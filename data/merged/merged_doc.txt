Mechanism of feature learning in convolutional neural networks Daniel Beaglehole∗,2 Adityanarayanan Radhakrishnan∗,3,4 Parthe Pandit1 Mikhail Belkin1,2 1Halıcıoğlu Data Science Institute, UC San Diego. 2Computer Science and Engineering, UC San Diego. 3Massachusetts Institute of Technology. 4Broad Institute of MIT and Harvard. ∗Equal contribution. Abstract Understanding the mechanism of how convolutional neural networks learn features from image data is a fundamental problem in machine learning and computer vision. In this work, we identify such a mechanism. We posit the Convolutional Neural Feature Ansatz, which states that covariances of filters in any convolutional layer are proportional to the average gradient outer product (AGOP) taken with respect to patches of the input to that layer. We present extensive empirical evidence for our ansatz, including identifying high correlation between covariances of filters and patch-based AGOPs for convolutional layers in standard neural architectures, such as AlexNet, VGG, and ResNets pre-trained on ImageNet. We also provide supporting theoretical evidence. We then demonstrate the generality of our result by using the patch-based AGOP to enable deep feature learning in convolutional kernel machines. We refer to the resulting algorithm as (Deep) ConvRFM and show that our algorithm recovers similar features to deep convolutional networks including the notable emergence of edge detectors. Moreover, we find that Deep ConvRFM overcomes previously identified limitations of convolutional kernels, such as their inability to adapt to local signals in images and, as a result, leads to sizable performance improvement over fixed convolutional kernels. 1 Introduction Neural networks have achieved impressive empirical results across various tasks in natural language pro- cessing [8], computer vision [39], and biology [50]. Yet, our understanding of the mechanisms driving the successes of these models is still emerging. One such mechanism of central importance is that of neural feature learning, which is the ability of networks to automatically learn relevant input transformations from data [37, 43, 55, 56]. An important line of work [5, 14, 23, 32, 35, 43, 52, 55] has demonstrated how feature learning in fully connected neural networks provides an advantage over classical, non-feature-learning models such as kernel machines. Recently, the work [37] identified a connection between a mathematical operator, known as average gradient outer product (AGOP) [17, 21, 47, 48], and feature learning in fully connected networks. This work subsequently demonstrated that the AGOP could be used to enable similar feature learning in kernel machines operating on tabular data. In contrast to the case for fully connected networks, there are few prior works [3, 24] analyzing feature learning in convolutional networks, which have been transformative in computer vision [19, 39]. The work [24] demonstrates an advantage of feature learning in convolutional networks by showing that these models are able to threshold noise and identify signal in image data unlike convolutional kernel methods including Convolutional Neural Tangent Kernels [4]. The work [3] analyzes how deep convolutional networks can correct features in early layers by simultaneous training of all layers. While these prior works identify advantages of feature learning in convolutional networks, they do not identify a general operator that captures such feature learning. The connection between AGOP and feature learning 1 arXiv:2309.00570v1 [stat.ML] 1 Sep 2023in fully connected neural networks [37] suggests that a similar connection should exist for feature learning in convolutional networks. Moreover, such a mechanism could be used to learn analogous features with any machine learning model such as convolutional kernel machines. In this work, we establish a connection between convolutional neural feature learning and the AGOP, which we posit as the Convolutional Neural Feature Ansatz (CNFA). Unlike the fully connected case from [37] where feature learning is characterized by AGOP with respect to network inputs, we demonstrate that con- volutional feature learning is characterized by AGOP with respect to patches of network inputs. We present empirical evidence for the CNFA by demonstrating high average Pearson correlation (in most cases > .9) be- tween AGOP on patches and the covariance of filters across all layers of pre-trained convolutional networks on ImageNet [40] and across all layers of SimpleNet [18] trained on several standard image classification datasets. We additionally prove that the CNFA holds for one step of gradient descent for deep convolu- tional networks. To demonstrate the generality of our identified convolutional feature learning mechanism, we leverage the AGOP on patches to enable feature learning in convolutional kernel machines. We refer to the resulting algorithm as ConvRFM. We demonstrate that ConvRFM captures features similar to those learned by the first layer of convolutional networks. In particular, on various image classification benchmark datasets such as SVHN [33] and CIFAR10 [26], we observe that ConvRFM recovers features corresponding to edge detectors. We further enable deep feature learning with convolutional kernels by developing a layerwise training scheme with ConvRFM, which we refer to as Deep ConvRFM. We demonstrate that Deep ConvRFM learns features similar to those learned by deep convolutional neural networks. Furthermore, we show that Deep ConvRFM overcomes limitations of convolutional kernels identified in [24] and exhibits local feature adaptivity. Lastly, we demonstrate that Deep ConvRFM provides improvement over CNTK and ConvRFM on several standard image classification datasets, indicating a benefit to deep feature learning. Our results advance understanding of how convolutional networks automatically learn features from data and provide a path toward integrating convolutional feature learning into general machine learning models. 2 Convolutional Neural Feature Ansatz (CNFA) Let f : Rc×P ×Q → R denote a convolutional neural network (CNN) operating on P ×Q resolution images with c color channels. The ℓth convolutional layer of a CNN involves applying a function hℓ : Rcℓ−1×Pℓ−1×Qℓ−1 → Rcℓ×Pℓ×Qℓ defined recursively as hℓ(x) = ϕ(� Wℓ ∗hℓ−1(x)) with h1 = x, � Wℓ ∈ Rcℓ×cℓ−1×q×q denoting cℓ filters of size cℓ−1 ×q ×q, ∗ denoting the convolution operation, and ϕ denoting an elementwise activation function. To understand how features emerge in convolutional networks, we abstract a convolutional network to a function of the form f(x) = g(W1x[1, 1], . . . , W1x[i, j], . . . , W1x[P, Q]), i ∈ [P], j ∈ [Q] ; (1) where W1 ∈ Rc1×cq2 is a matrix of c1 stacked filters of size cq2 and x[i, j] ∈ Rcq2 denotes the patch of x centered at coordinate (i, j). This abstraction is helpful since it allows us to consider feature learning in convolutional networks with arbitrary architecture (e.g., pooling layers, batch normalization, etc.) after any given convolutional layer. Up to rotation and reflection by the left singular vectors, the feature extraction properties of W1 are determined by the singular values and right singular vectors of W1. These singular values and vectors can be recovered from the matrix W T 1 W1, which is the empirical (uncentered) covariance of filters in the first layer. This argument extends to analyze features selected at layer ℓ of a CNN by considering a function of the form f(x) = gℓ(Wℓhℓ−1(x)[1, 1], . . . , Wℓhℓ−1(x)[Pℓ−1, Qℓ−1]). We refer to the matrix W T ℓ Wℓ as a Convolutional Neural Feature Matrix (CNFM) and note that this matrix is proportional to the (uncentered) empirical covariance matrix of filters in layer ℓ. We use the form of convolutional networks presented in Eq. (1) to state our Convolutional Neural Feature Ansatz (CNFA). Let Gℓ(x) := gℓ(Wℓhℓ−1(x)[1, 1], . . . , Wℓhℓ−1(x)[Pℓ−1, Qℓ−1]). Then, after training f for at least one epoch of (stochastic) gradient descent on standard loss functions: W ⊤ ℓ Wℓ ∝ n � p=1 � (i,j)∈S ∇hℓ−1(x)[i,j]Gℓ(x) � ∇hℓ−1(x)[i,j]Gℓ(x) �⊤ ; (2) where S = {(i, j)}i∈[Pℓ−1],j∈[Qℓ−1] denotes the set of indices of patches utilized in the convolution operation in layer ℓ. The CNFA (Eq. 2) mathematically implies that the convolutional neural feature matrices are 2AlexNet VGG11 VGG13 VGG16 VGG19 VGG11 BN VGG13 BN VGG16 BN VGG19 BN ResNet18 ResNet34 ResNet50 ResNet101 ResNet152 Pearson Correlation 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 CNFA Verification for Pre-trained Models on ImageNet Initial CNFM and Trained NFM Trained CNFM and AGOP A VGG11 AGOP Initial NFM Trained NFM ResNet18 AlexNet B Figure 1: A. Correlation between initial CNFM and trained CNFM (red) and trained CNFM with AGOP (green) for convolutional layers in VGG, AlexNet, and ResNet on ImageNet (224 × 224 resolution color images). B. Initial CNFM, trained CNFM, and AGOP matrices for the first convolutional layer of ResNet18, VGG11, and AlexNet on ImageNet. proportional to the average gradient outer product (AGOP) with respect to the patches of the input to layer ℓ. The CNFA implies that the structure of covariance matrices of filters in convolutional networks, an object studied in prior work [49], corresponds to AGOP over patches. Intuitively, the CNFA implies that convolutional features are constructed by identifying and amplifying those pixels in any patch that most change the output of the network. We now present extensive empirical evidence corroborating our ansatz. We subsequently present supporting theoretical evidence. 2.1 Empirical evidence for CNFA We now provide empirical evidence for the ansatz by computing the correlation between CNFMs and the AGOP for each convolutional layer in various CNNs. We provide three lines of evidence by computing correlations for the following models: (1) AlexNet [27], all VGGs [46], and all ResNet [19] models pre-trained on ImageNet [40] ; (2) SimpleNet models [18] trained on SVHN [33], GTSRB [20], CIFAR10 [26], CIFAR100, and ImageNet32 [10]; and (3) shallow CNNs across 10 standard computer vision datasets from PyTorch upon varying pooling and patch size of convolution operations. The first set of experiments provides evidence for the ansatz in large-scale state-of-the-art models on ImageNet. The second set provides evidence for the ansatz across standard computer vision datasets. The last set provides evidence for the ansatz holding across architecture choices. CNFA verification for pre-trained state-of-the-art models on ImageNet. We begin by providing evidence for the ansatz on pre-trained state-of-the-art models on ImageNet. In Fig. 1, we present these correlations for AlexNet, all VGG models and all ResNet models pre-trained on ImageNet, which are available for download from the PyTorch library [36].4 As a control, we verify that weights at the end of training are far from initialization (see the red bars in Fig. 1A). Note that despite the complexity involved in training these models (e.g., batch normalization, skip connections, custom optimization procedures, data augmentation) the Pearson correlation between the AGOP and CNFMs are remarkably high (> .9 for each layer of AlexNet and VGG13). In Fig. 1B, we additionally visualize the AGOP and CNFM for the first convolutional layer in AlexNet, VGG11, and ResNet18 to demonstrate the qualitative similarity between these matrices. In addition, in Appendix Fig. 7, we verify that these correlations are lower at initialization than at the end of training indicating that the ansatz is, in fact, a consequence of training. CNFA verification for SimpleNet on CIFAR10, CIFAR100, ImageNet32, SVHN, GTSRB. To verify the ansatz on other datasets, we also trained the SimpleNet model on five datasets including 4We evaluate all correlations between AGOP and CNFMs for all convolutional layers of AlexNet and all VGGs. To simplify computation on ResNets, we evaluate correlations between AGOP and CNFMs for the first layer in each BasicBlock and each Bottleneck, as defined in PyTorch. We note that for ResNet152, this computation involves computing correlation between matrices in 50 Bottleneck blocks. 3Input Image +CNFM +AGOP Layer 2 Layer 4 Layer 1 Layer 3 VGG11 +CNFM +AGOP AlexNet Input Image +CNFM +AGOP VGG11 +CNFM +AGOP AlexNet Figure 2: Comparison of features extracted by CNFMs and AGOPs across layers of VGG11 and AlexNet for two input images. These visualizations provide further supporting evidence that the CNFMs and AGOPs of early layers are performing an operation akin to edge detection. CIFAR10/100, ImageNet32, SVHN, and GTSRB. We note SimpleNet had achieved state-of-the-art results on several of these tasks at the time of its release (e.g., > 95% test accuracy on CIFAR10). We train SimpleNet models using the same optimization procedure provided from [18] (i.e., Adadelta [57] with weight decay and manual learning rate scheduling). We use a small initialization scheme of normally distributed weights with a standard deviation of 10−4 for convolutional layers. We note that we were able to recover high test accuracies across all datasets consistent with the results from [18] (see test accuracies for these trained SimpleNet models in Appendix Fig. 8). As shown in Appendix Fig. 8, we observe consistently high correlation between AGOPs and CNFMs across layers of SimpleNet. CNFA is robust to hyperparameter choices. We lastly study the effect of patch size and architecture choices on the CNFA for networks trained using the Adam optimizer [25]. We generally observe that larger patch sizes slightly reduce the correlation between AGOP and CNFMs, and that max pooling layers (in contrast to no pooling or average pooling) lead to higher correlation (Appendix Fig. 9). Interestingly, these results indicate that the choices used in state-of-the-art CNNs (max pooling layers and patch size of 3) are consistent with those that lead to highest correlation between AGOP and CNFMs. 2.2 Visualizing features captured by CNFM and AGOP We now visualize how the CNFM operates on patches of images to select features and demonstrate that AGOP over patches captures similar features. Both the CNFM and AGOP yield an operator on patches of images. Thus, to visualize how these matrices select features, we expand input images into individual patches, then apply either the CNFM or the AGOP to each patch. We then reduce the expanded image back to its original size by taking the norm over the spatial dimensions of each expanded patch. Formally, the 4value for each coordinate (i, j) ∈ Pℓ−1 × Qℓ−1 is replaced with ∥M 1 2 ℓ hℓ−1(X)[i, j]∥ where Mℓ := W T ℓ Wℓ. Our visualization reflects the magnitude of the patch in the image of the patch transformation. For example, if Mℓ is an edge detector, then ∥M 1 2 ℓ hℓ−1(X)[i, j]∥ will be large, if and only if the patch centered at coordinate (i, j) contains an edge. This visualization technique emerges naturally from the convolution operation in CNNs, where a post- activation hidden unit is generated by applying a filter to each patch independently of the others. Further, this visualization characterizes how a trained CNN extracts features across patches of any image. This is in contrast to visualization techniques based on saliency maps [41, 44, 45, 59], which consider gradients with respect to an entire input image and for a single sample. In addition to the high correlation between AGOP and CNFMs in the previous section, in Fig. 2, we observe that the AGOP and CNFMs transform input images similarly at any given layer of the CNN. For 224 × 224 images from ImageNet, CNFMs and AGOPs extracted from a pre-trained VGG11 model both emphasize objects and their edges in the image. We note these visualizations corroborate hypotheses from prior work that the first layer weights of deep CNNs learn an operator corresponding to edge detection [58]. Moreover, our results imply that the mathematical origin of edge detectors in convolutional neural networks is the average gradient outer product. In the following section, we will corroborate this claim by demonstrating that such edge detectors can be recovered without the use of any neural network through estimating the average gradient outer product of convolutional kernel machines. 2.3 Supporting Theoretical Evidence for CNFA The following theorem (proof in Appendix A) proves the ansatz for general convolutional networks after 1 step of full-batch gradient descent. Theorem 1. Let f denote a function that operates on m patches of size q, i.e., let f(v1, v2, . . . , vm) : Rq × . . . × Rq → R with f(v1, v2, . . . , vm) = g(Wv1, Wv2, . . . , Wvm) where W ∈ Rk×q and g(z1, . . . , zm) : Rk × . . . × Rk → R. Assume g(0) = 0 and ∂g(0) ∂zℓ = ∂g(0) ∂zℓ′ ̸= 0 for all ℓ, ℓ′ ∈ [m]. If W is trained for one step of gradient descent with mean squared loss on data {((v(p) 1 , . . . v(p) m ), yp)}n p=1 from initialization W (0) = 0, then for the point (u1, . . . , um): W (1)T W (1) ∝ m � r=1 ∂f (1)(u1, . . . , um) ∂vr ∂f (1)(u1, . . . , um) ∂vr T ; (3) where f (1)(v1, v2, . . . vm) := g(W (1)v1, W (1)v2, . . . , W (1)vm). We note the assumptions of Theorem 1 hold for several types of convolutional networks. As a simple example, the assumptions hold for convolutional networks with activation function ϕ satisfying ϕ(0) = 0 and ϕ′(0) ̸= 0 (e.g., tanh activation) with remaining layers initialized as constant matrices. Furthermore, we note that while the above theorem is stated for the first layer of a convolutional network, the same proof strategy applies for deeper layers by considering the subnetwork Gℓ(x). 3 CNFA as a general mechanism for convolutional feature learning We now show that the CNFA allows us to introduce a feature learning mechanism in any machine learning model on patches to capture features akin to those of convolutional networks. Given recent work connecting neural networks to kernel machines [22], we focus on convolutional kernels given by the Convolutional Neural Tangent Kernel (CNTK) [4] as our candidate model class. Intuitively, these models can be thought of as combining kernels evaluated across pairs of patches in images. While such models have achieved impressive performance [1, 6, 7, 29, 38, 42], these models do not automatically learn features from data unlike CNNs. Thus, as demonstrated in prior work [24, 52], there are tasks where CNTKs are significantly outperformed by corresponding CNNs. A major consequence of the CNFA is that we can now enable feature learning in CNTKs by leveraging the AGOP over patches. In particular, we can first solve kernel regression with the CNTK and then use 5Algorithm 1 Convolutional Recursive Feature Machine (ConvRFM) Input: X, y, KM, T, q ▷ Train data: (X, y), kernel: KM, iters.: T, and patch size: q Output: α, M ▷ Solution to kernel regression: α, and feature matrix: M M = Icq2 ▷ Initialize M to be the identity matrix of size cq2 × cq2 for t ∈ T do Ktrain = KM(X, X) ▷ KM(X, X)i,j := KM(xi, xj) α = yK−1 train M = 1 n � x∈X � (u,v)∈S(∇x[u,v]f(x))(∇x[u,v]f(x))T ▷ f(x) = αKM(X, x) end for the AGOP of the trained predictor over patches of images to learn features. We call our method the Convolutional Recursive Feature Machine (ConvRFM), as it is the convolutional variant of the original RFM [37]. We will demonstrate that ConvRFM accurately captures first layer feature learning in CNNs and can recover edge detectors as features when trained on standard image classification datasets. To account for deep convolutional feature learning, we extend ConvRFM to Deep ConvRFMs by sequentially learning features in a manner similar to layerwise training in CNNs. We show that Deep ConvRFM: (1) improves performance of CNTKs on local signal adaptivity tasks considered in [24] ; and (2) improves performance of CNTKs on several image classification tasks. 3.1 Convolutional Recursive Feature Machine (ConvRFM) We present the algorithm for ConvRFM in Algorithm 1. The ConvRFM algorithm recursively learns a feature extractor on patches of a given image by implementing the AGOP across patches of training data. Namely, the ConvRFM first builds a predictor with a fixed convolutional kernel. Then, we compute the AGOP of the trained predictor with respect to image patches, which we denote as the feature matrix, M. Lastly, we transform image patches with M and then repeat the previous steps. We provide a concrete example of this algorithm for the convolutional neural network Gausssian process (CNNGP) [9, 28] of a one hidden layer convolutional network with fully connected last layer operating on black and white images below. The CNNGP of a one hidden layer convolutional network with fully connected last layer, activation ϕ, and filter size q is given by K(x, z) = 1 PQ P � i=1 Q � j=1 ˇϕ(x[i, j]T z[i, j], ∥x[i, j]∥, ∥z[i, j]∥) ; where x, z ∈ RP ×Q, x[i, j] ∈ Rq2 denotes the vectorized q × q patch of x centered at coordinate (i, j), and ˇϕ(aT b, ∥a∥, ∥b∥) denotes the dual activation [15] of ϕ. For the case of ReLU activation, this dual activation has a well known form [9] and is given by ˇϕ(aT b, ∥a∥, ∥b∥) = 1 π � aT b � π − arccos � aT b ∥a∥∥b∥ �� + � ∥a∥2∥b∥2 − aT b � . In ConvRFM, we modify the inner product in the kernel above to be a Mahalanobis inner product, con- structing kernels of the form KM(x, z) := 1 PQ P � i=1 Q � j=1 ˇϕ(x[i, j]T Mz[i, j], x[i, j]T Mx[i, j], z[i, j]T Mz[i, j]) ; where M is a learned positive semi-definite matrix. In particular, M is updated as the AGOP of the esti- mator constructed by solving kernel regression with KM. In our experiments, we analyze performance when replacing ˇϕ with the Mahanolobis Laplace kernel used in [37] and with the CNTK of a deep convolutional ReLU network with fully connected last layer. We will make clear our choice of ˇϕ by denoting our method as CNTK-ConvRFM or Laplace-ConvRFM. 6Eigenvectors of CNTK-ConvRFM AGOP (SVHN) A Input Image +AGOP CNTK-ConvRFM +CNFM CNN B C Input Image +AGOP CNTK-ConvRFM SVHN +AGOP Laplace-ConvRFM SVHN Eigenvectors of Laplace-ConvRFM AGOP (SVHN) Figure 3: Features extractors learned by ConvRFM using CNTK (CNTK-ConvRFM) and Laplace kernel (Laplace-ConvRFM), which appear to operate as universal edge detectors. A. Top 8 eigenvectors of CNTK- ConvRFM and Laplace-ConvRFM trained on SVHN. We use 3 × 3 patches for CNTK-ConvRFM and 7 × 7 patches for Laplace-ConvRFM. B. Comparison of patch operators learned by CNTK-ConvRFM (given by the AGOP taken with respect to patches) and CNNs (given by the CNFM). C. Applying patch-based AGOP operators from ConvRFMs trained on SVHN to images from ImageNet. ConvRFM captures first layer features of convolutional neural networks. We now demonstrate that ConvRFM recovers features similar to those learned by first layers of CNNs. In Fig. 3A, we visualize the top eigenvectors of the feature matrix of CNTK-ConvRFM (filter size 3 × 3) and Laplace-ConvRFM (filter size 7 × 7) trained on SVHN. Training details for all methods are presented in Appendix B. We observe that these top eigenvectors resemble edge detectors [16]. In Fig. 3B, we visualize how the feature matrix of the CNTK-ConvRFM and the CNFM of the corresponding finite width CNN trained on SVHN transform SVHN images. Even though both operators arise from vastly different training procedures (solving kernel regression vs. training a CNN), we observe that both operators appear to extract similar features (corresponding to edges of digits) from SVHN images. We provide additional evidence for similarity between ConvRFM and CNN features in Appendix Fig. 10. To demonstrate further evidence of the universality of edge detector features arising from AGOP of CNTK-ConvRFM and Laplace-ConvRFM, we analyze how these AGOPs transform arbitrary images. In particular, in Fig. 3C, we apply these operators extracted from models trained on SVHN to images on ImageNet. We again observe that these operators remarkably extract edges from corresponding ImageNet images, which are of vastly different resolution (224 × 224 instead of 32 × 32) and contain vastly different objects. Such experiments provide conclusive evidence that AGOP with respect to patches of convolutional kernels recovers features akin to edge detectors. We present further experiments demonstrating emergence of edge detectors from convolutional kernels trained on CIFAR10 and GTSRB in Appendix Figs. 11 and 12. In particular, the eigenvectors of the AGOP often resemble Gabor filters with different orientations. In Figure 11, we see that horizontally, vertically, and diagonally aligned eigenvectors identify edges of the same alignment. 3.2 Deep feature learning with Deep ConvRFM ConvRFM is capable of only extracting features by linearly transforming patches of input images, which is analogous to extracting such features using the first layer of a CNN. In contrast, the CNFA implies that deep convolutional networks are capable of learning features in intermediate layers. To enable deep feature learning, we introduce Deep ConvRFM (see Algorithm 2) by sequentially learning features with AGOP in a manner similar to layerwise training in CNNs. In particular, Deep ConvRFM iterates the following steps: 1. Construct a predictor, �f, by training a convolutional kernel machine with kernel KM. 7Layer 1 Layer 2 Layer 3 Input Image +AGOP (Deep ConvRFM) +CNFM (CNN) +CNFM (CNN) +CNFM (CNN) +CNFM (CNN) +AGOP (Deep ConvRFM) +AGOP (Deep ConvRFM) +AGOP (Deep ConvRFM) Input Image Input Image Input Image Figure 4: Visualizations of features for each layer of Deep ConvRFM and the corresponding CNN on SVHN and the noisy digits task from [24]. 2. Update M to be the AGOP with respect to patches of the trained predictor. 3. Transform the data, x, with random features given by ϕ(Wx) where W denotes a set of convolutional filters with weights sampled according to N(0, M) and ϕ is a nonlinearity. Note that while we utilize random features and sample convolutional filters in Deep ConvRFM, we never utilize backpropgation to learn features or train models. Features are learned via the AGOP and models are trained by solving kernel regression, which is a convex optimization problem. For the base kernel for Deep ConvRFM, we utilize the deep CNTK [4] as implemented in the Neural Tangents library [34].5 Deep ConvRFM learns similar features to deep CNNs. We now present evidence that Deep Con- vRFMs learn similar features to those learned by deep CNNs. We analyze features learned by deep ConvRFM and the corresponding CNN on the local signal adaptivity synthetic tasks from [24] and SVHN. For the syn- thetic task from [24], we consider classification of MNIST digits embedded in a larger image of i.i.d. Gaussian noise. Dataset and training details are presented in Appendix B. In Fig. 4, we observe that AGOPs at each layer of Deep ConvRFM and and CNFMs at each layer of the corresponding CNN transform examples from both datasets similarly. 5In order to take gradient with respect to patches using Neural Tangents, we used a workaround that involved expanding images into their patch representations. This workaround unfortunately leads to heavy memory utilization, which limited our analysis of Deep ConvRFMs. Algorithm 2 Deep Convolutional Recursive Feature Machine (Deep ConvRFM) Input: X, y, {Kℓ}ℓ, T, L, q, k ▷ kernels: {Kℓ}ℓ, depth: L, channels: k Output: αL, {Mℓ}L ℓ=1 X1 = X ▷ Initialize embedding for ℓ ∈ L do αℓ, Mℓ = ConvRFM(Xℓ, y, Kℓ, T, q) Sample k filters Wℓ,k′ ∼ N(0, Mℓ) ▷ Wℓ ∈ Rk×q×q Xℓ+1 = ϕ(Wℓ ∗ Xℓ) ▷ ϕ: element-wise non-linearity end for 8A B 50 55 60 65 70 75 80 85 90 95 100 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 Test Accuracy Noise Std. Dev. Black and White Horizontal Bars in Noise Depth 3 - CNTK Depth 3 - ConvRFM Depth 3 - Deep ConvRFM 3-CNN 45 50 55 60 65 70 75 80 85 90 95 0 1 2 3 4 5 6 7 Test Accuracy Noise Std. Dev. MNIST Digits in Noise Depth 3 - CNTK Depth 3 - ConvRFM Depth 3 - Deep ConvRFM 3-CNN Figure 5: Test accuracy of CNTK, ConvRFM, Deep ConvRFM, and the corresponding CNN on local signal adaptivity tasks from [24] as a function of noise level. A. Identifying black and white bars in noisy images. B. MNIST digits placed randomly in noisy background image. Deep ConvRFM overcomes limitations of convolutional kernels. In the work [24] the authors posited local signal adaptivity, the ability to suppress noise and amplify signal in images, as a potential explanation for the superiority of convolutional neural networks over convolutional kernels. As supporting evidence, [24] demonstrated that convolutional networks generalized far better than convolutional kernels on image classification tasks in which images were embedded in a noisy background. We now demonstrate that by incorporating feature learning through patch-AGOPs, Deep ConvRFM exhibits local signal adaptivity on the tasks considered in [24] and thus, similar to CNNs, yield significantly improved performance over convolutional kernels. In particular, we begin by comparing performance of CNTK, Conv RFM, Deep ConvRFM, and corresponding CNNs on the following two image classification tasks from [24]: (1) images of black and white horizontal bars placed in a random position on larger images of Gaussian noise ; (2) MNIST images placed in a random position on larger images of Gausssian noise. The work [24] demonstrated that CNNs, unlike CNTK, could learn to threshold the background noise and amplify the signal in these tasks thus far outperforming CNTKs when the amount of background noise was large. In Fig. 5, we demonstrate that for these tasks CNNs, ConvRFMs, and Deep ConvRFMs all extract local signals and dim background noise through the AGOP, and thus far outperform CNTKs. Moreover, we observe that Deep ConvRFMs can provide up to a 5% improvement in performance over ConvRFM on the synthetic MNIST task, indicating a benefit to deep feature learning. Benefit of deep feature learning on real-world image classification tasks. Lastly, we analyze per- formance of CNTK, ConvRFM, Deep ConvRFM, and the corresponding three convolutional layer CNN on standard image classification datasets available for download from PyTorch. Consistent with our observa- tions for synthetic tasks from [24], we observe in Fig. 6A that ConvRFM and Deep ConvRFM provide an improvement over CNTK across almost all tasks. Moreover, we observe that ConvRFM and Deep Con- vRFM outperform CNTKs consistently when the corresponding CNN outperforms the CNTK. In Fig. 6B, we analyze the impact of deep feature learning by increasing the number of feature learning layers in Deep ConvRFM, i.e., the number of layers for which we utilize the AGOP to learn features. We observe that adding more layers of feature learning leads to consistent performance boost in the local signal adaptivity tasks from [24] and on select datasets such as SVHN and EMNIST [13]. 4 Discussion In this work, we identified a mathematical mechanism of feature learning in deep convolutional networks, which we posited as the Convolutional Neural Feature Ansatz (CNFA). Namely, the ansatz stated that features selected by convolutional networks, given by empirical covariance matrices of filters at any given layer, can be recovered by computing the average gradient outer product (AGOP) of the trained network 93-CNN Deep( ConvRFM ConvRFM CNTK Dataset 84.48±0.33 87.28 85.02 81.38 SVHN 64.90±0.16 67.97 67.97 67.74 CIFAR-10 29.78±0.17 37.64 37.64 37.64 CIFAR-100 93.35±0.34 93.02 93.02 91.76 GTSRB 88.35±0.22 87.60 86.73 86.01 EMNIST A B Test accuracy (%) 0 5 10 15 20 0 1 2 3 % Improvement Number of Feature Learning Layers Performance of Deep RFM SVHN EMNIST Noisy MNIST Digits Horizontal Bars in Noise Figure 6: A. Performance comparison of Deep ConvRFM with the corresponding CNTK and CNN on benchmark image classification datasets from PyTorch. B. Effect of number of feature learning layers on Deep ConvRFM performance. with respect to image patches. We presented empirical and theoretical evidence for the ansatz. Notably, we showed that convolutional filter covariances of neural networks pre-trained on ImageNet (AlexNet, VGG, ResNet) are highly correlated with AGOP with respect to patches (in many cases, Pearson correlation > .9). Since the AGOP with respect to patches can be computed on any function operating on image patches, we could use the AGOP to enable feature learning in any machine learning model operating on image patches. Thus, building on the RFM algorithm for fully connected networks from [37], we integrated the AGOP to enable deep feature learning in convolutional kernel machines, which could not apriori learn features, and referred to the resulting algorithms as ConvRFM and Deep ConvRFM. We demonstrated that ConvRFM and Deep ConvRFM recover features similar to those of deep convolutional neural networks, including evidence that features learned by these models can serve as universal edge detectors, akin to features learned in convolutional networks. Moreover, we demonstrated that ConvRFM and Deep ConvRFM overcome prior limitations of convolutional kernels, including the Convolutional Neural Tangent Kernel (CNTK), such the inability to adapt to localized signals in images [24]. Lastly, we showed a benefit to deep feature learning by demonstrating improvement in performance of Deep ConvRFM over ConvRFM and the CNTK on standard image classification benchmarks. We now conclude with a discussion of implications of our results and future directions. Identifying mechanisms driving success of deep learning. Understanding the mechanisms driving success of neural networks is an important problem for developing effective, interpretable and safe machine learning models. The complexities of training deep neural networks such as custom training procedures and layer structures (batch normalization, dropout, residual connections, etc.) can make it difficult to pinpoint overarching principles leading to effectiveness of these models. The fact that correlation between convolutional neural feature matrices (CNFMs) and AGOPs is high for convolutional networks pre-trained on ImageNet with all of these inherent complexities baked in, provides strong evidence that the connection between AGOP and CNFMs is key to identifying the core principles making these networks successful. Emergence of universal edge detectors with average gradient outer product. Detecting edges in images is a well-studied task in computer vision and classical approaches involved applying fixed convolutional filters to detect edges in images [2, 16, 54]. For example, AlexNet automatically learned filters in its first convolutional layer that were remarkably similar to Gabor filters [30]. Similarly, there was evidence that other convolutional networks pre-trained on ImageNet learned features akin to edge detection in the first layer [58]. Yet, it had been unclear how such filters automatically emerge through training. We demonstrated that the AGOP with respect to patches of a large class of convolutional models (convolutional neural networks and convolutional kernels) trained on various standard image classification tasks consistently recovered edge detectors (see Fig. 2, Fig. 3A, B). We further showed the universality of these edge detector features by demonstrating that features learned by ConvRFM on SVHN automatically identified edges in ImageNet 10images. This strongly suggests that edge detectors emerge from the underlying nature of the task rather than specific properties of architectures. Our findings indicate that understanding connections between AGOP and classical edge detection approaches is a promising direction for understanding emergence of features in the first layer of convolutional neural networks and for identifying simple algorithms to capture deeper convolutional features. Reducing computational complexity of convolutional kernels. In this work, we provided an ap- proach for enabling feature learning in convolutional kernels by iteratively training convolutional kernel ma- chines and computing AGOP of the trained predictor. Given that convolutional kernels are able to achieve impressive accuracy on standard datasets without any feature learning [1, 6, 7, 29, 42], these methods have the potential to provide state-of-the-art results upon incorporating feature learning. Yet, in contrast to the case of classical kernel machines such as those used in [37], evaluating the kernel for an effective CNTK (such as those with Global Average Pooling [4]) can be a far more computationally intensive process than simply training a convolutional neural network. For example, according to Neural Tangents [34], the CNTK of a Myrtle kernel [42] can take anywhere from 300 to 500 GPU hours for CIFAR10. Given that Deep ConvRFM involves constructing a kernel matrix and computing AGOP to capture features at each layer, reducing the evaluation time of convolutional kernels through strategies such as random feature approximations is key to making these approaches scalable. Acknowledgements A.R. is supported by the Eric and Wendy Schmidt Center at the Broad Institute. We acknowledge support from the National Science Foundation (NSF) and the Simons Foundation for the Collaboration on the Theoretical Foundations of Deep Learning6 through awards DMS-2031883 and #814639 as well as the TILOS institute (NSF CCF-2112665). This work used the programs (1) XSEDE (Extreme science and engineering discovery environment) which is supported by NSF grant numbers ACI-1548562, and (2) ACCESS (Advanced cyberinfrastructure coordination ecosystem: services & support) which is supported by NSF grants numbers #2138259, #2138286, #2138307, #2137603, and #2138296. Specifically, we used the resources from SDSC Expanse GPU compute nodes, and NCSA Delta system, via allocations TG-CIS220009. Code Availability All code is available at https://github.com/aradha/convrfm. References [1] B. Adlam, J. Lee, S. Padhy, Z. Nado, and J. Snoek. Kernel regression with infinite-width neural networks on millions of examples. arXiv preprint arXiv:2303.05420, 2023. [2] S. Albawi, T. A. Mohammed, and S. Al-Zawi. Understanding of a convolutional neural network. In International Conference on Engineering and Technology, pages 1–6. IEEE, 2017. [3] Z. Allen-Zhu and Y. Li. Backward feature correction: How deep learning performs deep learning. arXiv preprint arXiv:2001.04413, 2020. [4] S. Arora, S. S. Du, W. Hu, Z. Li, R. Salakhutdinov, and R. Wang. On exact computation with an infinitely wide neural net. In Advances in Neural Information Processing Systems, 2019. [5] J. Ba, M. A. Erdogdu, T. Suzuki, Z. Wang, D. Wu, and G. Yang. High-dimensional asymptotics of feature learning: How one gradient step improves the representation. Advances in Neural Information Processing Systems, 35:37932–37946, 2022. 6https://deepfoundations.ai/ 11[6] A. Bietti. Approximation and learning with deep convolutional models: a kernel perspective. arXiv preprint arXiv:2102.10032, 2021. [7] A. Bietti and J. Mairal. Group invariance, stability to deformations, and complexity of deep convolu- tional representations. The Journal of Machine Learning Research, 20(1):876–924, 2019. [8] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sas- try, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, and D. Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, 2020. [9] Y. Cho and L. Saul. Kernel methods for deep learning. In Advances in Neural Information Processing Systems, 2009. [10] P. Chrabaszcz, I. Loshchilov, and F. Hutter. A downsampled variant of imagenet as an alternative to the cifar datasets. arXiv preprint arXiv:1707.08819, 2017. [11] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition, 2014. [12] A. Coates, H. Lee, and A. Y. Ng. An analysis of single layer networks in unsupervised feature learning. In International Conference on Artificial Intelligence and Statistics, 2011. [13] G. Cohen, S. Afshar, J. Tapson, and A. Van Schaik. Emnist: Extending mnist to handwritten letters. In International Joint Conference on Neural Networks, pages 2921–2926. IEEE, 2017. [14] A. Damian, J. Lee, and M. Soltanolkotabi. Neural networks can learn representations with gradient descent. In Conference on Learning Theory, pages 5413–5452. PMLR, 2022. [15] A. Daniely, R. F. Frostig, and Y. Singer. Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity. In Advances in Neural Information Processing Systems, 2016. [16] R. C. Gonzales and P. Wintz. Digital image processing. Addison-Wesley Longman Publishing Co., Inc., 1987. [17] W. Härdle and T. M. Stoker. Investigating smooth multiple regression by the method of average derivatives. Journal of the American statistical Association, 84(408):986–995, 1989. [18] S. H. Hasanpour, M. Rouhani, M. Fayyaz, and M. Sabokrou. Lets keep it simple, using simple ar- chitectures to outperform deeper and more complex architectures. arXiv preprint arXiv:1608.06037, 2016. [19] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition, 2016. [20] S. Houben, J. Stallkamp, J. Salmen, M. Schlipsing, and C. Igel. Detection of traffic signs in real-world images: The German Traffic Sign Detection Benchmark. In International Joint Conference on Neural Networks, number 1288, 2013. [21] M. Hristache, A. Juditsky, J. Polzehl, and V. Spokoiny. Structure adaptive approach for dimension reduction. Annals of Statistics, pages 1537–1566, 2001. [22] A. Jacot, F. Gabriel, and C. Hongler. Neural Tangent Kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems, 2018. [23] A. Jacot, E. Golikov, C. Hongler, and F. Gabriel. Feature learning in l_2-regularized dnns: Attrac- tion/repulsion and sparsity. Advances in Neural Information Processing Systems, 35:6763–6774, 2022. [24] S. Karp, E. Winston, Y. Li, and A. Singh. Local signal adaptivity: Provable feature learning in neural networks beyond kernels. Advances in Neural Information Processing Systems, 34:24883–24897, 2021. 12[25] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015. [26] A. Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, University of Toronto, 2009. [27] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. Advances in Neural Information Processing Systems, 25, 2012. [28] J. Lee, Y. Bahri, R. Novak, S. Schoenholz, J. Pennington, and J. Sohl-Dickstein. Deep neural networks as Gaussian processes. In International Conference on Learning Representations, 2017. [29] Z. Li, R. Wang, D. Yu, S. S. Du, W. Hu, R. Salakhutdinov, and S. Arora. Enhanced convolutional neural tangent kernels. arXiv preprint arXiv:1911.00809, 2019. [30] S. Luan, C. Chen, B. Zhang, J. Han, and J. Liu. Gabor convolutional networks. IEEE Transactions on Image Processing, 27(9):4357–4366, 2018. [31] S. Maji, E. Rahtu, J. Kannala, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013. [32] A. Mousavi-Hosseini, S. Park, M. Girotti, I. Mitliagkas, and M. A. Erdogdu. Neural networks efficiently learn low-dimensional representations with sgd. arXiv preprint arXiv:2209.14863, 2022. [33] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with unsupervised feature learning. Advances in Neural Information Processing Systems (NIPS), 2011. [34] R. Novak, L. Xiao, J. Hron, J. Lee, A. A. Alemi, J. Sohl-Dickstein, and S. Schoenholz. Neural Tan- gents: Fast and easy infinite neural networks in Python. In International Conference on Learning Representations, 2020. [35] S. Parkinson, G. Ongie, and R. Willett. Linear neural network layers promote learning single-and multiple-index models. arXiv preprint arXiv:2305.15598, 2023. [36] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems, 2019. [37] A. Radhakrishnan, D. Beaglehole, P. Pandit, and M. Belkin. Feature learning in neural networks and kernel machines that recursively learn features. arXiv preprint arXiv:2212.13881, 2022. [38] A. Radhakrishnan, G. Stefanakis, M. Belkin, and C. Uhler. Simple, fast, and flexible framework for matrix completion with infinite width neural networks. Proceedings of the National Academy of Sciences, 119(16):e2115064119, 2022. [39] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, 2021. [40] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and F.-F. Li. ImageNet large scale visual recognition challenge. International Journal of Computer Vision, 2015. [41] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra. Grad-cam: Visual expla- nations from deep networks via gradient-based localization. In Proceedings of the IEEE International Conference on Computer Vision, pages 618–626, 2017. [42] V. Shankar, A. Fang, W. Guo, S. Fridovich-Keil, J. Ragan-Kelley, L. Schmidt, and B. Recht. Neural kernels without tangents. In International Conference on Machine Learning, pages 8614–8623. PMLR, 2020. 13[43] Z. Shi, J. Wei, and Y. Lian. A theoretical analysis on feature learning in neural networks: Emergence from inputs and advantage over fixed features. In International Conference on Learning Representations, 2022. [44] A. Shrikumar, P. Greenside, and A. Kundaje. Learning important features through propagating acti- vation differences. In International Conference on Machine Learning, pages 3145–3153. PMLR, 2017. [45] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013. [46] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. [47] S. Trivedi and J. Wang. The expected jacobian outerproduct: Theory and empirics. arXiv preprint arXiv:2006.03550, 2020. [48] S. Trivedi, J. Wang, S. Kpotufe, and G. Shakhnarovich. A consistent estimator of the expected gradient outerproduct. In UAI, pages 819–828, 2014. [49] A. Trockman, D. Willmott, and J. Z. Kolter. Understanding the covariance structure of convolutional filters. arXiv preprint arXiv:2210.03651, 2022. [50] K. Tunyasuvunakool, J. Adler, Z. Wu, T. Green, M. Zielinski, A. Žídek, A. Bridgland, A. Cowie, C. Meyer, A. Laydon, S. Velankar, G. Kleywegt, A. Bateman, R. Evans, A. Pritzel, M. Figurnov, O. Ronneberger, R. Bates, S. Kohl, and D. Hassabis. Highly accurate protein structure prediction for the human proteome. Nature, 596:1–9, 2021. [51] B. S. Veeling, J. Linmans, J. Winkens, T. Cohen, and M. Welling. Rotation equivariant cnns for digital pathology. In Medical Image Computing and Computer Assisted Intervention, pages 210–218. Springer, 2018. [52] N. Vyas, Y. Bansal, and P. Nakkiran. Limitations of the ntk for understanding generalization in deep learning. arXiv preprint arXiv:2206.10012, 2022. [53] C. Yadav and L. Bottou. Cold case: The lost mnist digits. In Advances in Neural Information Processing Systems 32. Curran Associates, Inc., 2019. [54] R. Yamashita, M. Nishio, R. K. G. Do, and K. Togashi. Convolutional neural networks: an overview and application in radiology. Insights into imaging, 9:611–629, 2018. [55] G. Yang and E. J. Hu. Tensor Programs IV: Feature learning in infinite-width neural networks. In International Conference on Machine Learning, 2021. [56] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How transferable are features in deep neural networks? In Advances in Neural Information Processing Systems, volume 27, 2014. [57] M. D. Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012. [58] M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. In European Con- ference on Computer Vision, pages 818–833. Springer, 2014. [59] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning deep features for discriminative localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2921–2929, 2016. 14A Theoretical Evidence for Deep Convolutional Feature Ansatz Proof of Theorem 1. Gradient descent proceeds as follows: B(1) = B(0) + η n � p=1 m � ℓ=1 ∂g(B(0)v(p) 1 , . . . , B(0)v(p) m ) ∂zℓ (yp − f(v(p) 1 , . . . , v(p) m ))v(p) ℓ T . Since B(0) = 0, g(0) = 0 and ∂g(0) ∂zℓ = G for fixed nonzero G ∈ Rk, the above expression reduces to: B(1) = η n � p=1 m � ℓ=1 Gypv(p) ℓ T . Thus, we conclude that B(1)T B(1) = η2 n � p,p′=1 m � ℓ,ℓ′=1 ypyp′v(p) ℓ GT Gv(p′) ℓ′ T = � η2m2GT G �   n � p,p′=1 m � ℓ,ℓ′=1 ypyp′v(p) ℓ v(p′) ℓ′ T   ∝ n � p,p′=1 m � ℓ,ℓ′=1 ypyp′v(p) ℓ v(p′) ℓ′ T . Now, we finish the proof by showing the right hand side of Eq. (3) is proportional to the same quantity above. First, we have that ∂f (1)(u1, . . . , um) ∂vr = B(1)T ∂g(B(1)u1, . . . , B(1)um) ∂zr . Thus, letting u = (u1, . . . um) and B(1)u = (B(1)u1, . . . , B(1)um), we have that m � r=1 ∂f (1)(u) ∂vr ∂f (1)(u) ∂vr T = m � r=1 B(1)T ∂g(B(1)u) ∂zr ∂g(B(1)u) ∂zr T B(1)T = η2 m � r=1 � n � p=1 m � ℓ=1 ypv(p) ℓ GT ∂g(B(1)u) ∂zr �   n � p′=1 m � ℓ′=1 ∂g(B(1)u) ∂zr T Gy′ pv(p′) ℓ′ T   = η2 n � p,p′=1 m � ℓ,ℓ′=1 ypyp′v(p) ℓ v(p′) ℓ′ T � m � r=1 GT ∂g(B(1)u) ∂zr ∂g(B(1)u) ∂zr T G � ∝ n � p,p′=1 m � ℓ,ℓ′=1 ypyp′v(p) ℓ v(p′) ℓ′ T . B Experimental Details Neural network comparisons. For all neural network experiments, we reported the best test accuracy across all epochs. For the Adam experiments, we trained CNNs without bias, with learning rate 10−4, width 64, without padding and with minibatch size 128. For EMNIST, CIFAR-10/100, SVHN, GTSRB the networks were trained for 500 epochs. For the toy datasets, the networks were trained for 25 epochs. For SGD experiments, the setup was identical except the learning rate was 10−1, and EMNIST, CIFAR-10/100, SVHN, GTSRB were trained for 2000 epochs, and toy datasets for 100 epochs. 15Visualizations. For the visualizations in Figs. 3, 12, the toy tasks were visualized with ∥M 1/2x∥, while CIFAR and SVHN were visualized with ∥Mx∥. Further, for the neural networks in CIFAR and SVHN, the initial weight matrices were subtracted before using the CNFMs. For Figure 4, the visualizations were also done with the full M matrix and subtracting the initial weights. The weight matrices were extracted after 250 epochs. For visualization, the M matrix that gave the best performance of the 5 iterations of ConvRFM was selected, and the CNN neural feature matrices were extracted at the end of training. Deep ConvRFM. For Deep-ConvRFM experiments, we greedily selected the best performing M matrix among 5 rounds of ConvRFM for each depth. Further, we tuned regularization among 10−8, 10−5, 10−3 and divided the train and test kernel matrices by the maximum value of the train kernel matrix. In all of the experiments, we used the same architecture as the CNN. In particular, we sampled 64 filters in each layer and removed bias. Further, to ensure that the ConvRFM did not have access to a kernel of additional depth, we reduced the depth of the kernel by 1 with each layer of Deep ConvRFM. Instead of sampling from M, we generated filters sampled from standard normal distribution and applied M 1/2 to each filter. CNFA verification. For the CNFA verification experiments in Figure 9, we used the uncentered corre- lation (commonly-known as cosine similarity) to measure similarity between the AGOP and CNFM. The correlation was averaged over ten datasets: Fine-Grained Visual Classification of Aircraft [31], PatchCame- lyon [51], CIFAR-10, STL-10 [12], GTSRB, SVHN, Caltech101, DTD [11], QMNIST [53], EMNIST. We used zero padding in all layers, a learning rate of 2 × 10−4, 500 epochs of training with the Adam optimizer, and minibatch size of 128. For datasets with multiple color channels, the RGB color channels were scaled and centered to have means [125.3/255, 123/125, 113.9/225] and standard deviations [63.0/225, 62.1/225, 66.7/225], respectively. Datasets with images larger than 32x32 resolution were resized using PyTorch’s resize transform to 32x32 resolution. All layers were initialized from a standard normal distribution. The first layer was initialized with standard deviation 5 × 10−3, while the remaining layers were sampled with standard deviation 10−2. Convolutional kernel implementation. To implement convolutional kernels, we used the Neural Tan- gents library [34]. Mahalanobis kernels were not implemented directly in this library at the time of pub- lication. To implement ConvRFM, we performed the following procedure: (1) unfold each image into all patches (without any padding), (2) applied M 1/2 to each patch independently, (3) reshaped the images to be 2-dimensional, and (4) set the stride of the first-convolutional layer in the kernel to the patch size, then ran kernel regression. 16ResNet18-Trained ResNet18-Init ResNet34-Trained ResNet34-Init Pearson Correlation 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Comparison of CNFA at Initialization and After Training Figure 7: Comparison of correlation between CNFMs and AGOP for randomly initialized ResNets and pre- trained ResNets on ImageNet. 17Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6 Layer 7 Layer 8 Layer 9 Layer 10 Layer 11 Layer 12 Correlation 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 CNFA Verification for SimpleNet across 5 Datasets Initial CNFM and Trained CNFM Trained CNFM and AGOP A SimpleNet Test Accuracy (%) Dataset 95.13 CIFAR10 76.63 CIFAR100 95.97 SVHN 96.99 GTSRB 51.50 ImageNet32 B SimpleNet Performance Figure 8: A. Correlation between initial CNFM and trained CNFM (red) and trained CNFM and AGOP (green) for each convolutional layer of SimpleNets trained on 5 datasets. B. Performance of SimpleNet on the 5 corresponding datasets. 18Layer 1 Layer 2 Layer 3 Layer 4 Correlation 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 NFM and AGOP after training (CNN-VEC) Patch Size = 3 Patch Size = 5 Patch Size = 7 Layer 1 Layer 2 Layer 3 Layer 4 Correlation 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 NFM at initialization and AGOP (CNN-VEC) Patch Size = 3 Patch Size = 5 Patch Size = 7 Layer 1 Layer 2 Layer 3 Layer 4 Correlation 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 NFM at initialization and AGOP (Avg. Pooling) Patch Size = 3 Patch Size = 5 Patch Size = 7 Layer 1 Layer 2 Layer 3 Layer 4 Correlation 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 NFM and AGOP after training (Avg. Pooling) Patch Size = 3 Patch Size = 5 Patch Size = 7 Layer 1 Layer 2 Layer 3 Layer 4 Correlation 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 NFM at initialization and AGOP (Max Pooling) Patch Size = 3 Patch Size = 5 Patch Size = 7 Layer 1 Layer 2 Layer 3 Layer 4 Correlation 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 NFM and AGOP after training (Max Pooling) Patch Size = 3 Patch Size = 5 Patch Size = 7 A B C D E F Figure 9: Correlation (cosine similarity) of NFM and AGOP across patch sizes and pooling types on ten datasets. (A) CNN-VEC, trained NFM and AGOP, (B) Max pooling, trained NFM and AGOP, (C) Average pooling, trained NFM and AGOP, (D) CNN-VEC, initial NFM and AGOP, (E) Max pooling, initial NFM and AGOP, (F) Average pooling, initial NFM and AGOP. 19A B Input Images +AGOP (ConvRFM) +CNFM (CNN) Input Images +AGOP (ConvRFM) +CNFM (CNN) Input Images +AGOP (ConvRFM) +CNFM (CNN) C Black and White Horizontal Bars MNIST in Noisy Background CIFAR10 Figure 10: Comparison of feature extraction performed by patch-AGOPs and CNFMs from ConvRFM and the corresponding CNN. A. Visualizations for models trained on horizontal bars in noisy backgrounds (the toy task from [24]). B. Visualizations for models trained on MNIST images in noisy backgrounds, which was also the task considered in [24]. C. Visualizations for models trained on CIFAR10. 20Eigenvectors of CNTK-ConvRFM (SVHN) Eigenvectors of Laplace-ConvRFM (SVHN) Eigenvectors of CNTK-ConvRFM (GTSRB) Input Images Figure 11: Top eigenvectors of patch-AGOP from Laplace-ConvRFM and CNTK-ConvRFM trained on standard image classification datasets (SVHN and GTSRB) act as universal edge detectors of different orientations. We visualize images after applying each top eigenvector to image patches of ImageNet images. 21A Eigenvectors of CNTK-ConvRFM AGOP (CIFAR10) Eigenvectors of CNTK-ConvRFM AGOP (GTSRB) Input Image +CNFM (VGG11) (ImageNet) +CNFM (AlexNet) (ImageNet) +AGOP (CNTK-ConvRFM) (SVHN) +AGOP (Laplace-ConvRFM) (SVHN) +AGOP (CNTK-ConvRFM) (GTSRB) +AGOP (CNTK-ConvRFM) (CIFAR10) B Figure 12: A. Visualization of top 8 eigenvectors of patch-AGOP from CNTK-ConvRFM trained on CI- FAR10 and GTSRB. B. Comparison of feature extraction on ImageNet data performed by CNFMs from VGG11, AlexNet pre-trained on ImageNet and patch-AGOP operators of CNTK-ConvRFM trained on SVHN, GTSRB, CIFAR10 and Laplace-ConvRFM trained on SVHN. 22
On the Implicit Bias of Adam Matias D. Cattaneo∗ Princeton University cattaneo@princeton.edu Jason M. Klusowski∗ Princeton University jason.klusowski@princeton.edu Boris Shigida∗ Princeton University bs1624@princeton.edu Abstract In previous literature, backward error analysis was used to find ordinary differential equations (ODEs) approximating the gradient descent trajectory. It was found that finite step sizes implicitly regularize solutions because terms appearing in the ODEs penalize the two-norm of the loss gradients. We prove that the exis- tence of similar implicit regularization in RMSProp and Adam depends on their hyperparameters and the training stage, but with a different “norm” involved: the corresponding ODE terms either penalize the (perturbed) one-norm of the loss gradients or, on the contrary, hinder its decrease (the latter case being typical). We also conduct numerical experiments and discuss how the proven facts can influence generalization. September 4, 2023 1 Introduction Gradient descent can be seen as a numerical method solving the ordinary differential equation (ODE) ˙θ = −∇E(θ), where E(·) is the loss function and ∇E(θ) denotes its gradient. Starting at θ(0), it creates a sequence of guesses θ(1), θ(2), . . ., which lie close to the solution trajectory θ(t) governed by aforementioned ODE. Since the step size h is finite, one could search for a modified differential equation ˙˜θ = −∇ �E(˜θ) such that θ(n) − ˜θ(nh) is exactly zero, or at least closer to zero than θ(n) − θ(nh), that is, all the guesses of the descent lie exactly on the new solution curve or closer compared to the original curve. This approach to analysing properties of numerical method is called backward error analysis in the numerical integration literature (see Chapter IX in [5]). [1] first used this idea for full-batch gradient descent and found that the modified loss function �E(˜θ) = E(˜θ) + (h/4)∥∇E(˜θ)∥2 makes the trajectory of the solution to ˙˜θ = −∇ �E(˜θ) approximate the sequence {θ(n)}∞ n=0 one order of h better than the original differential equation, where ∥ · ∥ denotes the Euclidean norm. In related work, [22] obtained the correction term for full-batch gradient descent up to any chosen order, also studying the global error (uniform in the iteration number) as opposed to the local (one-step) error. The analysis was later extended to mini-batch gradient descent in [28]. Assume that the training set is split in batches of size B and there are m batches per epoch (so the training set size is mB), the cost function is rewritten E(θ) = (1/m) �m−1 k=0 ˆEk(θ) with mini-batch costs denoted ∗Equal contribution Preprint. Under review. arXiv:2309.00079v1 [cs.LG] 31 Aug 2023ˆEk(θ) = (1/B) �kB+B j=kB+1 Ej(θ). It was obtained in that work that after one epoch, the mean iterate of the algorithm, averaged over all possible shuffles of the batch indices, is close to the solution to ˙θ = −∇ �ESGD(θ), where the modified loss is given by �ESGD(θ) = E(θ) + h 4m m−1 � k=0 ��∇ ˆE(θ) ��2. More recently, [8] studied the gradient descent with heavy-ball momentum iteration θ(n+1) = θ(n) − h∇E(θ(n)) + β(θ(n) − θ(n−1)), where β is the momentum parameter. They proved that it is close to the continuous trajectory of the piecewise first-order ODE ˙˜θ = −1 − βn+1 1 − β ∇E(˜θ(t)) − hγn(1 + β) 2(1 − β)3 ∇2E(˜θ(t))∇E(˜θ(t)), t ∈ [nh, (n + 1)h], where γn = � 1 − β2n+2� − 4(n + 1)βn+1 (1 − β) (1 + β). This result is stated here in a full-batch setting and is a special case of their main theorem, which includes mini-batches. In another recent work, [34] introduce a regularization term λ · ∥∇E(θ)∥ to the loss function as a way to ensure finding flatter minima, which have been observed empirically to have a smaller test error. The only difference between their term and the first-order correction coming from backward error analysis (up to a coefficient) is that the norm is not squared. Using backward error analysis to approximate the discrete dynamics with a modified ODE for adaptive algorithms such as RMSProp [30] and Adam [17] (which is an improvement over RMSProp and AdaGrad[4]) is currently missing in the literature. [1] note that “it would be interesting to use backward error analysis to calculate the modified loss and implicit regularization for other widely used optimizers such as momentum, Adam and RMSprop”. [28] reiterate that they “anticipate that backward error analysis could also be used to clarify the role of finite learning rates in adaptive optimizers like Adam”. In the same context, [8] agree that “RMSProp ... and Adam ..., albeit being powerful alternatives to SGD with faster convergence rates, are far from well-understood in the aspect of implicit regularization”. In a similar context, in Appendix G to [22] it is mentioned that “its [Adam’s] counter term and discretization error are open questions”. This paper fills the gap in the literature by conducting backward error analysis for (mini-batch, and full-batch as a special case) Adam and RMSProp. Our main contributions are listed below. • In Theorem 3.1 and Theorem 4.2, we provide a global second-order in h continuous ODE approxi- mation to Adam and RMSProp in the general mini-batch setting. For the full-batch special case, it was shown in prior work [21] that the continuous-time limit of both these algorithms is a (perturbed by ε) signGD flow ˙θ = − ∇E(θ) |∇E(θ)| + ε component-wise, where ε is the numerical stability parameter; we make this more precise by finding an additional “bias” term on the right (linearly depending on h). • We analyze the full-batch case in more detail. We find that the bias term does something different from penalizing the two-norm of the loss gradient as in the case of gradient descent: it either penalizes the perturbed one-norm of the loss gradient, defined as ∥v∥1,ε = �p i=1 � v2 i + ε, or, on the contrary, hinders its decrease (depending on hyperparameters and the training stage). See the summary of our theoretical finding for the full-batch case in Section 2. We also obtain the backward error analysis result for heavy-ball momentum gradient descent (which was derived before in [8]) as a special case: see Example 2.3. • We provide numerical evidence consistent with our results. In particular, we notice that often penalizing the perturbed one-norm appears to improve generalization, and hindering its decrease hurts it. The typical absence of implicit regularization appearing from backward error analysis in RMSProp and Adam (as opposed to GD) becomes one more previously unidentified possible explanation for poorer generalization of adaptive gradient algorithms compared to other methods. 2• Consistent with our theoretical results, we notice a phenomenon of rising and falling norm in our experiments, which is described as follows. The training of full-batch Adam first steeply decreases the training loss and the perturbed one-norm of the loss gradient, while increasing the test accuracy. Then, however, the perturbed norm rises, even though the training loss and test accuracy behave as expected: continue to decrease and increase respectively. Later the perturbed norm falls again. It seems that the height of the “hill” the perturbed norm graph rises to mid-training depends on how much the bias term hinders the decrease of this norm. To the best of our knowledge, this has not been noticed previously, though the phenomenon of the two-norm of the loss gradient rising while training convolutional neural networks with SGD may be related.2 Related work Backward error analysis of first-order methods. We provide the history of finding ordinary differential equations approximating different algorithms in the introduction. Recently, there have been other applications of backward error analysis related to machine learning. [18] show that the approximating continuous-time trajectories satisfy conservation laws that are broken in discrete time. [7] use backward error analysis while studying how to discretize continuous-time dynamical systems preserving stability and convergence rates. [27] find continuous-time approximations of discrete two-player differential games. Approximating gradient methods by differential equation trajectories. [21] prove that the trajectories of Adam and RMSProp are close to signGD dynamics, and investigate different training regimes of these algorithms empirically. SGD is approximated by stochastic differential equations and novel adaptive parameter adjustment policies are devised in [19]. Implicit bias of first-order methods. [29] prove that gradient descent trained to classify linearly separable data in the case of logistic loss converges to the direction of the max-margin vector (the solution to the hard margin SVM). This result has been extended to different loss functions in [24], to stochastic gradient descent in [25] and more generic optimization methods in [9], to the nonseparable case in [13], [14]. This line of research has been generalized to studying implicit biases of linear networks [12], [10], homogeneous neural networks [11], [23], [20]. [32] study the gradient flow of a diagonal linear network with squared loss and show that large initializations lead to minimum 2-norm solutions while small initializations lead to minimum 1-norm solutions. [6] extend this work to the case of non-zero step sizes and mini-batch training (SGD). [31] prove that Adam and RMSProp maximize the margin of homogeneous neural networks. Generalization of adaptive methods. [3] empirically investigate the edge-of-stability regime of adaptive gradient algorithms and the effect of sharpness (defined as the largest eigenvalue of the hessian) on generalization. [15] introduce a statistic that measures the uniformity of the hessian diagonal and argue that adaptive gradient algorithms are biased towards making this statistic smaller. [16] propose to improve generalization of adaptive methods by switching to SGD in the middle of training. Convergence of Adam. [26] investigate cases where Adam fails to converge to the optimal solution and argue that this is because of exponential averaging of the gradients only provides a short-term memory, and [33] propose ways of fixing the non-convergence issues by preventing the uncontrolled increase of the effective learning rate (i. e. learning rate divided by the square root of the exponential moving average of the squared gradients). Notation We denote the loss of the kth minibatch as a function of the network parameters θ ∈ Rp by Ek(θ), and in the full-batch setting we omit the index and write E(θ). ∇E means the gradient of E, and ∇ with indices means partial derivatives, e. g. ∇ijsE is a shortcut for ∂3E ∂θi∂θj∂θs . The norm without indices ∥·∥ is the two-norm of a vector, ∥·∥1 is the one-norm and ∥·∥1,ε is the perturbed one-norm 2See, e. g., the presentation http://videolectures.net/deeplearning2015_goodfellow_network_ optimization/. 3defined as ∥v∥1,ε = �p i=1 � v2 i + ε. (Of course, if ε > 0 the perturbed one-norm is not a norm, but ε = 0 makes it the one-norm.) 2 Implicit bias of full-batch Adam: an informal summary To avoid ambiguity and to provide the names and notations for hyperparameters, we define the algorithm below. Definition 2.1. The Adam algorithm is an optimization algorithm with numerical stability hyper- parameter ε > 0, squared gradient momentum hyperparameter ρ ∈ (0, 1), gradient momentum parameter β ∈ (0, 1), initialization θ(0) ∈ Rp, ν(0) = 0 ∈ Rp, m(0) = 0 ∈ Rp and the following update rule: for each n ≥ 0, j ∈ {1, . . . , p} ν(n+1) j = ρν(n) j + (1 − ρ) � ∇jEn(θ(n)) �2, m(n+1) j = βm(n) j + (1 − β)∇jEn(θ(n)), θ(n+1) j = θ(n) j − h m(n+1) j /(1 − βn+1) � ν(n+1) j /(1 − ρn+1) + ε . (1) Remark 2.2 (The ε hyperparameter is inside the square root). Note that the numerical stability hyperparameter ε > 0, which is introduced in these algorithms to avoid division by zero, is inside the square root in our definition. This way we avoid division by zero in the derivative too: the first derivative of x �→ �√x + ε �−1 is bounded for x ≥ 0. This is useful for our analysis. In the appendix, the original versions of RMSProp and Adam are also tackled, though with an additional assumption which requires that no component of the gradient can come very close to zero in the region of interest (in particular, no component can change the sign). This is true only for the initial period of learning (whereas the theorems below are true for the whole period). Practitioners do not seem to make a distinction between the version with ε inside vs. outside the square root: tutorials with both versions abound on machine learning related websites. Moreover, the popular Tensorflow variant of RMSProp has ε inside the square root3 even though in the documentation4 [17] is cited, where ε is outside. While conducting numerical experiments, we also noted that moving ε inside or outside the square root does not change the behavior of the algorithms qualitatively. Summary of our main result (in the full-batch case) Having provided the definition, we are ready to informally describe our theoretical result (in the full-batch special case). Assume E(θ) is the loss, whose partial derivatives up to the fourth order are bounded. Let {θ(n)} be iterations of Adam as defined in Definition 2.1. Our main result for this case is finding an ODE whose solution trajectory ˜θ(t) is h2-close to {θ(n)}, meaning that for any positive time horizon T > 0 there exists a constant C > 0 such that for any step size h ∈ (0, T) we have ∥˜θ(nh) − θ(n)∥ ≤ Ch2 (for n between 0 and ⌊T/h⌋). The ODE is written the following way (up to terms that rapidly go to zero as n grows): for the component number j ∈ {1, . . . , p} ˙˜θj(t) = − 1 � |∇jE(˜θ(t))|2 + ε � ∇jE(˜θ(t)) + bias � (2) with initial conditions ˜θj(0) = θ(0) j for all j, where the bias term is bias := h 2 � 1 + β 1 − β − 1 + ρ 1 − ρ + 1 + ρ 1 − ρ · ε |∇jE(˜θ(t))|2 + ε � ∇j ��∇E(˜θ(t)) �� 1,ε. (3) Depending on hyperparameter values and the training stage, the bias term can take two extreme forms, and during most of the training the reality is usually in between. The extreme cases are as follows. 3https://github.com/keras-team/keras/blob/f9336cc5114b4a9429a242deb264b707379646b7/ keras/optimizers/rmsprop.py#L190 4https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/experimental/ RMSprop 4ε “small” ε “large” β ≥ ρ ∥∇E(θ)∥1-penalized ∥∇E(θ)∥2 2-penalized ρ > β −∥∇E(θ)∥1-penalized ∥∇E(θ)∥2 2-penalized Table 1: Implicit bias of Adam: special cases. “Small” and “large” are in relation to squared gradient components. • If √ε is small compared to all components of ∇E(˜θ(t)), i. e. minj ��∇jE(˜θ(t)) �� ≫ √ε, which is the case during the initial learning stage, then bias = h 2 �1 + β 1 − β − 1 + ρ 1 − ρ � ∇j ��∇E(˜θ(t)) �� 1,ε. (4) For small ε, the perturbed one-norm is indistinguishable from the usual one-norm, and for β > ρ it is penalized (in much the same way as the squared two-norm is implicitly penalized in the case of GD), but for ρ > β its decrease is actually hindered by this term (so the bias is opposite to penalization). The ODE in (2) can be approximately rewritten as ˙˜θj(t) = − ∇j �E(˜θ(t)) ��∇jE(˜θ(t)) ��, �E(θ) = E(θ) + h 2 �1 + β 1 − β − 1 + ρ 1 − ρ � ��∇E(θ) �� 1. (5) • If √ε is large compared to all gradient components, i. e. maxj ��∇jE(˜θ(t)) �� ≪ √ε, which may happen during the later learning stage, the fraction with ε is the numerator in (3) approaches one, the dependence on ρ cancels out, and ��∇E(˜θ(t)) �� 1,ε ≈ p � i=1 √ε � 1 + ��∇iE(˜θ(t)) ��2 2ε � = p√ε + 1 2√ε ��∇E(˜θ(t)) ��2. (6) In other words, ∥ · ∥1,ε becomes ∥ · ∥2/(2√ε) up to an additive constant (which is “eaten” by the gradient): bias = h 4√ε 1 + β 1 − β ∇j ��∇E(˜θ(t)) ��2. The form of the ODE in this case is ˙˜θj(t) = −∇j �E(˜θ(t)), �E(θ) = 1 √ε � E(˜θ(t)) + h 4√ε 1 + β 1 − β ��∇E(˜θ(t)) ��2 � . (7) These two extreme cases are summarized in Table 1. In Figure 1, we use the one-dimensional (p = 1) case to illustrate what kind of term is being implicitly penalized. = 1e-05 =0.95, =0.995 =0.95, =0.99 =0.95, =0.951 =0.95, =0.95 =0.95, =0.9 =0.95, =0.8 =0.95, =0.75 =0.95, =0.5 =0.95, =0.1 = 1.0 =0.95, =0.995 =0.95, =0.99 =0.95, =0.951 =0.95, =0.95 =0.95, =0.9 =0.95, =0.8 =0.95, =0.75 =0.95, =0.5 =0.95, =0.1 = 100.0 =0.95, =0.995 =0.95, =0.99 =0.95, =0.951 =0.95, =0.95 =0.95, =0.9 =0.95, =0.8 =0.95, =0.75 =0.95, =0.5 =0.95, =0.1 Figure 1: The graphs of x �→ � x 0 � 1+β 1−β − 1+ρ 1−ρ + 1+ρ 1−ρ · ε y2+ε � d � ε + y2 for different β and ρ. 5Example 2.3 (Backward Error Analysis for GD with Heavy-ball Momentum). Assume ε is very large compared to all squared gradient components during the whole training process, so that the form of the ODE is approximated by (7). Since Adam with a large ε and after a certain number of iterations approximates SGD with heavy-ball momentum with step size h 1−β √ε , linear step size change (and corresponding time change) gives exactly the equations in Theorem 4.1 of [8]. Taking β = 0 (no momentum), we get the implicit regularization of GD from [1]. This overview also applies to RMSProp by setting β = 0. See Section 4 for the formal result. 3 ODE approximating mini-batch Adam trajectories: full statement We only make one assumption, which is standard in the literature: the loss for each mini-batch is 4 times continuously differentiable partial derivatives up to order 4 of each mini-batch loss Ek are bounded by constants, i. e. there exists a positive constant M such that for θ in the region of interest sup k sup θ � sup i |∇iEk(θ)| ∨ sup i,j |∇ijEk(θ)| ∨ sup i,j,s |∇ijsEk(θ)| ∨ sup i,j,s,r |∇ijsrEk(θ)| � ≤ M. (8) We now state the main result for mini-batch Adam, whose proof is in the supplemental appendix. Theorem 3.1. Assume (8) holds. Let {θ(n)} be iterations of Adam as defined in Definition 2.1, ˜θ(t) be the continuous solution to the piecewise ODE ˙˜θj(t) = − M (n) j (˜θ(t)) R(n) j (˜θ(t)) + h � M (n) j (˜θ(t)) � 2P (n) j (˜θ(t)) + ¯P (n) j (˜θ(t)) � 2R(n) j (˜θ(t))3 − 2L(n) j (˜θ(t)) + ¯L(n) j (˜θ(t)) 2R(n) j (˜θ(t)) � . (9) for t ∈ [nh, (n + 1)h] with the initial condition ˜θ(0) = θ(0), where R(n) j (θ) := � � � � n � k=0 ρn−k(1 − ρ)(∇jEk(θ))2/(1 − ρn+1) + ε, M (n) j (θ) := 1 1 − βn+1 n � k=0 βn−k(1 − β)∇jEk(θ), L(n) j (θ) := 1 1 − βn+1 n � k=0 βn−k(1 − β) p � i=1 ∇ijEk(θ) n−1 � l=k M (l) i (θ) R(l) i (θ) , ¯L(n) j (θ) := 1 1 − βn+1 n � k=0 βn−k(1 − β) p � i=1 ∇ijEk(θ)M (n) i (θ) R(n) i (θ) , P (n) j (θ) := 1 1 − ρn+1 n � k=0 ρn−k(1 − ρ)∇jEk(θ) p � i=1 ∇ijEk(θ) n−1 � l=k M (l) i (θ) R(l) i (θ) , ¯P (n) j (θ) := 1 1 − ρn+1 n � k=0 ρn−k(1 − ρ)∇jEk(θ) p � i=1 ∇ijEk(θ)M (n) i (θ) R(n) i (θ) . Then, for any fixed positive time horizon T > 0 there exists a constant C such that for any step size h ∈ (0, T) we have ��˜θ(nh) − θ(n)�� ≤ Ch2, n = 0, 1, . . . , ⌊T/h⌋. (10) Remark 3.2 (Backward error analysis of Adam in the full-batch setting). In the full-batch setting Ek ≡ E, the terms in Theorem 3.1 simplify to R(n) j (θ) = � |∇jE(θ)|2 + ε, M (n) j (θ) = ∇jE(θ), 6L(n) j (θ) = � β 1 − β − (n + 1)βn+1 1 − βn+1 � ∇j∥∇E(θ)∥1,ε, ¯L(n) j (θ) = ∇j∥∇E(θ)∥1,ε, P (n) j (θ) = � ρ 1 − ρ − (n + 1)ρn+1 1 − ρn+1 � ∇jE(θ)∇j∥∇E(θ)∥1,ε, ¯P (n) j (θ) = ∇jE(θ)∇j∥∇E(θ)∥1,ε. If the iteration number n is large, (9) rapidly becomes as described in (2) and (3). 4 ODE approximating mini-batch RMSProp trajectories: full statement We also study the properties of RMSProp using the same arguments as in Theorem 3.1. Up to rapidly vanishing terms, the resulting ODE is the same as for Adam with β = 0. See Remark 4.3 below. Definition 4.1. The RMSProp algorithm is an optimization algorithm with numerical stability parameter ε > 0, squared gradient momentum parameter ρ ∈ (0, 1), initialization θ(0) ∈ Rp, ν(0) = 0 ∈ Rp and the following update rule: for each n ≥ 0, j ∈ {1, . . . , p} ν(n+1) j = ρν(n) j + (1 − ρ) � ∇jEn(θ(n)) �2, θ(n+1) j = θ(n) j − h � ν(n+1) j + ε ∇jEn(θ(n)). (11) We now state the main result for mini-batch RMSProp, whose proof is in the supplemental appendix. Theorem 4.2. Assume (8) holds. Let {θ(n)} be iterations of RMSProp as defined in Definition 4.1, ˜θ(t) be the continuous solution to the piecewise ODE ˙˜θj(t) = −∇jEn(˜θ(t)) R(n) j (˜θ(t)) + h    ∇jEn(˜θ(t)) � 2P (n) j (˜θ(t)) + ¯P (n) j (˜θ(t)) � 2R(n) j (˜θ(t))3 − �p i=1 ∇ijEn(˜θ(t)) ∇iEn(˜θ(t)) R(n) i (˜θ(t)) 2R(n) j (˜θ(t))    . (12) for t ∈ [nh, (n + 1)h] with the initial condition ˜θ(0) = θ(0), where R(n) j (θ) := � � � � n � k=0 ρn−k(1 − ρ)(∇jEk(θ))2 + ε, P (n) j (θ) := n � k=0 ρn−k(1 − ρ)∇jEk(θ) p � i=1 ∇ijEk(θ) n−1 � l=k ∇iEl(θ) R(l) i (θ) , ¯P (n) j (θ) := n � k=0 ρn−k(1 − ρ)∇jEk(θ) p � i=1 ∇ijEk(θ)∇iEn(θ) R(n) i (θ) . Then, for any fixed positive time horizon T > 0 there exists a constant C such that for any step size h ∈ (0, T) we have ��˜θ(nh) − θ(n)�� ≤ Ch2, n = 0, 1, . . . , ⌊T/h⌋. Remark 4.3 (Backward error analysis of RMSProp in the full-batch setting). In the full-batch setting Ek ≡ E, the terms in Theorem 4.2 simplify to R(n) j (θ) = � |∇jE(θ)|2 (1 − ρn+1) + ε, P (n) j (θ) = n � k=0 ρn−k(1 − ρ)∇jE(θ) p � i=1 ∇ijE(θ) n−1 � l=k ∇iE(θ) � |∇iE(θ)|2(1 − ρl+1) + ε , ¯P (n) j (θ) = (1 − ρn+1)∇jE(θ) p � i=1 ∇ijE(θ) ∇iE(θ) � |∇iE(θ)|2(1 − ρn+1) + ε . If the iteration number n is large, (12) rapidly becomes as described in (2) and (3) with β = 0. 70.5 1.0 1.5 2.0 2.5 0.5 1.0 1.5 2.0 2.5 3.0 3.5 rho = 0.999, h = 0.001, eps = 1e-06 adam, beta = 0.9 adam, beta = 0.99 adam, beta = 0.999 0.5 1.0 1.5 2.0 2.5 0.5 1.0 1.5 2.0 2.5 3.0 3.5 beta = 0.9, h = 0.001, eps = 1e-06 adam, rho = 0.9 adam, rho = 0.99 adam, rho = 0.999 Figure 2: Increasing β moves the trajectory of Adam towards the regions with smaller one-norm of the gradient (if ε is sufficiently small); increasing ρ does the opposite. The violet line is the line of global minima, and the cross denotes the limiting point of minimal one-norm of the gradient. All Adam trajectories start at (2.8, 3.5). 5 Discussion First conclusion. Recall that from [8] the ODE approximating the dynamics of full-batch heavy-ball momentum GD is close to ˙θ = 1 1 − β ∇E(θ) + h 1 + β 4(1 − β)3 ∇ ∥∇E(θ)∥2 � �� � regularization . The first-order term regularizes the training process by penalizing the two-norm of the gradient of the loss. We can conclude with high confidence that this kind of regularization is typically absent in RMSProp (if ε is small) and Adam with ρ > β (if ε is small). This may partially explain why these algorithms generalize worse than their non-adaptive counterparts. Second conclusion. However, the bias term in (3) does contain a kind of “norm” which is the perturbed one-norm ∥v∥1,ε = �p i=1 � v2 i + ε. If √ε is small compared to gradient components, which is usually true except at the end of the training, we can conclude from (5) with moderate confidence that it is only in the case β > ρ that the perturbed norm is penalized, and decreasing ρ or increasing β moves the trajectory towards regions with lower “norm”. Third conclusion. There is currently no theory that would indicate that penalizing the (perturbed) one-norm of the gradient improves generalization. However, reasoning by analogy (with the case of the two-norm), we can conjecture with lower confidence that at least in some stable regimes of training increasing β and decreasing ρ should improve the test error. 6 Illustration: simple bilinear model We now analyze the effect of the first-order term for Adam in the same model as [1] and [8] have studied. Namely, assume the parameter θ = (θ1, θ2) is 2-dimensional, and the loss is given by E(θ) := 1/2(y − θ1θ2x)2, where x, y are fixed scalars x = 2, y = 3/2. The loss is minimized on the hyperbola θ1θ2 = y/x. We graph the trajectories of Adam in this case: Figure 2 shows that increasing β forces the trajectory to the region with smaller 1-norm of the gradient of the loss ∥∇E(θ)∥1, and increasing ρ does the opposite. Figure 3 shows that increasing the learning rate moves Adam towards the region with smaller ∥∇E(θ)∥1 if β > ρ (just like in the case of gradient descent, except the norm is different if ε is small compared to gradient components), and does the opposite if ρ > β. All these observations are exactly what Theorem 3.1 predicts. 80.5 1.0 1.5 2.0 2.5 0.5 1.0 1.5 2.0 2.5 3.0 3.5 beta = 0.995, rho=0.75, eps=1e-06 adam, h = 1e-05 adam, h = 0.0001 adam, h = 0.001 0.5 1.0 1.5 2.0 2.5 0.5 1.0 1.5 2.0 2.5 3.0 3.5 beta = 0.9, rho=0.999, eps=1e-06 adam, h = 1e-05 adam, h = 0.0001 adam, h = 0.001 Figure 3: The setting is the same as in Figure 2. Increasing the learning rate moves the Adam trajectory towards the regions with smaller one-norm of the gradient if β is significantly larger than ρ and does the opposite if ρ is larger than β. 7 Numerical experiments We offer some preliminary empirical evidence of how the first-order term shows up in deep neural networks. [21] divides training regimes of Adam into three categories: the spike regime when ρ is sufficiently larger than β, in which the training loss curve contains very large spikes and the training process is obviously unstable; the (stable) oscillation regime when ρ is sufficiently close to β, in which the loss curve contains fast and small oscillations; the divergence regime when β is sufficiently larger than ρ, in which the optimization diverges. We of course exclude the last regime. Since it is very unlikely that an unstable Adam trajectory is close to the piecewise ODE emerging from backward error analysis, we exclude this regime as well, and confine ourselves to considering the oscillation regime (in which ρ and β do not have to be equal, but should not be too far apart). This is the regime [21] recommend to use in practice. We train Resnet-50 on the CIFAR-10 dataset with full-batch Adam and calculate the quantity ∥∇E(θ)∥1,ε at the first point the training loss drops below 0.01. Figure 4 shows that in the stable oscillation regime increasing ρ seems to increase the perturbed one-norm. This is consistent with backward error analysis (the smaller ρ, the more this “norm” is penalized). We also observe that increasing ρ seems to decrease the test accuracy (see the same figure). The opposite effect was noticed in [3], which we think is the case for the spike regime (where the trajectory of Adam is definitely far from the piecewise ODE trajectory at the later stages of training): it is intuitively plausible that increasing the number and magnitude of the spikes should increase the test accuracy by reducing overfitting. The left part of Figure 5 shows that increasing β seems to decrease the perturbed one-norm. This is consistent with backward error analysis (the larger β, the more this norm is penalized). Similarly, the right part shows that increasing β seems to increase the test accuracy, if anything. Note that the effective learning rate does not depend on β (as is the case for gradient descent with heavy-ball momentum), so we compare training with the same effective learning rate. The picture confirms the finding in [8] (for momentum gradient descent) that increasing the momentum parameter almost always improves the test accuracy. We also train Resnet-101 on CIFAR-10 with full-batch Adam, investigating how increasing β influences the training process. Figure 6 shows the training loss curves and the perturbed one-norm curve (the graphs of ∥∇E∥1,ε as functions of the epoch number). Note that the training loss decreases monotonically to zero, the larger β the faster. The “norm” decreases, then rises again, and then decreases further until convergence. Throughout most of the training, the larger β the smaller the “norm” (so the norm behaves with respect to β in the opposite way the training loss does). The “hills” of the “norm” curves are higher with smaller β and almost unnoticeable when β = ρ. (This should be treated with caution: “hills” are not fully explained by ρ > β.) This is completely consistent with backward analysis because the larger ρ with respect to β, the more ∥∇E∥1,ε is prevented from 90.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99 3100 3200 3300 3400 3500 3600 3700 3800 Perturbed 1-norm 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99 81.5 82.0 82.5 83.0 83.5 84.0 84.5 85.0 Test accuracy Figure 4: Resnet-50 on CIFAR-10 trained with full-batch Adam. The test accuracy seems to fall as ρ increases (in the stable “small oscillations” regime of training). The hyperparameters are as follows: h = 7.5 · 10−5, ε = 10−8, β = 0.99. The test accuracies plotted here are maximal after more than 3600 epochs (they become almost constant much earlier). The perturbed norms are calculated at the same epoch number 900. (It is fair to compare Adam with different parameters at one epoch since the effective learning rates are the same.) 0.96 0.97 0.98 0.99 1.00 3000 3500 4000 4500 5000 Perturbed 1-norm 0.96 0.97 0.98 0.99 1.00 76 78 80 82 84 Test accuracy Figure 5: Resnet-50 on CIFAR-10 trained with full-batch Adam. The perturbed one-norm seems to fall as β increases (in the stable oscillation regime of training), and the test accuracy seems to rise. The hyperparameters are as follows: h = 10−4, ρ = 0.999, ε = 10−8. Both metrics are calculated when the loss first drops below the threshold 0.1. falling by the bias term in (10). The last picture in Figure 6 shows how penalizing the “norm” seems to correspond to increasing the test accuracy in this case. Further evidence on how the perturbed one-norm behaves during training is available in Figure 7, where we train Resnet-101 on CIFAR-100 with increasing ρ. We see that the “hills” are there even if β > ρ, but their height seems to be larger for larger ρ. 8 Limitations and future directions We think that backward error analysis applied to real-world machine learning optimization tasks has some limitations, some of them general to the whole literature and some of them specific to adaptive algorithms. First, the assumption similar to (8) is either explicitly or implicitly present in all previous work on backward error analysis of gradient-based machine learning algorithms, as far as we know. This relatively weak assumption is not true if at least one activation function in the neural network is ReLU: the loss is not even differentiable (though it is very common to ignore this). Moreover, there 100 500 1000 1500 2000 epoch 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 Train loss train h=0.0001, =0.9572, =0.999, =1e-08 train h=0.0001, =0.9687, =0.999, =1e-08 train h=0.0001, =0.9771, =0.999, =1e-08 train h=0.0001, =0.9833, =0.999, =1e-08 train h=0.0001, =0.9878, =0.999, =1e-08 train h=0.0001, =0.9911, =0.999, =1e-08 train h=0.0001, =0.9935, =0.999, =1e-08 train h=0.0001, =0.9952, =0.999, =1e-08 train h=0.0001, =0.9965, =0.999, =1e-08 train h=0.0001, =0.9974, =0.999, =1e-08 0 500 1000 1500 2000 epoch 10000 20000 30000 40000 50000 Perturbed one-norm train h=0.0001, =0.9572, =0.999, =1e-08 train h=0.0001, =0.9687, =0.999, =1e-08 train h=0.0001, =0.9771, =0.999, =1e-08 train h=0.0001, =0.9833, =0.999, =1e-08 train h=0.0001, =0.9878, =0.999, =1e-08 train h=0.0001, =0.9911, =0.999, =1e-08 train h=0.0001, =0.9935, =0.999, =1e-08 train h=0.0001, =0.9952, =0.999, =1e-08 train h=0.0001, =0.9965, =0.999, =1e-08 train h=0.0001, =0.9974, =0.999, =1e-08 0.960 0.965 0.970 0.975 0.980 0.985 0.990 0.995 74 76 78 80 Test accuracy Figure 6: Resnet-101 on CIFAR-10 trained with full-batch Adam. First picture from the top: training loss curves. Second picture: curves plotting ∥∇E∥1,ε after each epoch. Third picture: test accuracy the moment loss drops below 0.01 as a function of β. Hyperparameters: h = 10−4, ρ = 0.999, ε = 10−8. 110 200 400 600 800 1000 epoch 10000 20000 30000 40000 Perturbed one-norm train h=0.0001, =0.97, =0.92, =1e-08 train h=0.0001, =0.97, =0.9354, =1e-08 train h=0.0001, =0.97, =0.9479, =1e-08 train h=0.0001, =0.97, =0.9579, =1e-08 train h=0.0001, =0.97, =0.966, =1e-08 train h=0.0001, =0.97, =0.9726, =1e-08 train h=0.0001, =0.97, =0.9778, =1e-08 train h=0.0001, =0.97, =0.9821, =1e-08 train h=0.0001, =0.97, =0.9856, =1e-08 train h=0.0001, =0.97, =0.9883, =1e-08 train h=0.0001, =0.97, =0.9906, =1e-08 train h=0.0001, =0.97, =0.9924, =1e-08 train h=0.0001, =0.97, =0.9939, =1e-08 train h=0.0001, =0.97, =0.995, =1e-08 train h=0.0001, =0.97, =0.996, =1e-08 Figure 7: Resnet-101 trained on CIFAR-100 with full-batch Adam. We plot ∥∇E∥1,ε after each epoch. Hyperparameters: h = 10−4, β = 0.97, ε = 10−8. is evidence that large-batch algorithms often operate at the edge of stability ([2], [3]), in which the largest eigenvalue of the hessian can be quite large, making it unclear whether the higher-order partial derivatives can safely be assumed bounded near optimality. Second, note that the constant in (10) depends on ε and goes to infinity as ε goes to zero. Theoretically, very small ε may mean that the trajectory of the piecewise ODE is only close to the actual Adam trajectory for unrealistically small learning rates, at least at the later stages of learning. (For the initial learning period, this is not a problem.) It is also true of Proposition 1 in [21]: the real trajectory may be far away even from the sign-GD dynamics. This is especially noticeable in the large-spike regime of training (see Section 7 and [21]) which, despite being obviously pretty unstable, can still minimize the training loss well and lead to acceptable test errors. We believe that these considerations can fruitfully guide future work in this area. Acknowledgments and Disclosure of Funding We specially thank Boris Hanin for his insightful comments and suggestions. Cattaneo gratefully acknowledges financial support from the National Science Foundation through DMS-2210561 and SES-2241575. Klusowski gratefully acknowledges financial support from the National Science Foundation through CAREER DMS-2239448, DMS-2054808, and HDR TRIPODS CCF-1934924. References [1] David Barrett and Benoit Dherin. “Implicit Gradient Regularization”. In: International Con- ference on Learning Representations. 2021. URL: https://openreview.net/forum?id= 3q5IqUrkcF. [2] Jeremy Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar. “Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability”. In: International Conference on Learning Representations. 2021. URL: https://openreview.net/forum? id=jh-rTtvkGeM. [3] Jeremy M Cohen, Behrooz Ghorbani, Shankar Krishnan, Naman Agarwal, Sourabh Medapati, Michal Badura, Daniel Suo, David Cardoze, Zachary Nado, George E Dahl, et al. “Adaptive gradient methods at the edge of stability”. In: arXiv preprint arXiv:2207.14484 (2022). [4] John Duchi, Elad Hazan, and Yoram Singer. “Adaptive subgradient methods for online learning and stochastic optimization.” In: Journal of machine learning research 12.7 (2011). 12[5] Christian Lubich Ernst Hairer and Gerhard Wanner. Geometric numerical integration. 2nd ed. Springer-Verlag, Berlin, 2006. ISBN: 3-540-30663-3. [6] Mathieu Even, Scott Pesme, Suriya Gunasekar, and Nicolas Flammarion. “(S) GD over Diagonal Linear Networks: Implicit Regularisation, Large Stepsizes and Edge of Stability”. In: arXiv preprint arXiv:2302.08982 (2023). [7] Guilherme França, Michael I Jordan, and René Vidal. “On dissipative symplectic integration with applications to gradient-based optimization”. In: Journal of Statistical Mechanics: Theory and Experiment 2021.4 (2021), p. 043402. [8] Avrajit Ghosh, He Lyu, Xitong Zhang, and Rongrong Wang. “Implicit regularization in Heavy-ball momentum accelerated stochastic gradient descent”. In: The Eleventh International Conference on Learning Representations. 2023. URL: https://openreview.net/forum? id=ZzdBhtEH9yB. [9] Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. “Characterizing implicit bias in terms of optimization geometry”. In: International Conference on Machine Learning. PMLR. 2018, pp. 1832–1841. [10] Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. “Implicit bias of gradient descent on linear convolutional networks”. In: Advances in neural information processing systems 31 (2018). [11] Ziwei Ji and Matus Telgarsky. “Directional convergence and alignment in deep learning”. In: Advances in Neural Information Processing Systems 33 (2020), pp. 17176–17186. [12] Ziwei Ji and Matus Telgarsky. “Gradient descent aligns the layers of deep linear networks”. In: arXiv preprint arXiv:1810.02032 (2018). [13] Ziwei Ji and Matus Telgarsky. “Risk and parameter convergence of logistic regression”. In: arXiv preprint arXiv:1803.07300 (2018). [14] Ziwei Ji and Matus Telgarsky. “The implicit bias of gradient descent on nonseparable data”. In: Conference on Learning Theory. PMLR. 2019, pp. 1772–1798. [15] Kaiqi Jiang, Dhruv Malik, and Yuanzhi Li. “How Does Adaptive Optimization Impact Local Neural Network Geometry?” In: arXiv preprint arXiv:2211.02254 (2022). [16] Nitish Shirish Keskar and Richard Socher. “Improving generalization performance by switch- ing from adam to sgd”. In: arXiv preprint arXiv:1712.07628 (2017). [17] Diederick P Kingma and Jimmy Ba. “Adam: A method for stochastic optimization”. In: International Conference on Learning Representations. 2015. [18] Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel LK Yamins, and Hidenori Tanaka. “Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics”. In: arXiv preprint arXiv:2012.04728 (2020). [19] Qianxiao Li, Cheng Tai, and Weinan E. “Stochastic Modified Equations and Adaptive Stochas- tic Gradient Algorithms”. In: Proceedings of the 34th International Conference on Machine Learning. Ed. by Doina Precup and Yee Whye Teh. Vol. 70. Proceedings of Machine Learning Research. PMLR, Aug. 2017, pp. 2101–2110. URL: https://proceedings.mlr.press/ v70/li17f.html. [20] Kaifeng Lyu and Jian Li. “Gradient descent maximizes the margin of homogeneous neural networks”. In: arXiv preprint arXiv:1906.05890 (2019). [21] Chao Ma, Lei Wu, and E Weinan. “A qualitative study of the dynamic behavior for adap- tive gradient algorithms”. In: Mathematical and Scientific Machine Learning. PMLR. 2022, pp. 671–692. [22] Taiki Miyagawa. “Toward Equation of Motion for Deep Neural Networks: Continuous-time Gradient Descent and Discretization Error Analysis”. In: Advances in Neural Information Processing Systems. Ed. by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho. 2022. URL: https://openreview.net/forum?id=qq84D17BPu. [23] Mor Shpigel Nacson, Suriya Gunasekar, Jason Lee, Nathan Srebro, and Daniel Soudry. “Lexi- cographic and depth-sensitive margins in homogeneous and non-homogeneous deep models”. In: International Conference on Machine Learning. PMLR. 2019, pp. 4683–4692. [24] Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique Pamplona Savarese, Nathan Srebro, and Daniel Soudry. “Convergence of gradient descent on separable data”. In: The 22nd International Conference on Artificial Intelligence and Statistics. PMLR. 2019, pp. 3420–3428. 13[25] Mor Shpigel Nacson, Nathan Srebro, and Daniel Soudry. “Stochastic gradient descent on separable data: Exact convergence with a fixed learning rate”. In: The 22nd International Conference on Artificial Intelligence and Statistics. PMLR. 2019, pp. 3051–3059. [26] Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. “On the convergence of adam and beyond”. In: arXiv preprint arXiv:1904.09237 (2019). [27] Mihaela C Rosca, Yan Wu, Benoit Dherin, and David Barrett. “Discretization drift in two- player games”. In: International Conference on Machine Learning. PMLR. 2021, pp. 9064– 9074. [28] Samuel L Smith, Benoit Dherin, David Barrett, and Soham De. “On the Origin of Implicit Regularization in Stochastic Gradient Descent”. In: International Conference on Learning Representations. 2021. URL: https://openreview.net/forum?id=rq_Qr0c1Hyo. [29] Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. “The implicit bias of gradient descent on separable data”. In: The Journal of Machine Learning Research 19.1 (2018), pp. 2822–2878. [30] Tijmen Tieleman, Geoffrey Hinton, et al. “Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude”. In: COURSERA: Neural networks for machine learning 4.2 (2012), pp. 26–31. [31] Bohan Wang, Qi Meng, Wei Chen, and Tie-Yan Liu. “The Implicit Bias for Adaptive Optimiza- tion Algorithms on Homogeneous Neural Networks”. In: Proceedings of the 38th International Conference on Machine Learning. Ed. by Marina Meila and Tong Zhang. Vol. 139. Pro- ceedings of Machine Learning Research. PMLR, July 2021, pp. 10849–10858. URL: https: //proceedings.mlr.press/v139/wang21q.html. [32] Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. “Kernel and rich regimes in overparametrized models”. In: Conference on Learning Theory. PMLR. 2020, pp. 3635–3673. [33] Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. “Adaptive Methods for Nonconvex Optimization”. In: Advances in Neural Information Processing Systems. Ed. by S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett. Vol. 31. Curran Associates, Inc., 2018. URL: https://proceedings.neurips.cc/ paper_files/paper/2018/file/90365351ccc7437a1309dc64e4db32a3-Paper.pdf. [34] Yang Zhao, Hao Zhang, and Xiuyuan Hu. “Penalizing gradient norm for efficiently improving generalization in deep learning”. In: International Conference on Machine Learning. PMLR. 2022, pp. 26982–26992. 14Supplementary Material for the Manuscript “On the Implicit Bias of Adam” by Matias D. Cattaneo, Jason M. Klusowski, and Boris Shigida September 4, 2023 Contents 1 Overview 1 2 RMSProp with ε outside the square root 2 3 RMSProp with ε inside the square root 5 4 Adam with ε outside the square root 7 5 Adam with ε inside the square root 10 6 Technical bounding lemmas 12 7 Proof of Theorem SA-2.3 21 8 Numerical experiments 27 1 Overview SA-1.1. This appendix provides some omitted details and proofs. We consider two algorithms: RMSProp and Adam, and two versions of each algorithm (with the numerical stability ε parameter inside and outside of the square root in the denominator). This means there are four main theorems: Theorem SA-2.4, Theorem SA-3.4, Theorem SA-4.4 and Theorem SA-5.4, each residing in the section completely devoted to one algorithm. The simple induction argument taken from [1], essentially the same for each of these theorems, is based on an auxiliary result whose corresponding versions are Theorem SA-2.3, Theorem SA-3.3, Theorem SA-4.3 and Theorem SA-5.3. The proof of this result is also elementary but long, and it is done by a series of lemmas in Section 6 and Section 7, culminating in Section SA-7.6. Out of these four, we only prove Theorem SA-2.3 since the other three results are proven in the same way with obvious changes. Section 8 contains some details about the numerical experiments. SA-1.2 Notation. We denote the loss of the kth minibatch as a function of the network parameters θ ∈ Rp by Ek(θ), and in the full-batch setting we omit the index and write E(θ). As usual, ∇E means the gradient of E, and nabla with indices means partial derivatives, e. g. ∇ijsE is a shortcut for ∂3E ∂θi∂θj∂θs . The letter T > 0 will always denote a finite time horizon of the ODEs, h will always denote the training step size, and we will replace nh with tn when convenient, where n ∈ {0, 1, . . .} is the step number. We will use the same notation for the iteration of the discrete algorithm � θ(k)� k∈Z≥0 , the piecewise ODE solution ˜θ(t) and some auxiliary terms for each of the four algorithms: see Definition SA-2.1, Definition SA- 1 arXiv:2309.00079v1 [cs.LG] 31 Aug 20233.1, Definition SA-4.1, Definition SA-5.1. This way, we avoid cluttering the notation significantly. We are careful to reference the relevant definition in all theorem statements. 2 RMSProp with ε outside the square root Definition SA-2.1. In this section, for some θ(0) ∈ Rp, ν(0) = 0 ∈ Rp, ρ ∈ (0, 1), let the sequence of p-vectors � θ(k)� k∈Z≥0 be defined for n ≥ 0 by ν(n+1) j = ρν(n) j + (1 − ρ) � ∇jEn � θ(n)��2 , θ(n+1) j = θ(n) j − h � ν(n+1) j + ε ∇jEn � θ(n)� . (2.1) Let ˜θ(t) be defined as a continuous solution to the piecewise ODE ˙˜θj(t) = − ∇jEn � ˜θ(t) � R(n) j � ˜θ(t) � + ε + h       ∇jEn � ˜θ(t) � � 2P (n) j � ˜θ(t) � + ¯P (n) j � ˜θ(t) �� 2 � R(n) j � ˜θ(t) � + ε �2 R(n) j � ˜θ(t) � − �p i=1 ∇ijEn � ˜θ(t) � ∇iEn(˜θ(t)) R(n) i (˜θ(t))+ε 2 � R(n) j (˜θ(t)) + ε �       (2.2) with the initial condition ˜θ(0) = θ(0), where R(n)(θ), P(n)(θ) and ¯P(n)(θ) are p-dimensional functions with components R(n) j (θ) := � � � � n � k=0 ρn−k(1 − ρ) � ∇jEk(θ) �2, P (n) j (θ) := n � k=0 ρn−k(1 − ρ)∇jEk (θ) p � i=1 ∇ijEk (θ) n−1 � l=k ∇iEl (θ) R(l) i (θ) + ε , ¯P (n) j (θ) := n � k=0 ρn−k(1 − ρ)∇jEk (θ) p � i=1 ∇ijEk (θ) ∇iEn (θ) R(n) i (θ) + ε . Assumption SA-2.2. 1. For some positive constants M1, M2, M3, M4 we have sup i sup k sup θ ��∇iEk(θ) �� ≤ M1, sup i,j sup k sup θ ��∇ijEk(θ) �� ≤ M2, sup i,j,s sup k sup θ ��∇ijsEk(θ) �� ≤ M3, sup i,j,s,r sup k sup θ ��∇ijsrEk(θ) �� ≤ M4. 2. For some R > 0 we have for all n ∈ � 0, 1, . . . , ⌊T/h⌋ � R(n) j � ˜θ(tn) � ≥ R, n � k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(tk) ��2 ≥ R2, where ˜θ(t) is defined in Definition SA-2.1. 2Theorem SA-2.3 (RMSProp with ε outside: local error bound). Suppose Assumption SA-2.2 holds. Then for all n ∈ � 0, 1, . . . , ⌊T/h⌋ � ����������� ˜θj(tn+1) − ˜θj(tn) + h ∇jEn � ˜θ(tn) � � �n k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(tk) ��2 + ε ����������� ≤ C1h3 for a positive constant C1 depending on ρ. The proof of Theorem SA-2.3 is conceptually simple but very technical, and we delay it until Section 7. For now assuming it as given and combining it with a simple induction argument gives a global error bound which follows. Theorem SA-2.4 (RMSProp with ε outside: global error bound). Suppose Assumption SA-2.2 holds, and n � k=0 ρn−k(1 − ρ) � ∇jEk � θ(k)��2 ≥ R2 for � θ(k)� k∈Z≥0 defined in Definition SA-2.1. Then there exist positive constants d1, d2, d3 such that for all n ∈ � 0, 1, . . . , ⌊T/h⌋ � ∥en∥ ≤ d1ed2nhh2 and ∥en+1 − en∥ ≤ d3ed2nhh3, where en := ˜θ(tn) − θ(n). The constants can be defined as d1 := C1, d2 :=  1 + M2√p R + ε � M 2 1 R(R + ε) + 1 � d1   √p, d3 := C1d2. Proof. We will show this by induction over n, the same way an analogous bound is shown in [1]. The base case is n = 0. Indeed, e0 = ˜θ(0) − θ(0) = 0. Then the jth component of e1 − e0 is [e1 − e0]j = [e1]j = ˜θj(t1) − θ(0) j + h∇jE0 � θ(0)� � (1 − ρ) � ∇jE0 � θ(0)��2 + ε = ˜θj(t1) − ˜θj(t0) + h∇jE0 � ˜θ(t0) � � (1 − ρ) � ∇jE0 � ˜θ(t0) ��2 + ε . By Theorem SA-2.3, the absolute value of the right-hand side does not exceed C1h3, which means ∥e1 − e0∥ ≤ C1h3√p. Since C1√p ≤ d3, the base case is proven. Now suppose that for all k = 0, 1, . . . , n − 1 the claim ∥ek∥ ≤ d1ed2khh2 and ∥ek+1 − ek∥ ≤ d3ed2khh3 is proven. Then ∥en∥ (a) ≤ ∥en−1∥ + ∥en − en−1∥ ≤ d1ed2(n−1)hh2 + d3ed2(n−1)hh3 = d1ed2(n−1)hh2 � 1 + d3 d1 h � (b) ≤ d1ed2(n−1)hh2 (1 + d2h) 3(c) ≤ d1ed2(n−1)hh2 · ed2h = d1ed2nhh2, where (a) is by the triangle inequality, (b) is by d3/d1 ≤ d2, in (c) we used 1 + x ≤ ex for all x ≥ 0. Next, combining Theorem SA-2.3 with (2.1), we have ���[en+1 − en]j ��� ≤ C1h3 + h ������� ∇jEn � ˜θ(tn) � √ A + ε − ∇jEn � θ(n)� √ B + ε ������� , (2.3) where to simplify notation we put A := n � k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(tk) ��2 , B := n � k=0 ρn−k(1 − ρ) � ∇jEk � θ(k)��2 . Using A ≥ R2, B ≥ R2, we have ���� 1 √ A + ε − 1 √ B + ε ���� = |A − B| �√ A + ε � �√ B + ε � �√ A + √ B � ≤ |A − B| 2R (R + ε)2 . (2.4) But since ����� � ∇jEk � ˜θ(tk) ��2 − � ∇jEk � θ(k)��2����� = ����∇jEk � ˜θ(tk) � − ∇jEk � θ(k)����� · ����∇jEk � ˜θ(tk) � + ∇jEk � θ(k)����� ≤ 2M1 ����∇jEk � ˜θ(tk) � − ∇jEk � θ(k)����� ≤ 2M1M2 √p ���˜θ(tk) − θ(k)��� , we have |A − B| ≤ 2M1M2 √p n � k=0 ρn−k(1 − ρ) ���˜θ(tk) − θ(k)��� . (2.5) Combining (2.4) and (2.5), we obtain ������� ∇jEn � ˜θ(tn) � √ A + ε − ∇jEn � θ(n)� √ B + ε ������� ≤ ����∇jEn � ˜θ(tn) ����� · ���� 1 √ A + ε − 1 √ B + ε ���� + ����∇jEn � ˜θ(tn) � − ∇jEn � θ(n)����� √ B + ε ≤ M1 · 2M1M2√p �n k=0 ρn−k(1 − ρ) ���˜θ(tk) − θ(k)��� 2R(R + ε)2 + M2√p ���˜θ(tn) − θ(n)��� R + ε = M 2 1 M2√p R(R + ε)2 n � k=0 ρn−k(1 − ρ) ���˜θ(tk) − θ(k)��� + M2√p R + ε ���˜θ(tn) − θ(n)��� (a) ≤ M 2 1 M2√p R(R + ε)2 n � k=0 ρn−k(1 − ρ)d1ed2khh2 + M2√p R + ε d1ed2nhh2, (2.6) where in (a) we used the induction hypothesis and that the bound on ∥en∥ is already proven. Now note that since 0 < ρe−d2h ≤ ρ, we have �n k=0 � ρe−d2h�k ≤ �∞ k=0 ρk = 1 1−ρ, which is rewritten as n � k=0 ρn−k(1 − ρ)ed2kh ≤ ed2nh. 4Then we can continue (2.6): ������� ∇jEn � ˜θ(tn) � √ A + ε − ∇jEn � θ(n)� √ B + ε ������� ≤ M2√p R + ε � M 2 1 R(R + ε) + 1 � d1ed2nhh2 (2.7) Again using 1 ≤ ed2nh, we conclude from (2.3) and (2.7) that ∥en+1 − en∥ ≤  C1 + M2√p R + ε � M 2 1 R(R + ε) + 1 � d1   √p � �� � ≤d3 ed2nhh3, finishing the induction step. SA-2.5 RMSProp with ε outside: full-batch. In the full-batch setting Ek ≡ E, the terms in (2.2) simplify to R(n) j (θ) = ��∇jE(θ) �� � 1 − ρn+1, P (n) j (θ) = n � k=0 ρn−k(1 − ρ)∇jE(θ) p � i=1 ∇ijE(θ) n−1 � l=k ∇iE(θ) ��∇iE(θ) �� � 1 − ρl+1 + ε , ¯P (n) j (θ) = � 1 − ρn+1� ∇jE(θ) p � i=1 ∇ijE(θ) ∇iE(θ) ��∇iE(θ) �� � 1 − ρn+1 + ε . If ε is small and the iteration number n is large, (2.2) simplifies to ˙˜θj(t) = − sign ∇jE(˜θ(t)) + h ρ 1 − ρ · �p i=1 ∇ijE(˜θ(t)) sign ∇iE(˜θ(t)) ���∇jE(˜θ(t)) ��� = ���∇jE(˜θ(t)) ��� −1 � −∇jE(˜θ(t)) + h ρ 1 − ρ∇j ���∇E(˜θ(t)) ��� 1 � . 3 RMSProp with ε inside the square root Definition SA-3.1. In this section, for some θ(0) ∈ Rp, ν(0) = 0 ∈ Rp, ρ ∈ (0, 1), let the sequence of p-vectors � θ(k)� k∈Z≥0 be defined for n ≥ 0 by ν(n+1) j = ρν(n) j + (1 − ρ) � ∇jEn � θ(n)��2 , θ(n+1) j = θ(n) j − h � ν(n+1) j + ε ∇jEn � θ(n)� . (3.1) Let ˜θ(t) be defined as a continuous solution to the piecewise ODE ˙˜θj(t) = − ∇jEn � ˜θ(t) � R(n) j � ˜θ(t) � + h      ∇jEn � ˜θ(t) � � 2P (n) j � ˜θ(t) � + ¯P (n) j � ˜θ(t) �� 2R(n) j � ˜θ(t) �3 − �p i=1 ∇ijEn � ˜θ(t) � ∇iEn(˜θ(t)) R(n) i (˜θ(t)) 2R(n) j (˜θ(t))      . (3.2) 5with the initial condition ˜θ(0) = θ(0), where R(n)(θ), P(n)(θ) and ¯P(n)(θ) are p-dimensional functions with components R(n) j (θ) := � � � � n � k=0 ρn−k(1 − ρ) � ∇jEk(θ) �2 + ε, P (n) j (θ) := n � k=0 ρn−k(1 − ρ)∇jEk (θ) p � i=1 ∇ijEk (θ) n−1 � l=k ∇iEl (θ) R(l) i (θ) , ¯P (n) j (θ) := n � k=0 ρn−k(1 − ρ)∇jEk (θ) p � i=1 ∇ijEk (θ) ∇iEn (θ) R(n) i (θ) . (3.3) Assumption SA-3.2. For some positive constants M1, M2, M3, M4 we have sup i sup k sup θ ��∇iEk(θ) �� ≤ M1, sup i,j sup k sup θ ��∇ijEk(θ) �� ≤ M2, sup i,j,s sup k sup θ ��∇ijsEk(θ) �� ≤ M3, sup i,j,s,r sup k sup θ ��∇ijsrEk(θ) �� ≤ M4. Theorem SA-3.3 (RMSProp with ε inside: local error bound). Suppose Assumption SA-3.2 holds. Then for all n ∈ � 0, 1, . . . , ⌊T/h⌋ � ����������� ˜θj(tn+1) − ˜θj(tn) + h ∇jEn � ˜θ(tn) � � �n k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(tk) ��2 + ε ����������� ≤ C2h3 for a positive constant C2 depending on ρ, where ˜θ(t) is defined in Definition SA-3.1. We omit the proof since it is essentially the same argument as for Theorem SA-2.3. Theorem SA-3.4 (RMSProp with ε inside: global error bound). Suppose Assumption SA-3.2 holds. Then there exist positive constants d4, d5, d6 such that for all n ∈ � 0, 1, . . . , ⌊T/h⌋ � ∥en∥ ≤ d4ed5nhh2 and ∥en+1 − en∥ ≤ d6ed5nhh3, where en := ˜θ(tn) − θ(n); ˜θ(t) and � θ(k)� k∈Z≥0 are defined in Definition SA-3.1. The constants can be defined as d4 := C2, d5 :=  1 + M2√p √ε � M 2 1 ε + 1 � d4   √p, d6 := C2d5. We omit the proof since it is essentially the same argument as for Theorem SA-2.4. 64 Adam with ε outside the square root Definition SA-4.1. In this section, for some θ(0) ∈ Rp, ν(0) = 0 ∈ Rp, β, ρ ∈ (0, 1), let the sequence of p-vectors � θ(k)� k∈Z≥0 be defined for n ≥ 0 by ν(n+1) j = ρν(n) j + (1 − ρ) � ∇jEn � θ(n)��2 , m(n+1) j = βm(n) j + (1 − β)∇jEn � θ(n)� , θ(n+1) j = θ(n) j − h m(n+1) j / � 1 − βn+1� � ν(n+1) j / (1 − ρn+1) + ε or, rewriting, θ(n+1) j = θ(n) j − h 1 1−βn+1 �n k=0 βn−k (1 − β) ∇jEk � θ(k)� � 1 1−ρn+1 �n k=0 ρn−k(1 − ρ) � ∇jEk � θ(k)��2 + ε . (4.1) Let ˜θ(t) be defined as a continuous solution to the piecewise ODE ˙˜θj(t) = − M (n) j � ˜θ(t) � R(n) j � ˜θ(t) � + ε + h       M (n) j � ˜θ(t) � � 2P (n) j � ˜θ(t) � + ¯P (n) j � ˜θ(t) �� 2 � R(n) j � ˜θ(t) � + ε �2 R(n) j � ˜θ(t) � − 2L(n) j � ˜θ(t) � + ¯L(n) j � ˜θ(t) � 2 � R(n) j � ˜θ(t) � + ε �       . (4.2) with the initial condition ˜θ(0) = θ(0), where R(n)(θ), P(n)(θ), ¯P(n)(θ), M(n)(θ), L(n)(θ), ¯L(n)(θ) are p-dimensional functions with components R(n) j (θ) := � � � � n � k=0 ρn−k(1 − ρ) � ∇jEk(θ) �2 / (1 − ρn+1), M (n) j (θ) := 1 1 − βn+1 n � k=0 βn−k (1 − β) ∇jEk (θ) , L(n) j (θ) := 1 1 − βn+1 n � k=0 βn−k(1 − β) p � i=1 ∇ijEk(θ) n−1 � l=k M (l) i (θ) R(l) i (θ) + ε , ¯L(n) j (θ) := 1 1 − βn+1 n � k=0 βn−k(1 − β) p � i=1 ∇ijEk(θ) M (n) i (θ) R(n) i (θ) + ε , P (n) j (θ) := 1 1 − ρn+1 n � k=0 ρn−k(1 − ρ)∇jEk(θ) p � i=1 ∇ijEk(θ) n−1 � l=k M (l) i (θ) R(l) i (θ) + ε , ¯P (n) j (θ) := 1 1 − ρn+1 n � k=0 ρn−k(1 − ρ)∇jEk(θ) p � i=1 ∇ijEk(θ) M (n) i (θ) R(n) i (θ) + ε . (4.3) Assumption SA-4.2. 1. For some positive constants M1, M2, M3, M4 we have sup i sup k sup θ ��∇iEk(θ) �� ≤ M1, 7sup i,j sup k sup θ ��∇ijEk(θ) �� ≤ M2, sup i,j,s sup k sup θ ��∇ijsEk(θ) �� ≤ M3, sup i,j,s,r sup k sup θ ��∇ijsrEk(θ) �� ≤ M4. 2. For some R > 0 we have for all n ∈ � 0, 1, . . . , ⌊T/h⌋ � R(n) j � ˜θ(tn) � ≥ R, 1 1 − ρn+1 n � k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(tk) ��2 ≥ R2, where ˜θ(t) is defined in Definition SA-4.1. Theorem SA-4.3 (Adam with ε outside: local error bound). Suppose Assumption SA-4.2 holds. Then for all n ∈ � 0, 1, . . . , ⌊T/h⌋ � ����������� ˜θj(tn+1) − ˜θj(tn) + h 1 1−βn+1 �n k=0 βn−k (1 − β) ∇jEk � ˜θ(tk) � � 1 1−ρn+1 �n k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(tk) ��2 + ε ����������� ≤ C3h3 for a positive constant C3 depending on β and ρ. We omit the proof since it is essentially the same argument as for Theorem SA-2.3. Theorem SA-4.4 (Adam with ε outside: global error bound). Suppose Assumption SA-4.2 holds, and 1 1 − ρn+1 n � k=0 ρn−k(1 − ρ) � ∇jEk � θ(k)��2 ≥ R2 for � θ(k)� k∈Z≥0 defined in Definition SA-4.1. Then there exist positive constants d7, d8, d9 such that for all n ∈ � 0, 1, . . . , ⌊T/h⌋ � ∥en∥ ≤ d7ed8nhh2 and ∥en+1 − en∥ ≤ d9ed8nhh3, where en := ˜θ(tn) − θ(n). The constants can be defined as d7 := C3, d8 :=  1 + M2√p R + ε � M 2 1 R(R + ε) + 1 � d7   √p, d9 := C3d8. Proof. Analogously to Theorem SA-2.4, we will prove this by induction over n. The base case is n = 0. Indeed, e0 = ˜θ(0) − θ(0) = 0. Then the jth component of e1 − e0 is [e1 − e0]j = [e1]j = ˜θj(t1) − θ(0) j + h∇jE0 � θ(0)� ����∇jE0 � θ(0)����� + ε = ˜θj(t1) − ˜θj(t0) + h∇jE0 � ˜θ(t0) � �� ∇jE0 � ˜θ(t0) ��2 + ε . 8By Theorem SA-4.3, the absolute value of the right-hand side does not exceed C3h3, which means ∥e1 − e0∥ ≤ C3h3√p. Since C3√p ≤ d9, the base case is proven. Now suppose that for all k = 0, 1, . . . , n − 1 the claim ∥ek∥ ≤ d7ed8khh2 and ∥ek+1 − ek∥ ≤ d9ed8khh3 is proven. Then ∥en∥ (a) ≤ ∥en−1∥ + ∥en − en−1∥ ≤ d7ed8(n−1)hh2 + d9ed8(n−1)hh3 = d7ed8(n−1)hh2 � 1 + d9 d7 h � (b) ≤ d7ed8(n−1)hh2 (1 + d8h) (c) ≤ d7ed8(n−1)hh2 · ed8h = d7ed8nhh2, where (a) is by the triangle inequality, (b) is by d9/d7 ≤ d8, in (c) we used 1 + x ≤ ex for all x ≥ 0. Next, combining Theorem SA-4.3 with (4.1), we have ���[en+1 − en]j ��� ≤ C3h3 + h ���� N ′ √ D′ + ε − N ′′ √ D′′ + ε ���� , (4.4) where to simplify notation we put N ′ := 1 1 − βn+1 n � k=0 βn−k(1 − β)∇jEk � θ(k)� , N ′′ := 1 1 − βn+1 n � k=0 βn−k(1 − β)∇jEk � ˜θ(tk) � , D′ := 1 1 − ρn+1 n � k=0 ρn−k(1 − ρ) � ∇jEk � θ(k)��2 , D′′ := 1 1 − ρn+1 n � k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(tk) ��2 . Using D′ ≥ R2, D′′ ≥ R2, we have ���� 1 √ D′ + ε − 1 √ D′′ + ε ���� = ��D′ − D′′�� �√ D′ + ε � �√ D′′ + ε � �√ D′ + √ D′′ � ≤ ��D′ − D′′�� 2R (R + ε)2 . (4.5) But since ����� � ∇jEk � θ(k)��2 − � ∇jEk � ˜θ(tk) ��2����� = ����∇jEk � θ(k)� − ∇jEk � ˜θ(tk) ����� · ����∇jEk � θ(k)� + ∇jEk � ˜θ(tk) ����� ≤ 2M1 ����∇jEk � θ(k)� − ∇jEk � ˜θ(tk) ����� ≤ 2M1M2 √p ���θ(k) − ˜θ(tk) ��� , we have ��D′ − D′′�� ≤ 2M1M2√p 1 − ρn+1 n � k=0 ρn−k(1 − ρ) ���θ(k) − ˜θ(tk) ��� . (4.6) Similarly, ��N ′ − N ′′�� ≤ 1 1 − βn+1 n � k=0 βn−k(1 − β) ����∇jEk � θ(k)� − ∇jEk � ˜θ(tk) ����� ≤ 1 1 − βn+1 n � k=0 βn−k(1 − β)M2 √p ���θ(k) − ˜θ(tk) ��� . (4.7) 9Combining (4.5), (4.6) and (4.7), we get ���� N ′ √ D′ + ε − N ′′ √ D′′ + ε ���� ≤ ��N ′�� · ���� 1 √ D′ + ε − 1 √ D′′ + ε ���� + ��N ′ − N ′′�� √ D′′ + ε ≤ 1 1 − βn+1 n � k=0 βn−k(1 − β)M1 · 2M1M2√p 2R(R + ε)2 (1 − ρn+1) n � k=0 ρn−k(1 − ρ) ���θ(k) − ˜θ(tk) ��� + M2√p (R + ε) (1 − βn+1) n � k=0 βn−k(1 − β) ���θ(k) − ˜θ(tk) ��� = M 2 1 M2√p R(R + ε)2 (1 − ρn+1) n � k=0 ρn−k(1 − ρ) ���θ(k) − ˜θ(tk) ��� + M2√p (R + ε) (1 − βn+1) n � k=0 βn−k(1 − β) ���θ(k) − ˜θ(tk) ��� (a) ≤ M 2 1 M2√p R(R + ε)2 (1 − ρn+1) n � k=0 ρn−k(1 − ρ)d7ed8khh2 + M2√p (R + ε) (1 − βn+1) n � k=0 βn−k(1 − β)d7ed8khh2, (4.8) where in (a) we used the induction hypothesis and that the bound on ∥en∥ is already proven. Now note that since 0 < ρe−d8h < ρ, we have �n k=0 � ρe−d8h�k ≤ �n k=0 ρk = � 1 − ρn+1� / (1 − ρ), which is rewritten as 1 1 − ρn+1 n � k=0 ρn−k(1 − ρ)ed8kh ≤ ed8nh. By the same logic, 1 1 − βn+1 n � k=0 βn−k(1 − β)ed8kh ≤ ed8nh. Then we can continue (4.8): ���� N ′ √ D′ + ε − N ′′ √ D′′ + ε ���� ≤ M2√p R + ε � M 2 1 R(R + ε) + 1 � d7ed8nhh2 (4.9) Again using 1 ≤ ed8nh, we conclude from (4.4) and (4.9) that ∥en+1 − en∥ ≤  C3 + M2√p R + ε � M 2 1 R(R + ε) + 1 � d7   √p � �� � ≤d9 ed8nhh3, finishing the induction step. 5 Adam with ε inside the square root Definition SA-5.1. In this section, for some θ(0) ∈ Rp, ν(0) = 0 ∈ Rp, β, ρ ∈ (0, 1), let the sequence of p-vectors � θ(k)� k∈Z≥0 be defined for n ≥ 0 by ν(n+1) j = ρν(n) j + (1 − ρ) � ∇jEn � θ(n)��2 , m(n+1) j = βm(n) j + (1 − β)∇jEn � θ(n)� , θ(n+1) j = θ(n) j − h m(n+1) j / � 1 − βn+1� � ν(n+1) j / (1 − ρn+1) + ε . (5.1) 10Let ˜θ(t) be defined as a continuous solution to the piecewise ODE ˙˜θj(t) = − M (n) j � ˜θ(t) � R(n) j � ˜θ(t) � + h      M (n) j � ˜θ(t) � � 2P (n) j � ˜θ(t) � + ¯P (n) j � ˜θ(t) �� 2R(n) j � ˜θ(t) �3 − 2L(n) j � ˜θ(t) � + ¯L(n) j � ˜θ(t) � 2R(n) j � ˜θ(t) �      . (5.2) with the initial condition ˜θ(0) = θ(0), where R(n)(θ), P(n)(θ), ¯P(n)(θ), M(n)(θ), L(n)(θ), ¯L(n)(θ) are p-dimensional functions with components R(n) j (θ) := � � � � n � k=0 ρn−k(1 − ρ) � ∇jEk(θ) �2 / (1 − ρn+1) + ε, M (n) j (θ) := 1 1 − βn+1 n � k=0 βn−k (1 − β) ∇jEk (θ) , L(n) j (θ) := 1 1 − βn+1 n � k=0 βn−k(1 − β) p � i=1 ∇ijEk(θ) n−1 � l=k M (l) i (θ) R(l) i (θ) , ¯L(n) j (θ) := 1 1 − βn+1 n � k=0 βn−k(1 − β) p � i=1 ∇ijEk(θ)M (n) i (θ) R(n) i (θ) , P (n) j (θ) := 1 1 − ρn+1 n � k=0 ρn−k(1 − ρ)∇jEk(θ) p � i=1 ∇ijEk(θ) n−1 � l=k M (l) i (θ) R(l) i (θ) , ¯P (n) j (θ) := 1 1 − ρn+1 n � k=0 ρn−k(1 − ρ)∇jEk(θ) p � i=1 ∇ijEk(θ)M (n) i (θ) R(n) i (θ) . (5.3) Assumption SA-5.2. For some positive constants M1, M2, M3, M4 we have sup i sup k sup θ ��∇iEk(θ) �� ≤ M1, sup i,j sup k sup θ ��∇ijEk(θ) �� ≤ M2, sup i,j,s sup k sup θ ��∇ijsEk(θ) �� ≤ M3, sup i,j,s,r sup k sup θ ��∇ijsrEk(θ) �� ≤ M4. Theorem SA-5.3 (Adam with ε inside: local error bound). Suppose Assumption SA-5.2 holds. Then for all n ∈ � 0, 1, . . . , ⌊T/h⌋ � ����������� ˜θj(tn+1) − ˜θj(tn) + h 1 1−βn+1 �n k=0 βn−k (1 − β) ∇jEk � ˜θ(tk) � � 1 1−ρn+1 �n k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(tk) ��2 + ε ����������� ≤ C4h3 for a positive constant C4 depending on β and ρ. We omit the proof since it is essentially the same argument as for Theorem SA-2.3. Theorem SA-5.4 (Adam with ε inside: global error bound). Suppose Assumption SA-5.2 holds for � θ(k)� k∈Z≥0 defined in Definition SA-5.1. Then there exist positive constants d10, d11, d12 such that for all n ∈ � 0, 1, . . . , ⌊T/h⌋ � ∥en∥ ≤ d10ed11nhh2 and ∥en+1 − en∥ ≤ d12ed11nhh3, 11where en := ˜θ(tn) − θ(n). The constants can be defined as d10 := C4, d11 :=  1 + M2√p √ε � M 2 1 ε + 1 � d10   √p, d12 := C4d11. 6 Technical bounding lemmas We will need the following lemmas to prove Theorem SA-2.3. Lemma SA-6.1. Suppose Assumption SA-2.2 holds. Then sup θ ���P (n) j (θ) ��� ≤ C5, (6.1) sup θ ��� ¯P (n) j (θ) ��� ≤ C6, (6.2) with constants C5, C6 defined as follows: C5 := pM 2 1 M2 R + ε · ρ 1 − ρ, C6 := pM 2 1 M2 R + ε . Proof of Lemma SA-6.1. The proof is done in the following simple steps. SA-6.2 Proof of (6.1). This bound is straightforward: sup θ ���P (n) j (θ) ��� = sup θ ������ n � k=0 ρn−k(1 − ρ)∇jEk (θ) p � i=1 ∇ijEk (θ) n−1 � l=k ∇iEl (θ) R(l) i (θ) + ε ������ ≤ pM 2 1 M2 R + ε (1 − ρ) n � k=0 ρn−k(n − k) ≤ pM 2 1 M2 R + ε (1 − ρ) ∞ � k=0 ρkk = C5. SA-6.3 Proof of (6.2). This bound is straightforward: sup θ ��� ¯P (n) j (θ) ��� = sup θ ������ n � k=0 ρn−k(1 − ρ)∇jEk (θ) p � i=1 ∇ijEk (θ) ∇iEn (θ) R(n) i (θ) + ε ������ ≤ pM 2 1 M2 R + ε (1 − ρ) n � k=0 ρn−k ≤ pM 2 1 M2 R + ε = C6. This concludes the proof of Lemma SA-6.1. Lemma SA-6.4. Suppose Assumption SA-2.2 holds. Then the first derivative of t �→ ˜θj(t) is uniformly over j and t ∈ [0, T] bounded in absolute value by some positive constant, say D1. Proof. This follows immediately from h ≤ T, (6.1), (6.2) and the definition of ˜θ(t) given in (2.2). Lemma SA-6.5. Suppose Assumption SA-2.2 holds. Then sup t∈[0,T ] sup j ����� � ∇jEn � ˜θ(t) ��·����� ≤ C7, (6.3) 12sup n,k sup t∈[tn,tn+1] ������� p � i=1 ∇ijEk � ˜θ(t) �   ˙˜θi(t) + ∇iEn � ˜θ(t) � R(n) i � ˜θ(t) � + ε   ������� ≤ C8h, (6.4) sup k≤n sup t∈[0,T ] ������� p � i=1 ∇ijEk � ˜θ(t) � n−1 � l=k ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε ������� ≤ (n − k)C9, (6.5) ����� � P (n) j � ˜θ(t) ��·����� ≤ C10 + C14, (6.6) ���� � ¯P (n) j (˜θ(t)) �·���� ≤ C15, (6.7) ��������    p � i=1 ∇ijEk � ˜θ(t) � ∇iEn � ˜θ(t) � R(n) i � ˜θ(t) � + ε    ·�������� ≤ C13, (6.8) �����������       ∇jEn � ˜θ(t) � � 2P (n) j � ˜θ(t) � + ¯P (n) j � ˜θ(t) �� 2 � R(n) j � ˜θ(t) � + ε �2 R(n) j � ˜θ(t) �       ·����������� ≤ C17, (6.9) ���������     �p i=1 ∇ijEn � ˜θ(t) � ∇iEn(˜θ(t)) R(n) i (˜θ(t))+ε 2 � R(n) j (˜θ(t)) + ε �     ·��������� ≤ C18, (6.10) with constants C7, C8, C9, C10, C11, C12, C13, C14, C15, C16, C17, C18 defined as follows: C7 := pM2D1, C8 := pM2 �M1 (2C5 + C6) 2(R + ε)2R + pM1M2 2(R + ε)2 � , C9 := pM1M2 R + ε , C10 := D1p2 M1M 2 2 R + ε · ρ 1 − ρ, C11 := D1pM1M2 R , C12 := D1p2 M1M3 R + ε , C13 := C12 + pM2 �D1pM2 R + ε + M1 (R + ε)2 C11 � = D1p2 R + ε � M1M3 + M 2 2 + M 2 1 M 2 2 (R + ε)R � , C14 := M1C13 ρ 1 − ρ, C15 := D1p2M1M 2 2 R + ε + D1p2M 2 1 M3 R + ε + D1p2M1M 2 2 R + ε + pM 2 1 M2C11 (R + ε)2 , C16 := 2C11 R(R + ε)3 + C11 (R + ε)4 , C17 := D1pM2 · (2C5 + C6) 2 (R + ε)2 R + M1 � 2 (C10 + C14) + C15 � 2 (R + ε)2 R + M1 (2C5 + C6) C16 2 , 13C18 := 1 2(R + ε) � p2D1M1M3 R + ε + p2D1M 2 2 R + ε + pM1M2C11 (R + ε)2 � + 1 2 · pM1M2 R + ε · C11 (R + ε)2 . Proof of Lemma SA-6.5. We divide this argument in several steps. SA-6.6 Proof of (6.3). This bound is straightforward: ����� � ∇jEn � ˜θ(t) ��·����� = ������ p � i=1 ∇ijEn � ˜θ(t) � ˙˜θi(t) ������ ≤ C7. SA-6.7 Proof of (6.4). By (2.2) we have for t = t− n+1 ������� ˙˜θj(t) + ∇jEn � ˜θ(t) � R(n) j � ˜θ(t) � + ε ������� ≤ h �M1 (2C5 + C6) 2(R + ε)2R + pM1M2 2(R + ε)2 � , giving (6.4) immediately. SA-6.8 Proof of (6.5). This bound follows from the assumptions immediately. SA-6.9 Proof of (6.6). We will prove this by bounding the two terms in the expression d dtP (n) j � ˜θ(t) � = n � k=0 ρn−k(1 − ρ) p � u=1 ∇juEk � ˜θ(t) � ˙˜θu(t) p � i=1 ∇ijEk � ˜θ(t) � n−1 � l=k ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε + n � k=0 ρn−k(1 − ρ)∇jEk � ˜θ(t) � p � i=1 d dt      ∇ijEk � ˜θ(t) � n−1 � l=k ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε      . (6.11) It is easily shown that the first term in (6.11) is bounded in absolute value by C10: ������� n � k=0 ρn−k(1 − ρ) p � u=1 ∇juEk � ˜θ(t) � ˙˜θu(t) p � i=1 ∇ijEk � ˜θ(t) � n−1 � l=k ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε ������� ≤ D1p2 M1M 2 2 R + ε (1 − ρ) n � k=0 ρkk ≤ D1p2 M1M 2 2 R + ε (1 − ρ) ∞ � k=0 ρkk = C10. For the proof of (6.6), it is left to show that the second term in (6.11) is bounded in absolute value by C14. To bound �p i=1 d dt � ∇ijEk � ˜θ(t) � �n−1 l=k ∇iEl(˜θ(t)) R(l) i (˜θ(t))+ε � , we can use �������� p � i=1 d dt      ∇ijEk � ˜θ(t) � n−1 � l=k ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε      �������� 14≤ ������� p � i=1 d dt � ∇ijEk � ˜θ(t) �� n−1 � l=k ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε ������� + �������� p � i=1 ∇ijEk � ˜θ(t) � n−1 � l=k d dt      ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε      �������� By the Cauchy-Schwarz inequality applied twice, ������� p � i=1 d dt � ∇ijEk � ˜θ(t) �� n−1 � l=k ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε ������� ≤ � � � � p � i=1 p � s=1 � ∇ijsEk � ˜θ(t) ��2 � � � � p � u=1 ˙˜θu(t)2 � � � � � � p � i=1 ������� n−1 � l=k ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε ������� 2 ≤ M3p · D1 √p · � � � � � � p � i=1 ������� n−1 � l=k ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε ������� 2 ≤ (n − k)C12. Next, for any n and j ���� d dtR(n) j � ˜θ(t) ����� = 1 R(n) j � ˜θ(t) � ������ n � k=0 ρn−k(1 − ρ)∇jEk � ˜θ(t) � p � i=1 ∇ijEk � ˜θ(t) � ˙˜θi(t) ������ ≤ 1 R(n) j � ˜θ(t) �D1pM1M2 n � k=0 ρn−k(1 − ρ) ≤ C11. (6.12) This gives �������� d dt      ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε      �������� ≤ ���� �p s=1 ∇isEl � ˜θ(t) � ˙˜θs(t) ���� R(l) i � ˜θ(t) � + ε + ����∇iEl � ˜θ(t) ����� · ���� d dtR(l) i � ˜θ(t) ����� � R(l) i � ˜θ(t) � + ε �2 ≤ D1pM2 R + ε + M1 (R + ε)2 C11. We have obtained �������� p � i=1 d dt      ∇ijEk � ˜θ(t) � n−1 � l=k ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε      �������� ≤ (n − k)C13. (6.13) This gives a bound on the second term in (6.11): �������� n � k=0 ρn−k(1 − ρ)∇jEk � ˜θ(t) � p � i=1 d dt      ∇ijEk � ˜θ(t) � n−1 � l=k ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε      �������� ≤ M1 n � k=0 ρn−k(1 − ρ)(n − k)C13 ≤ C14, concluding the proof of (6.6). 15SA-6.10 Proof of (6.7). We will prove this by bounding the four terms in the expression d dt      n � k=0 ρn−k(1 − ρ)∇jEk � ˜θ(t) � p � i=1 ∇ijEk � ˜θ(t) � ∇iEn � ˜θ(t) � R(n) i � ˜θ(t) � + ε      = Term1 + Term2 + Term3 + Term4, where Term1 := n � k=0 ρn−k(1 − ρ) d dt � ∇jEk � ˜θ(t) �� p � i=1 ∇ijEk � ˜θ(t) � ∇iEn � ˜θ(t) � R(n) i � ˜θ(t) � + ε , Term2 := n � k=0 ρn−k(1 − ρ)∇jEk � ˜θ(t) � p � i=1 d dt � ∇ijEk � ˜θ(t) �� ∇iEn � ˜θ(t) � R(n) i � ˜θ(t) � + ε , Term3 := n � k=0 ρn−k(1 − ρ)∇jEk � ˜θ(t) � p � i=1 ∇ijEk � ˜θ(t) � d dt � ∇iEn � ˜θ(t) �� R(n) i � ˜θ(t) � + ε , Term4 := − n � k=0 ρn−k(1 − ρ)∇jEk � ˜θ(t) � p � i=1 ∇ijEk � ˜θ(t) � ∇iEn � ˜θ(t) � d dtR(n) i � ˜θ(t) � � R(n) i � ˜θ(t) � + ε �2 . To bound Term1, use ����� d dt � ∇jEk � ˜θ(t) ������� ≤ D1pM2, giving |Term1| ≤ D1p2M1M 2 2 R + ε n � k=0 ρn−k(1 − ρ) ≤ D1p2M1M 2 2 R + ε . To bound Term2, use ����� d dt � ∇ijEk � ˜θ(t) ������� ≤ D1pM3, giving |Term2| ≤ D1p2M 2 1 M3 R + ε n � k=0 ρn−k(1 − ρ) ≤ D1p2M 2 1 M3 R + ε . To bound Term3, use ����� d dt � ∇iEn � ˜θ(t) ������� ≤ D1pM2, giving |Term3| ≤ D1p2M1M 2 2 R + ε n � k=0 ρn−k(1 − ρ) ≤ D1p2M1M 2 2 R + ε . To bound Term4, use (6.12), giving |Term4| ≤ pM 2 1 M2C11 (R + ε)2 n � k=0 ρn−k(1 − ρ) ≤ pM 2 1 M2C11 (R + ε)2 . SA-6.11 Proof of (6.8). This is proven in (6.13). 16SA-6.12 Proof of (6.9). (6.12) gives �������� d dt      1 R(n) j � ˜θ(t) �      �������� = ���� d dtR(n) j � ˜θ(t) ����� R(n) j � ˜θ(t) �2 ≤ C11 R2 , (6.14) �������� d dt      1 R(n) j � ˜θ(t) � + ε      �������� = ���� d dtR(n) j � ˜θ(t) ����� � R(n) j � ˜θ(t) � + ε �2 ≤ C11 (R + ε)2 , (6.15) ����������� d dt            1 � R(n) j � ˜θ(t) � + ε �2            ����������� = 2 ���� d dtR(n) j � ˜θ(t) ����� � R(n) j � ˜θ(t) � + ε �3 ≤ 2C11 (R + ε)3 . (6.16) Combining two bounds above, we have ������ d dt �� R(n) j � ˜θ(t) � + ε �−2 R(n) j (˜θ(t))−1 ������� ≤ ������ d dt �� R(n) j � ˜θ(t) � + ε �−2������� R(n) j (˜θ(t)) + ���� d dt � R(n) j (˜θ(t))−1����� � R(n) j � ˜θ(t) � + ε �2 ≤ C16. We are ready to bound �����������       ∇jEn � ˜θ(t) � � 2P (n) j � ˜θ(t) � + ¯P (n) j � ˜θ(t) �� 2 � R(n) j � ˜θ(t) � + ε �2 R(n) j � ˜θ(t) �       ·����������� ≤ ���������� � ∇jEn � ˜θ(t) ��· � 2P (n) j � ˜θ(t) � + ¯P (n) j � ˜θ(t) �� 2 � R(n) j � ˜θ(t) � + ε �2 R(n) j � ˜θ(t) � ���������� + + ���������� ∇jEn � ˜θ(t) � � 2P (n) j � ˜θ(t) � + ¯P (n) j � ˜θ(t) ��· 2 � R(n) j � ˜θ(t) � + ε �2 R(n) j � ˜θ(t) � ���������� + ��������� ∇jEn � ˜θ(t) � � 2P (n) j � ˜θ(t) � + ¯P (n) j � ˜θ(t) �� 2 × �� R(n) j � ˜θ(t) � + ε �−2 R(n) j (˜θ(t))−1 �·������ ≤ C17. 17SA-6.13 Proof of (6.10). Since ������� p � i=1 ∇ijEn � ˜θ(t) � ∇iEn � ˜θ(t) � R(n) i � ˜θ(t) � + ε ������� ≤ pM1M2 R + ε and, as we have already seen in the argument for (6.7), ��������    p � i=1 ∇ijEn � ˜θ(t) � ∇iEn � ˜θ(t) � R(n) i � ˜θ(t) � + ε    ·�������� ≤ p2D1M1M3 R + ε + p2D1M 2 2 R + ε + pM1M2C11 (R + ε)2 , we are ready to bound ���������     �p i=1 ∇ijEn � ˜θ(t) � ∇iEn(˜θ(t)) R(n) i (˜θ(t))+ε 2 � R(n) j (˜θ(t)) + ε �     ·��������� ≤ C18. The proof of Lemma SA-6.5 is concluded. Lemma SA-6.14. Suppose Assumption SA-2.2 holds. Then the second derivative of t �→ ˜θj(t) is uniformly over j and t ∈ [0, T] bounded in absolute value by some positive constant, say D2. Proof. This follows from the definition of ˜θ(t) given in (2.2), h ≤ T and that the first derivatives of all three terms in (2.2) are bounded by Lemma SA-6.5. Lemma SA-6.15. Suppose Assumption SA-2.2 holds. Then ����� � ∇jEn � ˜θ(t) ��··����� ≤ C19, (6.17) ����� � R(n) j � ˜θ(t) ��··����� ≤ C20, (6.18) ������ �� R(n) j � ˜θ(t) � + ε �−2�··������ ≤ C21, (6.19) ����� � R(n) j � ˜θ(t) �−1�··����� ≤ C22, (6.20) ������ �� R(n) j � ˜θ(t) � + ε �−2 R(n) j � ˜θ(t) �−1 �··������ ≤ C23, (6.21) ��������    p � i=1 ∇ijEk � ˜θ(t) � n−1 � l=k ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε    ··�������� ≤ (n − k)C24, (6.22) with constants C19, C20, C21, C22, C23, C24 defined as follows: C19 := p2M3D2 1 + pM2D2, C20 := C11 R2 pM1M2D1 + 1 Rp2M 2 2 D2 1 + 1 Rp2M1M3D2 1 + 1 RpM1M2D2, C21 := 6C2 11 (R + ε)4 + 2C20 (R + ε)3 , C22 := 2C2 11 R3 + C20 R2 , 18C23 := C21 R + 4C2 11 R2(R + ε)3 + C22 (R + ε)2 , C24 := p  2C11 � D1M 2 2 p + D1M1M3p � (R + ε)2 + M1M2 � 2C2 11 (R + ε)3 + C20 (R + ε)2 � + 2D2 1M2M3p2 + M2 � D2 1M3p2 + D2M2p � + M1 � D2 1M4p2 + D2M3p � R + ε � . Proof of Lemma SA-6.15. We divide this argument in several steps. SA-6.16 Proof of (6.17). This bound is straightforward: ����� � ∇jEn � ˜θ(t) ��··����� = ������ p � i=1 p � s=1 ∇ijsEn � ˜θ(t) � ˙˜θs(t) ˙˜θi(t) + p � i=1 ∇ijEn � ˜θ(t) � ¨˜θt(t) ������ ≤ C19. SA-6.17 Proof of (6.18). Note that � R(n) j � ˜θ(t) ��·· = � R(n) j � ˜θ(t) �−1�· n � k=0 ρn−k(1 − ρ)∇jEk � ˜θ(t) � p � i=1 ∇ijEk � ˜θ(t) � ˙˜θi(t) + R(n) j � ˜θ(t) �−1 n � k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(t) ��· p � i=1 ∇ijEk � ˜θ(t) � ˙˜θi(t) + R(n) j � ˜θ(t) �−1 n � k=0 ρn−k(1 − ρ)∇jEk � ˜θ(t) � p � i=1 � ∇ijEk � ˜θ(t) ��· ˙˜θi(t) + R(n) j � ˜θ(t) �−1 n � k=0 ρn−k(1 − ρ)∇jEk � ˜θ(t) � p � i=1 ∇ijEk � ˜θ(t) � ¨˜θi(t), giving by (6.14) ����� � R(n) j � ˜θ(t) ��··����� ≤ C11 R2 pM1M2D1 n � k=0 ρn−k(1 − ρ) + 1 Rp2M 2 2 D2 1 n � k=0 ρn−k(1 − ρ) + 1 Rp2M1M3D2 1 n � k=0 ρn−k(1 − ρ) + 1 RpM1M2D2 � k=0 ρn−k(1 − ρ) ≤ C20. SA-6.18 Proof of (6.19). Note that �� R(n) j � ˜θ(t) � + ε �−2�·· = 6 �� R(n) j � ˜θ(t) ��·�2 � R(n) j � ˜θ(t) � + ε �4 − 2 � R(n) j � ˜θ(t) ��·· � R(n) j � ˜θ(t) � + ε �3 , giving by (6.12) and (6.18) ������ �� R(n) j � ˜θ(t) � + ε �−2�··������ ≤ C21. SA-6.19 Proof of (6.20). The bound follows from (6.12), (6.18) and � R(n) j � ˜θ(t) �−1�·· = 2 �� R(n) j � ˜θ(t) ��·�2 R(n) j � ˜θ(t) �3 − � R(n) j � ˜θ(t) ��·· R(n) j � ˜θ(t) �2 . 19SA-6.20 Proof of (6.21). Putting a := � R(n) j � ˜θ(t) � + ε �−2 , b := R(n) j � ˜θ(t) �−1 , use |a| ≤ 1 (R + ε)2 , |b| ≤ 1 R, |˙a| ≤ 2C11 (R + ε)3 , ���˙b ��� ≤ C11 R2 , |¨a| ≤ C21, ���¨b ��� ≤ C22, and (ab)·· = ¨ab + 2˙a˙b + a¨b. SA-6.21 Proof of (6.22). Putting a := ∇ijEk � ˜θ(t) � , b := ∇iEl � ˜θ(t) � , c := � R(l) i � ˜θ(t) � + ε �−1 , we have |a| ≤ M2, |˙a| ≤ pM3D1, |¨a| ≤ p2M4D2 1 + pM3D2, |b| ≤ M1, ���˙b ��� ≤ pM2D1, ���¨b ��� ≤ p2M3D2 1 + pM2D2, |c| ≤ 1 R + ε, |˙c| ≤ C11 (R + ε)2 , |¨c| ≤ 2C2 11 (R + ε)3 + C20 (R + ε)2 . (6.22) follows. The proof of Lemma SA-6.15 is concluded. Lemma SA-6.22. Suppose Assumption SA-2.2 holds. Then the third derivative of t �→ ˜θj(t) is uniformly over j and t ∈ [0, T] bounded in absolute value by some positive constant, say D3. Proof. By (6.5), (6.13) and (6.22) ������� p � i=1 ∇ijEk � ˜θ(t) � n−1 � l=k ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε ������� ≤ (n − k)C9, ��������    p � i=1 ∇ijEk � ˜θ(t) � n−1 � l=k ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε    ·�������� ≤ (n − k)C13, ��������    p � i=1 ∇ijEk � ˜θ(t) � n−1 � l=k ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε    ··�������� ≤ (n − k)C24. From the definition of t �→ P (n) j � ˜θ(t) � , it means that its derivatives up to order two are bounded. Similarly, the same is true for t �→ ¯P (n) j � ˜θ(t) � . It follows from (6.19) and its proof that the derivatives up to order two of t �→ � R(n) j � ˜θ(t) � + ε �−2 R(n) j � ˜θ(t) �−1 20are also bounded. These considerations give the boundedness of the second derivative of the term t �→ ∇jEn � ˜θ(t) � � 2P (n) j � ˜θ(t) � + ¯P (n) j � ˜θ(t) �� 2 � R(n) j � ˜θ(t) � + ε �2 R(n) j � ˜θ(t) � in (2.2). The boundedness of the second derivatives of the other two terms is shown analogously. By (2.2) and since h ≤ T, this means sup j sup t∈[0,T ] ��� ... ˜θ j(t) ��� ≤ D3 for some positive constant D3. 7 Proof of Theorem SA-2.3 Lemma SA-7.1. Suppose Assumption SA-2.2 holds. Then for all n ∈ � 0, 1, . . . , ⌊T/h⌋ � , k ∈ {0, 1, . . . , n − 1} we have ����∇jEk � ˜θ(tk) � − ∇jEk � ˜θ(tn) ����� ≤ C7(n − k)h (7.1) Proof. (7.1) follows from the mean value theorem applied n − k times. Lemma SA-7.2. In the setting of Lemma SA-7.1, for any l ∈ {k, k + 1, . . . , n − 1} we have ������� ∇jEk � ˜θ(tl) � − ∇jEk � ˜θ(tl+1) � − h p � i=1 ∇ijEk � ˜θ(tn) � ∇iEl � ˜θ(tn) � R(l) i � ˜θ(tn) � + ε ������� ≤ � C19/2 + C8 + (n − l − 1)C13 � h2. Proof. By the Taylor expansion of t �→ ∇jEk � ˜θ(t) � on the segment [tl, tl+1] at tl+1 on the left ������ ∇jEk � ˜θ(tl) � − ∇jEk � ˜θ(tl+1) � + h p � i=1 ∇ijEk � ˜θ(tl+1) � ˙˜θi � t− l+1 � ������ ≤ C19 2 h2. Combining this with (6.4) gives ������� ∇jEk � ˜θ(tl) � − ∇jEk � ˜θ(tl+1) � − h p � i=1 ∇ijEk � ˜θ(tl+1) � ∇iEl � ˜θ(tl+1) � R(l) i � ˜θ(tl+1) � + ε ������� ≤ � C19/2 + C8 � h2. (7.2) Now applying the mean-value theorem n − l − 1 times, we have ������� p � i=1 ∇ijEk � ˜θ(tl+1) � ∇iEl � ˜θ(tl+1) � R(l) i � ˜θ(tl+1) � + ε − p � i=1 ∇ijEk � ˜θ(tl+2) � ∇iEl � ˜θ(tl+2) � R(l) i � ˜θ(tl+2) � + ε ������� ≤ C13h, · · · ������� p � i=1 ∇ijEl � ˜θ(tn−1) � ∇iEk � ˜θ(tn−1) � R(l) i � ˜θ(tn−1) � + ε − p � i=1 ∇ijEk � ˜θ(tn) � ∇iEl � ˜θ(tn) � R(l) i � ˜θ(tn) � + ε ������� ≤ C13h, 21and in particular ������� p � i=1 ∇ijEk � ˜θ(tl+1) � ∇iEl � ˜θ(tl+1) � R(l) i � ˜θ(tl+1) � + ε − p � i=1 ∇ijEk � ˜θ(tn) � ∇iEl � ˜θ(tn) � R(l) i � ˜θ(tn) � + ε ������� ≤ (n − l − 1)C13h. Combining this with (7.2), we conclude the proof of Lemma SA-7.2. Lemma SA-7.3. In the setting of Lemma SA-7.1, ������� ∇jEk � ˜θ(tk) � − ∇jEk � ˜θ(tn) � − h p � i=1 ∇ijEk � ˜θ(tn) � n−1 � l=k ∇iEl � ˜θ(tn) � R(l) i � ˜θ(tn) � + ε ������� ≤ � (n − k)(C19/2 + C8) + (n − k)(n − k − 1) 2 C13 � h2. Proof. Fix n ∈ Z≥0. Note that ������� ∇jEk � ˜θ(tk) � − ∇jEk � ˜θ(tn) � − h p � i=1 ∇ijEk � ˜θ(tn) � n−1 � l=k ∇iEl � ˜θ(tn) � R(l) i � ˜θ(tn) � + ε ������� = �������� n−1 � l=k      ∇jEk � ˜θ(tl) � − ∇jEk � ˜θ(tl+1) � − h p � i=1 ∇ijEk � ˜θ(tn) � ∇iEl � ˜θ(tn) � R(l) i � ˜θ(tn) � + ε      �������� ≤ n−1 � l=k ������� ∇jEk � ˜θ(tl) � − ∇jEk � ˜θ(tl+1) � − h p � i=1 ∇ijEk � ˜θ(tn) � ∇iEl � ˜θ(tn) � R(l) i � ˜θ(tn) � + ε ������� (a) ≤ n−1 � l=k � C19/2 + C8 + (n − l − 1)C13 � h2 = � (n − k)(C19/2 + C8) + (n − k)(n − k − 1) 2 C13 � h2, where (a) is by Lemma SA-7.2. Lemma SA-7.4. Suppose Assumption SA-2.2 holds. Then for all n ∈ � 0, 1, . . . , ⌊T/h⌋ � ������ n � k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(tk) ��2 − R(n) j � ˜θ(tn) �2 ������ ≤ C25h (7.3) and ������ n � k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(tk) ��2 − R(n) j � ˜θ(tn) �2 − 2hP (n) j � ˜θ(tn) � ������ ≤ C26h2 (7.4) with C25 and C26 defined as follows: C25(ρ) := 2M1C7 ρ 1 − ρ, C26(ρ) := M1 |C19 + 2C8 − C13| ρ 1 − ρ + � M1C13 + |C19 + 2C8 − C13| C9 + (C19 + 2C8 − C13)2 4 � ρ(1 + ρ) (1 − ρ)2 + � C13C9 + C13 2 |C19 + 2C8 − C13| � ρ � 1 + 4ρ + ρ2� (1 − ρ)3 + C2 13 4 · ρ � 1 + 11ρ + 11ρ2 + ρ3� (1 − ρ)4 . 22Proof. Note that ����� � ∇jEk � ˜θ(tk) ��2 − � ∇jEk � ˜θ(tn) ��2����� ≤ ����∇jEk � ˜θ(tk) � − ∇jEk � ˜θ(tn) ����� · ����∇jEk � ˜θ(tk) � + ∇jEk � ˜θ(tn) ����� (a) ≤ C7(n − k)h · 2M1, where (a) is by (7.1). Using the triangle inequality, we can conclude ������ n � k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(tk) ��2 − R(n) j � ˜θ(tn) �2 ������ ≤ 2M1C7h(1 − ρ) n � k=0 (n − k)ρn−k = 2M1C7h(1 − ρ) n � k=0 kρk = 2M1C7 ρ 1 − ρh. (7.3) is proven. We continue by showing ����� � ∇jEk � ˜θ(tk) ��2 − � ∇jEk � ˜θ(tn) ��2 −2∇jEk � ˜θ(tn) � h p � i=1 ∇ijEk � ˜θ(tn) � n−1 � l=k ∇iEl � ˜θ(tn) � R(l) i � ˜θ(tn) � + ε ������� ≤ 2M1 � (n − k)(C19/2 + C8) + (n − k)(n − k − 1) 2 C13 � h2 + 2(n − k)C9 � (n − k)(C19/2 + C8) + (n − k)(n − k − 1) 2 C13 � h3 + � (n − k)(C19/2 + C8) + (n − k)(n − k − 1) 2 C13 �2 h4. (7.5) To prove this, use ���a2 − b2 − 2bKh ��� ≤ 2|b| · |a − b − Kh| + 2|K| · h · |a − b − Kh| + (a − b − Kh)2 with a := ∇jEk � ˜θ(tk) � , b := ∇jEk � ˜θ(tn) � , K := p � i=1 ∇ijEk � ˜θ(tn) � n−1 � l=k ∇iEl � ˜θ(tn) � R(l) i � ˜θ(tn) � + ε , and bounding |a − b − Kh| (a) ≤ � (n − k)(C19/2 + C8) + (n − k)(n − k − 1) 2 C13 � h2, |b| ≤ M1, |K| ≤ (n − k)C9, where (a) is by Lemma SA-7.3. (7.5) is proven. We turn to the proof of (7.4). By (7.5) and the triangle inequality ������ n � k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(tk) ��2 − R(n) j � ˜θ(tn) �2 − 2hP (n) j � ˜θ(tn) � ������ 23≤ (1 − ρ) n � k=0 ρn−k � Poly1(n − k)h2 + Poly2(n − k)h3 + Poly3(n − k)h4� = (1 − ρ) n � k=0 ρk � Poly1(k)h2 + Poly2(k)h3 + Poly3(k)h4� , where Poly1(k) := 2M1 � k(C19/2 + C8) + k(k − 1) 2 C13 � = M1C13k2 + M1(C19 + 2C8 − C13)k, Poly2(k) := 2kC9 � k(C19/2 + C8) + k(k − 1) 2 C13 � = C13C9k3 + (C19 + 2C8 − C13) C9k2, Poly3(k) := � k(C19/2 + C8) + k(k − 1) 2 C13 �2 = C2 13 4 k4 + C13 2 (C19 + 2C8 − C13) k3 + 1 4 (C19 + 2C8 − C13)2 k2. It is left to combine this with n � k=0 kρk ≤ ∞ � k=0 kρk = ρ (1 − ρ)2 , n � k=0 k2ρk ≤ ∞ � k=0 k2ρk = ρ(1 + ρ) (1 − ρ)3 , n � k=0 k3ρk ≤ ∞ � k=0 k3ρk = ρ � 1 + 4ρ + ρ2� (1 − ρ)4 , n � k=0 k4ρk ≤ ∞ � k=0 k4ρk = ρ � 1 + 11ρ + 11ρ2 + ρ3� (1 − ρ)5 . This gives ������ n � k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(tk) ��2 − R(n) j � ˜θ(tn) �2 − 2hP (n) j � ˜θ(tn) � ������ ≤ � M1C13 ρ(1 + ρ) (1 − ρ)2 + M1 |C19 + 2C8 − C13| ρ 1 − ρ � h2 + � C13C9 ρ � 1 + 4ρ + ρ2� (1 − ρ)3 + |C19 + 2C8 − C13| C9 ρ(1 + ρ) (1 − ρ)2 � h3 + � C2 13 4 · ρ � 1 + 11ρ + 11ρ2 + ρ3� (1 − ρ)4 + C13 2 |C19 + 2C8 − C13| ρ � 1 + 4ρ + ρ2� (1 − ρ)3 + 1 4 (C19 + 2C8 − C13)2 ρ(1 + ρ) (1 − ρ)2 � h4 (a) ≤ � M1 |C19 + 2C8 − C13| ρ 1 − ρ + � M1C13 + |C19 + 2C8 − C13| C9 + (C19 + 2C8 − C13)2 4 � ρ(1 + ρ) (1 − ρ)2 + � C13C9 + C13 2 |C19 + 2C8 − C13| � ρ � 1 + 4ρ + ρ2� (1 − ρ)3 +C2 13 4 · ρ � 1 + 11ρ + 11ρ2 + ρ3� (1 − ρ)4 � h2, where in (a) we used that h < 1. (7.4) is proven. 24Lemma SA-7.5. Suppose Assumption SA-2.2 holds. Then ��������    � � � � n � k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(tk) ��2 + ε    −1 − � R(n) j � ˜θ(tn) � + ε �−1 +h P (n) j � ˜θ(tn) � � R(n) j � ˜θ(tn) � + ε �2 R(n) j � ˜θ(tn) � ���������� ≤ C25(ρ)2 + R2C26(ρ) 2R3(R + ε)2 h2. Proof. Note that if a ≥ R2, b ≥ R2, we have �������� 1 √a + ε − 1 √ b + ε + a − b 2 �√ b + ε �2 √ b �������� = (a − b)2 2 √ b �√ b + ε � �√ a + ε � �√ a + √ b � � 1 √ b + ε + 1 √ a + √ b � � �� � ≤2/R ≤ (a − b)2 2R3(R + ε)2 . By the triangle inequality, �������� 1 √a + ε − 1 √ b + ε + c 2 �√ b + ε �2 √ b �������� ≤ (a − b)2 2R3(R + ε)2 + |a − b − c| 2 �√ b + ε �2 √ b ≤ (a − b)2 2R3(R + ε)2 + |a − b − c| 2R (R + ε)2 Apply this with a := n � k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(tk) ��2 , b := R(n) j � ˜θ(tn) �2 , c := 2hP (n) j � ˜θ(tn) � and use bounds |a − b| ≤ 2M1C7 ρ 1 − ρh, |a − b − c| ≤ C26(ρ)h2 by Lemma SA-7.4. SA-7.6. We are finally ready to prove Theorem SA-2.3. Proof of Theorem SA-2.3. By (6.9) and (6.10), the first derivative of the function t �→       ∇jEn � ˜θ(t) � � 2P (n) j � ˜θ(t) � + ¯P (n) j � ˜θ(t) �� 2 � R(n) j � ˜θ(t) � + ε �2 R(n) j � ˜θ(t) � − �p i=1 ∇ijEn � ˜θ(t) � ∇iEn(˜θ(t)) R(n) i (˜θ(t))+ε 2 � R(n) j (˜θ(t)) + ε �       25is bounded in absolute value by a positive constant C27 = C17 + C18. By (2.2), this means ������� ¨˜θj(t) + d dt    ∇jEn � ˜θ(t) � R(n) j � ˜θ(t) � + ε    ������� ≤ C27h. Combining this with ������ ˜θj(tn+1) − ˜θj(tn) − ˙˜θj � t+ n � h − ¨˜θj � t+ n � 2 h2 ������ ≤ D3 6 by Taylor expansion, we get �������� ˜θj(tn+1) − ˜θj(tn) − ˙˜θj � t+ n � h + h2 2 · d dt    ∇jEn � ˜θ(t) � R(n) j � ˜θ(t) � + ε    ������� t=t+ n �������� ≤ �D3 6 + C27 2 � h3. (7.6) Using ������� ˙˜θj(tn) + ∇jEn � ˜θ(tn) � R(n) j � ˜θ(tn) � + ε ������� ≤ C28h with C28 defined as C28 := M1 (2C5 + C6) 2(R + ε)2R + pM1M2 2(R + ε)2 by (2.2), and calculating the derivative, it is easy to show �������� d dt    ∇jEn � ˜θ(t) � R(n) j � ˜θ(t) � + ε    ������� t=t+ n − FrDer �������� ≤ C29h (7.7) for a positive constant C29, where FrDer := FrDerNum � R(n) j � ˜θ(tn) � + ε �2 R(n) j � ˜θ(tn) � FrDerNum := ∇jEn � ˜θ(tn) � ¯P (n) j � ˜θ(tn) � − � R(n) j � ˜θ(tn) � + ε � R(n) j � ˜θ(tn) � p � i=1 ∇ijEn � ˜θ(tn) � ∇iEn � ˜θ(tn) � R(n) i � ˜θ(tn) � + ε , C29 := � pM2 R + ε + M 2 1 M2p (R + ε)2R � C28. From (7.6) and (7.7), by the triangle inequality ����� ˜θj(tn+1) − ˜θj(tn) − ˙˜θj � t+ n � h + h2 2 FrDer ����� ≤ �D3 6 + C27 + C29 2 � h3, which, using (2.2), is rewritten as ���������� ˜θj(tn+1) − ˜θj(tn) + h ∇jEn � ˜θ(tn) � R(n) j � ˜θ(tn) � + ε − h2 ∇jEn � ˜θ(tn) � P (n) j � ˜θ(tn) � � R(n) j � ˜θ(tn) � + ε �2 R(n) j � ˜θ(tn) � ���������� 26≤ �D3 6 + C27 + C29 2 � h3. It is left to combine this with Lemma SA-7.5, giving the assertion of the theorem with C1 = D3 6 + C27 + C29 2 + M1 C2 25 + R2C26 2R3(R + ε)2 . 8 Numerical experiments SA-8.1 Models. We use small modifications of default Keras Resnet-50 and Resnet-101 architectures1 for training on CIFAR-10 and CIFAR-100 (since image sizes are not the same as Imagenet), after verifying their correctness. The first convolution layer conv1 has 3 × 3 kernel, stride 1 and “same” padding. Then comes batch normalization, and relu. Max pooling is removed, and otherwise conv2_x to conv5_x are as described in [2], see Table 1 there (downsampling is performed by the first convolution of each bottleneck block, same as in this original paper, not the middle one as in version 1.52; all convolution layers have learned biases). After conv5 there is global average pooling, 10 or 100-way fully connected layer (for CIFAR-10 and CIFAR-100 respectively), and softmax. SA-8.2 Data augmentation. We subtract the per-pixel mean and divide by standard deviation, and we use the data augmentation scheme from [3], following [2], section 4.2. We take inspiration and some code snippets from [4] (though we do not use their models). During each pass over the training dataset, each 32 × 32 initial image is padded evenly with zeros so that it becomes 36 × 36, then random crop is applied so that the picture becomes 32 × 32 again, and finally random (probability 0.5) horizontal (left to right) flip is used. References [1] Avrajit Ghosh, He Lyu, Xitong Zhang, and Rongrong Wang. “Implicit regularization in Heavy-ball momentum accelerated stochastic gradient descent”. In: The Eleventh International Conference on Learning Representations. 2023. url: https://openreview.net/forum?id=ZzdBhtEH9yB. [2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. “Deep residual learning for image recognition”. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2016, pp. 770–778. [3] Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu. “Deeply-supervised nets”. In: Artificial intelligence and statistics. Pmlr. 2015, pp. 562–570. [4] Chia-Hung Yuan. Training CIFAR-10 with TensorFlow2(TF2). https : / / github . com / lionelmessi6410/tensorflow2-cifar. 2021. 1https://github.com/keras-team/keras/blob/v2.13.1/keras/applications/resnet.py 2https://catalog.ngc.nvidia.com/orgs/nvidia/resources/resnet_50_v1_5_for_pytorch 27
Information Theoretically Optimal Sample Complexity of Learning Dynamical Directed Acyclic Graphs Mishfad Shaikh Veedu∗, Deepjyoti Deka †, and Murti V. Salapaka∗ Abstract In this article, the optimal sample complexity of learning the underlying interac- tion/dependencies of a Linear Dynamical System (LDS) over a Directed Acyclic Graph (DAG) is studied. The sample complexity of learning a DAG’s structure is well-studied for static systems, where the samples of nodal states are independent and identically distributed (i.i.d.). However, such a study is less explored for DAGs with dynamical systems, where the nodal states are temporally correlated. We call such a DAG underly- ing an LDS as dynamical DAG (DDAG). In particular, we consider a DDAG where the nodal dynamics are driven by unobserved exogenous noise sources that are wide-sense stationary (WSS) in time but are mutually uncorrelated, and have the same power spec- tral density (PSD). Inspired by the static settings, a metric and an algorithm based on the PSD matrix of the observed time series are proposed to reconstruct the DDAG. The equal noise PSD assumption can be relaxed such that identifiability conditions for DDAG reconstruction are not violated. For the LDS with WSS (sub) Gaussian exogenous noise sources, it is shown that the optimal sample complexity (or length of state trajectory) needed to learn the DDAG is n = Θ(q log(p/q)), where p is the number of nodes and q is the maximum number of parents per node. To prove the sample complexity upper bound, a concentration bound for the PSD estimation is derived, under two different sampling strategies. A matching min-max lower bound using generalized Fano’s inequality also is provided, thus showing the order optimality of the proposed algorithm. 1 Introduction Learning the interdependency structure in a network of agents, from passive time series observations, is a salient problem with applications in neuroscience Bower and Beeman (2012), finance Kim et al. (2011), meteorology Ghil et al. (2002), etc. Reconstructing the exact structure with the dependency/causation directions has a wide range of applications. For example, the identification of causation structure among the shares helps in obtaining robust portfolio management in the stock market Kim et al. (2011). Similarly, causal graphs are useful in understanding dynamics and identifying contributing factors of a public epidemic emergency situation Yang et al. (2020). ∗Department of Electrical Engineering, University of Minnesota Twin Cities †Theoretical Division T-5, Los Alamos National Laboratory. The authors acknowledge support from the Information Science and Technology Institute (ISTI) at Los Alamos National Laboratory. The research work conducted at Los Alamos National Laboratory is done under the auspices of the National Nuclear Security Administration of the U.S. Department of Energy under Contract No. 89233218CNA000001. The first and the third authors acknowledge the support of NSF through Award Number 2030096 and ARPA-E through the Award number DE-AR0001016. 1 arXiv:2308.16859v1 [stat.ML] 31 Aug 2023The structure of directed interactions in a network of agents is conveniently represented using directed graphs, with the agents as nodes and the directed interactions as directed edges. If the underlying graph doesn’t have cycles, it is called a Directed Acyclic Graph (DAG). In general, it is not possible to reconstruct the exact structure with the direction of different edges. Instead, in many networks it is possible to retrieve only the Markov equivalence graphs, the set of graphs satisfying the same conditional dependence property, from data without any intervention Ghoshal and Honorio (2018). In applications such as finance Kim et al. (2011), climate science Ghil et al. (2002) etc., the agent states, instead of being temporally independent, can evolve over time due to past directed interactions. Such temporal evolution can be represented by a linear dynamical system (LDS). In LDS, the interaction between agent states is captured by a linear time-invariant function. In this paper, we study the identifiability and present the first sample complexity results for learning a DAG of LDS, which we term as Dynamical DAG or DDAG. This is distinguished from static DAG, where the agent states are temporally independent and the DAG does not correspond to temporal dynamics. 1.1 Related Work Static DAG Learning: The problem of obtaining an upper bound on the sample complexity of learning static DAGs goes back twenty-five years Friedman and Yakhini (1996), Zuk et al. (2006). However, tight characterization of optimal rates for DAG learning is a harder problem compared to undirected networks Gao et al. (2022), primarily due to the order identification step. Identifiability conditions for learning static DAGs with linear interactions and excited by equal variance Gaussian noise were given in Peters and B¨uhlmann (2014). Several polynomial time algorithms have been proposed for static DAG reconstruction using samples of states at the graph nodes; see Ghoshal and Honorio (2017a,b, 2018); Chen et al. (2019); Gao et al. (2022); Park (2020); Park and Raskutti (2017), and the reference therein. An information- theoretic lower bound on structure estimation was studied in Ghoshal and Honorio (2017a). In Gao et al. (2022), it was shown that the order optimal sample complexity for static Gaussian graphical model with equal variance is n = Θ(q log(p/q)), where p is the number of nodes and q is the maximum number of parents. The authors showed that the algorithm given in Chen et al. (2019) provides an upper bound that matches a min-max lower bound for the number of samples. However, similar results for DDAGs, with underlying temporal directed interaction between agent states, have not been studied, to the best of our knowledge. LDS Learning: Graph reconstruction, in general, is challenging in a network of LDS (di- rected, undirected, or bi-directed), as it involves time-dependencies between collected samples of nodal states. Learning the conditional independence structure in LDS with independent and identically distributed (white) excitation was explored in Basu and Michailidis (2015), Loh and Wainwright (2011), Songsiri and Vandenberghe (2010), Simchowitz et al. (2018), Faradonbeh et al. (2018), and the references therein. However, the methods in the cited papers do not extend to LDS that is excited by WSS (wide-sense stationary) noise, which makes correlations in state samples more pronounced. For LDS with WSS noise, Tank et al. (2015), Dahlhaus (2000), Materassi and Salapaka (2013), and Materassi and Innocenti (2009), estimated the conditional correlation structure, which contains true edges in the network and extra edges between some two-hop neighbors Materassi and Salapaka (2012). A consistent algorithm for the recovery of exact topology in a large class of applications with WSS noise was provided in Talukdar et al. (2020), with the corresponding sample-complexity analysis developed in Doddi et al. (2022), using a neighborhood-based regression framework. However, 2the developed algorithms and related sample complexity results do not extend to directed graphs and hence exclude DDAG reconstruction. DDAG reconstruction from the time-series data has been explored using the framework of directed mutual information in Quinn et al. (2015) but without rate characterization for learning from finite samples. Contribution: This article presents an information-theoretically optimal sample com- plexity analysis for learning a dynamical DAG (DDAG, i.e., a DAG underlying an LDS excited by WSS noise of equal power spectral density), using samples of state trajectories of the corresponding LDS. To the best of our knowledge, this is the first paper to study and prove sample complexity analysis for DDAGs. We consider learning under two sampling scenarios, viz; 1) restart and record, 2) continuous sampling. While the former pertains to samples collected from multiple disjoint (independent) trajectories of state evolution, the latter includes samples from a single but longer trajectory of the state evolution (see Fig. 2). Surprisingly, the results in this article show that the estimation errors are not influenced by the sampling strategy (restart and record or continuous) as long as the collected samples are over a determined threshold given by n = O(q log(p/q)) is obtained, where p is the number of nodes and q is the maximum number of parents per node. We also provide a match- ing information-theoretic lower-bound, max � log p 2β2+β4 , q log(p/q) M2−1 � , where β and M are system parameters (see Definition 2.4); thus obtaining an order optimal bound n = Θ(q log(p/q)). Our learning algorithm relies on first deriving rules for DAG estimation using the Power Spectral Density Matrix (PSDM) of nodal states, inspired by the estimator for static DAGs based on covariance matrices in Chen et al. (2019). Subsequently, the sample complexity associated with learning is derived by obtaining concentration bounds for the PSDM. In this regard, characterization of non-asymptotic bounds of PSDMs for a few spectral estimators have been obtained Fiecas et al. (2019); Veedu et al. (2021); Zhang and Wu (2021) previ- ously. A unified framework of concentration bounds for a general class of PSDM estimators was recently presented in Lamperski (2023). Our concentration bounds of the PSDM are reached using different proof steps, based on Rademacher random variables and symmetriza- tion argument Wainwright (2019). The rest of the paper is organized as follows. Section 2 introduces the system model and the preliminary definitions for LDS and DDAGs. Section 3 discusses Algorithm 1 and the main results for DDAG reconstruction from PSDM. Section 4 provides a concentration bound for the error in estimating the PSDM and a sample complexity upper bound for DDAG reconstruction using Algorithm 1. Section 5 contains a sample complexity lower bound. Notations: Bold faced small letters, x denote vectors; Bold faced capital letters, A denote matrices; For a time-series, x, ˘x(t) denotes the value of x at time t, x(ω) denotes the discrete time Fourier transform of x, x(ω) := �∞ k=−∞ ˘x(k)e−iωk, ω ∈ Ω = [0, 2π]; diag(v1, . . . , vp) operator creates a diagonal matrix with diagonal entries v1, . . . , vp; ΦxAB or ΦAB denotes the matrix obtained by selecting rows A and columns B in Φx; A∗ denotes conjugate transpose of A. ∥A∥ is the spectral norm of A and ∥v∥2 is the Euclidean norm of vector v. 2 System model of DDAG We describe different aspects of DDAG (DAG underlying an LDS) and the sampling strategies considered. We begin with some necessary DAG terminologies. DDAG terminology: The dynamical directed acyclic graph (DDAG), is given by G := (V, E), where V = {1, . . . , p}, and E is edge set of directed edge i −→ j. A directed path from i to j is a path of the form i = v0 −→ v1 −→ . . . −→ vℓ −→ vℓ+1 = j, where vk ∈ V and 3(vk, vk+1) ∈ E for every k = 0, . . . , ℓ. A cycle is a directed path from i to i, which does not exist in DDAG G. For G, pa(i) := {j ∈ V : (j, i) ∈ E} denotes the parents set and desc(i) denotes the descendants of i, the nodes that have a directed path from i. The set nd(i) := V \ desc(i) denotes the non-descendants set and the set an(i) ⊂ nd(i) denotes the ancestors set, the nodes that have a directed path to i. At set C ⊆ V is called ancestral if for every i ∈ C, pa(i) ⊆ C. Figure 2 shows an example DDAG with the node definitions. A node set C ⊆ V is said to be a topological ordering on G if for every i, j ∈ C, i ∈ desc(j) in G implies i > j. Gp,q denotes the family of DDAGs with p nodes and at most q parents per node. Without a loss of generalizability, we use the same terminology for the DDAG and the 1 2 5 3 6 4 7 Q1 Q2 Q4 Q3 Q5 R2 R4 R6 R8 R10 R12 R15 R16 R18 R20 Fig. 1: An electric circuit with transistors and tuned ampl transistors introduce direction, and inductors and capaci time-lag dependency across the circuit. Fig. 2: A causal graph representation of the electric circu Here, each node represents a transistor Figure 1: An example DDAG. Node 1 is an ancestor and node 7 is a descendant of every node in the graph. The set {1, 2, 5} is an ancestral set but {2, 5} is not. an(3) = {1, 2}, desc(3) = {4, 7}, nd(3) = {1, 2, 5, 6}. underlying DAG. LDS Model excited by equal PSD WSS noise: For the DDAG G = (V, E) ∈ Gp,q, we consider a linear dynamical system (LDS) with p scalar state variables, corresponding to nodes in V . Node i is equipped with time series measurements, {˘xi(k)}k∈Z, 1 ≤ i ≤ p. The LDS evolves according to the linear time-invariant model, ˘xi(k) = p � (i,j)∈E,j̸=i (˘hij ⋆ ˘xj)(k) + ˘ei(k), k ∈ Z, (1) where transfer function ˘hij ̸= 0 when directed edge (i, j) ∈ E. The exogenous noise {˘ei(k)}k∈Z, 1 ≤ i ≤ p, are zero mean wide sense stationary Gaussian processes, uncorre- lated across nodes. Taking the discrete-time Fourier transform (DTFT) of (1) provides the frequency representation for every ω ∈ Ω = [0, 2π], xi(ω) = p � (i,j)∈E,j̸=i Hij(ω)xj(ω) + ei(ω), 1 ≤ i ≤ p, (2) where xi(ω) = F{˘xi} := �∞ k=−∞ ˘xi(k)e−iωk, ei(ω) = F{˘ei}, and Hij(ω) = F{˘hij}. The model in (2) can be represented in the matrix form to obtain the following LDS, x(ω) = H(ω)x(ω) + e(ω), ∀ω ∈ Ω, (3) where e(ω) is the WSS noise. In this article, we are interested in the LDS with Φe(ω) = σ(ω)diag(α1, . . . , αp), where αi are known and can be a function of ω. For the simplicity of analysis, henceforth it is assumed that Φe(ω) = σ(ω)I. Remark 2.1. The assumption Φe(ω) = σ(ω)I is a restrictive assumption. However, we would like to remark that some form of restriction is required due to the impossibility of DAG 4reconstruction in a general setup due to identifiability issues Shimizu et al. (2006). However, the assumption can be relaxed to incorporate the identifiability conditions on Φei and H to retrieve the topological ordering, similar to Ghoshal and Honorio (2018). Furthermore, our results on DDAG reconstruction require the equal PSD to hold only at some known ω ∈ Ω which is less restrictive. The power spectral density matrix (PSDM) of the time-series x at the angular frequency ω ∈ Ω is given by Φx(ω) = F {Rx(t)} = ∞ � k=−∞ Rx(k)e−iωk, (4) where Rx(k) := E[˘x(k)˘xT (0)] is the auto-correlation matrix of the time-series x at lag k. The (i, j)-th entry of Φx is denoted by Φij. For the LDS (3), the PSDM is given by Φx(ω) = (I − H(ω))−1Φe(ω)((I − H(ω))−1)∗. (5) Consider the following additional non-restrictive assumptions on the power spectral density and correlation matrix of the LDS states. Assumption 2.2. There exists an M ∈ R such that 1 M ≤ λmin(Φx) ≤ λmax(Φx) ≤ M, where λmin and λmax respectively denote minimum and maximum eigenvalues. Assumption 2.3. The auto-correlation matrix of the time-series x at lag k, Rx(k) := E[˘x(k)˘xT (0)] satisfies ∥Rx(k)∥ ≤ Cρ−|k|, for some positive constants C, ρ ∈ R, ρ > 1. In the remaining paper, following these assumptions, our interest will be limited to the following family of DDAGs and corresponding LDS. Definition 2.4. Hp,q(β, σ, M) denotes the family of LDS given by (3) such that the corre- sponding DDAG, G(V, E) ∈ Gp,q (p nodes with each node having a maximum q parents), with |Hij(ω)| ≥ β, ∀(i, j) ∈ E, ω, Φe(ω) = σ(ω)I, and M−1 ≤ λmin(Φx(ω)) ≤ λmax(Φx(ω)) ≤ M, ∀ω ∈ Ω Sampling Strategy for LDS states: We consider two sampling settings (see Fig. 2 for details): i) restart and record: The sampling is performed as follows: start recording and stop it after N measurements. For the next trajectory, the procedure is restarted with an indepen- dent realization and record for another epoch of N samples; repeat the process another n − 2 times providing n i.i.d trajectories of N samples each. ii) continuous sampling: Here, a single trajectory of length n × N is taken consecutively. Then, the observations are divided into n segments, each having N consecutive samples. The data collected using either strategy is thus grouped into n trajectories of N samples each. For the finite length rth trajectory, {˘xr(t)}N−1 t=0 , define xr(ω) = 1 √ N N−1 � k=0 ˘xr(k)e−iωk, r = 1, . . . , n, ω ∈ Ω. (6) Note that xr(ω), if exists, is a zero mean random variable with the covariance matrix given by �Φx(ω) := E {xr(ω)[xr(ω)]∗} = 1 N N−1 � k=−(N−1) (N − |k|)Rx(k)e−iωk, ∀ω ∈ Ω. (7) 5Start Start Stop x1 Stop x1 Start Stop x2 Start Stop xn x2 xn Figure 2: (a) shows restart and record sampling. (b) shows continuous sampling. For the restart and record setting (unlike for continuous sampling), {xr(ω)}n r=1 are i.i.d. Further, as N −→ ∞, �Φx(ω) −→ Φx(ω) uniformly in Ω Stoica et al. (2005). 3 Reconstructing DDAGs from PSDM In this section, we discuss some results on how the PSDM, Φx, can be employed to completely reconstruct the DDAG, G when the time series is generated according to (3). Applying these results, inspired from the algorithm for static setting Chen et al. (2019), we propose Algorithm 1 for reconstructing the DDAG, G. First, we prove that conditional PSD (CPSD) of i conditioned on C, defined as f(i, C, ω) := Φii(ω) − ΦiC(ω)Φ−1 CC(ω)ΦCi(ω), (8) is a metric sufficient to obtain a topological ordering on the DDAG, G, which aids in the reconstruction of G. Notice that unlike Chen et al. (2019)’s static setting, our algorithm will require the use of CPSD to unveil the dependencies in the DDAG and further affect the sample complexity of learning. 3.1 CPSD and Topological Ordering Here, we will show that CPSD is the minimum for the nodes that have all the parents included in the conditioning set. We start by proving the result for the source nodes. Lemma 3.1. Consider the LDS described by (3). For any ω ∈ Ω, let α∗ := min k∈V Φkk(ω). Then Φii(ω) = α∗ if and only if i is a source node. Proof. Let T(ω) = (I − H(ω))−1 and Φe(ω) = σ(ω)In. Then, Φx(ω) = σ(ω)T(ω)T∗(ω). By Cayley-Hamilton theorem, there exists constants a0(ω), . . . , an(ω) such that (I − H(ω))−1 = �n−1 k=0 ak(ω)(I − H(ω))k. Using induction, it can be shown that non-diagonal entries of (I − H(ω))k are zero if and only if there no k − hop path between i and j (almost always). Similarly, (i, i)th entry is 1 if and only if there is no k-hop path between them (almost always). Then (ignoring (ω)), Φii = (Φx)ii = σ �n k=1 TikTik = σ(T2 ii + � k̸=i T2 ik). If i is a source node, then Tik = 0 for every k ̸= i and Tii = 1, which implies Φii = σT2 ii. For non-source nodes, Tik ̸= 0 for some k, which gives Φii > σ. 3.1.1 Conditional PSD deficit The following is the definition of conditional PSD deficit, which is helpful in proving the subsequent results and in retrieving the DDAG. 6Definition 3.2 (The CPSD deficit). ∆ := min ω∈[−π,π] min j∈V min C⊊nd(j), Pa(j)\C̸=∅ f(j, C, ω) − σ(ω) (9) The following lemma shows that f(i, C, ω) from Eq. 8 can be used as a metric to obtain a topological ordering of G. Lemma 3.3. Consider the DDAG, G, governed by (3). Let j ∈ V and let C ⊆ V \ {j} be an ancestral set. Then for every ω ∈ Ω, f(j, C, ω) = σ(ω) : if Pa(j) ⊆ C, f(j, C, ω) ≥ ∆ + σ(ω) > σ(ω) : if Pa(j) ⊈ C. Proof. See Appendix A Corollary 3.4. If Pa(i) ⊆ C and Pa(i) ⊈ D, then f(i, C, ω) − f(i, D, ω) ≥ ∆. Lemma 3.5. ∆ ≥ β2σ. Proof. Applying (15), f(j, C, ω) − σ = HjD(ΦD − ΦDCΦ−1 CCΦCD)H∗ jD ≥ σ|D|β2. Then ∆ ≥ min ω min j |σ|D|β2 = σ|D|β2 (a) ≥ σβ2, where (a) follows because |D| ≥ 1 if C ⊈ Pa(j). To determine the DDAG G’s structure, first determine a topological ordering of nodes as follows: Beginning with an empty set S, we iteratively add node i in S where (i, C∗ i ) ∈ arg min C⊆S,|C|≤q 1≤j≤p, j /∈S f(j, C, ω), (10) and f comes from (8). The following Lemma shows that S is a valid topological ordering w.r.t. G. Lemma 3.6. S is a valid topological ordering with respect to G. Proof. In the first step, C = ∅ and f(i, C, ω) = Φii. By Lemma 3.1 and Lemma 3.3, Φii = σ if i is a source node, where as Φii ≥ σ + ∆ if i is not a source node. Thus, the first node in the ordered set S, S1, is always a source node. Induction assumption: Nodes S1 to Sn in S follow topological order. For the n + 1, by Lemma 3.3, for every C ⊆ S, f(k, C, ω) is minimum for k ∈ V \ S if and only if Pa(k) ⊆ C. Thus nodes S1 to Sn+1 follow a topological order, which proves the result. Identification of the parents: Parents of a node are identified from the ordered set by applying Corollary 3.4. Let D = C \ {k}. As shown in Corollary 3.4, if Pa(i) ⊆ C and k ∈ Pa(i), then f(i, C, ω) − f(i, D, ω) ≥ ∆. Thus, from the set S, for every node Si, one can eliminate nodes S1, . . . , Si−1 by checking if the difference is greater than ∆. If the difference is greater than ∆ for some Sk, then Sk is a parent of Si. That is, 7Lemma 3.7. Let (i, C∗ i ) be a solution of (10) and let Pi := {j ∈ C∗ i | |f(i, C∗ i , ω) − f(i, C∗ i \ j, ω)| ≥ ∆} . Then, Pa(i) = Pi. Applying the above procedure and Lemma 3.3-Lemma 3.7, one can formulate Algorithm 1 to obtain the ordering of the DDAG, G and eventually reconstruct the DDAG exactly. In Algorithm 1, �f(i, C, ω) := �Φii(ω)− �ΦiC(ω)�Φ−1 CC(ω)�ΦCi(ω), an empirical estimate of f(i, C, ω), is employed instead of f and γ = ∆/2. The following lemma proves that if the empirical estimate �f(·) is close enough to the original f(·), then Algorithm 1 reconstructs the DDAG exactly. Algorithm 1 Ordering algorithm Input: Estimated PSDM, �Φ(ω): ∆, z = ejω, ω ∈ (−π, π] Output: �G 1. Initialize the ordering, S ←−() 2. For i = 1, . . . , p (a) Compute � j∗, C∗ j � ∈ arg min C⊆S,|C|≤q 1≤j≤p, j /∈S �f(j, C, ω) (b) S ←− (S, j∗) 3. �G = (V, �E), V ←− {1, . . . , p}, �E ←− ∅ 4. For i = 1, . . . , p (a) Parents of i, Pi := � j ∈ C∗ i ��� | �f(Si, C∗ i , ω) − �f(Si, C∗ i \ j, ω)| ≥ γ � (b) For k ∈ Pi i. Do �E ←− �E ∪ (k, i) 5. Return �G Lemma 3.8. If | �f(i, C, ω)−f(i, C, ω)| < ∆/4, for every i, ω, C, then Algorithm 1 reconstructs the DAG, G successfully. That is, G = �G. Proof. See Appendix B Therefore, it suffices to derive the conditions under which |f(·) − �f(·)| < ∆/4. In the following section, we derive a concentration bound to guarantee a small error, which in turn is applied in obtaining the upper bound on the sample complexity of estimating G. 4 Finite Sample Analysis of Reconstructing DDAGs In Lemma 3.8, it was shown that the DDAG, G, can be reconstructed exactly if the error in estimating f(i, C, ω) (given by (8)) is small enough. In this section, a concentration bound 8on the error in estimating Φx from finite data is obtained, which is used later to obtain a concentration bound on the error in estimating the metric f. Recall that we consider n state trajectories (see Fig. 2 for the two sampling strategies) with each trajectory being of length N samples, i.e � {˘xr(k)}N−1 k=0 �n r=1. The DFTs for each trajectory, xr(ω), is a complex Gaussian with mean zero and covariance matrix, �Φx(ω)), i.e., for every r = 1, . . . , n, xr(ω) ∼ N(0, �Φx(ω)), as given in Eq. 7. Increasing N ensures that �Φx is close to Φx. To estimate the PSDM, we thus rely on the spectrogram method and estimate �Φx(ω)) using finite n samples for xr. 4.1 Non-Asymptotic Estimation Error in Spectrogram Method Let �Φx(ω) := 1 n �n r=1 xr(ω)[xr(ω)]∗. Let the estimation error in estimating Φx by �Φx(ω) be Q := �Φx(ω) − Φx(ω). Applying the triangle inequality, ∥Q∥ ≤ ∥Qapprox∥+∥Q2∥, where Qapprox := �Φx(ω)−Φx(ω) and Q2 := �Φx(ω) − �Φx(ω). Note that �Φx(ω) is the covariance of the DFT of each trajectory. To bound the estimation error, we bound both Qapprox and Q2. The following Lemma shows that Qapprox is small if N (length of each trajectory) is large. Lemma 4.1 (Lemma 5.1, Doddi et al. (2022)). Consider an LDS given by (3) that satis- fies Assumption 2.3. Let Qapprox = �Φx(ω) − Φx(ω) where �Φx(ω) is given in Eq. 7. Then ∥Qapprox∥ < ε1 if N > 2Cρ−1 (1−ρ−1)2ε1 . The next step is to characterize Q2, the error in estimating �Φx using �Φx(ω). Since E (xr(ω)[xr(ω)]∗) = �Φx(ω)), �Φx(ω) is an unbiased estimator of �Φx(ω). The following theorem provides a concentration bound on ∥Q2∥. The concentration bound is applicable under two sampling scenarios; the restart and record setting and continuous sampling setting, as shown in Fig. 2. Theorem 4.2. Suppose {˘xr(k)}N−1 k=0 , 1 ≤ r ≤ n be the time series measurements obtained from an LDS governed by (3), satisfying Assumption 2.2. Then P �����Φx(ω) − �Φx(ω)) ��� ≥ ϵ � ≤ exp � − ϵ2n 128M2 + 6p � , ∀ω ∈ [−π, π]. (11) Proof. See Appendix C By combining Lemma 4.1 and Theorem 4.2, the following corollary is obtained, which gives a concentration bound on the estimation error, ∥Q∥. Corollary 4.3. Consider an LDS governed by (3) that satisfies Assumptions 2.2 and 2.3. Let {˘xr(k)}N−1 k=0 , 1 ≤ r ≤ n be the time series measurements obtained for the LDS. Suppose that N > 2Cρ−1 (1−ρ−1)2ε1 , where 0 < ε1. Let 0 < ε1, ε2 < ε be such that ε2 = ε − ε1. Then ∀ω ∈ Ω, P ����Φxx(ω) − �Φxx(ω)) ��� ≥ ε � ≤ exp � − ε2 2n 128M2 + 6p � , (12) P ����ΦCC(ω) − �ΦCC(ω)) ��� ≥ ε � ≤ exp � − ε2 2n 128M2 + 6q � , and (13) P ����Φii(ω) − �Φii(ω)) ��� ≥ ε � ≤ exp � − ε2 2n 128M2 + 6 � . (14) 94.2 Sample Complexity Bounds: Upper Bound In the previous subsection, concentration bounds on the estimation errors in PSDM were obtained. Here, a concentration bound on the error in estimating f is obtained, which is used to obtain a concentration bound in reconstructing the DDAG, G. The following result provides a concentration bound on |f − �f|. Lemma 4.4. Consider an LDS governed by (3) that satisfies Assumptions 2.2 and 2.3. Let {˘xr(k)}N−1 k=0 , 1 ≤ r ≤ n be the time series measurements obtained for the LDS. Suppose that N > 2Cρ−1 (1−ρ−1)2ε1 , where 0 < ε1. Let 0 < ε1, ε2 < ε be such that ε2 = ε − ε1. Then there exists a c0 ∈ R such that, for any ω ∈ Ω, P � |f(i, C, ω) − �f(i, C, ω) ≥ ε � ≤ c0e � − ε2 2n 10368M6 +6(q+1) � , where q is the maximum number of parents any node has in G. Proof. See Appendix D Based on Lemma 4.4, the following upper bound on the probability of error in estimating G can be obtained. Theorem 4.5. Suppose q ≤ p/2. Consider an LDS that belongs to Hp,q(β, σ, M) (Definition 2.4) that satisfies Assumptions 2.2 and 2.3. Let {˘xr(k)}N−1 k=0 , 1 ≤ r ≤ n be the time series measurements of the LDS and let �G be the DDAG reconstructed by Algorithm 1. Suppose that N > 2Cρ−1 (1−ρ−1)2ε1 , where 0 < ε1 < ∆/4. Let 0 < ε1, ε2 < ε < ∆/4 be such that ε2 = ε − ε1. Then P � G(ω) ̸= �G(ω) � ≤ δ if n ≳ M6 (q log(p/q) − log δ) ϵ2 2 . Proof. Applying the bound in Lemma 4.4, P � G(ω) ̸= ˆG(ω) � = P   � k∈V,C⊆V \{k},|C|≤q � |f(i, C, ω) − �f(i, C, ω)| > ϵ �   (a) ≤ � k∈V,C⊆V \{k},|C|≤q P �� |f(i, C, ω) − �f(i, C, ω)| > ϵ �� (b) ≤ p ��(p − 1) 1 � + · · · + �(p − 1) q �� c0e � − ϵ2n 10368M6 +6(q+1) � (c) ≲ c1p × q × (p/q)qe � − ϵ2n 10368M6 +6(q+1) � ≈ exp � log p + log q + q log (p/q) + � − ϵ2n 10368M6 + 6(q + 1) �� ≈ exp � log p + log q + q log (p/q) + 6q − ϵ2n M6 � ≲ exp � q log (p/q) − ϵ2n M6 � < δ, 10where (a) follows by union bound, (b) follows since |V | = p and there are �p−1 k � number of combinations with |C| = k. (c) follows by applying Stirling’s approximation, �n k � ≤ (ne/k)k. Thus, n ≳ M6(q log(p/q)−log δ) ϵ2 . By selecting the threshold γ in the algorithm appropriately, we can get a sample complexity n ≳ M6q log(p/q) ∆2 . In the following section, a matching lower bound is derived. 5 Sample Complexity Bounds: Lower Bound The lower bound for reconstructing DDAG is derived using information-theoretic techniques, in particular Fano’s inequality, and by restricting the interested family of graphs to a finite set. Notice that, except for a couple of non-trivial facts in dynamic setup, this is a direct extension of the lower bound for the static case provided in Gao et al. (2022). The approach is to construct restricted ensembles of graphical models and then to lower bound the probability of error using Generalized Fano’s inequality. Theorem 5.1 below provides the lower bound. For completeness, the proof is provided in the Appendix E.2. Theorem 5.1. Suppose q ≤ p/2. If n ≤ (1 − 2δ) max � log p 2β2 + β4 , q log(p/q) M2 − 1 � then inf �G max H∈Hp,q(β,σ,M) P{(G(H) ̸= �G)} ≥ δ, That is, if n ≤ (1 − 2δ) max � log p 2β2+β4 , q log(p/q) M2−1 � , then any given estimator fails to recon- struct the DDAG with probability greater than δ. The lower-bound provides the fundamental limit on reconstructing the DDAG from finite samples. Theorem 5.1 provides the lower bound Ω � log p 2β2+β4 � q log(p/q) M2−1 � . Notice that the upper bound in Theorem 4.5 is O(q log p q). Thus we obtain a matching order bound when the lower bound is dominated by the second term. Conclusion In this article, we characterized the optimal sample complexity for structure identification with directions in linear dynamical networks. Inspired by the static setting, a metric and an algorithm were proposed based on the power spectral density matrix to exactly reconstruct the DAG. It is shown that the optimal sample complexity is n = Θ(q log(p/q)). For the upper bound characterization, we obtained a tight concentration bound for power spectral density matrix. An information-theoretic min-max lower bound also was provided for (sub) Gaussian linear dynamical sysytems. It was shown that the upper-bound is order optimal with respect to the lower bound. 11Appendices A Proof of Lemma 3.3 Let C ⊆ V \ {j} be an ancestral set and let D = nd(j) \ C. Then, xj(ω) = HjC(ω)XC(ω) + HjD(ω)XD(ω) + ej(ω). Applying ΦejC = ΦejD = 0, we obtain ΦjC(ω) = HjC(ω)ΦCC(ω) + HjD(ω)ΦDC(ω) and Φj(ω) = HjC(ω)ΦCHjC(ω)∗ + HjD(ω)ΦDC(ω)H∗ jC(ω) + HjC(ω)ΦCD(ω)H∗ jD(ω) + HjD(ω)ΦD(ω)H∗ jD(ω) + Φejej(ω). Then f(j, C, ω) = Φj − ΦjCΦ−1 C ΦCj = Φejej + HjD(ΦD − ΦDCΦ−1 CCΦCD)H∗ jD. Notice that when Pa(j) ⊆ C, HjD = 0, and f(j, C, ω) = Φejej, which shows the first part. To prove the second part, suppose Pa(j) ∩ D ̸= ∅. We need to show that HjD(ΦD − ΦDCΦ−1 CCΦCD)H∗ jD > 0. Let A = nd(j) = C ∪ D and B = desc(j) ∪ {j}. From Talukdar et al. (2018); Veedu et al. (2021), Φ−1 AA =S + L, where S = (IA − H∗ AA)Φ−1 eA (IA − HAA), L = HBAΦ−1 eB HBA − Ψ∗Λ−1Ψ, Ψ = H∗ ABΦ−1 eA (I − HAA) + (I − H∗ BB)Φ−1 eB HBA, and Λ = H∗ ABΦ−1 eA HAB + (I − H∗ BB)Φ−1 eB (I − HBB). Notice that since B is the set of descendants of j, HAB = 0, as cycles can be formed otherwise. Then, L = 0 and Φ−1 AA = (IA − H∗ AA)Φ−1 eA (IA − HAA). Φ−1 AA = � ΦDD ΦDC ΦCD ΦCC �−1 = � KDD KDC KCD KCC � = 1 σ (I − H∗ AA) (I − HAA) By Scur’s complement, (ΦD−ΦDCΦ−1 CCΦCD)−1 = KDD = 1 σ(ID−H∗ DD−HDD+(H∗ AAHAA)D×D). Moreover, HAA = � HDD HDC HCD HCC � and (H∗ AAHAA)D×D = H∗ DDHDD + H∗ CDHCD. Since C is ancestral, HCD = 0 and KDD = 1 σ(ID − HDD)∗(ID − HDD). Since G is a DAG, the rows and columns of H can be rearranged to obtain a lower triangular matrix with zeros on the diagonal. Thus eigenvalues of (ID −HDD) and its inverse are all ones. Hence minimum eigenvalue of K−1 DD is greater than σ. Applying Rayleigh Ritz theorem on HjDK−1 DDH∗ jD, we have HjD(ΦD − ΦDCΦ−1 CCΦCD)H∗ jD = HjDK−1 DDH∗ jD ≥ σ|D|β2 (15) which is strictly greater than zero if D is non-empty. 12B Proof of Lemma 3.8 The proof is done in two steps. First, we show that S in Algorithm 1 is a topological ordering. Then, we show that step (4) in Algorithm 1 can identify the parents of every node in G. The first step is shown via induction. Since | �f(i, C, ω) − f(i, C, ω)| < ∆/4 for empty set, |Φii − �Φii| < ∆/4 for every i. Recall from Lemma 3.3 that Φii − Φjj > ∆ if i is a source node and j is a non-source node. Then, �Φjj ≥ Φjj − ∆/4 ≥ Φii + 3∆/4 ≥ �Φii + ∆/2. Thus, i ∈ arg min 1≤k≤p �Φkk if and only if i ∈ arg min 1≤k≤p Φkk and thus S1 is always a source node. For the induction step, assume that S1, . . . , Sn forms a correct topologically ordered set w.r.t. G. Let C ⊆ S(1 : n). If Pa(i) ⊆ C and Pa(j) ⊈ C, then by applying Lemma 3.3, �f(j, C, ω) > f(j, C, ω) − ∆/4 ≥ σ + 3∆/4 = f(i, C, ω) + 3∆/4 ≥ �f(i, C, ω) + ∆/2. Thus, i ∈ arg min k∈V \S �f(k, C, ω) if and only if i ∈ arg min k∈V \S f(k, C, ω) and thus (S, Sn+1) forms a topological order w.r.t. G, by Lemma 3.6. To prove the second step, let C ⊆ S(1 : i). Since S(1 : i) is a valid topological ordering, Pa(i) ⊆ S(1 : i − 1). Let k ∈ Pa(i) and let D = C \ {k}. Then, as shown in Corollary 3.4 f(i, C, ω) − f(i, D, ω) ≥ ∆, and ∆ ≤ |f(i, C, ω) − f(i, D, ω)| ≤ |f(i, C, ω) − �f(i, C, ω)| + | �f(i, C, ω) − �f(i, D, ω)| + | �f(i, D, ω) − f(i, D, ω)| < ∆/4 + ∆/4 + | �f(i, C, ω) − �f(i, D, ω)| =⇒ | �f(i, C, ω) − �f(i, D, ω)| > ∆/2. Suppose k /∈ Pa(i) but k ∈ S(1 : i). Then, for D = C \ {k}, f(i, C, ω) − f(i, D, ω) = 0. Repeating the same series of inequalities above by exchanging f and �f, we obtain | �f(i, C, ω)− �f(i, D, ω)| < ∆/2. Thus, from the set S, for every node Si, one can check nodes S1, . . . , Si−1 and verify if the difference of including and excluding the node is greater than ∆/2. If the difference is greater than ∆/2 for some k, then k is a parent of i, and if not, then the node is not a parent of i. That is, let Ci = {S1, . . . , Si−1}, i > 1, and let �Pi := � j ∈ Ci ��� | �f(Si, Ci, ω) − �f(Si, Ci \ {j}, ω)| > ∆/2 � . Then, Pa(i) = �Pi. C Proof of Theorem 4.2 By the variational form of spectral norm Horn and Johnson (2012), ∥Q∥ = sup v∈Cp,∥v∥=1 |v∗Qv|, where the max is taken over a p-dimensional unit complex sphere, Sp := {v ∈ Cp : ∥v∥2 = 1}. The first step here is to reduce supremum to finite maximization using finite covers of a unit ball, which is done using a δ cover. A δ-cover of a set A is a set v1, . . . , vm such that for every v ∈ A, there exists an i ∈ 1, . . . , m such that ∥vi − v∥2 ≤ δ. The following Lemma is obtained by extending example 5.8 in Wainwright (2019) to the complex field. 13Lemma C.1. Let v1, . . . , vm be a δ-covering of the unit sphere Sp. Then there exists such a covering with m ≤ (1 + 2/δ)2p vectors. Proof. The proof follows by extending (5.9) in Wainwright (2019), to the complex field. Let v ∈ Sp and let vj be such that v = vj + ∆, where ∥∆∥ ≤ δ. Then, v∗Qv = (vj)∗Qvj + 2ℜ{∆∗Qvj} + ∆∗Q∆. Applying triangle inequality, |v∗Qv| ≤ |(vj)∗Qvj| + 2∥∆∥∥Q∥∥vj∥ + |∆∥2∥Q∥ ≤ |(vj)∗Qvj| + 2δ∥Q∥ + δ2∥Q∥ ≤ |(vj)∗Qvj| + 1 2∥Q∥ for δ ≤ 0.22474. Thus, ∥Q∥ = max v∈Sp |v∗Qv| ≤ max j=1,...,m |(vj)∗Qvj| + 1 2∥Q∥ and ∥Q∥ ≤ 2 max j=1,...,m |(vj)∗Qvj| Next, we find an upper bound for E � eλ∥Q∥� , which is treated with Chernoff-type bounding technique to obtain the desired result. E � eλ∥Q∥� ≤ E � exp � 2λ max j=1,...,m |(vj)∗Qvj| �� ≤ m � j=1 E � e2λ(vj)∗Qvj� + E � e−2λ(vj)∗Qvj� (16) Next, we complete the proof for the restart and record sampling and the continuous sampling separately. C.1 Restart and Record Sampling Under the restart and record sampling settings, for any given ω ∈ Ω, {xr(ω)}n r=1 is i.i.d. Thus E � exp � t(vj)∗Qvj�� = E � exp � t(vj)∗(�Φx(ω) − �Φx(ω)))vj�� = E � exp � t n n � r=1 (vj)∗xr(ω)[xr(ω)]∗vj − (vj)∗�Φx(ω)vj �� = n � r=1 E � exp � t n(vj)∗xr(ω)[xr(ω)]∗vj − (vj)∗�Φx(ω)vj �� = � E � exp � t n(vj)∗x1(ω)[x1(ω)]∗vj − (vj)∗�Φx(ω)vj ���n = � E � exp � t n|v∗xr(ω)|2 − v∗�Φx(ω)v ���n 14Let ε ∈ {−1, +1} be a Rademacher variable independent of xr. It can be shown that Proposition 4.11 in Wainwright (2019) will hold for complex numbers also. Then Exr(ω) � exp � t n|v∗xr(ω)|2 − v∗�Φx(ω)v �� ≤ Exr(ω),ε � exp �2tε n |v∗xr(ω)|2 �� (17) = ∞ � k=0 (2t/n)2k 2k! E � |v∗xr(ω)|4k� (18) Recall that �Φx is a positive definite matrix and v∗xr ∼ N(0, η), where η = v∗�Φxv ≤ λmax(�Φx) ≤ M. The even moments of y ∼ N(0, η) is given by E{y2k} = η2k(2k − 1)!! = (2k)! 2kk! η2k. Then E � |v∗xr(ω)|4k� ≤ (4k)! 22k(2k)!M2k. Therefore using the inequality (4k)! ≤ 22k[(2k)!]2, Exr(ω) � exp � t n|v∗xr(ω)|2 − v∗�Φx(ω)v �� ≤ 1 + ∞ � k=1 (2t/n)2k 2k! (4k)! 22k(2k)!M2k ≤ 1 + ∞ � k=1 (2t/n)2k 2k! 22k[(2k)!]2 22k(2k)! M2k = 1 + ∞ � k=1 �2Mt n �2k = 1 1 − � 2Mt n �2 ≤ exp �8M2t2 n2 � whenever 2Mt n < 3/4, where the final inequality follows by applying 1 − x ≥ e−2x for x ∈ [0, 3/4] (to be precise 0.77). Thus, E � exp � t(vj)∗Qvj�� ≤ exp �8M2t2 n � , ∀|t| ≤ 3n 8M . Applying Lemma C.1 and the bound 2m ≤ 2(1 + 2/0.22474)2p ≤ 2e4.6p ≤ e5p+0.693 ≤ e6p, From (16), E � eλ∥Q∥� ≤ E � exp � 2λ max j=1,...,m |(vj)∗Qvj| �� ≤ m � j=1 E � e2λ(vj)∗Qvj� + E � e−2λ(vj)∗Qvj� ≤ 2m exp �32M2λ2 n � ≤ exp �32M2λ2 n + 6p � , ∀|λ| ≤ 3n 16M . Applying Chernoff-type bounding approach, P (∥Q∥ ≥ t) ≤ e−λtE � eλ∥Q∥� ≤ exp � −λt + 32M2λ2 n + 6p � , ∀|λ| ≤ 3n 16M . 15The tightest bound is given by g∗(t) := inf |λ|≤ 3n 16M � −λt + 32M2λ2 n + 6p � , where the objective is convex. Taking derivative w.r.t. λ and equating to zero, λ∗ = tn 64M2 and g∗ = − t2n 64M2 + 32M2 n t2n2 642M4 +6p = 6p− t2n 128M2 , if t is such that t ≤ 12M, which is reasonable as we can always pick M ≥ 1. Thus, P (∥Q∥ ≥ t) ≤ exp � − t2n 128M2 + 6p � The theorem statement follows. C.2 Continuous Sampling In the continuous sampling setting, the samples ˘x(0), . . . , ˘x(N −1), ˘x(N), . . . , ˘x(2N −1), . . . , ˘x((n − 1)N), . . . , ˘x(nN − 1) are sampled continuously and are correlated with each other. Thus, xr(ω) and xs(ω), r ̸= s, 1 ≤ r, s ≤ n, can be correlated, in contrast to the restart and record (RR) setting, where the xr(ω) and xs(ω), r ̸= s are i.i.d. For any given ω ∈ Ω, let x(ω) := [[x1(ω)]T , [x2(ω)]T , . . . , [xn(ω)]T ]T ∈ Cpn×1 be the vectorized form of {xr(ω)}n r=1 and let C(ω) := E{x(ω)x∗(ω)} be the covariance matrix of x(ω). Under the RR setting, C(ω) ∈ Cpn×nn will be a block-diagonal matrix (of block size p×p), whereas in the continuous sampling, the non-block-diagonal entries of C(ω) can be non-zero. However, the vector x(ω), with correlated entries, can be written as a linear transformation of i.i.d. vector w ∈ Cpn×1 with unit variance, i.e. x(ω) = C1/2(ω)w, where C1/2 is the square-root of C. As shown in Appendix E.1, when {˘e(k)}n k=1 in the linear time-invariant model (1) are Gaussian, xr(ω) and thus x(ω) are Gaussian distributed. In this case, a candidate is w ∼ N(0, Ipn). It can be verified that E{x(ω)x∗(ω)} = C1/2(ω)E{ww∗}C1/2(ω) = C(ω). Notice that the covariance matrix C(ω) is a block matrix, defined as C =   C11 C12 . . . C1n C21 C22 . . . C2n ... Cn1 Cn2 . . . Cnn   , where Crs(ω) ∈ Cp×p, 1 ≤ r, s ≤ n, where the entries of Crs(ω) is given by E {xr(ω)[xs(ω)]∗}. Recall that xr(ω) = 1 √ N N−1 � ℓ=0 ˘x((r − 1)N + ℓ)e−iωℓ. Let Ir = � 0| . . . |0|Ip×p| . . . |0 � ∈ Rp×np be such that rth block is identity matrix. Then xr(ω) = Irx(ω). The estimated PSDM is then given by �Φx(ω) = 1 n n � r=1 xr(ω)[xr(ω)]∗ = 1 n n � r=1 Irx(ω)x∗(ω)I∗ r. Substituting x(ω) = C1/2(ω)w, and letting B(ω) := (C1/2)∗(ω) �n r=1 I∗ ruu∗IrC1/2(ω) 16E [exp (tu∗Qu)]=E � exp � t n n � r=1 u∗xr(ω)[xr(ω)]∗u − u∗�Φx(ω)u �� =E � exp � t n n � r=1 [x∗(ω)I∗ ruu∗Irx(ω) − E {x∗(ω)I∗ ruu∗Irx(ω)}] �� =E � exp � t n n � r=1 � w∗(C1/2)∗(ω)I∗ ruu∗IrC1/2(ω)w − E � w∗(C1/2)∗(ω)I∗ ruu∗IrC1/2(ω)w ���� =E � exp � t n � w∗(C1/2)∗(ω) n � r=1 (I∗ ruu∗Ir)C1/2(ω)w − E � w∗(C1/2)∗(ω) n � r=1 (I∗ ruu∗Ir)C1/2(ω)w ���� = E � exp � t n [w∗B(ω)w − E {w∗B(ω)w}] �� Notice that I∗ ru = � 0, . . . , 0, uT , , . . . , 0 �T is a column vector, I∗ ruu∗Ir =   0 0 0 . . . 0 . . . . . . 0 ... . . . uu∗ ���� (r,r)th block . . . 0 ... ... · · · ... 0 0 0 . . . 0   and n � r=1 I∗ ruu∗Ir =   uu∗ 0 . . . 0 0 uu∗ . . . 0 ... ... · · · 0 0 0 . . . uu∗   , i.e., rank(I∗ ruu∗Ir) = 1 and rank(B(ω)) ≤ n. Let B(ω) = U(ω)Λ(ω)U∗(ω) be the eigen value decomposition of B(ω), where Λ = diag(λ1, . . . , λn). Consequently, omitting ω from the notations, E [exp (tu∗Qu)] = E � exp � t n [w∗Bw − E {w∗Bw}] �� = E � exp � t n [w∗UΛU∗w − E {w∗UΛU∗w}] �� (a) = E � exp � t n [w∗Λw − E {w∗Λw}] �� = E � exp � t n n � i=1 λi � w2 i − E � w2 i �� �� = n � i=1 E � exp �tλi n � w2 i − E � w2 i ���� , where (a) follows because w is invariant under unitary transformations Cui et al. (2019). Let ε ∈ {+1, −1} be a uniform random variable independent of w. Similar to (17), we can now 17apply the Rademacher random variable trick. Ewi � exp � λ � w2 i − E � w2 i ���� ≤ Ewi,ε � exp � 2λεw2 i �� = ∞ � k=0 (2λ)2k (2k)! E � w4k i � ≤ ∞ � k=0 (2λ)2k 2k! (4k)! (2k)!22k ≤ ∞ � k=0 (2λ)2k = 1 1 − 4λ2 ≤ exp(8λ2), for every |λ| < 3/8. Thus, (with the substitution λ = tλi/n and the upperbound λi ≤ ∥B∥) E [exp (tu∗Qu)] = n � i=1 E � exp �tλi n � w2 i − E � w2 i ���� ≤ n � i=1 exp �8t2λ2 i n2 � = exp � 8t2 n2 n � i=1 λ2 i � ≤ exp �8t2 n ∥C∥2 � , ∀ |t| ≤ 3n 8∥C∥, where we have used ∥B∥ ≤ ∥C∥ in the final equality. Now, combining this with the δ−cover argument, E � et∥Q∥� ≤ m � j=1 E � e2t(vj)∗Qvj� + E � e−2t(vj)∗Qvj� ≤ 2m exp �32∥C∥2t2 n � ≤ exp �32∥C∥2t2 n + 6p � , ∀|t| ≤ 3n 16∥C∥. Finally, applying Chernoff bound, P (∥Q∥ ≥ t) ≤ exp � − t2n 128∥C∥2 + 6p � . 18C.2.1 Tight upper bound for ∥C∥ An explicit expression for C is given as follows: Crs(ω) := E {xr(ω)[xs(ω)]∗} = 1 N N−1 � ℓ=0 N−1 � k=0 E � ˘x((r − 1)N + ℓ)[˘x((s − 1)N + k)]T � e−iω(ℓ−k) = 1 N N−1 � ℓ=0 N−1 � k=0 R˘x((r − s)N + ℓ − k)e−iω(ℓ−k) = 1 N N−1 � τ=−N+1 (N − |τ|)R˘x((r − s)N + τ)e−iωτ = N−1 � τ=−N+1 � 1 − |τ| N � R˘x((r − s)N + τ)e−iωτ = N−1 � τ=−N+1 � 1 − |τ| N � e−iωτR˘x((r − s)N + τ). Let ατ = e−iωτ � 1 − |τ| N � . Then C = N−1 � τ=−N+1 ατ   R˘x(τ) R˘x(−N + τ) . . . R˘x((1 − n)N + τ) R˘x(N + τ) R˘x(τ) . . . R˘x((2 − n)N + τ) ... ... ... R˘x((n − 1)N + τ) R˘x((n − 2)N + τ) . . . R˘x(τ)   . Notice that ˘g(τ) := 1 − |τ|/N is a triangle function, and the Fourier transform of ˘g(τ), g(ω) has the property that |g(ω)| ≤ 1. Then for any u ∈ Cnp such that ∥u∥2 ≤ 1, u∗Cu = n � i,j=1 [ui]∗ N−1 � τ=−N+1 ατR˘x((i − j)N + τ)uj (a) ≤ Fτ{u}Fτ{Rx(tN + τ)}Fτ{u} (b) ≤ ∥Φx(ω)∥ ≤ M, where (a) follows by taking Fourier transform with respect to τ (Fτ denotes Fourier trans- form with respect to the variable τ) and (b) since ∥Fτ{u}∥2 ≤ 1. Thus, P (∥Q∥ ≥ t) ≤ exp � − t2n 128M2 + 6p � , similar to the restart and record case. D Proof of Lemma 4.4 Notice that ∥Ax∥2 ≤ ∥A∥∥x∥2 for every matrix A and vector x. Applying this identity with x = [1, 0 . . . , 0], ∥ΦCi∥2 ≤ ∥ΦAA∥, where A = [k, C]. Then, applying CBS inequality for 19complex vectors, |x∗Ay| ≤ ∥x∥2∥Ay∥2 ≤ ∥x∥2∥A∥∥y∥2, the error can be upper bounded as |f(i, C, ω) − �f(i, C, ω)| = |(Φii − ΦiCΦ−1 CCΦCi) − (�Φii − �ΦiC �Φ−1 CC �ΦCi)| = |(Φii − �Φii) + (�ΦiC �Φ−1 CC �ΦCi − ΦiCΦ−1 CCΦCi)| ≤ |Φii − �Φii| + |�ΦiC(�Φ−1 CC − Φ−1 CC)�ΦCi)| + |(�ΦiC − ΦiC)Φ−1 CC �ΦCi)| + |ΦiCΦ−1 CC(�ΦCi − ΦCi)| ≤ |Φii − �Φii| + ∥�ΦiC∥2∥�Φ−1 CC − Φ−1 CC∥∥�ΦCi∥2 + ∥�ΦiC − ΦiC∥2∥Φ−1 CC∥∥�ΦCi∥2 + ∥ΦiC∥2∥Φ−1 CC∥∥�ΦCi − ΦCi∥2 ≤ |Φii − �Φii| + ∥�Φ−1 CC − Φ−1 CC∥∥�ΦCi∥2 2 + M∥�ΦiC − ΦiC∥2∥�ΦCi∥2 + M2∥�ΦCi − ΦCi∥2 ≤ |Φii − �Φii| + ∥�Φ−1 CC − Φ−1 CC∥∥�ΦCi − ΦCi∥2 2 + ∥�Φ−1 CC − Φ−1 CC∥∥ΦCi∥2 2 + M∥�ΦiC − ΦiC∥2 � ∥�ΦCi − ΦCi∥2 + ∥ΦCi∥2 � + M2∥�ΦCi − ΦCi∥2 ≤ |Φii − �Φii| + ∥�Φ−1 CC − Φ−1 CC∥∥�ΦCi − ΦCi∥2 2 + ∥�Φ−1 CC − Φ−1 CC∥M2 + M∥�ΦiC − ΦiC∥2 � ∥�ΦCi − ΦCi∥2 + M � + M2∥�ΦCi − ΦCi∥2 = |Φii − �Φii| + ∥�Φ−1 CC − Φ−1 CC∥∥�ΦCi − ΦCi∥2 2 + ∥�Φ−1 CC − Φ−1 CC∥M2 + M∥�ΦCi − ΦCi∥2 2 + M2∥�ΦCi − ΦCi∥2 + M2∥�ΦCi − ΦCi∥2 ≤ |Φii − �Φii| + ∥�Φ−1 CC − Φ−1 CC∥∥�ΦCi − ΦCi∥2 2 + M2∥�Φ−1 CC − Φ−1 CC∥ + M∥�ΦCi − ΦCi∥2 2 + 2M2∥�ΦCi − ΦCi∥2. The above expression can be bounded above if we can bound the three errors, ∥�Φii−Φii∥ = ϵi, ∥�ΦAA − ΦAA∥ = ϵA, and ∥�Φ−1 CC − Φ−1 CC∥ = ϵCinv. Simplifying the above expression, |f(i, C, ω) − �f(i, C, ω)| ≤ |Φii − �Φii| + ∥�Φ−1 CC − Φ−1 CC∥∥�ΦCi − ΦCi∥2 2 + M2∥�Φ−1 CC − Φ−1 CC∥ + M∥�ΦCi − ΦCi∥2 2 + 2M2∥�ΦCi − ΦCi∥2 ≤ ϵi + ϵCinvϵ2 A + 2M2ϵCinv + Mϵ2 A + 2M2ϵA ≤ ϵi + ϵCinv(ϵ2 A + 2M2) + 3M2ϵA ≤ ϵi + 3M2ϵCinv + 3M2ϵA Pick ϵi = 3M2ϵCinv = 3M2ϵA = ϵ/3. Then |f(i, C, ω) − �f(i, C, ω)| < ϵ. From Section 5.8 in Horn and Johnson (2012), ∥ΦCC − �ΦCC∥ ≤ ∥ΦCC∥∥Φ−1 CC∥−1∥�Φ−1 CC − Φ−1 CC∥ M2 1 − M2 ∥�Φ−1 CC−Φ−1 CC∥ ∥Φ−1 CC∥ , ≤ M4∥�Φ−1 CC − Φ−1 CC∥ 1 − M∥�Φ−1 CC − Φ−1 CC∥ ≤ ϵ =⇒ ∥�Φ−1 CC − Φ−1 CC∥ ≤ ϵ M4 + Mϵ ≤ ϵ M4 . 20Therefore, to guarantee that ∥�Φ−1 CC − Φ−1 CC∥ < ϵ, it is sufficient to guarantee that ∥�ΦCC − ΦCC∥ < ϵ since M ≥ 1. Rewriting Corollary 4.3, P � |Φii − �Φii| ≥ ϵ � ≤ e− ϵ2n 128M2 +6, (19) P � ∥ΦAA − �ΦAA∥ > ϵ � ≤ e− ϵ2n 128M2 +6(q+1), and (20) P � ∥ΦCC − �ΦCC∥ > ϵ � ≤ e− ϵ2n 128M2 +6q, ∀ϵ ≥ 0. (21) Plugging these bounds in the above expressions gives the concentration upper bound P � |f(i, C, ω) − �f(i, C, ω) ≥ ϵ � ≤ P � |Φii − �Φii| ≥ ϵ/3 � + P � ∥�Φ−1 CC − Φ−1 CC∥ ≥ ϵ/(9M2) � + P � ∥�ΦAA − ΦAA∥ ≥ ϵ/(9M2) � ≤ P � |Φii − �Φii| ≥ ϵ/3 � + P � ∥�ΦCC − ΦCC∥ ≥ ϵM2/(9) � + P � ∥�ΦAA − ΦAA∥ ≥ ϵ/(9M2) � ≤ e � − ϵ2n 1152M2 +6 � + e � − ϵ2M2n 10368 +6q � + e � − ϵ2n 10368M6 +6(q+1) � ≤ c0e � − ϵ2n 10368M6 +6(q+1) � . E Lower bound: Proof of Theorem 5.1 E.1 Density function in frequency domain Consider an autoregressive (AR) model ˘x(k) = T1 � l=0 ˘H(l)˘x(k − l) + ˘e(k), ∀k ∈ Z, (22) where ˘e(k) = [˘e1(k), . . . , ˘en(k)]T , �x(k) = [˘x1(k), . . . , ˘xn(k)]T , ˘ei is a stochastic process such that the Fourier transform e(ω) exists. To understand the problem, let us assume ˘e(k) ∼ N(0, σ2I) for k = 0, . . . , T2, i.i.d. and zero otherwise. i.e., ˘x(k) is non zero only for k = 210, . . . , T1 + T2. Then e(ω) = T2 � k=0 ˘e(k)e−jωk = T2 � k=0 ˘e(k) cos(ωk) − j T2 � k=0 ˘e(k) sin(ωk). =⇒ cov(e(ω)) = E{e(ω)e∗(ω)} = E      T2 � k1=0 ˘e(k1)e−jωk1     T2 � k2=0 ˘e(k2)e−jωk2   ∗   = E      T2 � k1=0 T2 � k2=0 ˘e(k1)[˘e(k2)]∗e−jωk1ejωk2      = E �� T2 � k=0 ˘e(k)[˘e(k)]∗ �� = T2 � k=0 E {˘e(k)[˘e(k)]∗} = σ2I T2 � k=0 1. Thus, e(ω) ∼ N � 0, (T2 + 1)σ2I � . Now, consider the general LDS (3), with ˘e(k) ∼ N(0, σkI), k ∈ Z. Then as shown above, e(ω) ∼ N (0, Φe(ω)), where Φe(ω) = � k∈Z σkI. It follows that x(ω) = (I − H(ω))−1e(ω) and x(ω) ∼ N(0, Φx), where Φx(ω) = (I − H(ω))−1Φe(ω)((I − H(ω))−1)∗. Thus, the density function f(ω) is Gaussian, where the covariance matrix is the power spectral density matrix, a function of ω. E.2 Proof of lowerbound The proof is based on Generalized Fano’s inequality. Lemma E.1 (Generalized Fano’s method). Gao et al. (2022) Consider a class of obser- vational distribution F and a subclass F′ = {F1, . . . , Fr} ⊆ F with r distributions and the estimators �θ. Then inf �θ max F∈F E{I(θ(F) ̸= �θ)} ≥ αr 2 � 1 − nβr + log 2 log r � , where n is the number of samples, αr := max k̸=j I(θ(Fk) ̸= θ(Fj)), βr := max k̸=j KL(Fk||Fj), with KL(P||Q) := EP � log P Q � = EP [log P] − EP [log Q] being the KL divergence. Corollary E.2. Consider subclass of graphs G′ = {G1, . . . , Gr} ⊆ Gp,q, and let Hi be the distribution corresponding to a distinct Gi ∈ G′. Then, any estimator �G := � ω∈Ω �G(ω) of Gi 22is δ unreliable, inf �G sup Gi∈G′ P{(G(Hi) ̸= �G} ≥ δ, if n ≤ (1 − 2δ) log r − log 2 βr Therefore, building a lower bound involves finding a subclass that has 1) small βr and 2) large r. First, we can find an upper bound of r by upper bounding the number of directed graphs possible with at most q parents. Overall, p2 number of possible positions are there and at most pq many edges. The number of possible ways to choose k edges is �p2 k � . Thus r = pq � k=1 �p2 k � ≤ pq �p2 pq � ≲ pq(p/q)pq. Therefore, log r ≲ log(pq) + pq log(p/q) ≲ pq log(p/q) Similarly, it can be shown that log r ≳ pq log(p/q) Gao et al. (2022). Consider the LDS (3), with ˘e(k) ∼ N(0, σkI), i.i.d. across time. Then as shown in Appendix E.1, e(ω) ∼ N (0, Φe(ω)) and x(ω) ∼ N(0, Φx), where Φe(ω) = � k∈Z σkI and Φx(ω) = (I − H(ω))−1Φe(ω)((I − H(ω))−1)∗. Ensemble A: Consider all possible DAGs in G′ with i.i.d. Gaussian exogenous distribu- tion such that Φe(ω) exists. For the two distributions, Fk and Fj such that for any ω ∈ Ω, Fk(ω) ∼ N(0, Φ(k)(ω)) and Fj(ω) ∼ N(0, Φ(j)(ω)), KL(Fk(ω)||Fj(ω)) = 1 2 � EFj(ω)[x∗(ω)[Φ(k)(ω)]−1x(ω)] − EFj[x∗(ω)[Φ(j)(ω)]−1x(ω)] � = 1 2 � EFj(ω)[tr(x(ω)[Φ(k)(ω)]−1x∗(ω))] − p � = 1 2 � tr([Φ(k)(ω)]−1Φ(j)(ω)) − p � ≤ 1 2 �� ∥[Φ(k)(ω)]−1∥2 F ∥Φ(j)(ω)∥2 F − p � ≤ 1 2 � pM2 − p � ≤ (M2 − 1)p Therefore one of the lower bounds is inf �G sup G∈G′ P{(G ̸= �G)} ≥ δ, if n ≤ (1 − 2δ)pq log(p/q)) − log 2 (M2 − 1)p ≲ q log(p/q) M2 − 1 . Ensemble B: Here, we consider graphs in Hp,q(β, σ, M) (recall the definition from 3.1.1) with a single edge u −→ v with Hvu(ω) = β for every ω ∈ Ω, i.e. constant matrix. For LDS with i.i.d. Gaussian noise with PSD matrix Φx that satisfies this condition, H is such that Hvu ̸= 0 and Hij = 0 otherwise. Here, the total number of graphs, r = 2 �p 2 � = (p2 − p) ≈ p2 23Notice that [Φ−1 x ]ij = (Iij − Hij − H∗ ji + �p k=1 H∗ kiHkj)/σ. Thus (ignoring ω), [Φ−1 x ]uv = −H∗ vu+�p k=1 H∗ kuHkv σ = −H∗ vu σ = −β/σ and [Φ−1 x ]ij = 0 if i, j ̸= u, v. Then, x∗Φ−1 x x = � ij x∗ i [Φ−1 x ]ijxj = 1 σ � p � i=1 |xi|2(1 + � k |Hki|2) + x∗ u[Φ−1 x ]uvxv + x∗ v[Φ−1 x ]vuxu � = 1 σ  � i̸=u |xi|2 + (1 + |Hvu|2)|xu|2 − 2βℜ{x∗ uxv}   = 1 σ �� i |xi|2 + β2|xu|2 − 2βℜ{x∗ uxv} � = 1 σ  � i̸=v |xi|2 + |xv − βxu|2   Therefore, KL(F uv||F jk) = EF uv � log F uv − log F jk� = 1 2σEF uv � |xv|2 + |xv − βxu|2 − |xk|2 − |xk − βxj|2� = 1 2σ � β2σ + EF uv � β2|xj|2 − 2βℜ{x∗ jxk} �� Considering all the cases of (u, v) vs (j, k) it can be shown that KL(F uv||F jk) ≤ β2 + β4/2 Gao et al. (2022). Thus, n ≳ log p β2+β4/2 gives the second lower bound. The lower bound follows by combining ensembles A and B. References S. Basu and G. Michailidis. Regularized estimation in sparse high-dimensional time series models. The Annals of Statistics, pages 1535–1567, 2015. J. M. Bower and D. Beeman. The book of GENESIS: exploring realistic neural models with the GEneral NEural SImulation System. Springer Science & Business Media, 2012. W. Chen, M. Drton, and Y. S. Wang. On causal discovery with an equal-variance assumption. Biometrika, 106(4):973–980, 2019. W. Cui, X. Zhang, and Y. Liu. Covariance matrix estimation from linearly-correlated gaussian samples. IEEE Transactions on Signal Processing, 67(8):2187–2195, 2019. R. Dahlhaus. Graphical interaction models for multivariate time series. Metrika, 51:157–172, 2000. H. Doddi, D. Deka, S. Talukdar, and M. Salapaka. Efficient and passive learning of networked dynamical systems driven by non-white exogenous inputs. In International Conference on Artificial Intelligence and Statistics, pages 9982–9997. PMLR, 2022. 24M. K. S. Faradonbeh, A. Tewari, and G. Michailidis. Finite time identification in unstable linear systems. Automatica, 96:342–353, 2018. M. Fiecas, C. Leng, W. Liu, and Y. Yu. Spectral analysis of high-dimensional time series. Electronic Journal of Statistics, 13(2):4079–4101, 2019. N. Friedman and Z. Yakhini. On the sample complexity of learning bayesian networks. In Proceedings of the Twelfth international conference on Uncertainty in artificial intelligence, pages 274–282, 1996. M. Gao, W. M. Tai, and B. Aragam. Optimal estimation of gaussian dag models. In In- ternational Conference on Artificial Intelligence and Statistics, pages 8738–8757. PMLR, 2022. M. Ghil, M. R. Allen, M. D. Dettinger, K. Ide, D. Kondrashov, M. E. Mann, A. W. Robertson, A. Saunders, Y. Tian, F. Varadi, and P. Yiou. Advanced spectral methods for climatic time series. Reviews of Geophysics, 40(1):3–1–3–41, 2002. doi: 10.1029/2000RG000092. URL https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2000RG000092. A. Ghoshal and J. Honorio. Information-theoretic limits of bayesian network structure learn- ing. In Artificial Intelligence and Statistics, pages 767–775. PMLR, 2017a. A. Ghoshal and J. Honorio. Learning identifiable gaussian bayesian networks in polynomial time and sample complexity. Advances in Neural Information Processing Systems, 30, 2017b. A. Ghoshal and J. Honorio. Learning linear structural equation models in polynomial time and sample complexity. In A. Storkey and F. Perez-Cruz, editors, Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, volume 84 of Proceedings of Machine Learning Research, pages 1466–1475. PMLR, 09–11 Apr 2018. URL https://proceedings.mlr.press/v84/ghoshal18a.html. R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, USA, 2nd edition, 2012. ISBN 0521548233. Y. S. Kim, S. T. Rachev, M. L. Bianchi, I. Mitov, and F. J. Fabozzi. Time series analysis for financial market meltdowns. Journal of Banking & Finance, 35(8):1879–1891, 2011. A. Lamperski. Non-asymptotic pointwise and worst-case bounds for classical spectrum esti- mators. arXiv preprint arXiv:2303.11908, 2023. P.-L. Loh and M. J. Wainwright. High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity. Advances in neural information processing sys- tems, 24, 2011. D. Materassi and G. Innocenti. Unveiling the connectivity structure of financial networks via high-frequency analysis. Physica A: Statistical Mechanics and its Applications, 388(18): 3866–3878, 2009. ISSN 0378-4371. doi: https://doi.org/10.1016/j.physa.2009.06.003. URL https://www.sciencedirect.com/science/article/pii/S0378437109004324. D. Materassi and M. V. Salapaka. On the problem of reconstructing an unknown topology via locality properties of the wiener filter. IEEE Transactions on Automatic Control, 57 (7):1765–1777, July 2012. ISSN 0018-9286. doi: 10.1109/TAC.2012.2183170. 25D. Materassi and M. V. Salapaka. Reconstruction of directed acyclic networks of dynamical systems. In 2013 American Control Conference, pages 4687–4692. IEEE, 2013. G. Park. Identifiability of additive noise models using conditional variances. The Journal of Machine Learning Research, 21(1):2896–2929, 2020. G. Park and G. Raskutti. Learning quadratic variance function (qvf) dag models via overdis- persion scoring (ods). J. Mach. Learn. Res., 18(224):1–44, 2017. J. Peters and P. B¨uhlmann. Identifiability of gaussian structural equation models with equal error variances. Biometrika, 101(1):219–228, 2014. C. J. Quinn, N. Kiyavash, and T. P. Coleman. Directed information graphs. IEEE Transac- tions on Information Theory, 61(12):6887–6909, 2015. doi: 10.1109/TIT.2015.2478440. S. Shimizu, P. O. Hoyer, A. Hyv¨arinen, A. Kerminen, and M. Jordan. A linear non-gaussian acyclic model for causal discovery. Journal of Machine Learning Research, 7(10), 2006. M. Simchowitz, H. Mania, S. Tu, M. I. Jordan, and B. Recht. Learning without mixing: Towards a sharp analysis of linear system identification. In Conference On Learning Theory, pages 439–473. PMLR, 2018. J. Songsiri and L. Vandenberghe. Topology selection in graphical models of autoregressive processes. The Journal of Machine Learning Research, 11:2671–2705, 2010. P. Stoica, R. L. Moses, et al. Spectral analysis of signals, volume 452. Pearson Prentice Hall Upper Saddle River, NJ, 2005. S. Talukdar, D. Deka, M. Chertkov, and M. Salapaka. Topology learning of radial dynamical systems with latent nodes. In 2018 Annual American Control Conference (ACC), pages 1096–1101. IEEE, 2018. S. Talukdar, D. Deka, H. Doddi, D. Materassi, M. Chertkov, and M. V. Salapaka. Physics informed topology learning in networks of linear dynamical systems. Automatica, 112: 108705, 2020. ISSN 0005-1098. doi: https://doi.org/10.1016/j.automatica.2019.108705. A. Tank, N. J. Foti, and E. B. Fox. Bayesian structure learning for stationary time series. In Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence, pages 872–881, 2015. M. S. Veedu, D. Harish, and M. V. Salapaka. Topology learning of linear dynamical systems with latent nodes using matrix decomposition. IEEE Transactions on Automatic Control, Early Access:1–1, 2021. doi: 10.1109/TAC.2021.3124979. M. J. Wainwright. Random matrices and covariance estimation, page 159–193. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019. doi: 10.1017/9781108627771.006. X. Yang, T. Xu, P. Jia, H. Xia, L. Guo, L. Zhang, and K. Ye. Transportation, germs, culture: a dynamic graph model of covid-19 outbreak. Quantitative Biology, 8:238–244, 2020. D. Zhang and W. B. Wu. Convergence of covariance and spectral density estimates for high- dimensional locally stationary processes. The Annals of Statistics, 49(1):233–254, 2021. 26O. Zuk, S. Margel, and E. Domany. On the number of samples needed to learn the cor- rect structure of a bayesian network. In Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence, pages 560–567, 2006. 27
Everything, Everywhere All in One Evaluation: Using Multiverse Analysis to Evaluate the Influence of Model Design Decisions on Algorithmic Fairness Jan Simson Florian Pfisterer Christoph Kern A vast number of systems across the world use algorithmic decision making (ADM) to (partially) automate decisions that have previously been made by humans. When designed well, these systems promise more objective decisions while saving large amounts of resources and freeing up human time. However, when ADM systems are not designed well, they can lead to unfair decisions which discriminate against societal groups. The downstream effects of ADMs critically depend on the decisions made during the systems’ design and implementation, as biases in data can be mitigated or reinforced along the modeling pipeline. Many of these design decisions are made implicitly, without knowing exactly how they will influence the final system. It is therefore important to make explicit the decisions made during the design of ADM systems and understand how these decisions affect the fairness of the resulting system. To study this issue, we draw on insights from the field of psychology and introduce the method of multiverse analysis for algorithmic fairness. In our proposed method, we turn implicit design decisions into explicit ones and demonstrate their fairness implications. By combining decisions, we create a grid of all possible “universes” of decision combinations. For each of these universes, we compute metrics of fairness and performance. Using the resulting dataset, one can see how and which decisions impact fairness. We demonstrate how multiverse analyses can be used to better understand variability and robustness of algorithmic fairness using an exemplary case study of predicting public health coverage of vulnerable populations for potential interventions. Our results illustrate how decisions during the design of a machine learning system can have surprising effects on its fairness and how to detect these effects using multiverse analysis. 1 arXiv:2308.16681v1 [stat.ML] 31 Aug 20231 Introduction Across the world, more and more decisions are being made with the support of machine learning (ML) and algorithms; so called algorithmic decision making (ADM). Examples of such systems can be found in finance for loan approvals (Mukerjee et al. 2002), the labor market for hiring decisions or filtering resumes (Faliagka, Ramantas, and Tzimas 2012) and the criminal justice system to assess risks of recidivism (Angwin et al. 2016). While these systems are very promising when designed well, raising hopes of more accurate and objective decisions, their impact can be quite the opposite when designed wrongly. There are many examples of ADM systems discriminating against people (Mehrabi et al. 2021). One prominent example was the robodebt system, where the Australian government used an algorithm to detect potential social security overpayments. Due to serious flaws in the design of the system it often overestimated debts and put the burden on the accused to prove the contrary (Henriques-Gomes 2023). Other examples include the Dutch childcare benefits system using an ADM system that was much more likely to accuse immigrants of having committed fraud (International 2021). These fairness problems often occur because algorithms replicate biases in the underlying training data. However, biases can be amplified throughout the machine learning pipeline depending on how exactly data is processed and turned into outputs (Kern et al. 2021; Rodolfa, Saleiro, and Ghani 2020). Unfortunately, no single straightforward method exists to prevent biases in the machine learning pipeline (Agrawal et al. 2021). Understanding how modeling decisions interact with fairness is therefore a prerequisite for effectively mitigating unintended outcomes in practice. A systematic mapping of design decisions to fairness outcomes can critically guide the model selection process as multiple models may achieve similar accuracy, but can considerably differ in their fairness properties (Black, Raghavan, and Barocas 2022). As a result, preventing algorithms from introducing new or reinforcing existing biases requires careful study and evaluation of the – often implicit – decisions made while designing a machine learning system. To facilitate this objective in a systematic and efficient way, we introduce the method of multiverse analysis for algorithmic fairness. Multiverse analyses were introduced to psychology with the intent to improve reproducibility and create more robust research (Steegen et al. 2016). We adapt this methodology across domains to work in the context of machine learning with a focus on evaluating metrics of algorithmic fairness. In the following, we present a generalizable approach of using multiverse analysis to estimate the effect of decisions during the design of a machine learning or ADM system on fairness outcomes. We demonstrate the feasibility of this approach using a case study of predicting public health coverage in US census data. We provide modular source code to allow streamlined adaptation of the proposed method in other use cases and contexts. 1.1 Multiverse Analysis Multiverse analyses were first introduced in psychology by Steegen et al. (2016) in response to the reproducibility crisis affecting the field (OPEN SCIENCE COLLABORATION 2015). 2The goal of this analysis type is to investigate the invariance of results to researchers’ analysis decisions. Specifically, when analyzing a dataset, researchers make many implicit and explicit choices (Simmons, Nelson, and Simonsohn 2011), often without the option of confirming whether a choice is correct or incorrect. This leads to many plausible scenarios when analyzing data, as one traverses a garden of forking paths (Gelman and Loken 2014), where each fork corresponds to a decision. The multitude of these scenarios becomes especially evident when multiple researchers analyze the same data, coming to staggeringly different results (Breznau et al. 2022). Multiverse analysis focuses on the pre-processing steps applied to a dataset: Steps such as selecting the observations and predictor variables to include in a dataset or scaling and binning their values. Based on the different decisions made and paths taken when pre-processing a dataset, analysts will end up with one of many possible datasets for the actual analysis. In a multiverse analysis, the goal is to make this variation explicit by using the complete grid of decisions and their options to generate all plausible datasets. Using all potential datasets, a multiverse analysis re-runs the analysis on each of them to receive the distribution of results instead of a single result point (Figure 1, Steps 1 - 3). We adapt this methodology for the machine learning context with a special focus on using it to generate insights on metrics of algorithmic fairness. Besides multiverse analyses, a highly related type of analysis emerged around the same time in the specification curve analysis (Simonsohn 2018). Similarly to a multiverse analysis, a specification curve analysis uses the complete grid of possible decision combinations to estimate the variability of research findings. This data is used to create a specification curve, a graph displaying the distribution of the effect size or coefficient of interest using a single curve. All decision combinations are ordered by the metric creating a single, monotonically increasing curve. The actual decisions are one-hot encoded and displayed as a binary rug below the curve, clearly identifying all options that produced a certain value on the curve. Both multiverse analysis and specification curve analysis focus on bringing the diversity in pre-processing (and beyond) to light and we incorporated the essential ideas from both approaches. 1.2 Multiverse Analysis for Algorithmic Fairness In our proposed adaptation of multiverse analysis for algorithmic fairness, one starts by compiling a list of all potentially relevant decisions that are being made during the design of a particular system. We differentiate between different kinds of decisions in this context: (1) decisions which are already made explicitly with a consideration of their different options e.g. choice of model and its hyperparameters, and (2) decisions which are made explicitly, but without any consideration for alternatives e.g. log-transforming an income column because it is common practice. In a multiverse analysis, the goal is to turn both types of decisions into completely explicitly made decisions and evaluate their impacts. There are also decisions which may initially not even be considered as such e.g. modifying classification cutoffs post- hoc due to external constraints. Conducting a multiverse analysis invites reflection on the 3Identify plausible decisions 1 Generate multiverse 2 Calculate importance of decisions 4 Examine important decisions in detail 5 Traverse universes 3 Figure 1: Steps to conduct a multiverse analysis for algorithmic fairness. Steps 1 - 3 apply to multiverse analyses in general, whereas steps 4 - 5 are unique to multiverse analyses for algorithmic fairness. 4modeling pipeline such that implicit decisions may surface and are turned into explicit ones. One of the key differences in the present analysis compared to a classic multiverse analysis is that we will evaluate machine learning systems, whereas classical multiverse analyses will typically evaluate the outcomes of null-hypothesis-significance-tests (NHST) across analysis choices. While many of the decision points apply to any machine learning system (e.g., choice of algorithm, how to preprocess certain variables, cross-validation splits), many of them are also domain-specific (e.g., coding of certain variables, how to set classification thresholds, how fairness is operationalized). We focus on decisions made during the pre-processing of data, in line with the original approach of multiverse analysis (Steegen et al. 2016). We extend this approach to incorporate decisions relevant to algorithmic fairness, particularly with regard to protected attributes and the translation of predictions into real-world actions or interventions. Similarly to a classical multiverse analysis, we use the resulting garden of forking paths to generate a grid of all possible universes of decisions combinations, the multiverse. For each of these universes, we compute the resulting fairness metric of the machine learning system and collect it as a data point. Based on the resulting dataset of decision universes and corresponding fairness scores, we evaluate how individual decisions influence the fairness metric and explore the most important decisions in more detail (Figure 1). 1.2.1 Related Research Existing work has described the effects of specific pre-processing or modeling decisions in isolation, such as the influence of different imputation methods (Caton, Malisetty, and Haas 2022), of the model architecture and hyperparameters (Sukthanker et al. 2022) on fairness in different contexts. Multiverse analyses have also been used to model the performance distribution in hyperparameter-space (Bell et al. 2022), but not yet for analyzing algorithmic fairness. The field of hyperparameter-optimization (HPO) (Feurer and Hutter 2019; Bischl et al. 2023), tries to optimize the process of tuning machine learning model hyperparameters. This field typically focuses on optimizing algorithm performance by employing efficient search strategies. These search strategies allow achieving higher performance without requiring exploration of the complete hyperparameter space. However, such search patterns usually focus on finding the optimal configuration and usually yield non-i.i.d. optimization traces. This makes them unsuitable for assessing the influence and robustness of any particular decisions. While algorithmic fairness and the importance of pre-processing are also explored (Perrone et al. 2021), they are typically only examined on the sidelines. Here, we draw on insights and methodology from the field of HPO, in particular the functional analysis of variance (FANOVA) (Hooker 2007; Hutter, Hoos, and Leyton-Brown 2014) to allow a more interpretable and efficient analysis of the results from the multiverse analysis. Our focus, however, is on uncovering and systematically exploring variation induced by the different decisions instead of finding the setting that optimizes fairness metrics. 51.3 Case Study We illustrate how multiverse analysis can enrich the machine learning fairness toolkit using a case study of predicting public health insurance coverage. Accurate and fair prediction of public health insurance coverage in the United States is an important issue as access to healthcare is quite expensive in the US, with the country spending almost 16% of its gross domestic product per capita on healthcare (Ortiz-Ospina and Roser 2017). Whether or not someone is covered by health insurance has large effects on their health and financial situation: People with insurance have better self-reported health, have more preventative doctors appointments, improved depression outcomes, and fewer personal bankruptcies (Sommers, Gawande, and Baicker 2017). Given the complexity of US public healthcare system it is easy for people to fall through its cracks, missing the chance for coverage due to not understanding how it works or by not fulfilling all necessary requirements. To combat this, one might want to set up either financial or informational interventions targeting low-income individuals who are at risk of not being covered by public health insurance. Determining who and who not to target with such an intervention could be facilitated with an ADM system. However, given the vulnerability of the target population and ethical implications of distributing interventions, fair and well-calibrated predictions are of the utmost importance in such a scenario. We implement our case study using the ACSPublicCoverage dataset (Ding et al. 2021). We use this particular dataset as it is rich enough for us to implement a wide range of design decisions and because many other well-established datasets used in the fairness literature suffer from non-trivial quality issues (Ding et al. 2021; Fabris et al. 2022; Bao et al. 2022): UCI Adult (Kohavi and Becker 1996), the most popular dataset in the fairness literature (Fabris et al. 2022), uses an arbitrary threshold of 50,000 USD to create a binary task of income prediction. This threshold has been shown to greatly influence the accuracy of predictions in certain groups, biasing measures of algorithmic fairness and threatening external validity (Ding et al. 2021). The ACSPublicCoverage dataset is one of the datasets which have been specifically developed in response to the issues in UCI Adult. Here, we operationalize having public insurance coverage as being covered by either Medicare, Medicaid, Medical Assistance (or any kind of government-assistance plan for those with low incomes or a disability) or Veterans Affairs Health Care, following the official Guidance for Health Insurance Data Users from the US Census Bureau (Bureau 2021). In line with the original task setup by Ding et al. (2021), only individuals with an age below 65 years and a yearly income of less than $30,000 are examined. Low-income households are also more likely to rely on public health insurance (Keisler-Starkey and Bunch 2022). As there are no clear guidelines on how to set up an ADM system within this context (as would be the case in heavily regulated contexts such as credit scoring) one faces a multitude of decisions when designing a solution for this task, each of which can govern how bias is fed into 6the final system. A multiverse analysis for algorithmic fairness requires developers to make these design decisions explicit and shows their fairness implications in the present context. 2 Methodology 2.1 Fairness Metric While our proposed analysis works with multiple different fairness metrics, it requires one to choose a primary metric for analysis. For the present case study we used equalized odds difference (Agarwal et al. 2018; Hardt et al. 2016) as the primary fairness metric, as it quantifies the degree to which a system’s predictions are equally good across different groups defined by a protected attribute. Equalized odds require both the true positive rate (TPR) and the false positive rate (FPR) of a system’s predictions to be equal across all groups of the protected attribute. Values of the equalized odds difference can range from 0 to 1. A value of 0 corresponds to a perfectly fair model according to the metric, whereas a value of 1 corresponds to a completely unfair model. We use the implementation from the fairlearn package (Bird et al. 2020) to calculate the metric, where the differences in both the true positive rate and the false positive rate are calculated and the larger of the two is used as the metric. We consider race as the protected attribute in our case study given the persisting racial disparities in various domains, including health outcomes, in the US (Obermeyer et al. 2019). 2.2 Decision Space When conducting a multiverse analysis, the first step is the identification of relevant and plausible decisions to be made. Based on the literature on data science and machine learning workflows (Kuhn and Johnson 2020; Le Quy et al. 2022) we identified five distinct categories to structure and guide the identification of decisions: Data Selection, Preprocessing, Modeling, Evaluation and Post-Hoc decisions (Table 1). For this case study, we considered 10 distinct and orthogonal design decisions. Each of these decisions has two to five unique choice options, leading to a total of N = 122880 combinations of decisions or universes. An overview of the decisions and their respective options can be seen in Table 1, and a detailed description of each is provided below. We consider decisions roughly in the order they would be made during a typical analysis and sort them under the list of typical decision categories. As there is a potentially infinite list of possible decisions to consider, the present list is not intended to be exhaustive, but rather to highlight the most common and important categories of decisions one may typically encounter when designing a machine learning or ADM system. We also deliberately set the focus on decisions where alternative options are typically not considered or ones that are not identified as decisions at all. When adapting the methodology to a new system, this list can serve as an inspiration, however, one must also consider the domain-specific decisions unique to each applied problem. 7Table 1: Overview of the typical decision categories, the actual decisions examined in the case study and their respective options used to construct the multiverse. Decisions and Options Examined in Case Study Category Decision Options Data Selection Exclude Features (1) none; (2) race; (3) sex; (4) race-sex Exclude Subgroups (1) keep-all; (2) drop-smallest-1; (3) drop-smallest-2; (4) keep-largest-2; (5) drop-other Preprocessing Scale (1) do-not-scale; (2) scale Preprocess Age (1) none; (2) bins-10; (3) quantiles-3; (4) quantiles-4 Preprocess Income (1) none; (2) bins-10000; (3) quantiles-3; (4) quantiles-4 Encode Categorical (1) one-hot; (2) ordinal Modeling Model (1) logreg; (2) rf; (3) gbm; (4) elasticnet Evaluation Stratify Split (1) none; (2) target; (3) protected-attribute; (4) both Fairness Grouping (1) majority-minority; (2) separate Post-Hoc Cutoff (1) raw-0.5; (2) quantile-0.1; (3) quantile-0.25 2.2.1 Data Selection 2.2.1.1 Excluding Variables as Predictors (Exclude Features) Selecting features to train a model on presents a critical design decision. In the ADM context, it can be required to exclude certain protected features (such as sex/gender, race, ethnicity) as predictors due to legal constraints when designing a machine-learning system. However, as prominently shown in various studies this does not necessarily lead to increased fairness, as the protected attribute is often correlated with other (“legitimate”) features (Weerts 2021). We implement the following options for this decision in our case study: (1) use all features as predictors (incl. protected ones), (2) exclude race, the protected attribute in the case study, (3) exclude sex, a sensitive attribute and (4) exclude both race and sex as protected / sensitive attributes. 2.2.1.2 Excluding Subgroups of the Protected Attribute (Exclude Subgroups) When working with variables with an uneven distribution or very rare categories one may focus only on the most common groups, dropping data for smaller ones. This can be done to preserve the privacy of small groups or out of convenience to allow for an easier model interpretation downstream. However, the exclusion of subgroups of the population can potentially be harmful, with discriminatory differences in downstream model predictions. While we decided to include this practice as a decision in our analysis to (1) raise awareness of the issue and (2) represent 8the effects of the practice in our analysis, this should not be taken as an endorsement of this practice. We try to capture the implications of this practice via the attribute race. We therefore chose to include a decision of dropping certain groups from the training data based on their prevalence. To accurately compute the fairness metric, groups were not dropped from the test data used for evaluation. We include six options for this decision, with the fraction of discarded data in brackets1: (1) to keep all groups (0.00%), (2) to drop the smallest group (0.01%), (3) to drop the two smallest groups (0.33%), (4) to keep the two largest groups (27.45%) and (5) to drop the category “Some Other Race alone” specifically (15.81%). 2.2.2 Preprocessing 2.2.2.1 Scaling of Continuous Variables (Scale) It is common to scale continuous variables during preprocessing, centering them on a mean of µ = 0 and standard deviation of σ = 1 (also referred to as z-scaling). Scaling may be particularly advisable if kernel-based learners are used as it typically leads to improved performance for such models. We include two options for this decision: (1) to keep continuous variables as they are and (2) to scale continuous variables. 2.2.2.2 Binning of Continuous Variables (Preprocess Age, Preprocess Income) Another common practice is binning continuous variables, i.e., turning continuous variables into ordinal variables with discrete categories. The reasons to do this are plentiful: To deal with outliers, to address privacy concerns, or for a more tangible interpretation to name a few. We provide two distinct and orthogonal decisions here on whether or how to bin the variables age and income. We include four options for the variable age: (1) perform no binning, (2) bin into bins of size 10, (3) bin into three evenly sized quantiles, (4) bin into four evenly sized quantiles. Likewise, we include four options for the variable income: (1) perform no binning, (2) bin into bins of size 10, 000, (3) bin into three evenly sized quantiles, (4) bin into four evenly sized quantiles. 1Fractions of discarded training data are only reported for a non-stratified train-test split, as there are only very slight differences in the fraction of discarded data based on stratification strategy. 92.2.2.3 Encoding of Categorical Variables (Encode Categorical) Another common pre-processing step includes transforming categorical variables into a numerical format. When doing this one typically has two options: (1) One-hot (or dummy) coding each variable with K categories into K (or K − 1) new binary variables or (2) ordinally encoding each variable by assigning an integer value from 1 to K for each category. Ordinal encoding is only applicable, however, for variables with a natural ordering. For all ordinal variables (including continuous variables that have been binned), we include both options. Any variables without a natural ordering are always one-hot coded. 2.2.3 Modeling 2.2.3.1 Model Type (Model) A major choice when designing any statistical or machine learning system is which model type one decides to use. While there is a large number of potential models to explore here, we focused on the most commonly used ones in the context of ADM in the literature. We note that hyperparameter selection has shown to have an impact on fairness, but choose to focus on the simple case, as HPO has already been studied elsewhere (Perrone et al. 2021). We therefore support the following model types as options for this decision: (1) logistic regression (Cox 1958), (2) random forest (Ho 1995), (3) gradient boosted machine (Friedman 2001), and (4) elastic net (Zou and Hastie 2005). 2.2.4 Evaluation 2.2.4.1 Stratification of Train-Test Split (Stratify Split) Training and test sets are often created by simple random splitting of the full dataset. It can be beneficial, however, to perform this split conditional on certain groupings to ensure equal representation of all labels within both the train and test sets. We include four options for this decision: (1) to not stratify at all, using a completely random split instead, (2) to stratify using the target variable (public coverage), (3) to stratify using the protected attribute (race) and (4) to stratify using a combination of both variables. 2.2.4.2 Grouping of Protected Attribute (Fairness Grouping) When working with a fairness metric, it is necessary to specify for which groups of the protected attribute it is calculated. The present case study uses race as the protected attribute. For protected attributes with more than two categories, however, multiple comparisons can be 10computed. Depending on the application context one may, e.g., simplify these groups into the largest group (majority) and all other groups (minority)2. An important note regarding this decision is that it changes how the fairness metric is calculated: with two groups, the difference between those two groups is calculated, however, with more than two groups all possible differences between group-pairs are calculated and the largest difference between them is used (the standard procedure for this metric). Naturally, this has a strong influence on the fairness metric. We therefore conduct many of our later analyses separately for each of its options. We include two options for this decision: (1) The fairness metric is computed between the majority group and minority group and (2) the fairness metric is computed as the maximum of the metric as computed between all groups of the protected attribute (race)3. 2.2.5 Post-Hoc 2.2.5.1 Cutoff for Final Classification (Cutoff) At the end of the ML pipeline, the prediction models’ (risk) scores can be used to classify new observations based on a pre-specified classification threshold. By default a threshold of 0.5 would be used with every score equal or above classified as 1 (having coverage) and everything below as 0 (not having coverage). Actual interventions, however, are often based on the ranked list of scores such that (costly) interventions are targeted at the top X percent with the highest risk. With real-world scenarios often coming with resource-bound restrictions, one may for example only be able to provide an intervention for, say, 10% or 25% of the most in-need in the population. These real-world restrictions are typically not taken into account in fairness evaluations, despite having potentially devastating implications. We therefore also consider different cutoff values for the final predictions of the system. We support the following options for this decision: (1) use the default raw cutoff value of 0.5, (2) only treat the lowest 0.1 quantile as not having coverage, (2) only treat the lowest 0.25 quantile as not having coverage. 2.3 Analysis We examined the overall variation of the fairness metric and the degree to which it is explained by the outlined decisions. To estimate the importance of individual decisions and their interactions, 2Majority group: ‘White alone’; Minority group: ‘Asian alone’, ‘Two or More Races’, ‘Some Other Race alone’, ‘Black or African American alone’, ‘American Indian alone’, ‘Native Hawaiian and Other Pacific Islander alone’, ‘American Indian and Alaska Native tribes specified; or American Indian or Alaska Native, not specified and no other races’ and ‘Alaska Native alone’. 3This corresponds to the default behavior in the fairlearn library. 11we used a functional analysis of variance (FANOVA) (Hooker 2007; Hutter, Hoos, and Leyton- Brown 2014). Based on the results of the FANOVA, we examined the most important decisions and interactions in detail. As it can be costly to iterate over the complete multiverse grid, we also examined the feasibility of running our analyses on smaller subsets of the data and comparing results with analysis performed on the full dataset. The implementation for running a FANOVA by Hutter, Hoos, and Leyton-Brown (2014) used here has also been demonstrated to work well for assessing hyperparameter importance with only a subset of data from the hyperparameter grid available. 2.4 Technology Analyses were conducted using Python Version 3.8 (Van Rossum and Drake 2009) and pipenv (P. M. Team 2017) for reproducibility. The Python package scikit-learn (Pedregosa et al. 2011) was used for pre-processing and fitting of models, pandas (team 2020) for loading and modification of data, folktables (Ding et al. 2021) for retrieval of data, fairlearn (Bird et al. 2020) for computation of fairness metrics, fANOVA (Hutter, Hoos, and Leyton-Brown 2014) for calculation of variable importance and papermill (contributors 2017) for parameterized computation of decision universes. This reproducible document was generated using quarto (Allaire et al. 2022), R (R. C. Team 2022) Version 4.2, the R packages from the tidyverse (Wickham et al. 2019) and ggpubr (Kassambara 2023) for generation of figures. The source code of the analyses and this publication is available at https://github.com/reliable-ai/fairml- multiverse. 3 Results 3.1 Distribution of Metrics The multiverse analysis in our case study produced a total of N = 122880 values of the fairness metric across all possible analysis choices. When examining the distribution of the fairness metric across the multiverse of decisions, the large variation of the fairness metric becomes apparent, with values spanning the entire possible range of the metric from 0 to 1 (Figure 2). Overall performance of the resulting models was moderate with raw accuracies between 0.419 and 0.722. Performance and the fairness metric were almost uncorrelated with a Pearson correlation of r = 0.082. Raw performance varied largely based on the decision Cutoff, with three large clusters of similar performance (Figure 3). Within these three clusters of almost equal performance there was a large variance of the fairness metric, highlighting the opportunity to optimize algorithmic fairness without sacrificing performance. The fairness metric is calculated differently depending on how the groupings of the protected attribute are made: for the majority-minority grouping only a single comparison can be made, 12Wider distribution around 0.533 for separate−grouping Metric closely centered around 0.047 for majority−minority grouping 0 10000 20000 30000 0.00 0.25 0.50 0.75 1.00 Fairness Metric (Equalized Odds Difference) Count Fairness Grouping majority−minority separate Figure 2: Using a different grouping of the protected attribute strongly influences the fairness metric. Distribution of fairness metric (equalized odds difference) split by grouping of the protected attribute with vertical lines corresponding to mean values. Lower values on the fairness metric indicate smaller TPR and FPR differences across groups. 13Plateaus of similar performance with large variation in fairness 0.5 0.6 0.7 0.00 0.25 0.50 0.75 1.00 Fairness Metric (Equalized Odds Difference) Performance (Accuracy) 1000 2000 3000 Count 0 10000 20000 Cutoff quantile−0.1 quantile−0.25 raw−0.5 Figure 3: Performance and fairness are largely unrelated with three plateaus of low variance in performance, but high variance in fairness. Distribution of overall performance (raw accuracy) and fairness metric (equalized odds difference) across all multiverses. The marginal histogram shows performance for different options of the Cutoff decision. A marginal histogram of the fairness metric can be seen in Figure 2. 14whereas the largest value from multiple comparisons is used with the separate grouping. Because of this, we examined this decision first and in isolation. The implications of the two groupings on the fairness metric are visible in Figure 2: The majority-minority grouping leads to a roughly normal distribution of low values on the fairness metric, centered around a mean of M = 0.047 (SD = 0.030). The separate grouping on the other hand leads to much wider distribution of fairness values centered around a mean value of M = 0.533 (SD = 0.291). Since the decision has such a strong influence on the fairness metric and interacts with many of the other decisions, we chose to conduct further analyses only within these two groupings to differentiate results more clearly and allow for easier interpretation as well as visualization. The following analyses are therefore conducted once for the majority-minority grouping and once for the separate grouping of the protected attribute; the sample size in each arm of the following analyses is thus n = 61440. 3.2 Importance of Decisions We conducted two FANOVAs (Hooker 2007) as described in Hutter, Hoos, and Leyton-Brown (2014) to assess the importance of decisions on the fairness metric. This analysis decomposes the overall variance of the fairness metric into the fractions which are explained by each decision. These variance decompositions are used to assess the relative importance of decisions. Moreover, the FANOVA also allows computing explained variance for interactions of decisions. This is highly useful, as the overall interaction space between decisions is quite large with 511 possible (interaction and main) effects. Using the resulting importance values from the FANOVA, one can see which decisions are associated with a high variation in fairness scores, whether it be by themselves or in conjunction with others. This allows assessing the most consequential decisions on a one-by-one case. Table 2 contains a ranked list of the most important decisions and decision interactions in our case study alongside their respective importance. When using a majority-minority grouping the most important decision is whether and which subgroups one chooses to exclude during model training, whereas the most important decisions for the separate grouping is how the stratification of the train-test split is performed. The cutoff value used for the final predictions is important in both cases and often has effects in conjunction with other decisions as well. Specifically the interaction of the chosen cutoff value with the stratification strategy is highly important when using the separate grouping, accounting for more than 30% of the variance in the fairness metric. 3.2.0.1 Examining Individual Decisions We analyzed the three most important decisions or decision-interactions per grouping approach to further illustrate the methodology and how one would explore the results of the analysis. The results also highlight why one should investigate the decisions in a detailed manner and not just pick the most-fair and highest-performing universe’s model. 15Table 2: The 10 most important decisions or decision interactions and their relative importance for both groupings of the protected attribute. (a) Most important decisions for majority-minority grouping. Effect Type Decision / Interaction of Decisions Importance Std. Deviation main ExcludeSubgroups 0.254 0.001 main Cutoff 0.211 0.001 2-way int. ExcludeFeatures × ExcludeSubgroups 0.110 0.001 2-way int. Cutoff × ExcludeSubgroups 0.082 0.000 3-way int. Cutoff × ExcludeFeatures × ExcludeSubgroups 0.027 0.000 2-way int. Model × PreprocessIncome 0.020 0.000 main Model 0.017 0.000 2-way int. ExcludeSubgroups × Model 0.016 0.000 main ExcludeFeatures 0.014 0.000 3-way int. Model × PreprocessIncome × Scale 0.012 0.000 (b) Most important decisions for separate grouping. Effect Type Decision / Interaction of Decisions Importance Std. Deviation main StratifySplit 0.375 0.001 2-way int. Cutoff × StratifySplit 0.313 0.001 main Cutoff 0.082 0.000 4-way int. Cutoff × ExcludeFeatures × Model × StratifySplit 0.007 0.000 3-way int. Cutoff × Model × StratifySplit 0.007 0.000 3-way int. Cutoff × Model × PreprocessIncome 0.007 0.000 2-way int. Model × PreprocessIncome 0.006 0.000 2-way int. ExcludeFeatures × Model 0.006 0.000 3-way int. Model × PreprocessIncome × Scale 0.006 0.000 2-way int. Cutoff × PreprocessIncome 0.005 0.000 16For the majority-minority grouping, Exclude Subgroups was the most influential decision. Examining the decision leads to surprising results: One might expect that the exclusion of any subgroups in the training data will reduce fairness (as measured by the equalized odds difference), however, results show that dropping the “Other” group from the training data actually lead to slightly fairer models on average (Figure 4 A). Whether one should actually drop the “Other” group from the training data should still be evaluated very carefully, however. The least fair models were produced when retaining only the largest two groups of the protected attribute and dropping all others. The second most important decision was which Cutoff value to choose when making the final predictions. Here, the default value of 0.5 typically lead to higher values of the fairness metric (Figure 4 B). It is important, to be aware of the importance of this decision, as one might be forced by practical circumstances to use a different cutoff after deployment in which case model fairness would need to be re-evaluated. Last, examining the interplay between Exclude Subgroups and Exclude Features in Figure 4 (C), illustrates how the effect of keeping only the two largest groups of race or dropping the “Other” group is greatly amplified when race is included as a predictive feature. For the separate grouping, the effects are slightly more indistinct as computing the maximum fairness metric between all comparisons leads to more volatile results. The decisions Stratify Split, Cutoff and their interaction account for all three of the most important decisions with this grouping. When examining the decision separately, it can be seen how stratifying by the target variable leads to noticeably less fair models (Figure 4 D, most important) and how the raw cutoff value of 0.5 is suddenly not leading to the most fair models anymore (Figure 4 F, third most important). The effects of both variables become most clear, however, when examining their interaction, which was identified as explaining almost as much variance as the most important decision. While using a cutoff value corresponding to the top 10% quantile leads to the least fair model when stratifying by the target variable it surprisingly leads to the fairest models when using any other stratification strategy (Figure 4 E, second most important). 3.3 Scaling Multiverse Analysis for Algorithmic Fairness To assess the feasibility of running the multiverse analysis on a smaller subset of the grid, we also conducted the FANOVAs on different subsamples of the collected multiverse dataset. Specifically, we ran the analysis on random subsets of 1%, 5%, 10% and 20% of the data and calculated the correlation of variance decomposition or importance values with the FANOVA estimated on the full multiverse dataset. The estimates of variance decomposition are highly skewed, with a few highly important decisions and a very larger number of very low-importance decisions. We therefore calculated both, the Pearson correlation which is more sensitive to correlations of the more important decisions and the Spearman rank-correlation which is also sensitive to decisions with low importance estimates. To assess the consistency of this approach we computed the FANOVA on each subsample 50 times and calculated the correlation with the results from the full multiverse dataset every time. 17drop−other keep−all drop−smallest−1 drop−smallest−2 keep−largest−2 0.0 0.1 0.2 Fairness Metric Exclude Subgroups A raw−0.5 quantile−0.1 quantile−0.25 0.00 0.05 0.10 0.15 0.20 0.25 Fairness Metric Cutoff B 0.03 0.04 0.04 0.04 0.10 0.04 0.04 0.04 0.04 0.05 0.04 0.04 0.04 0.04 0.05 0.03 0.04 0.04 0.04 0.10 drop−other keep−all drop−smallest−1 drop−smallest−2 keep−largest−2 race−sex race sex none Exclude Features Exclude Subgroups 0.04 0.06 0.08 Fairness Metric C protected−attribute both none target 0.0 0.4 0.8 Fairness Metric Stratify Split D 0.69 0.68 0.35 0.56 0.21 0.22 0.25 0.99 0.41 0.50 0.58 0.96 protected−attribute both none target raw−0.5 quantile−0.25 quantile−0.1 Cutoff Stratify Split 0.4 0.6 0.8 Fairness Metric E raw−0.5 quantile−0.1 quantile−0.25 0.0 0.4 0.8 Fairness Metric Cutoff F Figure 4: Detailed analysis of most important decisions. Visualization of the fairness metric depending on the three most important decision / decision combinations and their respective options for majority-minority (A-C) and separate (D-F) groupings of the protected attribute. 18When calculating the Pearson correlation, the resulting mean correlation coefficient ranged from ¯r1% = 0.989 (SD = 0.004) at 1% to ¯r20% ≥ 0.999 (SD = 0) at 20% for the majority-minority grouping and ¯r1% = 0.997 (SD = 0.002) at 1% to ¯r20% ≥ 0.999 (SD = 0) at 20% for the separate grouping. Spearman rank-correlations were also high, but lower than the Pearson correlation coefficients and more inconsistent (Figure 5), which indicates that using sparse data to estimate the importance of decisions works well for the most important decisions and less-so to identify nuances between the less-important decisions. The resulting Spearman rank-correlation mean coefficients ranged from ¯ρ1% = 0.517 (SD = 0.032) at 1% to ¯ρ20% = 0.860 (SD = 0.012) at 20% for the majority-minority grouping and ¯ρ1% = 0.530 (SD = 0.03) at 1% to ¯ρ20% = 0.937 (SD = 0.007) at 20% for the separate grouping. 0.5 0.6 0.7 0.8 0.9 1.0 1% 5% 10% 20% Fraction of Multiverse Used for Analysis Correlation Fairness Grouping majority−minority separate Correlation Method Pearson Spearman Figure 5: Conducting the analysis with smaller subsets of the complete multiverse leads to similar results. Correlations of variance decomposition / importance estimates between full dataset and random subsets of different sizes. Random subsets were drawn 50 times with points corresponding to mean correlations and lines to +/- 1 standard deviation. Pearson correlation coefficients are consistently higher than Spearman correlation coefficients, indicating better estimation of high-importance decisions. 4 Discussion We demonstrate how multiverse analysis for algorithmic fairness provides a useful new method for evaluating the robustness of machine learning and ADM systems with respect to decisions along the modeling pipeline and their implications for algorithmic fairness. Our method 19provides a promising new methodology which can empower analysts to better understand how their decisions affect a system’s fairness and which decisions matter. We highlight the importance of making decisions during model design explicitly rather than implicitly. By applying this new methodology in a use case of predicting public health care coverage, we demonstrate the feasibility of this approach and show which decisions from our list of decisions affect fairness the most: Unsurprisingly the grouping of the protected attribute used for calculating the fairness metric has considerable effect on algorithmic fairness. More interestingly, this grouping does also affect the influence of almost all other decisions. Besides the grouping, we showed that the cutoff value used for making final decisions has a significant effect on the fairness metric, a decision often implemented post-hoc after model deployment without any consideration of fairness. We also observe that the exclusion of certain subgroups of the protected attribute during training affects fairness downstream, especially when the protected attribute is kept as a feature. Surprisingly, we also saw that the stratification strategy used for the train-test split had strong effects on the fairness metric. When interpreting the results from a multiverse analysis for algorithmic fairness, one should evaluate results with care and strictly avoid merely selecting the combination of decisions with the best fairness metric. Results should be seen as an indication of how susceptible the fairness of the model is to design decisions and which decisions warrant closer examination. Results from the analysis can also be used to guide the search of new options for the most important decisions. Final choices regarding the design of the system should be made using a combination of empirical results from the multiverse analysis and practical as well as ethical considerations within the context of the use case. This can often mean that decisions may be made that do not correspond to the optimal value in the fairness metric for a decision. The main goal of a multiverse analysis for algorithmic fairness is to facilitate making educated and explicit decisions. We recommend including complete results from the analysis alongside the final system. As we explored only a single use-case, we do not make any generalizable claims regarding the importance of any particular decisions, beyond the fact that these decisions can matter and are worth investigating. Another limitation of this case-study is that we only examined 10 distinct decisions, with many plausible alternative decisions which could’ve been examined in their place or additionally. As there is an infinite space of decisions one may consider, we decided to draw the line at these 10 decisions for illustrative purposes. A successful adoption of multiverse analysis for algorithmic fairness in different use cases and reporting of results could help identify a more exhaustive list of the most important decisions across contexts. Potential concerns regarding the computational cost of conducting a multiverse analysis for algorithmic fairness are valid, but can be addressed as we demonstrate that estimates of importance are robustly detected for important decisions even when exploring only 1% of the full multiverse. We encourage the use of the method during the design of future machine learning or ADM systems and provide an overview of the most important areas of decisions to guide analysts when adapting multiverse analysis for algorithmic fairness in their own context. We further provide a non-exhaustive list of exemplary decisions to serve as inspiration to identify potentially 20relevant decisions and a modular implementation that makes adoption to different use-cases easy. We posit that results from a multiverse analysis for algorithmic fairness can critically inform discussions between developers and stakeholders and advise joint reflections on the ultimate design of ADM systems. By successfully adapting multiverse analysis across disciplines we also highlight the feasibility of adapting methods and techniques between disciplines and hope that this will inspire further cross-discipline pollination of ideas and methodologies between research fields. 5 References Agarwal, Alekh, Alina Beygelzimer, Miroslav Dudík, John Langford, and Hanna Wallach. 2018. “A Reductions Approach to Fair Classification.” Agrawal, Ashrya, Florian Pfisterer, Bernd Bischl, Francois Buet-Golfouse, Srijan Sood, Jiahao Chen, Sameena Shah, and Sebastian Vollmer. 2021. “Debiasing Classifiers: Is Reality at Variance with Expectation?” https://doi.org/10.48550/arXiv.2011.02407. Allaire, J. J., Charles Teague, Carlos Scheidegger, Yihui Xie, and Christophe Dervieux. 2022. Quarto. https://doi.org/10.5281/zenodo.5960048. Angwin, Julia, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. “Machine Bias.” ProPublica, May, 254264. https://www.propublica.org/article/machine-bias-risk- assessments-in-criminal-sentencing. Bao, Michelle, Angela Zhou, Samantha Zottola, Brian Brubach, Sarah Desmarais, Aaron Horowitz, Kristian Lum, and Suresh Venkatasubramanian. 2022. “It’s COMPASlicated: The Messy Relationship Between RAI Datasets and Algorithmic Fairness Benchmarks.” https://doi.org/10.48550/arXiv.2106.05498. Bell, Samuel J., Onno P. Kampman, Jesse Dodge, and Neil D. Lawrence. 2022. “Modeling the Machine Learning Multiverse.” https://doi.org/https://doi.org/10.48550/arXiv.2206.0598 5. Bird, Sarah, Miroslav Dudík, Richard Edgar, Brandon Horn, Roman Lutz, Vanessa Milan, Mehrnoosh Sameki, Hanna Wallach, Kathleen Walker, and Allovus Design. 2020. “Fairlearn: A Toolkit for Assessing and Improving Fairness in AI.” Bischl, Bernd, Martin Binder, Michel Lang, Tobias Pielok, Jakob Richter, Stefan Coors, Janek Thomas, et al. 2023. “Hyperparameter Optimization: Foundations, Algorithms, Best Practices, and Open Challenges.” WIREs Data Mining and Knowledge Discovery 13 (2). https://doi.org/10.1002/widm.1484. Black, Emily, Manish Raghavan, and Solon Barocas. 2022. “Model Multiplicity: Opportunities, Concerns, and Solutions.” Breznau, Nate, Eike Mark Rinke, Alexander Wuttke, Hung H. V. Nguyen, Muna Adem, Jule Adriaans, Amalia Alvarez-Benjumea, et al. 2022. “Observing Many Researchers Using the Same Data and Hypothesis Reveals a Hidden Universe of Uncertainty.” Proceedings of the National Academy of Sciences 119 (44): e2203150119. https://doi.org/10.1073/pnas.22031 50119. 21Bureau, US Census. 2021. “ACS Health Insurance Coverage Recoding Programming Code.” https://www.census.gov/topics/health/health-insurance/guidance/programming- code/acs-recoding.html. Caton, Simon, Saiteja Malisetty, and Christian Haas. 2022. “Impact of Imputation Strategies on Fairness in Machine Learning.” Journal of Artificial Intelligence Research 74 (September). https://doi.org/10.1613/jair.1.13197. contributors, nteract. 2017. Papermill: Parametrize and Run Jupyter and Nteract Notebooks. https://github.com/nteract/papermill. Cox, David R. 1958. “The Regression Analysis of Binary Sequences.” Journal of the Royal Statistical Society: Series B (Methodological) 20 (2): 215232. Ding, Frances, Moritz Hardt, John Miller, and Ludwig Schmidt. 2021. “Retiring Adult: New Datasets for Fair Machine Learning,” 13. Fabris, Alessandro, Stefano Messina, Gianmaria Silvello, and Gian Antonio Susto. 2022. “Algorithmic Fairness Datasets: The Story so Far.” Data Mining and Knowledge Discovery, September. https://doi.org/10.1007/s10618-022-00854-z. Faliagka, Evanthia, Kostas Ramantas, and Giannis Tzimas. 2012. “Application of Machine Learning Algorithms to an Online Recruitment System.” Feurer, Matthias, and Frank Hutter. 2019. “Hyperparameter Optimization.” In, edited by Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren, 3–33. The Springer Series on Challenges in Machine Learning. Cham: Springer International Publishing. https://doi.org/10.1007/978- 3-030-05318-5_1. Friedman, Jerome H. 2001. “Greedy Function Approximation: A Gradient Boosting Machine.” The Annals of Statistics 29 (5): 1189–1232. https://doi.org/10.1214/aos/1013203451. Gelman, Andrew, and Eric Loken. 2014. “The Statistical Crisis in Science.” American Scientist 102 (6): 460. Hardt, Moritz, Eric Price, Eric Price, and Nati Srebro. 2016. “Equality of Opportunity in Supervised Learning.” Henriques-Gomes, Luke. 2023. “Robodebt: Five Years of Lies, Mistakes and Failures That Caused a $1.8bn Scandal.” The Guardian, March. https://www.theguardian.com/australia- news/2023/mar/11/robodebt-five-years-of-lies-mistakes-and-failures-that-caused-a-18bn- scandal. Ho, Tin Kam. 1995. “Random Decision Forests.” In, 1:278282. IEEE. Hooker, Giles. 2007. “Generalized Functional ANOVA Diagnostics for High-Dimensional Functions of Dependent Variables.” Journal of Computational and Graphical Statistics 16 (3): 709–32. https://www.jstor.org/stable/27594267. Hutter, Frank, Holger Hoos, and Kevin Leyton-Brown. 2014. “International Conference on Machine Learning.” In, 754–62. PMLR. https://proceedings.mlr.press/v32/hutter14.htm l. International, Amnesty. 2021. “Xenophobic Machines.” https://www.amnesty.org/en/wp- content/uploads/2021/10/EUR3546862021ENGLISH.pdf. Kassambara, Alboukadel. 2023. Ggpubr: ’Ggplot2’ Based Publication Ready Plots. https: //CRAN.R-project.org/package=ggpubr. Keisler-Starkey, Katherine, and Lisa N Bunch. 2022. “Health Insurance Coverage in the United 22States: 2021 - Appendix Table C3.” https://www.census.gov/content/dam/Census/library/ publications/2022/demo/p60-278.pdf. Kern, Christoph, Ruben L. Bach, Hannah Mautner, and Frauke Kreuter. 2021. “Fairness in Algorithmic Profiling: A German Case Study.” https://doi.org/10.48550/arXiv.2108.0413 4. Kohavi, Ronny, and Barry Becker. 1996. “Adult Data Set.” UCI Machine Learning Repository 5: 2093. Kuhn, Max, and Kjell Johnson. 2020. Feature engineering and selection: a practical approach for predictive models. Chapman & Hall/CRC data science series. Boca Raton London New York: CRC Press, Taylor & Francis Group. www.feat.engineering. Le Quy, Tai, Arjun Roy, Vasileios Iosifidis, Wenbin Zhang, and Eirini Ntoutsi. 2022. “A Survey on Datasets for Fairness-Aware Machine Learning.” WIREs Data Mining and Knowledge Discovery 12 (3): e1452. https://doi.org/10.1002/widm.1452. Mehrabi, Ninareh, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. 2021. “A Survey on Bias and Fairness in Machine Learning.” ACM Computing Surveys 54 (6): 115:1115:35. https://doi.org/10.1145/3457607. Mukerjee, Amitabha, Rita Biswas, Kalyanmoy Deb, and Amrit P. Mathur. 2002. “Multi–objective Evolutionary Algorithms for the Risk–return Trade–off in Bank Loan Management.” International Transactions in Operational Research 9 (5): 583–97. https://doi.org/10.1111/1475-3995.00375. Obermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” Science 366 (6464): 447–53. https://doi.org/10.1126/science.aax2342. OPEN SCIENCE COLLABORATION. 2015. “Estimating the Reproducibility of Psychological Science.” Science 349 (6251): aac4716. https://doi.org/10.1126/science.aac4716. Ortiz-Ospina, Esteban, and Max Roser. 2017. “Healthcare Spending.” Our World in Data, June. https://ourworldindata.org/financing-healthcare. Pedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning in Python.” Journal of Machine Learning Research 12: 28252830. Perrone, Valerio, Michele Donini, Muhammad Bilal Zafar, Robin Schmucker, Krishnaram Kenthapadi, and Cédric Archambeau. 2021. “AIES ’21: AAAI/ACM Conference on AI, Ethics, and Society.” In, 854–63. Virtual Event USA: ACM. https://doi.org/10.1145/3461 702.3462629. Rodolfa, Kit T., Pedro Saleiro, and Rayid Ghani. 2020. “Bias and Fairness.” In, 2nd ed. Chapman; Hall/CRC. Simmons, Joseph P., Leif D. Nelson, and Uri Simonsohn. 2011. “False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant.” Psychological Science 22 (11): 1359–66. https://doi.org/10.1177/095679761141 7632. Simonsohn, Uri. 2018. “Two Lines: A Valid Alternative to the Invalid Testing of U-Shaped Rela- tionships With Quadratic Regressions.” Advances in Methods and Practices in Psychological Science 1 (4): 538–55. https://doi.org/10.1177/2515245918805755. 23Sommers, Benjamin D., Atul A. Gawande, and Katherine Baicker. 2017. “Health Insurance Coverage and Health — What the Recent Evidence Tells Us.” New England Journal of Medicine 377 (6): 586–93. https://doi.org/10.1056/NEJMsb1706645. Steegen, Sara, Francis Tuerlinckx, Andrew Gelman, and Wolf Vanpaemel. 2016. “Increasing Transparency Through a Multiverse Analysis.” Perspectives on Psychological Science 11 (5): 702–12. https://doi.org/10.1177/1745691616658637. Sukthanker, Rhea, Samuel Dooley, John P. Dickerson, Colin White, Frank Hutter, and Micah Goldblum. 2022. “On the Importance of Architectures and Hyperparameters for Fairness in Face Recognition.” https://doi.org/10.48550/arXiv.2210.09943. Team, Pipenv Maintainer. 2017. Pipenv: Python Development Workflow for Humans. https: //github.com/pypa/pipenv. Team, R Core. 2022. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. team, The pandas development. 2020. Pandas-Dev/Pandas: Pandas. Zenodo. https: //doi.org/10.5281/zenodo.3509134. Van Rossum, Guido, and Fred L. Drake. 2009. Python 3 Reference Manual. Scotts Valley, CA: CreateSpace. Weerts, Hilde J. P. 2021. “An Introduction to Algorithmic Fairness.” https://doi.org/10.48550 /arXiv.2105.05595. Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse.” https://joss.theoj.o rg. Zou, Hui, and Trevor Hastie. 2005. “Regularization and Variable Selection via the Elastic Net.” Journal of the Royal Statistical Society. Series B (Statistical Methodology) 67 (2): 301–20. https://www.jstor.org/stable/3647580. 24
Branches of a Tree: Taking Derivatives of Programs with Discrete and Branching Randomness in High Energy Physics Michael Kagan1, ∗ and Lukas Heinrich2, ∗ 1SLAC National Accelerator Laboratory 2Technical University of Munich We propose to apply several gradient estimation techniques to enable the differentiation of programs with discrete randomness in High Energy Physics. Such programs are common in High Energy Physics due to the presence of branching processes and clustering-based analysis. Thus differentiating such programs can open the way for gradient based optimization in the context of detector design optimization, simulator tuning, or data analysis and reconstruction optimization. We discuss several possible gradient estimation strategies, including the recent Stochastic AD method, and compare them in simplified detector design experiments. In doing so we develop, to the best of our knowledge, the first fully differentiable branching program. I. INTRODUCTION Gradient-based optimization methods are at the core of many modern successes in Machine Learning (ML) and Artificial Intelligence (AI), especially Deep Learning. The development and application of these ML methods in High Energy Physics (HEP) has similarly enabled large performance improvements in a wide array of tasks, such as particle reconstruction, fast simulation, and parame- ter inference (for recent topical reviews, see e.g. [1–8]). Gradient-based optimization methods rely on automatic differentiation (AD) [9, 10], an algorithmic way to effi- ciently compute the derivatives of numerical software. AD is a general tool that can be applied to scientific software beyond ML, such as simulators and inference algorithms, and used for optimizing the parameters of this software. The broader application of AD to numerical software, potentially mixed with ML components, is often referred to as Differentiable Programming (DP). For instance, combining AD-enabled HEP software with ML can lead to optimizable hybrid physics-AI models with built-in physics knowledge from the HEP software, such as AI- augmented / AI-guided simulation and reconstruction, or analysis-by-synthesis inference methods with simula- tors in the loop [11–13]. Using such AD-integrated HEP software in ML models can be considered a complimen- tary approach to adding inductive bias into ML models through structure and architecture, such as symmetry equivariance and relational inductive bias. While interest is quickly growing to apply gradient- based optimization methods to a broader set challenges in HEP, such as detector design or end-to-end reconstruction, a major limitation has been the fact that standard AD can only compute derivatives of deterministic continuous functions or stochastic functions with reparametrized con- tinuous random variables [14–16]. Specifically, in HEP, many programs are both stochastic and rely on sampling discrete random variables or decisions, such as branching ∗ Corresponding authors contributed equally to this work: makagan@slac.stanford.edu,l.heinrich@tum.de points in parton showers, particle-material interactions, or clustering steps in jet building. Standard AD may not compute the desired derivative of such programs correctly, particularly when the discrete stochasticity depends on the parameter one aims to optimize (and thus differentiate with respect to). Instead, more careful consideration on how to compute the appropriate derivative is required. There are several methods for gradient-based optimiza- tion in programs with discrete randomness, which we ex- plore within the context of HEP applications. One method uses the score-based approach to estimating derivatives of expectations values [17], which has been examined sparsely within HEP and not for tasks such as detector design optimization. Recently, Arya et al. [18] proposed a new AD method for handling and composing programs with discrete randomness. Using these tools, we develop simplified differentiable HEP simulators,which nonetheless exhibit the critical behaviors which until now hampered progress, and case studies to examine the behavior of and the variance of these gradient estimators. A review of methods for computing derivatives of stochastic programs is found in Sec. II. Related work is discussed in Sec. III. Sec. IV presents applications and comparisons of different gradient estimators in HEP case studies, with an emphasis on detector design optimization. Contributions1: We introduce several methods to en- able differentiation through the discrete randomness to HEP programs. We show how the score function based approach can be used for design optimization in HEP. While the score function is often used outside of HEP for design optimization, and has been used for other tasks within HEP, it has not yet been explored within the context of HEP detector design optimization. We also introduce stochastic AD [18] to HEP and its ability to enable differentiable programming even in programs with discrete randomness. We provide the first application of stochastic AD to branching processes and in doing 1 Project code can be found at https://github.com/lukasheinrich/branches_of_a_tree/ arXiv:2308.16680v1 [stat.ML] 31 Aug 20232 1 def f(θ): 2 p = 0.5 3 b ∼ Bernoulli(p) 4 return g(θ) + b 1 def f(θ): 2 p = h(θ) 3 b ∼ Bernoulli(p) 4 return g(θ) + b FIG. 1: Assuming differentiable g : R → R and h : R → (0, 1), Left: Toy program without θ-dependence in the discrete stochastic behavior. As such, the derivative of the expected value of this program is dE[f(θ)] dθ = dg(θ) dθ and standard AD can correctly estimate this. Right: Toy program with θ-dependence in the discrete stochastic behavior through the Bernoulli parameter p. Standard AD would ignore this dependence, and the resulting derivative estimator would be the same as the program on the left. However the correct derivative should be dE[f(θ)] dθ = dg(θ) dθ + dh(θ) dθ . so we develop the first (to the best of our knowledge) differentiable branching program. To test these methods, we provide comparisons of gradient methods on detector design optimization case studies. II. REVIEW OF DIFFERENTIATION OF STOCHASTIC PROGRAMS In many HEP applications, a quantity of interest can be formulated as an expectated value of a function f(x, θ) over a parametrized density pθ(x): ¯f(θ) = Epθ(x)[f(x, θ)]. For optimizing such quantities one requires the gradients of these expectation values of stochastic programs, e.g. d dθEpθ(x)[f(x, θ)]. Importantly, the expected value of such stochastic programs may be continuous and differentiable, even when they depend on discrete randomness. For in- stance, the expected value of a Bernoulli random variable b ∼Bernoulli(θ) has the derivative d dθE[b] = d dθθ = 1. However, standard AD tools applied to such expecta- tions may not produce the desired result. For instance, Figure 1 shows two programs with discrete stochasticity. On the left, the discrete stochasticity does not depend on the parameter of differentiation θ, and standard AD will produce the correct derivative. On the right the discrete stochasticity depends on θ, standard AD will ignore this dependence and the resulting derivative will be incorrect as it will ignore this dependence. Handling these challenges requires more dedicated consideration on home to compute the appropriate derivative. We briefly review several approaches to gradient estimation below (see Ref. [19] for a detail review). Finite Differences (Numerical Differentiation): Fi- nite difference methods estimate derivatives by computing the difference between forward evaluations of a program and a perturbed version of the program, for instance: d dθEpθ(x)[f(x)] ≈ Epθ+ϵ(x)[f(x)] − Epθ(x)[f(x)] ϵ (1) Finite difference methods are prone to high vari- ance [10], and require large numbers of program eval- uations when θ is high dimensional. Central difference methods can reduce error. One large contributor to this large variance is that multiple independent evaluations of the program are used to estimate this gradient, introduc- ing separate stochastic evaluation paths of the program. Reparameterization Trick: In many cases, when sam- pling x ∼ pθ(x), we can smoothly reparametrize this sampling as z ∼ p(z) and x = g(z, θ), where p(z) is often a simple “base" distribution and g(·, θ) provides a differ- entiable, θ-dependent transformation of samples from the base to the desired distribution. For example, the normal distribution x ∼ N(µ, σ) is location-scale reparameteri- zable through z ∼ N(0, 1) and x = σz + µ ∼ N(µ, σ). This is particularly convenient for computing derivatives of expectation values of differentiable functions f(·), d dθEpθ(x)[f(x)] = d dθ � p(z)f(g(z, θ))dz = � p(z) df dg dg(z, θ) dθ dz (2) Many HEP simulators, which implement a structural causal model, can be considered as a reparametrization, mapping from noise variables to random variables x with physical meaning. However if the random variables x are discrete, the map is not differentiable, which limits the applicability of the reparametrization trick within a HEP context. Surrogate Methods: When a reparameterization is not possible, either because f(·) is non differentiable or pθ(x) does not admit a smooth reparameterization, surrogate methods can be used. In this case, an ML model S(z, θ), with z ∼ p(z) a chosen distribution, is trained to mimic the stochastic program. As such, surrogate methods try enable reparameterization though a ML model and thus enable differentiation, for instance: d dθEpθ(x)[f(x)] ≈ d dθ � p(z)S(z, θ)dz = � p(z)dS(z, θ) dθ dz (3) The quality of this derivative estimator will depend significantly on the quality of the surrogate model as an approximation of the original program. Moreover, the surrogate will learn a smooth approximation of non- differentiable elements of the program, but how this ap- proximation is performed and if bias is introduced is difficult to asses. Score function: When the parameter dependence of a differentiable distribution pθ(x) is known and differen-3 tiable with respect to the parameters, one can compute: d dθEpθ(x)[f(x)] = � pθ(x)d log pθ(x) dθ f(x)dx = Epθ(x) �d log pθ(x) dθ f(x) � (4) where d dθ log pθ(x) is known as the score function. This gradient estimator is also known as REINFORCE [17], and is commonly used in reinforcement learning. The benefit of this approach is that the f(x) does not need to be differentiable, only pθ(x) must be differentiable with respect to θ. As such, discrete random variables can be used in the computation of f(x). Control Variates: Score function based gradient esti- mates often have a large variance, which can make tasks like optimization with gradient descent slow and more difficult. A control variate, c(x, θ), can be subtracted from f(x) in Eqn. 4 to reduce the variance of the es- timator as long as it does not bias the estimator, i.e. as long as Epθ(x) � d log pθ(x) dθ c(x, θ) � = 0. Noting that Epθ(x) � d log pθ(x) dθ � = 0, one way to find a control vari- ate is to choose a c(θ) which does not depend on x. A common control variate, often also called a baseline, is the function mean ¯fθ = � pθ(x)f(x); in practice ¯fθ is often estimated using the mean of a mini-batch. We will use this baseline for the experiments in Sec. IV. More details on variance reduction methods for score based gradient estimation can be found in Ref. [20]. Proposal Distributions: In some case, we may not know or have access to pθ(x), for example when g(θ) is a simulator with parameters as input and sampling is done internally within the program. One approach can be to imagine the input to the program as a sample from a proposal distribution θ ∼ πψ(θ), where ψ are the parameters of the proposal distribution. For instance one could choose a normal distribution N(ψ, 1) for the proposal. One would then aim to optimize the mean of this proposal, d dψ Eπψ(θ)[g(θ)] = Eπψ �d log πψ(θ) dψ g(θ) � (5) Stochastic AD: The stochastic derivative of the expected value of a function f(·) of a discrete random variable x ∼ pθ(x) has the form [18]: d dθEpθ(x)[f(x)] = Epθ(x,y)[δ + β � f(y) − f(x) � ] (6) where δ is the standard derivative ∂f/∂θ as computed with AD, and the second term corresponds to the effect of a change in θ on the sampling of the discrete random variable x. The weight β depends on the underlying sampling distribution and y is an alternative value of the discrete random variable. Conceptually, for discrete X with consecutive integer range one can understand this result through the lens of reparameterization. One can reparamaterize the discrete distribution via the inversion method, e.g.: ω ∼ U[0, 1] x = {X : ω ∈ [CDFpθ(X), CDFpθ(X + 1) ) } For example, if x is a Bernoulli random variable with pa- rameter θ, then one can reparameterize x = H(ω > 1 − θ) where H(·) is the Heaviside step function. The boundaries which define the set of values of ω that result in a value of X are now dependent on the parameters θ. A change in parameters changes the boundaries, and thus changes the probabilities of different outcomes x. The second term of the stochastic AD derivative accounts for the infinitesimal change in probabilities as the boundaries are changed as well as the alternative value of the program y that would result from such a change. Importantly, one can define this derivative at each program evaluation and at each stochastic sampling within the program, allowing for the development of composition rules and of forward mode automatic differentiation. For a more detailed discussion of Stochastic AD, see Ref. [18]. The expectation on the right hand side of Equation 6 is taken with respect to the joint distribution pθ(x, y), which is often denoted the coupling. The marginal distributions of this coupling must be the same as the original sampling distribution, e.g. � dypθ(x, y) = � dxpθ(x, y) = pθ(x) to ensure both the primal evaluation of the program and the alternative proceed under the normal operation of the program. As such, there is a distribution over possible alternative programs. In practice, out of all possible alternatives from all of the discrete samplings within a program, only one alternative is randomly chosen. This pruning process treats the set of alternatives as a categorical distribution with the probability of a given alternative being the weight of the alternative relative to the total event weight (computed using the composition rules). This alternative may occur in the middle of the program, and is then tracked in parallel to the primal until completion of the alternative program. One can then use this program alternative y for computing even gradients using Equation 6. Variance Reduction with Random Number Reuse: While the marginals of the coupling are fixed, one has consider- able flexibility to choose the correlation structure between x and y. This is important because once an alternative is determined from a discrete sampling within the program, the alternative program will be run to completion and thus may require additional sampling of discrete random variables. The more correlated are the evaluations of the alternative completion to that of the primal evaluation, the lower the variance of the gradient estimator may be. As downstream discrete samplings also occur in the pri- mal program, one can reuse the reparameterized sampling in the primal program, i.e. the ω values sampled for the inversion method. By reusing ω values, less additional4 variance is added to the alternative program then if the downstream discrete random variable were sampled inde- pendently. In this work for the experiments in Sec. IV, we use a first-in first-out (FIFO) approach, where at each time step we store ω values in the FIFO while iterating over branches (particle) in the primal, and then pull ω values from the FIFO while iterating over branches in the alternative. If additional ω values are needed by the alternative, they are sampled independently of the primal. III. RELATED WORK Automatic differentiation [9, 10], and its use in gradi- ent based optimization, is ubiquitous in ML, statistics, and applied math. AD uses the chain rule to evaluate derivatives of a function that is represented as a computer program. AD takes as input program code and produces new code for evaluating the program and derivatives. AD typically builds a computational graph, or a directed acyclic graph of mathematical operations applied to an input. Gradients are defined for each operation of the graph, and the total gradient can be evaluated from input to output, called forward mode, and from output to input, called reverse mode or backpropagation in ML. AD is backbone of ML / DP frameworks like TensorFlow [21], JAX [22], and PyTorch [23]. Significant work has been performed on developing Monte Carlo estimators for gradients in machine learning, as discussed in the recent review [19], and gradients of stochastic computation graphs [24]. More recently meth- ods such as Stochastic AD [18, 25] have been developed to target derivatives of programs with discrete stochastic behavior in a compositional way, as well as developing specific applications with dedicated variance reduction methods (e.g. developing coupling for these applications). Extensions to AD have recently been proposed for differ- entiating the expectations of stochastic programs [26, 27], and to account for parametric discontinuities [28]. Differentiable programming approaches have begun to be explored in HEP. Examples include histogram fitting in pyHF [29, 30], in analysis optimization in Neos [12, 31], for modeling parton distribution functions (pdf) used by matrix element generators [32, 33], and for developing dif- ferentiable matrix element simulations with MadJax [11]. Related to our work, Ref. [34] studies differentiating a parton shower, which focuses on the derivative of the primal shower program but does not examine differen- tiation through discrete randomness in such branching programs. Within cosmology, the Differentiable Universe Initiative is developing a differentiable simulation and analysis toolchain for comological data analyses [35]. Within HEP, score function gradient estimators have been used within the context of jet grooming [36] and hierarchical jet clustering [37]. To the best of our knowl- edge, this work is the first application to detector design optimization in HEP. On HEP detector design, surrogate based optimization methods have been developed and explored for particle detectors in Ref. [38]. Surrogate methods have also been applied to neutrino detector design [39]. Detector design optimization with standard AD tools and with surrogate methods is discussed in Ref. [40]. A branch-and-bound type algorithm is explored in Ref. [41]. IV. APPLICATIONS We present a series of applications in a simplified sim- ulation of particles interacting with detector material. In these experiments, we examine how different gradient estimation methods can be applied and compare their performance in terms of the variance of the estimators. A. Particle Interaction Simulator The simplified simulator in this work aims to capture the salient features of particle physics simulators that model the traversal of particles through matter but be simple enough allow reimplementation in a programming language of choice for a detailed study of various gradient estimation in a self-contained setting. Algorithm1 Simplified Particle Interaction Simulator Require: E0 : energy threshold Require: ϵ : energy loss at interaction Require: m(x, θ) : R3 × Rn → [0, 1] : material map Require: P = {pi = (⃗xi⃗pi, Ei)}: initial particle list Require: H = ∅: list of hits 1: function Simulate(P,θ) 2: while not all p ∈ P below threshold do 3: P ← ∅ ▷ list of surviving particles 4: for for all particles in P do 5: if Ei < E0 then 6: continue ▷ particle below thr. 7: end if 8: pi = propagate(pi) 9: ρE ← sample Ber(ρE|mθ(xi)) 10: if ρE then ▷ particle interacts 11: H ← H ∪ {xi} ▷ add position to hits 12: ρsplit ← sample Ber(ρsplit|mθ(xi)) 13: if ρsplit then ▷ particle splits 14: pL, pR ← split(pi) 15: P ← P ∪ {pR, pL} 16: else 17: Ei ← Ei − ϵ ▷ lose energy 18: P ← P ∪ {pi} 19: end if 20: else 21: P ← P ∪ {pi} 22: end if 23: end for 24: end while 25: return H 26: end function The simulator Sθ(p) models the stochastic evolution of5 (a) Single particle energy loss (b) Particle shower with splitting FIG. 2: Event Displays of a single particle energy loss (left) and a particle shower with particle splitting (right). A primal event is shown in purple, while an alternative event, as determined using Stochastic AD, is shown in yellow. The material map is shown in grey. particles through two main processes. A binary splitting process p0 → pL, pR that splits a parent particle momen- tum evenly across two child particles and a energy loss process E → E − ϵ. The probability of a particle interac- tion is modeled as a function of the material map m(x). The simulation is performed by fixed time steps, first propagating particles in their direction of travel, and then querying the material map to determine if an interaction occurs, and if so which type of interaction (i.e. splitting or energy loss). Pseudo-code for the simulator can be found in Alg. 1. The detector is simulated as a continuous material map mθ(x0, x1) which takes as input a position (x0, x1) and outputs an interaction probability. This interaction probability is dependent on detector parameters θ, and we will examine examples where derivatives with respect to θ are sought. Only a single detector parameter is used in the following experiments, which is the detector inner radius which will be denoted θR. With r = � x2 0 + x2 1, and ϕ = arctan x0 x1 , the material map is defined as: mθ(x, y) = 1 2mstart(r, θR) mΦ(r, ϕ) mR(r) mend(r, θR) (7) where mstart(r, θR) = 1 1 + e−β(r−θR) mΦ(r, ϕ) = 1 1 + eβ sin(ω(ϕ+2r)) mR(r) = 1 1 + eβ cos(ω(r−2)) mend(r, θR) = 1 1 + eβ(r−θR−Rmax) . The terms mstart(r, θR) and mend(r, θR) determine the inner and outer radius of the detector, respectively. The terms mΦ(r, ϕ) and mR(r) determine the segmentation in ϕ and r, respectively. The constants β, ω, and Rmax control the sharpness of the smooth material map, the segmentation frequency in the azimuthal direction, and the maximum depth of the detector, respectively. The parameter that will be optimized is θR, the inner radius of the detector. Example material maps can be seen in grey in the event displays of Figure 2, where the darkness of the shade of grey indicates the strength of interaction. B. Single Particle Energy Loss In the single particle energy loss setting, interactions which cause splitting are turned off, i.e. there are no show- ers. At each step, the particle interacts with the detector with a probability pEloss = m(x, θ) which is dependent on material map parameters θ. This probability is large is high density regions of detector material and small in low density regions. A Bernoulli distribution with parameter m(x, θ) is sampled at each time step to determine if the interaction occurs and, if so, the particle deterministically loses energy ϵ = 1 GeV. All particles are set to have initial energy of 25 GeV and when the particle energy falls below E0 = 0.5 GeV, the particle is stopped. An example event display can be seen in Fig. 2a, where the primal particle trajectory is seen in purple, the alternative trajectory determined with Stochastic AD is seen in yellow, and the material map is seen in grey. In this example, there is only one detector parameter6 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 parameter 0 2 4 6 8 10 Loss per event primal primal median poly. fit 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 parameter 15 10 5 0 5 10 15 20 Grad Numeric StochAD SCORB SCORE grad from fit StochAD Score w/ Baseline Score 15 10 5 0 5 10 15 g [X( )] Score Numeric 50 0 50 FIG. 3: For simulations of single particle energy loss: (Left) The loss function and various gradient estimators of the loss are shown as a function of the detector radius parameter. Sample primal evaluations of the loss are shown as markers, and the interquantile interval is shown in black. The red dashed line shows the derivative of an polynomial interpolation of the mean loss. (Middle) The mean and standard deviation of the numeric, Stochastic AD (STAD), score function (SCORE), and score function with baseline (SCORB) gradient estimators as a function of the detector radius parameter. The gradient of the polynomial interpolation of the mean loss is shown in dashed black. (Right) Box plot of the four variance estimators evaluated at parameter value θR = 2.5m, with the mean shown as a dashed line. θ ≡ R, the inner radius of the detector, and derivatives are computed with respect to R using several methods of estimating gradients. The mean squared error between the radial position of points of particle interaction and a target radius ¯RT is used as a loss function that one may minimize for the purposes of design optimization. In this example we set ¯RT = 2m. The loss as a function of the detector radius parameter can be seen on the left in Fig. 3. The loss from individual primal simulation samples can be seen in grey, the median loss and interquartile range in black, and a polynomial interpolation of the average loss. Even though the simu- lation is stochastic, and one can see the variations of the loss at each parameter in the grey points, there is a clear minimum to the loss function. The gradient estimators and their standard deviations, calculated over the 5000 simulation runs, can be seen as a function of the detector radius parameter in the middle in Fig. 3. The distributions of gradient estimators evaluated at the parameter value θR = 2.5m can be seen in the box plot on the right in Fig. 3. As expected, numerical derivatives have the largest standard deviation, though it should be noted that this can depend highly on the size of the finite different ϵ and the method for calculating the numerical derivative. Similarly, score function gradi- ent estimators without baseline shows a high standard deviation, especially at large radius parameter where the loss function has larger standard deviation over differ- ent simulation samples. The score with baseline has a much reduced standard deviation over the score function without baseline across all parameter values. Stochastic AD shows the smallest standard deviation of all estima- tors, likely owing to the ability to couple much of the alternative program evaluation to that of the primal up to the alternative branching point. Across all gradient estimators, the mean of the gradient estimator, shown in solid lines, are close to the gradient of the polynomial fit of the loss (which serves as a rough guide to the gradient of the expected loss) within one standard deviation. A numerical comparison of gradient estimator mean and variance can be found in Tab. I. C. Branching Shower In the branching shower example, the same material map, and thus interaction probability, as the single par- ticle energy loss simulation is used but if an interaction occurs, the particle is deterministically split into two daughter particles each with half the energy of the parent particle and with an opening angle of 0.1 radians. Initial particles are set to have starting energy of 25 GeV and when any particle energy falls below 0.5 GeV, the parti- cle is stopped. The same loss function as in the single particle energy loss example is used here. An example event display can be seen in Fig. 2b, where the primal particle shower is seen in purple, the alternative shower determined with Stochastic AD is seen in yellow, and the material map is seen in grey. The loss function and the standard deviation of the gradients, as functions of the detector radius parameter, can be seen on the left and middle, respectively, in Fig. 4. As in the single particle energy loss example, the nu- merical gradients are found to have the largest standard deviation of gradients, with the score function without baseline estimator having the second largest standard deviation. Notably, the score function with baseline esti-7 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 parameter 0 2 4 6 8 10 Loss per event primal primal median poly. fit 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 parameter 15 10 5 0 5 10 15 20 Grad Numeric StochAD SCORB SCORE grad from fit StochAD Score w/ Baseline Score 15 10 5 0 5 10 15 g [X( )] Score Numeric 50 0 50 FIG. 4: For simulations of particle showers with splitting: (Left) The loss function and various gradient estimators of the loss are shown as a function of the detector radius parameter. Sample primal evaluations of the loss are shown as markers, and the interquantile interval is shown in black. The red dashed line shows the derivative of an polynomial interpolation of the mean loss. (Middle) The mean and standard deviation of the numeric, Stochastic AD (STAD), score function (SCORE), and score function with baseline (SCORB) gradient estimators as a function of the detector radius parameter. The gradient of the polynomial interpolation of the mean loss is shown in dashed black. (Right) Box plot of the four variance estimators evaluated at parameter value θR = 2.5m, with the mean shown as a dashed line. mator is found to have the smallest standard deviation, slightly smaller than the Stochastic AD gradient esti- mator. Unlike the single particle example, the splitting shower has many program branching points which can create alternative outputs that are significantly different from the primal shower. In turn, this leads to a reduc- tion in the correlation between the primal and alternative showers and ultimately to an increase in the gradient estimator standard deviation. A comparison of the distri- bution of gradient estimators, at detector parameter value θR = 2.5m, can be seen on the right in Fig. 4. While the mean values (dotted lines) in each box agree well across estimators, the variance estimates as well as the tails are significantly better behaved for Stochastic AD and score function with baseline estimators. Similarly, a compari- son of the mean and standard deviation of the gradient estimators evaluated at the parameter value θR = 2.5m for both the single particle energy loss and the splitting shower can be found in Tab. I. Estimator E-loss Shower StochAD 3.17 ± 4.47 2.53 ± 6.37 Score w/ Baseline 3.01 ± 6.59 2.47 ± 4.42 Score w/o Baseline 2.68 ± 17.18 2.76 ± 12.20 Numerical 3.83 ± 139.96 2.43 ± 74.85 TABLE I: Gradient estimator mean and standard deviation, for both the single particle energy loss and splitting shower, evaluated at parameter value θR = 2.5m and determined from 5,000 samples. The estimator with lowest standard deviation is shown in bold. It should be noted that there is considerable flexibility in Stochastic AD for how to couple the randomness in the primal and alternative programs after the point at which the alternative is produced, i.e. how to choose the join distribution over random variables in the primal and alternative programs. This selection of coupling can have a considerable impact on the Stochastic AD gradient estimator variance. In this work, we have used a simple approach of re-using random variables sampled in the primal for the alternative, without regard for where those random variables are re-used in the alternative. We have seen that this re-use can have a large impact; we observed that removing the re-use of random variables in the alternative can increase the Stochastic AD gradient estimator standard deviation by factors of 1.5 or more. More generally, a more careful strategy of re-using of random variables may considerably reduce the Stochastic AD gradient estimator variance. D. Design Optimization with Splitting Shower We test the ability to use the various gradient estima- tors to perform a gradient based optimization using of the detector radius parameter using the aforementioned loss with a target radial shower depth of ¯RT = 2. Each epoch consists of a single step of the optimization, with a mini-batch size of only 2 simulation runs used to esti- mate gradients in each epoch. The Adam optimizer [42] is used. A learning rate of 0.01 is used for the gradient descent parameter update. For all optimizations, the ini- tial detector radius parameter value is set to θinit = 3m and the optimization is run for 500 gradient steps. Each gradient method is used in 10 separate optimizations,8 and the average and standard deviation of the loss at each optimization step is shown in Fig. 5. As expected, the score function estimator without baseline and the numeric gradients shows the largest standard deviation to the extent that optimization is not feasible in this setting. We also see that the numeric and score function without baseline estimators are significantly slower at optimiz- ing the objective. The score with baseline estimator and Stochastic AD estimators show similar standard deviation and similar progress towards the loss minimum as a func- tion of optimization step. This suggests that Stochastic AD and score function with baseline estimators provide significantly better gradient estimates, even with very small sample sizes, and are likely interesting estimators for further study of detector design optimizations in more complex settings. 0 100 200 300 400 500 Steps 0 1 2 3 4 Loss Design Optimization numeric SCORE SCORB STAD FIG. 5: The mean and inter-quantile range of the loss versus epoch of detector design optimization is shown. Mean and quantiles are computed from 10 optimizations. V. CONCLUSION In this work, we discuss several strategies for differ- entiating stochastic programs, with a focus on methods capable of differentiating programs with discrete random- ness, and discuss their application to High Energy Physics detector design optimization. We develop the first appli- cation of Stochastic AD to branching processes and, more generally, the first differentiable branching program capa- ble of estimating gradients through the discrete processes within a particle shower. We also introduce score func- tion gradient estimators within this HEP detector design context. We find that Stochastic AD and score function gradient estimators, using control variates, provide the best gradient estimators in terms of smallest standard deviation among the gradient estimators examined within a case study of detector design. We show that both tech- niques can successfully be used for gradient-based HEP detector design on a toy detector simulator. More broadly, we believe that the careful study and application of techniques like Stochastic AD and score function estimation can open the way to a wide array of new differentiable programming applications in HEP and other sciences. ACKNOWLEDGEMENTS We thank Gaurav Arya, Frank Schäfer, and Moritz Schauer for the helpful discussions regarding Stochastic AD, and thank Gaurav Arya for the helpful feedback on the manuscript. We thank Michael Brenner for the helpful discussions regarding score function gradient estimators at the Aspen Center for Physics, as this work was par- tially performed at the Aspen Center for Physics, which is supported by National Science Foundation grant PHY- 2210452. We also thank the Munich Institute for Astro-, Particle and BioPhysics (MIAPbP) which is funded by the Deutsche Forschungsgemeinschaft (DFG, German Re- search Foundation) under Germany´s Excellence Strategy – EXC-2094 – 390783311, as this work was partially per- formed at the MIAPbP workshop on Differentiable and Probabilistic Programming for Fundamental Physics. MK is supported by the US Department of Energy (DOE) under grant DE-AC02-76SF00515. LH is sup- ported by the Excellence Cluster ORIGINS, which is funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excel- lence Strategy - EXC-2094-390783311. [1] A. Radovic et al., “Machine learning at the energy and intensity frontiers of particle physics,” Nature 560, 41–48 (2018). [2] K. Cranmer, U. Seljak, K. Terao, and the Particle Data Group, “Review of Particle Physics,” Progress of Theo- retical and Experimental Physics 2022, 083C01 (2022), chapter 41. [3] P. Calafiura, D. Rousseau, and K. Terao, Artificial Intel- ligence for High Energy Physics (WORLD SCIENTIFIC, 2022).9 [4] K. Cranmer, J. Brehmer, and G. Louppe, “The frontier of simulation-based inference,” Proceedings of the National Academy of Sciences 117, 30055–30062 (2020). [5] J. Shlomi, P. Battaglia, and J.-R. Vlimant, “Graph neural networks in particle physics,” Machine Learning: Science and Technology 2, 021001 (2020). [6] S. Thais et al., “Graph neural networks in particle physics: Implementations, innovations, and challenges,” (2022), arXiv:2203.12852. [7] A. Butter et al., “Machine learning and LHC event gener- ation,” SciPost Phys. 14, 079 (2023). [8] A. Adelmann et al., “New directions for surrogate models and differentiable programming for high energy physics detector simulation,” (2022), arXiv:2203.08806. [9] H. M. Bücker, G. F. Corliss, P. D. Hovland, U. Naumann, and B. Norris, eds., Automatic Differentiation: Applica- tions, Theory, and Implementations, Lecture Notes in Computational Science and Engineering (Springer, New York, NY, 2005). [10] A. G. Baydin, B. A. Pearlmutter, A. A. Radul, and J. M. Siskind, “Automatic differentiation in machine learning: a survey,” Journal of Machine Learning Research 18, 1–43 (2018). [11] L. Heinrich and M. Kagan, “Differentiable Matrix Ele- ments with MadJax,” J. Phys. Conf. Ser. 2438, 012137 (2023), arXiv:2203.00057. [12] N. Simpson and L. Heinrich, “neos: End-to-End- Optimised Summary Statistics for High Energy Physics,” J. Phys. Conf. Ser. 2438, 012105 (2023), arXiv:2203.05570. [13] S. Cheong et al., “Novel light field imaging device with enhanced light collection for cold atom clouds,” Journal of Instrumentation 17, P08021 (2022). [14] P. Glasserman, Monte Carlo Methods in Financial Engi- neering (Springer, 2013). [15] D. P. Kingma and M. Welling, “Auto-encoding vari- ational bayes,” Proceedings of the International Con- ference on Learning Representations, ICLR’14 (2014), arXiv:1312.6114. [16] D. Rezende, S. Mohamed, and D. Wierstra, “Stochas- tic backpropagation and approximate inference in deep generative models,” in Proceedings of the 31st Inter- national Conference on International Conference on Machine Learning - Volume 32, ICML’14 (2014) p. II–1278–II–1286. [17] R. Williams, “Simple statistical gradient-following algo- rithms for connectionist reinforcement learning,” Machine Learning 8, 229–256 (1992). [18] G. Arya, M. Schauer, F. Schäfer, and C. Rackauckas, “Automatic Differentiation of Programs with Discrete Ran- domness,” in Advances in Neural Information Processing Systems, Vol. 35 (Curran Associates, Inc., 2022). [19] S. Mohamed, M. Rosca, M. Figurnov, and A. Mnih, “Monte carlo gradient estimation in machine learning,” (2020), arXiv:1906.10652. [20] E. Greensmith, P. L. Bartlett, and J. Baxter, “Variance reduction techniques for gradient estimates in reinforce- ment learning,” Journal of Machine Learning Research 5, 1471–1530 (2004). [21] Martín Abadi et al., “TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems,” (2015), software available from tensorflow.org. [22] J. Bradbury et al., “JAX: composable transformations of Python+NumPy programs,” (2018). [23] Adam Paszke et al., “PyTorch: An Imperative Style, High- Performance Deep Learning Library,” in Advances in Neu- ral Information Processing Systems 32 (Curran Associates, Inc., 2019) pp. 8024–8035. [24] J. Schulman, N. Heess, T. Weber, and P. Abbeel, “Gra- dient estimation using stochastic computation graphs,” in Advances in Neural Information Processing Systems, Vol. 28 (Curran Associates, Inc., 2015). [25] G. Arya et al., “Differentiating metropolis-hastings to optimize intractable densities,” (2023), arXiv:2306.07961. [26] A. K. Lew, M. Huot, S. Staton, and V. K. Mansinghka, “ADEV: Sound automatic differentiation of expected val- ues of probabilistic programs,” Proceedings of the ACM on Programming Languages 7, 121–153 (2023). [27] E. Krieken, J. Tomczak, and A. Ten Teije, “Storchastic: A framework for general stochastic automatic differen- tiation,” in Advances in Neural Information Processing Systems, Vol. 34 (Curran Associates, Inc., 2021) pp. 7574– 7587. [28] S. Bangaru, J. Michel, K. Mu, G. Bernstein, T.-M. Li, and J. Ragan-Kelley, “Systematically differentiating paramet- ric discontinuities,” ACM Trans. Graph. 40, 107:1–107:17 (2021). [29] L. Heinrich, M. Feickert, and G. Stark, “pyhf: v0.6.3,” https://github.com/scikit-hep/pyhf/releases/tag/v0.6.3. [30] L. Heinrich, M. Feickert, G. Stark, and K. Cranmer, “pyhf: pure-Python implementation of HistFactory statistical models,” Journal of Open Source Software 6, 2823 (2021). [31] L. Heinrich and N. Simpson, “pyhf/neos: initial zenodo release [Link],” (2020). [32] S. Carrazza, J. M. Cruz-Martinez, and M. Rossi, “Pdfflow: Parton distribution functions on gpu,” Computer Physics Communications 264, 107995 (2021). [33] R. D. Ball et al., “An open-source machine learning frame- work for global analyses of parton distributions,” (2021), arXiv:2109.02671. [34] B. Nachman and S. Prestel, “Morphing parton showers with event derivatives,” (2022), arXiv:2208.02274. [35] Differentiable Universe Initiative, home page & software. [36] S. Carrazza and F. A. Dreyer, “Jet grooming through reinforcement learning,” Phys. Rev. D 100, 014014 (2019), arXiv:1903.09644. [37] J. Brehmer et al., “Hierarchical clustering in parti- cle physics through reinforcement learning,” (2020), arXiv:2011.08191. [38] S. Shirobokov et al., “Black-box optimization with local generative surrogates,” in Advances in Neural Informa- tion Processing Systems, Vol. 33 (Curran Associates, Inc., 2020) pp. 14650–14662. [39] C. Haack and L. Schumacher, “Machine-learning aided detector optimization of the pacific ocean neutrino exper- iment,” ICRC2023, 1059. [40] T. Dorigo et al. (MODE), “Toward the end-to-end op- timization of particle physics instruments with differ- entiable programming,” Rev. Phys. 10, 100085 (2023), arXiv:2203.13818. [41] T. Gorordo et al., “Geometry Optimization for Long-lived Particle Detectors,” (2022), arXiv:2211.08450. [42] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in 3rd International Conference on Learn- ing Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings (2015).
Astronomy & Astrophysics manuscript no. ms ©ESO 2023 1st September, 2023 Karhunen–Loève Data Imputation in High Contrast Imaging⋆ Bin B. Ren (任彬) iD ⋆⋆ Université Côte d’Azur, Observatoire de la Côte d’Azur, CNRS, Laboratoire Lagrange, Bd de l’Observatoire, CS 34229, 06304 Nice cedex 4, France; e-mail: bin.ren@oca.eu Received 04 July 2023 / Revised 30 August 2023 / Accepted 31 August 2023 ABSTRACT Detection and characterization of extended structures is a crucial goal in high contrast imaging. However, these structures face chal- lenges in data reduction, leading to over-subtraction from speckles and self-subtraction with most existing methods. Iterative post- processing methods offer promising results, but their integration into existing pipelines is hindered by selective algorithms, high computational cost, and algorithmic regularization. To address this for reference differential imaging (RDI), here we propose the data imputation concept to Karhunen–Loève transform (DIKL) by modifying two steps in the standard Karhunen–Loève image projection (KLIP) method. Specifically, we partition an image to two matrices: an anchor matrix which focuses only on the speckles to obtain the DIKL coefficients, and a boat matrix which focuses on the regions of astrophysical interest for speckle removal using DIKL com- ponents. As an analytical approach, DIKL achieves high-quality results with significantly reduced computational cost (∼3 orders of magnitude less than iterative methods). Being a derivative method of KLIP, DIKL is seamlessly integrable into high contrast imaging pipelines for RDI observations. Key words. (stars:) circumstellar matter – (Galaxies:) quasars: general – techniques: high angular resolution – techniques: image processing – methods: statistical 1. Introduction High contrast imaging aims at detecting and characterizing faint signals surrounding bright central sources (e.g., Oppenheimer & Hinkley 2009; Benisty et al. 2023; Currie et al. 2023). To reach this goal, coronagraphs are used to suppress central light, and post-processing – combined with observational techniques – is implemented to remove residual light and speckles (e.g., Pueyo 2018; Follette 2023). With these setups, existing observational techniques and methods are efficiently detecting and characteriz- ing point sources such as planets and brown dwarfs (e.g., Nielsen et al. 2019; Vigan et al. 2021), as well as circumstellar disks and quasar host galaxies in polarized light (e.g., Benisty et al. 2023; Gratadour et al. 2015). Total intensity detection for ex- tended structures, however, is still prone to data reduction arti- facts with most existing methods for ground-based observations (e.g., Ruane et al. 2019; Xie et al. 2022). Their characterization is limited by compromised data quality (e.g., Milli et al. 2012; Mazoyer et al. 2020; Olofsson et al. 2023; Xie et al. 2023). To detect and characterize extended structures in total inten- sity, several statistics-based methods and their derivative meth- ods are being used (e.g., Lafrenière et al. 2007; Soummer et al. 2012; Amara & Quanz 2012; Ren et al. 2018, 2020; Pairet et al. 2018, 2021; Flasseur et al. 2021; Samland et al. 2021; Juillard et al. 2022; Berdeu et al. 2022). Nevertheless, these methods are either prone to severe overfitting that can alter the morphology and surface brightness of such structures (e.g., Lafrenière et al. 2007; Soummer et al. 2012), or being highly selective on ref- erence regions and computationally intensive (e.g., Ren et al. ⋆ FITS images are only available at the CDS via anonymous ftp to cdsarc.cds.unistra.fr (130.79.128.5) or via https:// cdsarc.cds.unistra.fr/viz-bin/cat/J/A+A/ ⋆⋆ Marie Skłodowska-Curie Fellow 2018, 2020), or subject to regularization terms that may provide uncertain results on the morphology and surface brightness of extended structures (e.g., Flasseur et al. 2021; Pairet et al. 2021; Juillard et al. 2022). Meanwhile, a careful selection of data re- duction regions (e.g., Galicher & Marois 2011; Milli et al. 2012, 2017; Perrin et al. 2015) could provide better results than non- statistical methods, yet these regions need to be adjusted for dif- ferent systems, and such a selection is challenging when the disk morphology is face-on, complex, or even unknown. Using non- negative matrix factorization (NMF: Lee & Seung 2001), Ren et al. (2020) showed that data imputation with sequential NMF (DI-sNMF) could be a promising statistical method for simple disk morphology, and it can provide high-quality results with theoretically minimized signal alteration. However, the data im- putation concept has only been applied in Ren et al. (2020) for NMF in high contrast imaging, and it can provide high-quality results only when some strict requirements are satisfied (e.g., Ren et al. 2020; Olofsson et al. 2023; Xie et al. 2023). What is more, DI-sNMF is computationally inefficient due to its iterative nature (e.g., Ren et al. 2018), calling for other analytical methods that can implement the data imputation concept. In high contrast imaging, the principal-component-analysis- based Karhunen–Loève Image Projection (KLIP: Soummer et al. 2012; Amara & Quanz 2012) method is one of the analytical method standards for data reduction. KLIP decomposes refer- ence images into an orthogonal basis through Karhunen–Loève (KL) transform, and it has been included in post-processing pipelines (e.g., Wang et al. 2015; Gomez Gonzalez et al. 2016; Stolker et al. 2019; Lucas & Bottom 2020). It is widely used by surveys on both exoplanets (e.g., Nielsen et al. 2019; Vi- gan et al. 2021) and circumstellar disks (e.g., Esposito et al. 2020; Xie et al. 2022; Cugno et al. 2023; Wallack et al. 2023). Due to its over-fitting nature (e.g., Soummer et al. 2012; Pueyo Article number, page 1 of 7 arXiv:2308.16912v1 [astro-ph.IM] 31 Aug 2023A&A proofs: manuscript no. ms 2016), KLIP encounters difficulty in detecting and character- izing complex and extended structures. While iterative KLIP derivatives (e.g. Pairet et al. 2018; Ginski et al. 2021; Stapper & Ginski 2022) might help alleviate the over-subtraction and self- subtraction in certain observational setups, there are still persis- tent residual signals and self-subtraction artefacts that pose chal- lenges to data interpretation. As a data replacement (i.e., impu- tation) attempt with KLIP, Hunziker et al. (2018) and Xuan et al. (2018) used KLIP-based background subtraction in an interpo- lation approach: first perform KL decomposition, then conduct KLIP while masking out the central regions. However, due to the masking of central regions, the KLIP procedure within were per- formed on a non-orthogonal basis. Such a KLIP treatment can introduce over-fitting, which is manifested as a higher empiri- cal model for the background, and yields negative surrounding regions even before post-processing. Noticing this over-fitting, Xie et al. (2023) thus used DI-sNMF for background removal and post-processing to extract a double-spiraled system to en- able precise spiral motion measurement. After all, a proper data imputation with KLIP would offer a promising way to produce high-quality results with a high computational efficiency. In the upcoming era of Extremely Large Telescopes (ELTs), the reference differential imaging (RDI) observational setup, in- cluding star-hopping (Wahhaj et al. 2021), could likely dominate the observation modes to produce high-quality images of exo- planets, circumstellar disks, and quasar host galaxies (hereafter “astrophysical signals”) in total intensity. In an RDI observation, a reference star (or multiple reference stars), which does not host circumstellar signals, is used to empirically capture the point spread function and speckles of the target star or quasar to reveal the target’s astrophysical surroundings. With large collecting ar- eas (e.g., Gilmozzi & Spyromilio 2007), ELTs can reach high signal-to-noise ratios in a significantly smaller amount of time (e.g., Bowens et al. 2021) than existing telescopes. In compari- son, it might be suboptimal for the classical angular differential imaging (ADI: Marois et al. 2006) observation setup to be de- ployed on ELTs, since ADI requires a sufficient amount of field rotation of the sky for data processing (e.g., Milli et al. 2012; Xuan et al. 2018), yet sky rotation rate is a natural phenomenon that cannot be changed. To reduce and process the ELT data with high fidelity in RDI observations, iteratively methods could be potentially offer the best results. However, these iterative meth- ods might be limited due to their significantly high computa- tional cost than analytical methods, especially when there are high-resolution observations with a large number of datasets. Therefore, an analytical RDI data reduction method is needed to provide high-quality results in an efficient way. In this study, by engineering the mathematical basics for KLIP, we enable it with the data imputation concept (DIKL). By modifying two steps in the standard KLIP procedure, and illustrating the DIKL results, we expect DIKL to be a promis- ing method in RDI data reduction. In comparison with the es- tablished iterative DI-sNMF method which offers high-quality RDI results (e.g., Olofsson et al. 2023; Xie et al. 2023), such modifications allow an analytical data imputation by DIKL. It is computationally more efficient than iterative methods such as DI-sNMF by ∼3 orders of magnitude, and it can provide results with comparable quality as DI-sNMF in RDI datasets. 2. Method In high contrast imaging observations which require the usage of coronagraphs and adaptive optics systems, light signals from a central object (e.g., star, quasar) received on the detectors are superimposed on the astrophysical signals (e.g., Pueyo 2018; Follette 2023). The goal of post-processing is to extract the faint astrophysical signals (e.g., exoplanets, circumstellar disk, host galaxy) that are hidden behind these bright signals of non- astrophysical origin (e.g., adaptive optics halo, speckles, thermal background). For simplicity, we refer all these non-astrophysical signals as “speckles” hereafter. For an image which contains as- trophysical signals, we refer it as target t. For a set of images that only contain speckle signals, we refer them as reference images R.1 In post-processing, we use the reference images to obtain the representative features of the speckles, then remove these fea- tures from a target image to reveal astrophysical signals in the residuals. 2.1. Overview of KLIP For a reference image array containing nref ∈ N reference images each with npix ∈ N pixels, we denote the array as R ∈ Rnpix×nref. A column in R contains a reference image that is converted to a one-dimensional column vector (e.g., concatenating all columns of one image). We note that the notations here follow that of statisticians and computer scientists, instead of existing publica- tions by astronomers (e.g., Soummer et al. 2012; Pueyo 2016), to enable an efficient delivery of messages to non-astronomy readers in coding. Specifically, we have the reference array R = �R1, · · · , Rnref �, with Ri = (Ri1, · · · , Rinpix)⊤ for i ∈ {1, · · · , nref} and ⊤ for matrix transpose, where Ri j ∈ R for j ∈ {1, · · · , npix}. For a target image, we can convert it to a column vector t ∈ Rnpix×1. To remove the speckles from the target image, a statistics-based post-processing method first transforms the ref- erence array R to obtain a basis. This basis contains the represen- tative features of the reference array (i.e., components). We then remove the representative features from the target vector. After post-processing, astrophysical signals are expected to reside in the residual image from feature removal. The KLIP algorithm in Soummer et al. (2012) is consisted of two steps: the Karhunen–Loève (KL) transform of the reference matrix R to obtain the KL component basis, and the projection of the target vector t on the KL basis. For KLIP, the columns in both R and t are zero-spatial-mean. To reveal the astrophysi- cal signals, the KLIP projection is then removed from the target image. To distinguish the difference between KLIP and DIKL later in this work, here we denote the reference array for KLIP as matrix R(a) ∈ Rn(a) pix×nref, with n(a) pix ≤ npix allowing for the selec- tion of smaller regions in reference images. Similarly, we denote the target vector as t(a) ∈ Rn(a) pix×1 in KLIP processing. With these notations, we review the KLIP procedure as follows. In the KL transform step, for a reference array R(a), its co- variance matrix Σ(a) = R(a)⊤R(a) ∈ Rn(a) pix×n(a) pix is real symmetric. The spectral decomposition of Σ(a) is Σ(a) = R(a)⊤R(a) = Q(a)D(a)Q(a)⊤, (1) where D(a) = diag{λ(a) 1 , · · · , λ(a) nref} is a diagonal matrix whose di- agonal entries are the eigenvalues of Σ(a) with λ(a) k ≥ λ(a) k+1 > 0, and Q(a) ∈ Rnref×nref is an orthonormal matrix with its columns be- ing the corresponding eigenvectors with Q(a)⊤Q(a) = Q(a)Q(a)⊤ = I. Left multiply Eq. (1) by Q(a)⊤ and right multiply by Q(a), we have [R(a)Q(a)]⊤R(a)Q(a) = Q(a)⊤R(a)⊤R(a)Q(a) = D(a), 1 A target can serve as a reference in scenarios such as ADI, in which astrophysical signals move with respect to speckles in different images. Article number, page 2 of 7Ren: DIKL for High Contrast Imaging 101 102 103 104 (counts pixel 1) Fig. 1. Partitioning of an image for DIKL. (a) An anchor image used to construct KL basis. (b) A boat image used to construct DIKL basis. (c) A complete image without partitioning. Note: the images here are vectorized to constitute the columns of the R(a), R(b), and R matrices, respectively. where we now have diagonalized the covariance matrix. By defining Z(a) = R(a)Q(a), (2) we have an orthogonal KL component basis Z(a) ∈ Rn(a) pix×nref. The columns of the KL basis can be divided by the corresponding square root of the eigenvalues (e.g., Soummer et al. 2012), and thus creating an orthonormal basis Z(a). While we do not adopt this normalization since it does not impact the following target modeling procedure in the datasets used later in this study, such a normalization (e.g., VIP: Gomez Gonzalez et al. 2016) might be necessary for other datasets. In the KL projection step, for a given target image vector with zero-spatial-mean t(a) ∈ Rn(a) pix×1, we can project and remove the KL component to obtain the residual r(a) ∈ Rn(a) pix×1, r(a) = t(a) − Kklip � k=1 c(a) k Z(a) k , (3) where Kklip ∈ {1, · · · , nref} is the cutoff count of the KL compo- nents (Soummer et al. 2012), and c(a) k = t(a)⊤Z(a) k (4) is a scalar with c(a) k ∈ R, which denotes the projection from the target onto the k-th KL component of R(a). In the next step of this study, we will modify the KLIP proce- dure to introduce DIKL post-processing. To perform DIKL data reduction, we need to focus on different regions of the reference array R. For the full reference array R, we can reorder or dupli- cate certain of its rows to obtain two submatrices, � R(a) = S (a)R R(b) = S (b)R , (5) where R(a) ∈ Rn(a) pix×nref, R(b) ∈ Rn(b) pix×nref, with n(a) pix ≤ npix and n(b) pix ≤ npix. Selection matrices S (a) ∈ Bn(a) pix×npix and S (b) ∈ Bn(b) pix×npix are boolean, and they are used to select specific rows in R to form R(a) and R(b), respectively. For example, to select the j-th row from R and store it in the i-th row of R(a), we have S (a) ij = 1 (and S (a) i j = 0 otherwise). To be more informative on the naming con- ventions of the superscripts in Eq. (5), we refer the two selected matrices as an anchor matrix R(a) and a boat matrix R(b). The anchor matrix R(a) covers the regions which only host speckle signals, and the boat matrix R(b) can host astrophysical signals (and it can also contain the anchor matrix, which is adopted in this study), see Fig. 1 for an illustration of the selected regions. 2.2. DIKL for Reference Differential Imaging The DIKL method modifies KLIP in two aspects. Specifically, the DIKL components are constructed by applying the eigenvec- tors from the anchor matrix R(a) to the boat matrix R(b), and the DIKL projection adopts the projection coefficient between an- chor target t(a) and anchor KL basis Z(a). On the one hand, DIKL modifies the KL transform in Eq. (2). The DIKL basis follows Z′(b) = R(b)Q(a), (6) where we now instead use the eigenvectors from R(a) to construct the DIKL basis for R(b) regions. Here we use the prime symbol for the DIKL basis to distinguish it from a normal KL basis for the reference matrix R(b). In comparison, the KL basis for R(b) is Z(b), and it is not calculated nor used in this study. On the other hand, we modify the KL projection in Eq. (3). For a target vector t(b) ∈ Rn(b) pix×1 with zero-spatial-mean, DIKL uses the coefficients from Eq. (4) between t(a) and Z(a), then ap- plies the coefficients to the DIKL basis in Eq. (6). Specifically, the residual image after DIKL projection is r(b) = t(b) − Kklip � k=1 c(a) k Z′(b) k = t(b) − Kklip � k=1 t(a)⊤Z(a) k Z′(b) k = |S (b)t⟩ − Kklip � k=1 ⟨S (a)t|Z(a) k ⟩|Z′(b) k ⟩, (7) where the last equation adopts the bra–ket convention for the purpose of demonstration and connection with existing studies (e.g., Soummer et al. 2012). Article number, page 3 of 7A&A proofs: manuscript no. ms With the above procedure, Eqs. (6) and (7) establish the DIKL post-processing procedure, see Appendix A for a corre- sponding pseudocode and implementation instructions. On the one hand, there is no direct projection of t(b) onto the KL basis of R(b), which ensures the non-overfitting of non-speckle signals for DIKL. On the other hand, the projection coefficient is ob- tained from where speckle-only signals are expected (in both the reference images and the target image). Both ensure that astro- physical signals in the boat regions of t(b) are not captured in the coefficients in Eq. (7), and thus it serves as the data imputation step for DIKL. In fact, DIKL only performs KL transform to the speckles in the anchor matrix in this work, which avoids the high variation of signals in the boat matrix (e.g., regions that are close to the coronagraph) to dominate the classical KL transform (which is performed on the entire image), see Fig. 1. We note that, however, Eq. (6) is limited in the reduction of non-RDI data. For example, in ADI datasets, we need to mask out different pixels in an observation sequence (e.g., Milli et al. 2017; Ren et al. 2020) to focus on speckle-only signals, since angular diversity is used to capture the speckles in an ADI ob- servation sequence. We can indeed calculate an element in the covariance matrix by focusing only on the pixels that are not masked out in a pair of reference images, and perform spectral decomposition in Eq. (1) to obtain the corresponding eigenvec- tors. However, we cannot yet construct a basis following Eq. (2) for KLIP, nor Eq. (6) for DIKL, since weights in the eigenvec- tor matrix Q are assigned to data that are masked out (i.e., ar- tificially “missing”) in the reference array R. To apply DIKL to ADI datasets, we can interpolate the speckle-only signals for the masked out data (e.g., Perrin et al. 2015) or iteratively fill the missing data (e.g., Bailey 2012) in R for Eq. (6), yet such treat- ments beyond the current scope of this analytical study. 3. Application To demonstrate the DIKL method using on-sky RDI data, we re- trieved datasets from the SPHERE instrument at the Very Large Telescope (VLT). We adopted data from IRDIS (Dohlen et al. 2008) of SPHERE in the star-hopping mode (Wahhaj et al. 2021). The star-hopping mode hops between a target star and its reference star during an observation sequence, thus reaching a quasi-simultaneous capture of speckle change. We retrieved the UT 2021-09-06, UT 2022-02-07, and UT 2022-04-01 ob- servations of circumstellar disk hosts – HD 169142, PDS 201, and HD 129590 – as well as their reference stars from pro- grams 105.209E and 105.20GP. We preprocess the data using the IRDAP pipeline (van Holstein et al. 2017, 2020), and the stars are then located at the center of the images. In the IRDIS data, with each IRDIS pixel having a spatial scale of 12.25 mas (Maire et al. 2016), the control ring – interior to which the adaptive optics system can perform optimal dark hole correction to reveal faint objects – spans between 85 pixel and 115 pixel from the matrix centers. For further processing, we masked out the matrix ele- ments that are interior to a radius of 8 pixel from the center. We display the partitioning of an image in Fig. 1 for and illustration of the anchor region and the boat region in this study. For the three groups of IRDIS datasets, the images for post- processing are the central 350 × 350 pixel from the IRDAP pre- processed data. For one exposure, there are two channels from the preprocessed data, and we added them to produce one im- age. There are 128, 32, and 32 target images, with 36, 12, and 32 corresponding reference images, for HD 169142, PDS 201, and HD 129590, respectively. To convert a 2-dimensional image to a column in a matrix for this study, in practice, we created a 2-dimensional binary mask whose entries are 1 when the corre- sponding pixels are selected in Fig. 1(a)(b). For one image, we used the where function in numpy (Harris et al. 2020) to select the pixels in the images, and created a column for the selected matrix. By performing this for all reference images, we created the selected anchor matrix R(a) or the boat matrix R(b) for further post-processing. By performing this once for a target image, we created the selected anchor target vector t(a) or the boat target vector t(b). In post-processing, for the three IRDIS datasets that are in matrix form, we first followed Eq. (1) to perform spectral de- composition of the matrix elements in the anchor matrix (i.e., the IRDIS control ring). We then followed Eq. (6) to conduct DIKL transform on the boat matrix – the entire image (or the matrix elements interior to the outer edge of the control ring for the HD 129590 data) – using the eigenvectors from the covariance matrix of the anchor references. At last, to remove the speckles for a target image, we followed Eq. (7) to perform DIKL reduc- tion, where we adopted the KLIP projection coefficients from Eq. (4) between the anchor target and anchor references. We re- shaped the 1-dimensional vectors to 2-dimensional image, then derotated the reduction results for all target images to north-up and east-left according to their corresponding parallactic angles calculated using Pynpoint (Stolker et al. 2019). We calculated the element-wise median of the derotated reduction results as the combined image. We adopt the median-subtracted combined image as the final result. 3.1. Residual Variance Variance of residual images can be informative of the existence of astrophysical signals. By performing DIKL on the reference images of the HD 169142 dataset, we generated the correspond- ing fractional residual variance (FRV) curves (e.g., Soummer et al. 2012; Ren et al. 2018). Specifically, for a reduced refer- ence image, we divide its variance by that of the corresponding original image to generate the FRV. The FRV curves are then the FRV dependence as a function of KL components. Similarly, we generated the FRV curves for KLIP for comparison. For KLIP, FRV curves are expected to follow the fractional residual eigen- values (Soummer et al. 2012), as can be seen in Fig. 2. The FRV curves of DIKL converge to higher plateaus than those from KLIP. This convergence illustrates the less aggres- sive overfitting (or potentially non-overfitting of data; e.g., NMF: Ren et al. 2018) of DIKL. Similar as NMF, the FRV plateaus with DIKL is the indirect evidence that DIKL can retain more information of non-speckle signals than KLIP. In addition, the FRVs for DIKL reaches to stable values with ≈5 DIKL compo- nents, suggesting that the rest of the components have negligible contribution in the DIKL process. Due to the non-orthogonality of a DIKL basis, we witnessed a minor level of overfitting of the data, and thus we subtracted the median of the DIKL residuals to manually offset this effect. We present in Fig. 3 the correlation matrix for DIKL compo- nents: there exist crosstalks in the form of non-zero off-diagonal elements. However, given that the contribution of higher order DIKL components (≳5) are negligible in the FRVs in Fig. 2, the DIKL crosstalk only impacts the first few components, and the crosstalk does not contribute to data reduction beyond the first ≈5 DIKL components due to the reaching of FRV plateaus in Fig. 2. Article number, page 4 of 7Ren: DIKL for High Contrast Imaging 0 10 20 30 KL component 10 3 10 2 10 1 100 Fractional residual variance DIKL KLIP (theory) KLIP Fig. 2. Fractional residual variance as a function of KL components for RDI. DIKL reaches higher plateaus than KLIP, illustrating the ability of information retention using DIKL. (The data used to create this figure are available in the ancillary folder on arXiv.) 1 2 5 10 20 30 DIKL Component 1 2 5 10 20 30 DIKL Component 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 Correlation Fig. 3. Correlation matrix of DIKL components. While KL components are mutually orthogonal, most DIKL components are not. However, with the FRVs in Fig. 2 suggesting that only the first ∼5 DIKL compo- nents contributes to the removal of speckles, the crosstalk from higher order components therefore does not impact DIKL data reduction. (The data used to create this figure are available in the ancillary folder on arXiv.) 3.2. DIKL Imaging We used DIKL to reduce the star-hopping RDI observations of HD 169142, PDS 201, and HD 129590, which are known cir- cumstellar disk hosts. The three disks have inclinations from nearly face-on to roughly edge-on (e.g., Pohl et al. 2017; Wagner et al. 2020; Matthews et al. 2017; Olofsson et al. 2023). We also reduced the datasets with KLIP and DI-sNMF for comparison. Between the DIKL and KLIP results in Fig. 4, for the nearly face-on HD 169142, DIKL retrieves a two-ringed system, KLIP can nevertheless only recover the inner ring that is close to the coronagraph with compromised data quality. The DIKL result resembles the disk image obtained in polarized light, which thus demonstrates its superiority over KLIP in conserving face-on structures. Similarly, the PDS 201 and HD 129590 results have fine extended structures only seen in DIKL when compared with KLIP. In comparison, KLIP removes signals from the disks, al- tering disk morphology that poses challenges in data interpreta- tion (e.g., Wagner et al. 2020; Olofsson et al. 2023). 101 102 103 104 (counts pixel 1) Fig. 4. Comparison of reduction results using KLIP (left) and DIKL (right) for nearly face-on to roughly edge-on systems. For HD 169142 (left), PDS 201 (middle), and HD 129590 (right), DIKL reaches data quality that might be comparable with polarized light observations. (The data used to create this figure are available in the ancillary folder on arXiv.) In comparison with the DI-sNMF method, which is mathe- matically well-founded to deliver high-quality results (e.g., Ren et al. 2020; Olofsson et al. 2023; Xie et al. 2023), DIKL can pro- vide satisfactory results in Fig. 5. The DIKL results not only re- semble the morphology of disks from DI-sNMF (Olofsson et al. 2023; Ren et al. 2023), but also agree with the DI-sNMF surface brightness values within ∼10% for the disk-hosting regions. This further shows that DIKL can be a promising method in reaching high-quality results, and with a computational efficiency that is ∼3 orders of magnitude better than DI-sNMF. With the demonstrated superiority of DIKL over the clas- sical KLIP method in Fig. 4, as well as the consistency of its results with the established DI-sNMF method in Fig. 5, DIKL is a promising method that can produce high-quality results us- Article number, page 5 of 7A&A proofs: manuscript no. ms 1.0 0.5 0.0 0.5 1.0 Fig. 5. Validation of DIKL results (top) using DI-sNMF results (mid- dle), with the former method being more computationally efficient than the latter by ∼3 orders of magnitude. The difference between the two methods (bottom), obtained by first subtracting the DI-sNMF result from that of DIKL then divided it by the DI-sNMF result, is ≲10% for the disk signals. (The data used to create this figure are available in the ancillary folder on arXiv.) ing similar amount of calculation time as KLIP. However, given that the DIKL basis being a non-orthogonal basis in practice, we should not yet ascribe full credibility in the fine details of DIKL results for interpretation until its application to datasets from other instruments is validated. Nevertheless, DIKL possesses a distinctive advantage in producing high-quality preliminary out- comes with high computational efficiency as the first step for data analysis. Such datasets with promising results can be then reduced with more advanced methods including DI-sNMF to en- sure signal quality. 4. Summary For RDI data reduction in high-contrast imaging, we demon- strated a high-efficiency and analytical approach to perform data imputation with modified KLIP algorithm: DIKL. Specifically, we can modify two steps in KLIP to reach the purpose of data imputation. On the one hand, in component construction, we use the eigenvectors of the covariance matrix – from the regions which only host speckle signals (i.e., the anchor matrix, “a”) – to generate the DIKL basis for the regions hosting non-speckle signals (i.e., the boat matrix, “b”), see Eq. (6). On the other hand, in speckle removal, we adopt the coefficients from capturing the speckles in the anchor matrix, and apply them to the DIKL basis and thus capture the speckles for the regions that host astrophys- ical signals in the boat matrix, see Eq. (7). With the two modi- fications, we only need to perform spectral decomposition once in Eq. (1). What is more, the corresponding covariance matrix for the anchor matrix is faster to compute than KLIP, due to a re- duced number of matrix elements. As a result, the computational complexity of DIKL is similar as or less than that of KLIP. By avoiding the projection of astrophysical signals onto speckle features, DIKL can recover face-on structures that are normally overfit in RDI observations, see Fig. 4. On the one hand, in comparison with the mathematically well-founded it- erative DI-sNMF method in Ren et al. (2020), DIKL might po- tentially provide analytical results of low quality due to the non- orthogonal DIKL basis in Eq. (6). However, DIKL can approach a data quality similar to that of the latter in Fig. 5, since the crosstalk of the DIKL basis is negligible for higher order terms in Figs. 2 and 3. On the other hand, in comparison with the Hun- ziker et al. (2018) modification of KLIP, DIKL does not apply KL transform for the regions interior to the control ring while the latter does. Given that KL transform captures the variance of the signals (e.g., Soummer et al. 2012), while high contrast imaging observations have the highest variance next to the coro- nagraph (e.g., Pueyo 2018), the Hunziker et al. (2018) and Xuan et al. (2018) approach should not be adopted to realize the data imputation concept for KLIP. In comparison with that approach, DIKL is not sensitive to such variances, since it uses the control ring signals that are less prone to random noise for KL transform, and thus DIKL is theoretically more plausible for data imputa- tion. As a result, DIKL can be a promising analytical method that provides initial results, and with a ∼3 orders of magnitude higher computational efficiency than the existing iterative meth- ods (e.g., Ren et al. 2020). However, due to the crosstalk of DIKL basis, when the reference images are not stable, we recom- mend using DIKL for initial signal detection followed by other iterative data imputation methods for detailed characterization. Given that DIKL is a natural extension of the KLIP algo- rithm, it can be implemented in the existing high-contrast imag- ing packages (e.g., Wang et al. 2015; Gomez Gonzalez et al. 2016; Stolker et al. 2019; Lucas & Bottom 2020) for reference differential imaging data reduction. What is more, DIKL can perform data reduction for images containing negative values (from background removal), and thus it might potentially extract fainter disks than DI-sNMF which only takes in non-negative values. In the upcoming ELT era, we expect DIKL and its fu- ture derivative methods to provide high-quality results with high computational efficiency. Moving forward, on the one hand, we can assign different weights to the pixels in KL transform (e.g., Bailey 2012) to make it less prone to random noise or shot noise. On the other hand, for non-RDI data (e.g., angular differential imaging, spectral differential imaging), more careful derivations which can handle missing data for different regions from differ- ent images, including modifying the covariance matrix for KL transform and the KLIP procedure, are needed in the future. Acknowledgements. We thank the anonymous referee for their suggestions that increased the readability and clarity of this manuscript, Mamadou N’Diaye for discussions, and Chen Xie for discussions on reference differential imaging in the ELT era. Based on observations collected at the European Organisation for Astronomical Research in the Southern Hemisphere under ESO programmes 105.209E and 105.20GP. We thank the PIs for the two programs (M. Benisty and J. Olofsson) for the datasets that validated this study. This research has received funding from the European Union’s Horizon 2020 research and innovation pro- gramme under the Marie Skłodowska-Curie grant agreement No. 101103114. References Amara, A., & Quanz, S. P. 2012, MNRAS, 427, 948 Bailey, S. 2012, PASP, 124, 1015 Benisty, M., Dominik, C., Follette, K., et al. 2023, Astronomical Society of the Pacific Conference Series, 534, 605 Berdeu, A., Langlois, M., & Vachier, F. 2022, A&A, 658, L4 Article number, page 6 of 7Ren: DIKL for High Contrast Imaging Bowens, R., Meyer, M. R., Delacroix, C., et al. 2021, A&A, 653, A8 Cugno, G., Pearce, T. D., Launhardt, R., et al. 2023, A&A, 669, A145 Currie, T., Biller, B., Lagrange, A., et al. 2023, Astronomical Society of the Pacific Conference Series, 534, 799 Dohlen, K., Langlois, M., Saisse, M., et al. 2008, Proc. SPIE, 7014, 70143L Esposito, T. M., Kalas, P., Fitzgerald, M. P., et al. 2020, AJ, 160, 24 Flasseur, O., Thé, S., Denis, L., et al. 2021, A&A, 651, A62 Follette, K. B. 2023, arXiv, arXiv:2308.01354 Galicher, R., & Marois, C. 2011, P25 Gilmozzi, R., & Spyromilio, J. 2007, The Messenger, 127, 11 Ginski, C., Facchini, S., Huang, J., et al. 2021, ApJ, 908, L25 Gomez Gonzalez, C. A., Wertz, O., Christiaens, V., et al. 2016, VIP: Vortex Image Processing pipeline for high-contrast direct imaging of exoplanets, ASCL, 1603.003 Gratadour, D., Rouan, D., Grosset, L., et al. 2015, A&A, 581, L8 Harris, C. R., Millman, K. J., van der Walt, S. J., et al. 2020, Nature, 585, 357 Hunziker, S., Quanz, S. P., Amara, A., & Meyer, M. R. 2018, A&A, 611, A23 Juillard, S., Christiaens, V., & Absil, O. 2022, A&A, 668, A125 Lafrenière, D., Marois, C., Doyon, R., et al. 2007, ApJ, 660, 770 Lee, D. D., & Seung, H. S. 2001, in Advances in Neural Information Processing Systems 13, ed. T. K. Leen, T. G. Dietterich, & V. Tresp (MIT Press), 556 Lucas, M., & Bottom, M. 2020, JOSS, 5, 2843 Maire, A.-L., Langlois, M., Dohlen, K., et al. 2016, Proc. SPIE, 9908, 990834 Marois, C., Lafrenière, D., Doyon, R., et al. 2006, ApJ, 641, 556 Matthews, E., Hinkley, S., Vigan, A., et al. 2017, ApJ, 843, L12 Mazoyer, J., Arriaga, P., Hom, J., et al. 2020, Proc. SPIE, 11447, 1144759 Milli, J., Mouillet, D., Lagrange, A. M., et al. 2012, A&A, 545, A111 Milli, J., Vigan, A., Mouillet, D., et al. 2017, A&A, 599, A108 Nielsen, E. L., De Rosa, R. J., Macintosh, B., et al. 2019, AJ, 158, 13 Olofsson, J., Thébault, P., Bayo, A., et al. 2023, A&A, 674, A84 Oppenheimer, B. R., & Hinkley, S. 2009, ARA&A, 47, 253 Pairet, B., Cantalloube, F., & Jacques, L. 2018, arXiv, arXiv:1812.01333 —. 2021, MNRAS, 503, 3724 Perrin, M. D., Duchene, G., Millar-Blanchaer, M., et al. 2015, ApJ, 799, 182 Pohl, A., Benisty, M., Pinilla, P., et al. 2017, ApJ, 850, 52 Pueyo, L. 2016, ApJ, 824, 117 —. 2018, Direct Imaging as a Detection Technique for Exoplanets, 10 Ren, B., Benisty, M., Ginksi, C., et al. 2023, A&A, under review Ren, B., Pueyo, L., Chen, C., et al. 2020, ApJ, 892, 74 Ren, B., Pueyo, L., Zhu, G. B., et al. 2018, ApJ, 852, 104 Ruane, G., Ngo, H., Mawet, D., et al. 2019, AJ, 157, 118 Samland, M., Bouwman, J., Hogg, D. W., et al. 2021, A&A, 646, A24 Soummer, R., Pueyo, L., & Larkin, J. 2012, ApJ, 755, L28 Stapper, L. M., & Ginski, C. 2022, A&A, 668, A50 Stolker, T., Bonse, M. J., Quanz, S. P., et al. 2019, A&A, 621, A59 van Holstein, R. G., Snik, F., Girard, J. H., et al. 2017, Proc. SPIE, 10400, 1040015 van Holstein, R. G., Girard, J. H., de Boer, J., et al. 2020, A&A, 633, A64 Vigan, A., Fontanive, C., Meyer, M., et al. 2021, A&A, 651, A72 Wagner, K., Stone, J., Dong, R., et al. 2020, AJ, 159, 252 Wahhaj, Z., Milli, J., Romero, C., et al. 2021, A&A, 648, A26 Wallack, N. L., Ruffio, J.-B., Ruane, G., et al. 2023, AJ, accepted Wang, J. J., Ruffio, J.-B., De Rosa, R. J., et al. 2015, pyKLIP: PSF Subtraction for Exoplanets and Disks, ASCL, 1506.001 Xie, C., Choquet, E., Vigan, A., et al. 2022, A&A, 666, A32 Xie, C., Ren, B. B., Dong, R., et al. 2023, A&A, 675, L1 Xuan, W. J., Mawet, D., Ngo, H., et al. 2018, AJ, 156, 156 Appendix A: Pseudocode for DIKL implementation We present a pseudocode to implement DIKL in Algorithm 1. Specifically, we use the KL projection coefficient from Eq. (4), and apply it to the DIKL basis in Eq. (6), to obtain the DIKL projection. We then remove the DIKL projection from the target image to reveal the astrophysical signals in Eq. (7). To implement a standalone DIKL function, we need two ma- trix operations – matrix multiplication and eigendecomposition – both are available in modern scientific programming languages. Alternatively, to implement DIKL in the field of high contrast imaging, we can use existing KLIP frameworks (e.g., pyKLIP: Wang et al. 2015, VIP: Gomez Gonzalez et al. 2016, Pynpoint: Stolker et al. 2019, ADI.jl: Lucas & Bottom 2020), since there are only two additional commands (at the end of Algorithm 1) in Algorithm 1 DIKL algorithm. 1: Input: reference array R, target vector t, anchor selection mask S (a), and boat selection mask S (b); 2: Generate anchors: compute anchor reference R(a) and anchor target t(a) using selection mask S (a); ▷ See Eq. (5). 3: Generate boats: compute boat reference R(b) and boat target t(b) using selection mask S (b); ▷ See Eq. (5). 4: Q(a) ← eigenvectors of R(a)⊤R(a); ▷ See Eq. (1). 5: Z(a) ← R(a)⊤Q(a); ▷ KL transform in Eq. (2). 6: c(a) ← t(a)⊤Z(a); ▷ KL projection in Eq. (4). 7: Z′(b) ← R(b)⊤Q(a); ▷ DIKL transform in Eq. (6). 8: r(b) ← t(b) − c(a)Z′(b); ▷ DIKL projection in Eq. (7). 9: Output: DIKL residual r(b). addition to KLIP for RDI observations. There is one extra com- mand to select specific regions to obtain the anchor and boat ma- trices, and this selection is available using the mask commands in existing frameworks. In actual implementation, we recommend splitting Algo- rithm 1 into two functions for computational efficiency. One function performs KL and DIKL transforms, and returns both the KL basis from Eq. (2) and the DIKL basis from Eq. (6). The other uses the KL basis to generate the KL projection coeffi- cients from Eq. (4) for target t, then apply the coefficients to the DIKL basis to obtain the residuals in Eq. (7). In this way, we can use the same reference array R for the RDI reduction of different targets using DIKL. To implement DIKL on existing high contrast imaging pipelines, we detail the key modifications here. Specifically, in the KL transform we need the eigenvector matrix Q(a) in Eq. (1), and thus such an output is needed when eigendecomposition is conducted (e.g., pyKLIP2, Pynpoint3, and VIP4); for adi.jl5 where singular value decomposition is conducted, we can use the output directly. We can then multiply the boat reference matrix R(b) with the eigenvector matrix to obtain the DIKL basis Z′(b) in Eq. (6). At last, we multiply the KL projection coefficients c(a) from the pipelines for Eq. (4) with the DIKL basis Z′(b), then obtain the residuals in Eq. (7) after DIKL projection. 2 https://bitbucket.org/pyKLIP/pyklip/src/ ab0040da1ae442dce9503502fc92f1736615b37d/pyklip/klip. py#lines-137 3 https://github.com/PynPoint/PynPoint/blob/main/ pynpoint/util/psf.py#L86 4 https://github.com/vortex-exoplanet/VIP/blob/v1.4. 0/vip_hci/psfsub/svd.py#L442C21 5 https://github.com/JuliaHCI/ADI.jl/blob/main/src/ pca.jl#L46 Article number, page 7 of 7
1 Least Squares Maximum and Weighted Generalization-Memorization Machines Shuai Wang, Zhen Wang and Yuan-Hai Shao∗ Abstract—In this paper, we propose a new way of remembering by introducing a memory influence mechanism for the least squares support vector machine (LSSVM). Without changing the equation constraints of the original LSSVM, this mechanism, allows an accurate partitioning of the training set without over- fitting. The maximum memory impact model (MIMM) and the weighted impact memory model (WIMM) are then proposed. It is demonstrated that these models can be degraded to the LSSVM. Furthermore, we propose some different memory impact func- tions for the MIMM and WIMM. The experimental results show that that our MIMM and WIMM have better generalization performance compared to the LSSVM and significant advantage in time cost compared to other memory models. Index Terms—Generalization-memorization mechanism, Ker- nel, Support vector machine, Kernel functiontypesetting. I. INTRODUCTION Z ERO experience risk, also known as memory of training data, has been widely researched and discussed in ma- chine learning [1]–[3]. Traditional learning machines require to classify the training samples correctly as much as possible, but it is prone to fall into the overfitting problem. Therefore, to avoid overfitting, we commonly use regularization techniques but also reduce the memory ability, e.g.support vector ma- chines (SVMs) [4]. However, more powerful tools have been proposed in machine learning based on the zero empirical risks. For instance, Deep Neural Network (DNN) [1], [5], [6] has a structure of multiple hidden layers. Each neuron receives inputs from the neurons in the previous layer and generates outputs that serve as inputs to the neurons in the next layer. And each hidden layer contains multiple neurons to achieve the almost zero empirical risk. It is also realized by Recurrent Neural Network (RNN) [7]–[10] which is a neural network model commonly used in sequential data processing. Compared to traditional feed-forward neural networks, RNN considers temporal dependencies when processing sequential data. Information is allowed to be passed from the current time step to the next time step. This recurrent structure allows RNN to process sequence inputs of arbitrary length and to capture temporal dependencies in the sequence. The Long Short-Term Memory (LSTM) [11]–[13] is a particular RNN for solving the long-term dependency problem in RNN. Unlike the traditional RNN, the LSTM model introduces three gates (input gate, S. Wang is with the School of Mathematics and Statistics, Hainan University , Haikou, China. (e-mail: wangshuai282615@163.com). Z. Wang is with the School of Mathematical Sciences, Inner Mongolia University, Hohhot, 010021, P.R. China. (e-mail: wangzhen@imu.edu.cn). Y.H. Shao (*Corresponding author) is with the Management School, Hainan University, Haikou, P.R. China. (e-mail:shaoyuanhai21@163.com). Manuscript received xx, xx; revised xx, xx. forget gate and output gate) and a memory unit to effectively capture and remember critical information in long sequences. Unlike the LSTM model of memory, Devansh Arpitet al [5] investigated the role of memory in deep learning, linking it to ability, generalization, and adversarial robustness. It is also shown that the training data itself plays an important role in determining the degree of memory. Zhang et al [14].explored a new mechanism to improve model generalization through explicit memory and proposed the residual memory (ResMem) algorithm, a new approach to augment existing prediction models (e.g., neural networks) by fitting the residuals of the model with k-nearest-neighbor based moderators. Indeed, memory systems have been widely explored by researchers to enhance memorization capabilities in various domains. For instance, in the field of machine learning and ar- tificial intelligence, memory mechanisms have been proposed to assist learners in remembering and revising learning tasks [15]–[17].Rafferty et al. [18] presented an observable Partially Observable Markov Decision Process (POMDP) planning problem to address memory tasks [19], [20], while Settle and Meeder [21] developed a trainable memory retention model that optimizes revision schedules for effective memorization. In the realm of deep reinforcement learning, researchers have explored novel methods and optimal policies, elevating the ef- ficiency and engagement of learners [22]–[24] . In other works related to memory, researchers have focused on statistical char- acteristics of learners’ memory behavior rather than just time- series features [25], [26]. This approach has been extended to consider forgetting mechanisms and spaced repetition to improve memory retention [27], [28]. By transforming the optimization problem into a stochastic shortest path problem, these methods aim to enhance the learning process through efficient memory utilization and forgetting strategies [29]– [31]. Recently, Vapnik and Izmailov [4], [32] studied the memory problem of SVMs and introduced two RBF kernels in the SVMs to improve their memory capability, called SVMm. The two RBF kernels, one for generalization and one for memory, are used to memorize the training samples by properly tuning their parameters to achieve zero empirical risk and have a well generalization performance. Subsequently, a generalization- memorization machine (GMM) [33], [34] presented a more general model and explained the mechanism of SVMm more clearly. In this paper, another new memory mechanism is proposed. It contains two memory models by the least squares sense, i.e., a Maximum Impact Memory Model (MIMM) and a Weighted Impact Memory Model(WIMM). Their learning rate are much arXiv:2308.16456v1 [stat.ML] 31 Aug 20232 faster than the GMM and SVMm,while guaranteeing their zero empirical risks. The main contributions of this paper are as follow: • For the memory problem, we proposed the maximum memory impact (MIMM), which uses only the nearest training points for test point judgments and gives a sufficient condition for the empirical risk of the model to be zero. • For the MIMM model, we constructed a memory influ- ence function suitable for the model to ensure the memory capacity of the model. • We provide a clearer interpretation of the memory kernel of the model and derivatively give conditions for the model to degenerate to LSSVM. • Compared with other memory models, the two memory models we proposed, WIMM and MIMM, are shorter in terms of time cost in memorizing the same learning task machines. The next section provides a brief overview of the development of Support Vector Machines (SVM) and Least Squares Support Vector Machines (LSSVM). It also reviews the GMM models. The third section introduces the new objective function and the novel memory mechanism. This includes discussing memory cost and impact functions and how they contribute to solving the MIMM and WIMM models. The last section presents the numerical experiments conducted to validate the proposed MIMM and WIMM models. Conclusions drawn from these experiments are also discussed in this section II. REVIEW Consider a binary classification problem in a n-dimensional real space Rn. The training set is given by T = {(xi, yi)|i = 1, 2, ..., m}, where xi ∈ Rn is the ith sample, and yi ∈ {+1, −1} is the corresponding label. The training samples and their labels are organized into matrix X∈ Rn×m and diagonal matrix Y with diagonal elements Yii = yi (i = 1, ..., m), respectively. SVM [4], [35], [36] deals with this binary classification problem by finding a pair of parallel hyperplanes in the feature space, where the margin is maximized to separate the two classes as much as possible. Sch¨olkopf et al [37]. proposed a new class of regression and classification models based on the SVM, in which a parameter ν was introduced to not only effectively controls the number of support vectors but also suit for different data distributions well. Twin Support Vector Machine (TWSVM) was introduced by Jayadeva et al. [38]. The TWSVM approach aims to identify a pair of non-parallel hyperplanes that can effectively solve the clas- sification problem, resulting in a reduced problem size com- pared to traditional SVMs. To further accelerate the learning speed of SVMs,the Least Squares Support Vector Machine (LSSVM) [39], [40] was proposed by J.A.K. Suykens et al. Due to the equation constraints in the LSSVM formulation, it requires to solve a system linear equations rather than the quadratic programming problem in the SVM. However, the zero empirical risks are guaranteed in neither of these SVMs. Recently, Vapnik and Izmailov [4], [32], [41] proposed a new kernel function consist of two Gaussian kernels as K(x, x′) = τ exp{−σ2(x− ´x)2}+(1−τ) exp{−σ2 ∗(x− ´x)2} (where 0 ≤ τ ≤ 1,andσ∗ ≫ σ). This kernel function could greatly improve the memory ability of SVM. To memorize all the training samples,Wang et al. [33] pro- posed a generalization-memorization machine(GMM) under the principle of large margins, and this mechanism can obtain zero empirical risk easily. Hard Generalization-Mem -orization Machine (HGMM) [33] constructed a classification decision with f(x) =< w, φ(x) > +b + m � i=1 yiciδ(xi, x), and w ∈ Rd and b ∈ R by solving min w,b,c 1 2∥w∥2 + λ 2 ∥c∥2 s.t. yi(< w, φ(xi) > +b + m � j=1 yjcjδ(xi, xj)) ≥ 1, i = 1, ..., m, (1) where < ·, · > denotes the inner product, φ(·) is the mapping, and λ is the positive parameter, c = (c1, ..., cm)⊤ denotes the memory cost of the training sample, δ(xi, x) is a memory impact function that we define in advance. For a new sample x, if f(x) ¿ 0, it is classified as positive class with y = +1, otherwise it is classified as negative class with y = −1. In general, we can solve the pairwise problem of (1) min α 1 2α⊤Y(K(X, X) + 1 λ △ △⊤)Yα − 1⊤α, s.t. 1⊤Yα = 0, α ≥ 0, (2) where α ∈ Rm is a Lagrangian multiplier vector, K(·, ·) =< φ(·), φ(·) > is a kernel function, and 1 is a vector with the appropriate dimension. Specifically, a new sample x will be classified as +1 or -1 depending on the decision f(x) = m � i=1 yiαiK(xi, x) + b + m � i=1 yiciδ(xi, x). (3) Furthermore, by finding a non-zero component αk in the solution α(2) of the problem, we obtain b = yk − yk m � i=1 yi(αi K(xi, xk) + ciδ(xi, xk)). The above HGMM has good generalization ability for many problems, but it is time consuming for big data problems and cannot always classify all training samples quickly. For a memory problem, we not only need to be able to remember the training samples quickly, but also need to give labels quickly during testing. The optimization problem (1) with a memory cost function is a practical path to memorize the training samples. We consider the case where the constraints of this optimization problem are equivocal and propose a new construction on the optimization problem. From this perspective, for our machine learning model, we can solve the problem by solving a system of linear equations. In other words, we have a faster memory effect compared to HGMM, regardless of the complexity of the corresponding learning task. Also, we consider a new type of memory different from the weighted memory in HGMM and propose several constructions of new memory functions.3 III. MEMORY MODEL A. Weighted Impact Memory Model (WIMM) Our WIMM hires the decision function as f(x) =< w, φ(x) > +b + m � i=1 yiξiδ(xi, x), (4) where δ(xi, x) is the memory influence function, and it can be the similarity function between xi and x, e.g., δ(xi, xj) = 1 σ √ 2π exp (−∥ xi − xj ∥2 2σ2 ), σ > 0, (5) δ(xi, xj) = max{ρ− ∥ xi − xj ∥, 0}, ρ > 0, (6) δ(xi, xj) = � ∥ xi − xj ∥, if ∥ xi − xj ∥≤ ε, ε > 0, 0, else, (7) and δ(xi, xj) = � b ∥xi−xj∥, if xi ̸=, xj b > 0, 1, else. (8) The above functions measure the similarity between xi and xj. These influence functions are symmetric, and the memory of each training sample will have an effect on the prediction only if its memory cost is not zero. Then, when combined with the decision function (4), the effect of memory can be achieved. Therefore, our WIMM considers to min w,b,ξ 1 2∥w∥2 + γ 2 m � i=1 ξ2 i + λ m � i=1 m � j=1 yiyjξjδ(xi, xj), s.t. yi(< w, φ(xi) > +b + m � j=1 yjξjδ(xi, xj)) = 1, i = 1, ..., m, (9) where λ, γ is a positive parameter, ξ = (ξ1, ..., ξm)⊤ denotes the memory costs of training samples, and δ(xi, xj) is the memory impact function. Obviously, we use the decision function (4), set the memory cost as a variable and predefine the memory influence function in the decision. From the constraints of (9), it is necessary to remember all the training samples. The goal of problem (9) is to find the optimal strategy with the lowest possible memory cost as well as memory im- pact. To solve problem (9), we derive its Lagrangian function as L(w, b, ξ) = 1 2∥w∥2 + γ 2 m � i=1 ξ2 i + λ m � i=1 yi m � j=1 yjξjδ(xi, xj) + m � i=1 αi(1 − yi(< w, φ(xi) > +b + m � j=1 yjξjδ(xi, xj))), (10) where αi ∈ R is the Lagrangian multiplier with i = 1, . . . , m. Let its partial derivatives w.r.t. w, b, ξi and αi equal zeros, and we have                        ∂L ∂w = w − m � i=1 αiyiφ(xi), ∂L ∂b = m � i=1 αiyi, ∂L ∂ξi = cξi + λyi m � j=1 yjδ(xi, xj) − yiαi m � j=1 yjδ(xi, xj) = 0, ∂L ∂αi = 1 − yi(< w, φ(xi) >i +b + m � j=1 yjξjδ(xi, xj)). (11) Letting the partial derivative equal 0 gives                            w = m � i=1 αiyiφ(xi), m � i=1 αiyi = 0, ξi = αiyi m � j=1 yjδ(xi,xj)−λyi m � j=1 jδ(xi,xj) c , i = 1, . . . , m, yi(< w, φ(xi) >i +b + m � j=1 yjξjδ(xi, xj)) = 1, i = 1, . . . , m. (12) To facilitate the solution, we reformulate problem (12) as �YK(X, X)Y + Y △ △⊤Y Y1 1⊤Y 0 � �α b � = �1 + λ γ △ △⊤1 0 � , (13) where △ ∈ Rm×mand its elements are δ(xi, xj) with i, j = 1, ..., m.,, K(X, X) is a kernel matrix, α = (α1, ..., αm)⊤ and 1 = (1, ..., 1)⊤. After solving the above system of equations, the final decision is f(x) = m � i=1 yiαiK(xi, x) + b + m � i=1 yiξiδ(xi, x). (14) Furthermore, by in problem (12) we obtain ξ = (ξ1, ..., ξm)⊤. B. Maximum Impact Memory Model (MIMM) Different from the WIMM, our MIMM selects the closest training sample of the unknown sample to affect it by decision function as f(x) =< w, φ(x) > +b + yiξiδ(x, xk), (15) where xk is denoted as the centroid of the training point xi of the same kind. For example, suppose x+ and x− are positive and the negative class centroids, respectively. It is a straightforward way to use x+ or x− in δ(xk, x) as the memory influence function. Thus, our MIMM considers to min w,b,ξ 1 2∥w∥2 + γ 2 m � i=1 ξ2 i + λ m � i=1 ξiδi s.t. yi(< w, φ(xi) > +b) = 1 − ξiδi, i = 1, . . . , m, (16)4 where δi = δ(xk, xi) is the memory impact function we define. Instead of using all training samples in our MIMM decision, as in WIMM, we memorize the training samples by finding the closest training samples to the test sample points. Correspondingly, the Lagrangian function of (16) is L(w, b, ξ) = 1 2∥w∥2 + γ 2 m � i=1 ξ2 i + λ m � i=1 ξiδi + m � i=1 αi(1 − ξiδi − yi(< w, φ(xi) > +b)). (17) Find the partial derivatives of w.r.t. w, b, ξiand αi.,and we have                ∂L ∂w = w − m � i=1 αiyixi, ∂L ∂b = m � i=1 αiyi, ∂L ∂ξi = cξi + λδi − αiδi, ∂L ∂αi = 1 − ξiδi − yi(< w, φ(xi) > +b) i = 1, . . . , m. (18) Letting the partial derivative equal 0 gives                w = m � i=1 αiyixi, m � i=1 αiyi = 0, ξi = αiδi−λδi γ , i = 1, . . . , m, yi(< w, φ(xi) > +b) − 1 + ξiδi = 0, i = 1, . . . , m. (19) After simplifying the system of equations, we get: �YK(X, X)Y + YDD⊤Y Y1 1⊤Y 0 � �α b � = �1 + λ γ DD⊤1 0 � , (20) where D ∈ Rm×mand its a diagonal matrix with Dii = δi(i = 1, ..., m). Thus, the WIMM model obtains b and α by solving the system of linear equations (20), and then ξ by the optimality condition (19), the final decision is f(x) = m � i=1 yiαiK(xi, x) + b + yiξiδ(x, xk). (21) Indeed, the advantage of memorization becomes evident when we combine the memorization function with the LSSVM method. This combination allows us to carefully observe and analyze the impact of each memorization influence function on the overall performance of the model of the combined model. Specifically, consider a learner that incorporates a gen- eralized kernel Kg(x, x) = exp (− ∥xi−xj∥2 σ2 ) and a memory kernel Km, where the memory influence function is chosen as equations (5), (6), (7), and (8). Figure 1 illustrates the generated memory influence. With this memory influence, we can intuitively observe the range and degree of influence for each different influence function. We utilize the memory influence function to establish a rule, where classification is remembered only within a small region around the training data points. By adjusting the parameters of the influence function, we control the trade-off between generalization and memory in the algorithm. -2 -1 0 1 2 Generalization-memorization kernel1 0 1 2 3 4 5 -2 -1 0 1 2 Generalization-memorization kernel2 0 0.5 1 -2 -1 0 1 2 Generalization-memorization kernel3 0 0.5 1 1.5 -2 -1 0 1 2 Generalization-memorization kernel4 0 1 2 3 Fig. 1: Different types of memory kernels.1,2 ,3 and 4 with the influence function 5, 6, 7 and 8. Where red indicates the extent of memory influence and green indicates the extent of generalization influence. IV. DISCUSSION Proposition 1: The empirical risk of WIMM is zero if and only if problem (9) has at least one feasible solution. Similarly, the empirical risk of MIMM is zero if and only if problem (16) has at least one feasible solution. The feasibility of problems (9) or (16) depends on the proper- ties of the memory influence matrices △ or D. Generally, we have the following sufficient conditions for practical applica- tions. Proposition 2: The empirical risk of WIMM is zero if and only if matrix △ is nonsingular. Similarly, the empirical risk of MIMM is zero if and only if the D matrix is non-singular. Proof. We have considered the case where the △ matrix is non-singular. It can be shown that Y(K(X, X)+△)Y⊤ is also non-singular. Additionally, as r(1⊤Y) = 1, the problem (13) must have a unique solution. Similarly, it can be demonstrated that when the D matrix is non-singular, the problem (20) must also have a unique solution. This conclusion follows from proposition (1). □ Proposition 3: MIMM is equivalent to the LSSVM model if and only if D is a unit array and λ = 0. Proof. When D is a unitary matrix and λ = 0, the problem (20) is clearly in the form of a least squares system of linear equations to be solved. The proposition is proved and the conclusion holds. □ Proposition 4: WIMM is equivalent to the LSSVM model if and only if △ is a unit array and λ = 0. Proof. When △ is a unitary matrix and λ = 0, the problem (13) is clearly in the form of a system of linear equations solved by least squares. The proposition is proved and the conclusion holds. □ Fig. (2) gives information about the interrelationships be- tween the three memory kernels by comparing the memory5 Fig. 2: A memory relation diagram for MIMM,WIMM and SVMm [32], where yellow, pink and blue-gray denote the D, △ and km memory kernel matrices, respectively. kernels of equations (20), (13) and SVMm [32]. We can find that these three memory kernels are D, △ and km, which can be obtained by the matrix structure, D is a diagonal matrix, and △ and km are symmetric matrices. By tuning the parameters, D, △ and km can be varied to a unitary matrix. Thus there exists an intersection of these three memory kernels. Since △ can choose more than just one type of Gaussian kernel, △ contains km. Since △ and D have different influence functions, △ and D only have an intersection but no containment relationship. V. EXPERIMENTS This section utilizes several calibration datasets from UCI, for which Table (I) provides detailed information. We analyze the performance of our WIMM and MIMM models on various benchmark datasets, along with their execution times on large datasets. Additionally, we test the generalization performance of the two models and their ability to adapt to noise. The classical LSSVM utilizes linear kernels, while the SVMm and HGMM models employ linear generalization kernels and RBF memory kernels. In contrast, our WIMM and MIMM models both utilize linear kernels. All these models were implemented using MATLAB 2017a on a PC equipped with an Intel Core Duo processor (dual 4.2 GHz) and 32 GB of RAM. For the RBF kernel K(xi, xj) = exp(−σ ∥ xi − xj ∥2), we tested parameters σ from the set 2i|i = −6, −5, ..., 5, and for other models, we tested weighing parameters from the same set. To begin the comparison, we evaluated the memory performance of the linear kernel in WIMM and MIMM models on some small datasets, with the linear kernel in LSSVM used as the benchmark. To assess the memory capacity of the WIMM model, Table (II) presents the highest training and testing accuracies achieved by the WIMM model. This table provides valuable insights into the model’s ability to memorize and generalize effectively on the tested datasets. It can be observed from Table (II) that the WIMM model with the memory influence function (5, 6, 7) achieves a training accuracy of 100% on all datasets. However, the failure to reach 100% training accuracy can be attributed to the irreversibility of the △ term in the function and the impact of different influence functions on the data’s TABLE I: Details of benchmark datasets ID Name m n (a) Cleveland 173 13 (b) Ionosphere 351 34 (c) New-thyroid 215 4 (d) Parkinsons 195 22 (e) Sonar 208 60 (f) TicTacToe 958 27 (g) Vowel 988 13 (h) Wisconsin 683 9 (i) German 1000 20 (j) Shuttle 1829 9 (k) Segment 2308 19 (l) Waveform 5000 21 (m) TwoNorm 7400 20 (n) IJCNN01 49990 22 memory capacity. Among the various influence functions, the memory influence function (5) yields the highest test accuracy for most of the datasets. Consequently, for the remaining experiments, we utilize the memory influence function (5) as the basis for our WIMM model. Likewise, to evaluate the memory capacity of the MIMM model, Table (III) displays the maximum training and testing accuracies achieved by the MIMM model. It is evident from Table (III) that the MIMM model attains a training accuracy of 100% when using the memory influence function (8). The reason the other influence functions do not achieve 100% training accuracy is due to the irreversibility of D in these functions. The choice of different influence functions impacts the data’s memory capacity. Among the various influence functions, the memory influence function (8) yields the highest test accuracy for the majority of the datasets. As a result, for the subsequent experiments, we adopt the memory influence function (8) as the basis for our MIMM model. Next, to compare the running times of other memory models under optimal parameters, we recorded the execution times along with the corresponding accuracies on a larger dataset. This evaluation allows us to further assess the trade-off between the time consumed for memorization and the achieved performance on the same task for different memory models. For each dataset, approximately 70% of the total samples were randomly selected for training, ensuring that half of them belonged to the positive category and the other half to the negative category, while the remaining samples constituted the test set. This process was repeated five times, and the highest average training accuracy along with its standard deviation, the corresponding highest test accuracy, and the time taken to run the model once with optimal parameters were recorded for each dataset. The shortest time spent is indicated in bold in Table (IV). From Table (IV), it is evident that the test accuracies do not differ significantly. Notably, both WIMM and MIMM exhibit shorter execution times compared to HGMM and SVMm. This efficiency can be attributed to the fact that WIMM and MIMM models are solved as linear system of equations, whereas HGMM and SVMm are solved as quadratic programming problems. In practical applications, many tasks involve learning with labeled noise. Therefore, to examine the ability of WIMM and6 TABLE II: Testing and training accuracy of WIMM and LSSVM using memory effects. ID LSSVM WIMM1 WIMM2 WIMM3 WIMM4 LSSVM WIMM1 WIMM2 WIMM3 WIMM4 train(%) train(%) train(%) train(%) train(%) test(%) test(%) test(%) test(%) test(%) (a) 96.39±0.47 100 ± 0 100 ± 0 100 ± 0 90.77 ± 1.0 94.82±4.18 95.44±3.36 95.36±3.29 95.89 ± 4.49 95.89 ± 4.49 95.89 ± 4.49 92.85±6.16 (b) 89.46±0.47 100 ± 0 100 ± 0 100 ± 0 44.02 ± 1.6 88.3 ± 3.46 88.31±3.13 88.36±5.52 89.77 ± 3.86 89.77 ± 3.86 89.77 ± 3.86 42.24±9.23 (c) 94.08±0.86 100 ± 0 100 ± 0 100 ± 0 72.9 ± 2.08 93.66±3.41 97.79 ± 2.15 97.79 ± 2.15 97.79 ± 2.15 87.94±4.84 89.11 ± 7.7 88.31±4.52 (d) 91.42±1.15 100 ± 0 100 ± 0 100 ± 0 64.57±5.01 88.4 ± 7.44 94.73 ± 4.95 94.73 ± 4.95 94.73 ± 4.95 88.7 ± 5.84 91.8 ± 3.17 83.9 ± 4.3 (e) 87.99±1.36 100 ± 0 100 ± 0 100 ± 0 83.65 ± 1.4 79.48±2.85 86.79 ± 8.43 86.79 ± 8.43 86.79 ± 8.43 78.89±1.12 80.29±5.84 79.03±8.03 (f) 98.33±0.25 100 ± 0 100 ± 0 100 ± 0 100 ± 0 98.33 ± 1.0 98.33 ± 1.0 98.33 ± 1.0 98.33 ± 1.13 98.33 ± 1.13 98.33 ± 1.13 98.33 ± 1.62 98.33 ± 1.62 98.33 ± 1.62 98.33 ± 1.13 98.33 ± 1.13 98.33 ± 1.13 65.35±4.38 (g) 95.04±0.34 100 ± 0 100 ± 0 100 ± 0 75.05±7.08 95.04±2.08 100 ± 0 100 ± 0 100 ± 0 99.8 ± 0.28 99.8 ± 0.45 94.84±1.08 (h) 96.16±0.61 100 ± 0 100 ± 0 100 ± 0 95.65±0.71 96.18±2.45 96.63±1.52 96.65±2.25 96.78 ± 1.84 96.78 ± 1.84 96.78 ± 1.84 90.05±3.13 1,2 ,3 and 4 with the influence function 5, 6, 7 and 8. TABLE III: Testing and training accuracy of MIMM and LSSVM using memory effects. ID LSSVM MIMM1 MIMM2 MIMM3 MIMM4 LSSVM MIMM1 MIMM2 MIMM3 MIMM4 train(%) train(%) train(%) train(%) train(%) test(%) test(%) test(%) test(%) test(%) (a) 96.39±0.47 96.4 ± 0.79 94.83 ± 3.1 98.25 ± 0.9 100±0 94.82±4.18 95.28 ± 3.4 94.72±4.81 95.33±1.71 95.36 ± 2.6 95.36 ± 2.6 95.36 ± 2.6 (b) 89.46±0.47 100 ± 0 91.45±2.01 100 ± 0 100±0 88.3 ± 3.46 90.08±4.29 94.61 ± 3.64 94.61 ± 3.64 94.61 ± 3.64 88.61±1.67 89.75±4.63 (c) 94.08±0.86 100 ± 0 99.09±2.03 100 ± 0 100±0 93.66±3.41 98.64±1.18 98.57±1.28 98.72 ± 1.9 98.73 ± 1.9 98.73 ± 1.9 98.73 ± 1.9 (d) 91.42±1.15 100 ± 0 97.64±2.25 100 ± 0 100±0 88.4 ± 7.44 97.49 ± 1.49 97.49 ± 1.49 97.49 ± 1.49 97.07±1.93 96.42±3.03 96.3 ± 1.62 (e) 87.99±1.36 100 ± 0 86.85±2.73 100 ± 0 100±0 79.48±2.85 86.11±5.32 87.46±3.01 86.94 ± 2.2 88.47 ± 2.63 88.47 ± 2.63 88.47 ± 2.63 (f) 98.33±0.25 100 ± 0 98.33±1.44 100 ± 0 100±0 98.33 ± 1.0 98.33 ± 1.0 98.33 ± 1.0 98.33 ± 1.7 98.33 ± 1.7 98.33 ± 1.7 98.33 ± 0.93 98.33 ± 0.93 98.33 ± 0.93 98.33 ± 0.93 98.33 ± 0.93 98.33 ± 0.93 98.33 ± 1.13 98.33 ± 1.13 98.33 ± 1.13 (g) 95.04±0.34 100 ± 0 100 ± 0 100 ± 0 100±0 95.04±2.08 100 ± 0 100 ± 0 100 ± 0 100 ± 0 100 ± 0 100 ± 0 100 ± 0 100 ± 0 100 ± 0 100 ± 0 100 ± 0 100 ± 0 (h) 96.16±0.61 100 ± 0 97.23±1.29 100 ± 0 100±0 96.18±2.45 97.36±1.33 97.37 ± 0.82 97.37 ± 0.82 97.37 ± 0.82 97.08 ± 1.0 96.94±1.65 1,2 ,3 and 4 with the influence function 5, 6, 7 and 8. TABLE IV: Accuracy and time to train and test linear classifiers on benchmark datasets. ID SVMm HGMM WIMM MIMM SVMm HGMM WIMM MIMM train(%) train(%) train(%) train(%) test(%) test(%) test(%) test(%) time(s) time(s) time(s) time(s) (i) 100 ± 0 100 ± 0 100 ± 0 100 ± 0 76.1 ± 1.77 78.33±3.53 76.64±1.43 72.06±2.55 0.198s 0.196s 0.119s 0.105s (j) 100 ± 0 100 ± 0 100 ± 0 100 ± 0 99.95±0.12 100 ± 0 99.96±0.08 100 ± 0 0.738s 0.663s 0.344s 0.291s (h) 100 ± 0 100 ± 0 100 ± 0 100 ± 0 99.83±0.18 99.88±0.19 99.86±0.14 99.77±0.21 0.997s 1.256s 0.652s 0.393s (l) 100 ± 0 100 ± 0 100 ± 0 100 ± 0 90.16±0.81 88.27±0.61 86.42±0.85 83.24±0.67 5.835s 6.682s 2.362s 1.656s (m) 100 ± 0 100 ± 0 100 ± 0 100 ± 0 98.02±0.25 97.98±0.26 97.99 ± 0.5 95.09±0.56 15.793s 18.531s 5.888s 3.895s (n) ∗ ∗ 100 ± 0 100 ± 0 ∗ ∗ 93.80±0.26 97.37±0.08 787.645s 435.870s ’∗’ indicates a lack of sufficient operating memory. MIMM models to adapt to noise, we conducted experiments with datasets containing labeled noise. For certain datasets from Table (I), we randomly select 80% of the training samples to form the training set, while the remaining samples constitute the test set. We then introduce label noise to the training set, setting 5%, 10%, 15%, and gradually up to 50% of the labels to the opposite class. This process is repeated five times, and we record the highest test accuracy along with the corresponding average training accuracy for comparison with LSSVM. From Figures (3) and (4), we can observe the following trends: i) The training accuracy of LSSVM is not consistently 100% except for our WIMM and MIMM models. ii) The test performance of LSSVM is consistently lower than our model and exhibits instability with increasing label noise. iii) The test performance of our models (WIMM and MIMM) shows a gradual decline as the label noise increases in a regular manner. These observations suggest that our WIMM and MIMM models outperform LSSVM in handling labeled noise and offer more stable and robust performance under noisy conditions. Moreover, in many tasks, obtaining an adequate number of training samples can be particularly challenging. Hence, we further investigate the performance of these models in comparison to our WIMM and MIMM models under condi- tions of limited training samples. For selected datasets from Table (I), we randomly select 80% of the samples to form the training set, and the remaining samples constitute the test set. Subsequently, we vary the proportion of training samples used, ranging from 10% to 100%, in incremental steps. The models are tested on the dataset, and this process is repeated five times. We record the highest test accuracy along with the corresponding average training accuracy for comparison with LSSVM. From Figures (5) and (6), the following observations are made: i) Apart from our WIMM and MIMM models, the training accuracy of LSSVM is not consistently 100%. ii) The test performance of LSSVM is consistently inferior to our7 5% 15% 25% 35% 45% Percentage of noise used in Vowel 0.85 0.9 0.95 1 Training accuracy WIMM LSSVM 5% 15% 25% 35% 45% Percentage of noise used in Vowel 0.5 0.6 0.7 0.8 0.9 1 Test accuracy WIMM LSSVM 5% 15% 25% 35% 45% Percentage of noise used in Vowel 0.85 0.9 0.95 1 Training accuracy MIMM LSSVM 5% 15% 25% 35% 45% Percentage of noise used in Vowel 0.5 0.6 0.7 0.8 0.9 1 Test accuracy MIMM LSSVM Fig. 3: Training (left)/Testing (right) accuracy at different noise points 5% 15% 25% 35% 45% Percentage of noise used in Sonar 0.85 0.9 0.95 1 Training accuracy WIMM LSSVM 5% 15% 25% 35% 45% Percentage of noise used in Sonar 0.6 0.7 0.8 0.9 1 Test accuracy WIMM LSSVM 5% 15% 25% 35% 45% Percentage of noise used in Sonar 0.85 0.9 0.95 1 Training accuracy MIMM LSSVM 5% 15% 25% 35% 45% Percentage of noise used in Sonar 0.6 0.7 0.8 0.9 1 Test accuracy MIMM LSSVM Fig. 4: Training (left)/Testing (right) accuracy at different noise points8 10% 30% 50% 70% 90% Percentage of Vowel used 0.92 0.94 0.96 0.98 1 1.02 Training accuracy WIMM LSSVM 10% 30% 50% 70% 90% Percentage of Vowel used 0.4 0.5 0.6 0.7 0.8 0.9 1 Test accuracy WIMM LSSVM 10% 30% 50% 70% 90% Percentage of Vowel used 0.92 0.94 0.96 0.98 1 1.02 Training accuracy MIMM LSSVM 10% 30% 50% 70% 90% Percentage of Vowel used 0.5 0.6 0.7 0.8 0.9 1 Test accuracy MIMM LSSVM Fig. 5: Training(left)/Test(right) accuracy for different sample sizes. models, and its performance improves as more training data is used. iii) The test performance of our models (WIMM and MIMM) demonstrates a steady improvement as the number of training samples increases. These findings suggest that our WIMM and MIMM models outperform LSSVM, especially when training data is limited, and they consistently achieve higher test accuracy as the number of training samples grows. To compare the impact of different memory influence func- tions on the WIMM and MIMM models, we analyze the effect of memory kernel parameters on the models. Figures (7) and (8) illustrate this impact, with the LSSVM model used as a benchmark for comparison. In this experiment, we consider the models with parameters ranging from {0.1, 0.2, ..., 2}. Specifically, we focus on the Segment and Sonar datasets from Table (I). For these datasets, 80% of the training samples are randomly selected as the training set, and the remaining samples form the test set. The process is repeated five times, recording the highest test accuracy and its corresponding average training accuracy for comparison with LSSVM. As the WIMM model with the memory influence function (8) demonstrated poorer results in Table (II), we consider the influence case of this model. From Figures (7) and (8), we can make the following observations: i) The WIMM model is more sensitive to the parameters, and its ability to memorize is contingent on selecting the appropriate parameters. ii) The MIMM model, particularly with the memory influence function (8), exhibits greater stability, consistently memorizing the training samples while ensuring that the test performance remains superior to that of the LSSVM model. iii) Overall, our models consistently outperform the LSSVM model, provided that we select the right parameters. These findings emphasize the importance of parameter selection in the WIMM model, while the MIMM model offers a more robust performance with the chosen memory influence function. In general, our models demonstrate superior performance compared to the LSSVM model when the appropriate parameters are employed. VI. CONCLUSION We have presented two novel innovations in the traditional LSSVM framework: (i) We have proposed a replacement for the objective function of LSSVM, leading to improved perfor- mance. (ii) We have introduced a new memory generalization kernel that effectively incorporates the complete memory of the training data, achieving zero training error. As a result of these innovations, the MIMM and WIMM models have demonstrated superior generalization accuracy while maintain- ing the same computational complexity. Specifically, they still have involved solving a system of linear equations with a corresponding dimension, just like the current LSSVM im- plementation. Furthermore, our models have exhibited higher classification accuracy and have enhanced noise tolerance on certain datasets. Additionally, they have required less time and have cost to memorize training samples compared to existing memory models. In future work, we plan to extend our memory enhancement mechanism to other models and explore its applicability to a variety of other problems. In addition, we intend to consider multiple memory patterns in our memory model and introduce forgetting mechanisms to enrich the memory capacity to effectively solve a wider range of tasks.9 10% 30% 50% 70% 90% Percentage of Segment used 0.96 0.97 0.98 0.99 1 1.01 1.02 1.03 Training accuracy WIMM LSSVM 10% 30% 50% 70% 90% Percentage of Segment used 0.96 0.97 0.98 0.99 1 1.01 1.02 1.03 Test accuracy WIMM LSSVM 10% 30% 50% 70% 90% Percentage of Segment used 0.96 0.97 0.98 0.99 1 1.01 1.02 1.03 Training accuracy MIMM LSSVM 10% 30% 50% 70% 90% Percentage of Segment used 0.96 0.97 0.98 0.99 1 1.01 1.02 1.03 Test accuracy MIMM LSSVM Fig. 6: Training(left)/Test(right) accuracy for different sample sizes. 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2 Memory functions are used in Segment 0.85 0.9 0.95 1 Training accuracy WIMM1 WIMM2 WIMM3 LSSVM 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2 Memory functions are used in Segment 0.85 0.9 0.95 1 Test accuracy WIMM1 WIMM2 WIMM3 LSSVM 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2 Memory functions are used in Sonar 0.7 0.75 0.8 0.85 0.9 0.95 1 Training accuracy WIMM1 WIMM2 WIMM3 LSSVM 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2 Memory functions are used in Sonar 0.7 0.75 0.8 0.85 0.9 Test accuracy WIMM1 WIMM2 WIMM3 LSSVM Fig. 7: Training (left)/testing (right) accuracy with different influence functions.10 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2 Memory functions are used in Segment 0.984 0.986 0.988 0.99 0.992 0.994 0.996 0.998 1 Training accuracy MIMM1 MIMM2 MIMM3 MIMM4 LSSVM 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2 Memory functions are used in Segment 0.98 0.985 0.99 0.995 1 Test accuracy MIMM1 MIMM2 MIMM3 MIMM4 LSSVM 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2 Memory functions are used in Sonar 0.86 0.88 0.9 0.92 0.94 0.96 0.98 1 Training accuracy MIMM1 MIMM2 MIMM3 MIMM4 LSSVM 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2 Memory functions are used in Sonar 0.7 0.75 0.8 0.85 0.9 Test accuracy MIMM1 MIMM2 MIMM3 MIMM4 LSSVM Fig. 8: Training (left)/testing (right) accuracy with different influence functions. ACKNOWLEDGEMENT This work is supported in part by National Natural Science Foundation of China (Nos. 12271131, 62106112, 61866010,61966024 and 11871183), in part by the Natural Science Foundation of Hainan Province (No.120RC449), and in part by the Key Laboratory of Engineering Modeling and Statistical Computation of Hainan Province. REFERENCES [1] S. Chatterjee, “Learning and memorization,” pp. 755–763, 2018. [2] V. Feldman, “Does learning require memorization? a short tale about a long tail,” in Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, 2020, pp. 954–959. [3] V. Vapnik and R. Izmailov, “Reinforced svm method and memorization mechanisms,” Pattern Recognition, vol. 119, p. 108018, 2021. [4] C. Cortes and V. Vapnik, “Support-vector networks,” Machine learning, vol. 20, pp. 273–297, 1995. [5] D. Arpit et al., “A closer look at memorization in deep networks,” in Proceedings of the 34th International Conference on Machine Learning, vol. 70, 2017, pp. 233–242. [6] G. Cohen et al., “Dnn or k-nn: That is the generalize vs. memorize question,” arXiv preprint arXiv:1805.06822, 2018. [7] J. L. Elman, “Finding structure in time,” Cognitive science, vol. 14, no. 2, pp. 179–211, 1990. [8] Z. Yang et al., “Rethinking bias-variance trade-off for generalization of neural networks,” in International Conference on Machine Learning. PMLR, 2020, pp. 10 767–10 777. [9] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural network,” arXiv preprint arXiv:1503.02531, 2015. [10] I. Goodfellow, Y. Bengio, and A. Courville, Deep learning. MIT press, 2016. [11] C. M. Bishop, Neural networks for pattern recognition. Oxford university press, 1995. [12] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997. [13] C. Zhang et al., “Understanding deep learning (still) requires rethinking generalization,” Communications of the ACM, vol. 64, no. 3, pp. 107– 115, 2021. [14] Z. Yang et al., “Resmem: Learn what you can and memorize the rest,” arXiv preprint arXiv:2302.01576, 2023. [15] S. Wan and Z. Niu, “A hybrid e-learning recommendation approach based on learners’ influence propagation,” IEEE Transactions on Knowl- edge and Data Engineering, vol. 32, no. 5, pp. 827–840, 2019. [16] A. Cully and Y. Demiris, “Online knowledge level tracking with data- driven student models and collaborative filtering,” IEEE Transactions on Knowledge and Data Engineering, vol. 32, no. 10, pp. 2000–2013, 2019. [17] N. S. Clayton et al., “Elements of episodic–like memory in animals,” Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences, vol. 356, no. 1413, pp. 1483–1491, 2001. [18] A. N. Rafferty et al., “Faster teaching via pomdp planning,” Cognitive science, vol. 40, no. 6, pp. 1290–1332, 2016. [19] Q. Kang and W. P. Tay, “Sequential multi-class labeling in crowdsourc- ing,” IEEE Transactions on Knowledge and Data Engineering, vol. 31, no. 11, pp. 2190–2199, 2018. [20] I. P. Androulakis, “Dynamic programming: Stochastic shortest path problems.” 2009. [21] B. Settles and B. Meeder, “A trainable spaced repetition model for language learning,” in Proceedings of the 54th annual meeting of the association for computational linguistics (volume 1: long papers), 2016, pp. 1848–1858. [22] J. Ke et al., “Learning to delay in ride-sourcing systems: a multi- agent deep reinforcement learning framework,” IEEE Transactions on Knowledge and Data Engineering, vol. 34, no. 5, pp. 2280–2292, 2020. [23] Y. Zhang et al., “Cost-sensitive portfolio selection via deep reinforce- ment learning,” IEEE Transactions on Knowledge and Data Engineer- ing, vol. 34, no. 1, pp. 236–248, 2020. [24] U. Upadhyay, A. De, and M. Gomez Rodriguez, “Deep reinforcement learning of marked temporal point processes,” Advances in Neural Information Processing Systems, vol. 31, 2018. [25] J. R. Anderson, “Act: A simple theory of complex cognition.” American psychologist, vol. 51, no. 4, p. 355, 1996. [26] H. Pashler et al., “Predicting the optimal spacing of study: A multiscale context model of memory,” Advances in neural information processing systems, vol. 22, 2009. [27] J. Su et al., “Optimizing spaced repetition schedule by capturing the dynamics of memory,” IEEE Transactions on Knowledge and Data Engineering, 2023. [28] J. Ye, J. Su, and Y. Cao, “A stochastic shortest path algorithm for optimizing spaced repetition scheduling,” in Proceedings of the 28th11 ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022, pp. 4381–4390. [29] G. B. Maddox et al., “The role of forgetting rate in producing a benefit of expanded over equal spaced retrieval in young and older adults.” Psychology and aging, vol. 26, no. 3, p. 661, 2011. [30] S. Reddy et al., “Unbounded human learning: Optimal scheduling for spaced repetition,” in Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, 2016, pp. 1815–1824. [31] A. Nioche et al., “Improving artificial teachers by considering how people learn and forget,” in 26th International Conference on Intelligent User Interfaces, 2021, pp. 445–453. [32] V. Vapnik and R. Izmailov, “Reinforced svm method and memorization mechanisms,” Pattern Recognition, vol. 119, p. 108018, 2021. [33] Z. Wang and Y.-H. Shao, “Generalization-memorization machines,” arXiv preprint arXiv:2207.03976, 2022. [34] A. J. Smola and B. Sch¨olkopf, Learning with kernels. Citeseer, 1998, vol. 4. [35] V. N. Vapnik, “An overview of statistical learning theory,” IEEE trans- actions on neural networks, vol. 10, no. 5, pp. 988–999, 1999. [36] V. Vapnik, Estimation of dependences based on empirical data. Springer Science & Business Media, 2006. [37] B. Sch¨olkopf et al., “New support vector algorithms,” Neural computa- tion, vol. 12, no. 5, pp. 1207–1245, 2000. [38] Jayadeva, R. Khemchandani, and S. Chandra, “Twin support vector machines for pattern classification,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 29, pp. 905–910, 2007. [39] J. A. Suykens and J. Vandewalle, “Least squares support vector machine classifiers,” Neural processing letters, vol. 9, pp. 293–300, 1999. [40] C. Zhang et al., “Understanding deep learning (still) requires rethinking generalization,” Communications of the ACM, vol. 64, no. 3, pp. 107– 115, 2021. [41] M. Belkin et al., “Reconciling modern machine-learning practice and the classical bias–variance trade-off,” Proceedings of the National Academy of Sciences, vol. 116, no. 32, pp. 15 849–15 854, 2019.
Learning Channel Importance for High Content Imaging with Interpretable Deep Input Channel Mixing Daniel Siegismund, Mario Wieser, Stephan Heyse, and Stephan Steigele Genedata AG, Basel, Switzerland Abstract. Uncovering novel drug candidates for treating complex dis- eases remain one of the most challenging tasks in early discovery research. To tackle this challenge, biopharma research established a standardized high content imaging protocol that tags different cellular compartments per image channel. In order to judge the experimental outcome, the sci- entist requires knowledge about the channel importance with respect to a certain phenotype for decoding the underlying biology. In contrast to traditional image analysis approaches, such experiments are nowadays preferably analyzed by deep learning based approaches which, however, lack crucial information about the channel importance. To overcome this limitation, we present a novel approach which utilizes multi-spectral in- formation of high content images to interpret a certain aspect of cellu- lar biology. To this end, we base our method on image blending con- cepts with alpha compositing for an arbitrary number of channels. More specifically, we introduce DCMIX, a lightweight, scaleable and end-to- end trainable mixing layer which enables interpretable predictions in high content imaging while retaining the benefits of deep learning based methods. We employ an extensive set of experiments on both MNIST and RXRX1 datasets, demonstrating that DCMIX learns the biologically relevant channel importance without scarifying prediction performance. Keywords: Biomedical Imaging · Interpretable Machine Learning · Ex- plainable AI · Image Channel Importance. 1 Introduction High-Content Imaging (HCI) has developed to one of the main driving factors in biopharma early discovery research to reveal novel drug candidates for sophisti- cated treatment strategies such as cancer immunotherapies [21]. HCI is based on a standardized experimental protocol that allow for the systematic acquisition of multi-spectral images, e.g., in form of a cell painting assay protocol that requires a high number of channels with the benefit of a highly generalizable assay [3]. Here, high-content images are recorded by automated instruments on microtiter plates which allow for large-scale drug candidate testing and an automatic anal- ysis procedure to assess the mechanics of a drug candidate for a certain disease. When running such HCI experiments, scientists prepare typically a set of 4 to 15 arXiv:2308.16637v1 [cs.CV] 31 Aug 20232 D. Siegismund, M. Wieser et al. channels [23,31] with a specific fluorophore that tags a certain cellular protein or compartment. Subsequently, the scientist aims to analyze the experimental outcome with respect to the importance of the fluorescence channels to validate the findings or refine the experiment and, therefore, requires a fast and easy-to- use analysis workflow. This is particularly important as the specific functional or mechanistic knowledge is encoded via the specific staining per image channel [3] and hence required for decoding the underlying biology. However, to analyze such complex multi-channel cell-painting assays, the sci- entist requires the ability of sophisticated image analysis to distill the informa- tion from the multi-spectral information. In biopharma research, the traditional analysis [5] is gradually replaced by deep learning based approaches [40,14,48,39]. Despite the superior performance of such models in comparison to conventional segmentation based analysis [5], the scientist lacks informative insights in terms of understanding about which fluorescence channel influenced the decision [7]. In the past, various approaches have been proposed to extract the most rel- evant information from high-dimensional datasets. The most basic approach to determine the most relevant channels is a preprocessing step by applying an un- supervised dimensionality reduction method such as Principal Component Anal- ysis (PCA) [17]. However, employing such a preprocessing step does not guaran- tee for phenotype-specific channels as the method only optimizes for the direc- tions with the highest variance and not necessarily for the highest phenotypic information. More recently, attention-based approaches have been introduced for image channel selection [4,15,24,32] which suffer from high computational costs and poor scalability. In addition, there are model-agnostic approaches such as Shapely values [36,19] which, however, can suffer from sampling variability [27] and be time consuming in terms of highly complex models [6]. To overcome the aforementioned limitations, we present a simple yet effective method to estimate channel importance for HCI images. More specifically, we introduce a lightweight, easy to use mixing layer that is composed of a general- ized image blending mechanism with alpha compositing [50,1] which converts a d-dimensional channel image into a 2D image retaining all phenotype relevant information. This allows not only to incorporate an arbitrary number of chan- nels in a highly scalable fashion but also leads to a reduced network size with faster inference times while being able to facilitate the use of transfer learning of pretrained networks. To summarize, we make the following contributions: – We extend the imaging blending concepts of [50,1] and apply these to images with an arbitrary number of channels. – We encapsulate the generalized image blending into a lightweight, scalable and end-to-end trainable mixing layer, called DCMIX, to estimate channel importance for multi-spectral HCI data. – Experiments on MNIST as well as on the challenging multi-channel real- world imaging data set RXRX1 [42] with 31 different cell phenotype classes demonstrate that the proposed method learns the correct channel impor- tance without sacrificing its model performance.DCMIX 3 2 Related Work In this section, we review related work on interpretable and explainable machine learning [30]. Broadly spoken, we can distinguish between interpretable models that are interpretable by design and explainable models that try to explain existing models post-hoc [30]. Interpretable Machine Learning Methods can be separated into the fol- lowing model classes: score-based [44], rule-based [9], sparse [43] and neural net- works [12], among others [30]. In this review, we focus more closely on sparsity inducing and attention-based interpretable methods. Sparsity-based approaches introduce a sparsity constraint on the model coefficients to determine the feature importance. One of the most basic approaches is the least absolute shrinkage and selection operator (LASSO) introduced by [43] which is employing the L1-norm to ensure feature sparsity. This approach has subsequently been extended to various lines of research, including dealing with grouped features [49,33], esti- mating network graphs [13,34] or learning sparse representations in neural net- works [26,47]. Most closely related to our work is LassoNet [22] which employs a group lasso constraint based on the feature channels that are obtained from a pretrained feature extraction network. In contrast, our approach is end-to-end trainable and hence does not require a two step approach of feature extrac- tion and importance estimation. More recently, attention-based approaches [45] have emerged in the context of interpretable machine learning. [8] introduced an attention-based model for the analysis of electronic health records and [37] learns important features with an attentive mixture of experts approach. More- over, attention is used in the context of hyper spherical band/channel selection [4,15,24,32]. In contrast, our approach works on image blending and alpha com- positing and hence reducing high computational costs. Explainable Machine Learning Methods denote approaches that aim to ex- plain decisions of an already trained machine learning model post-hoc by learning a surrogate model [30]. In summary, we distinguish between attribution methods that try to quantify the attribution of a feature to the prediction [41], concept- based explanations trying to explain predictions with high-level concepts [20], symbolic metamodels employing symbolic regression as a surrogate [2] and coun- terfactual explanations [46]. In the context of our work, we focus on attribution models. [35] learns a surrogate classifier to explain an arbitrary black-box model based on submodular optimization. [38] introduced DeepLIFT to decompose the input contributions on the model prediction. In addition, Shapley values gained a wide adoption in the machine learning domain mainly for feature selection and model explain ability [36,19]. As a result, Lundberg & Lee [28] introduced Shapely additive explanations (SHAP) to explain model predictions based on Shapely regression values. Finally, Shapley values have been used in the con- text of HCI channel importance estimation [42]. More specifically, the authors adopt Shapely values to explain the channel importance of HCI images from a4 D. Siegismund, M. Wieser et al. pretrained black-box model. Opposed to our approach, this method requires the training of two separate models and hence does not allow for end-to-end training. Fig. 1. Blue arrows denote steps and gray boxes actions in our workflow, respectively. In the first step (1.), we take a multi-channel cellular image and split it into single channels. Subsequently, we mix the channel within our DCMIX layer to obtain the most important part of each channel. In the second step (2.), we take the blended image into our classification network. 3 Model As illustrated in Figure 1, we utilize a two step approach for estimating channel importance in multi-spectral bioimage classification settings by introducing a lightweight, easy to use and end-to-end trainable mixing layer. To do so, we propose a blending layer which combines the most important parts of the distinct channels into a new 2D image. After, we perform a classification based on the blended image. 3.1 Conception of the Image Blending Layer We start with an input image I ∈ Rh×w×c where h denotes the height, w the width and c the number of channels in the multi-spectral image. Subsequently, the image I is split into its distinct channels and processed in the DCMIX layer. The DCMIX layer is inspired by simple image blending and alpha compositingDCMIX 5 [50,1]. More specifically, the idea behind Alpha blending is to combine two images as follows: C = α1 · A1 + (1 − α1) · A2, (1) where ‘A1 ∈ ‘Rh×w×c and A2 ∈ Rh×w×c are the corresponding image matrices to blend and C ∈ Rh×w×c the blended image matrix. The trainable parameter α1 determines the transparency of each channel. In this work, we take advantage of the ideas proposed in [50,1] and generalize the idea by employing the trainable alpha values as weights for each channel that has to be blended with: C = n � i αi · Ai where: αi ≥ 0, (2) where αi is multiplied with each channel Ai. The parameter n defines the number of channels and C is the blended image which will be subsequently used for the further analysis. 3.2 Classifying Genetic Perturbations based on DCMIX-blended Images Our goal is to learn a classification model Fθ(y | C) of the blended image C for distinct classes of genetic perturbations yc where c is the number of genetic per- turbations to be predicted. In this work, our model F is a Deep Convolutional Neural Network which extracts a cascade of feature maps M l where l denotes the current layer. The last feature map is used as an input to the multi-class clas- sification head that predicts the genetic perturbation vector yc using a softmax output. 3.3 End-to-End Training Algorithm The model training is described in Algorithm 1. As an input, we use the multi- spectral images X and the genetic perturbation labels y. Subsequently, we draw minibatches from the training data X, y (line 1). For each of the minibatches, we obtain the blended images ci as well as the corresponding mixing factors αi. The blended images ci are fed in the neural network Fθ (line 3) and the correspond- ing predictions ˆyi are used to calculate the loss in line 5. Finally, we update the parameters θ and α based on the loss by using gradient descent (line 7). 4 Experiments A description of setups and additional hyperparameters can be found in the supplementary materials.6 D. Siegismund, M. Wieser et al. Algorithm 1 DCMIX training algorithm INPUT: X images, y labels OUTPUT: The prediction ˆy, mixing factors α 1: for minibatch xi, yi from X, y do 2: ci, αi ← DCMIX(xi) 3: ˆyi ← Fθ(ci) 4: 5: loss ← crossentropy( ˆyi, yi) 6: 7: update θ, α using gradient descent 8: end for 4.1 MNIST Dataset. To demonstrate the efficacy of DCMIX for estimating channel impor- tance, we generate an artificial dataset based on MNIST [10]. MNIST consists of 70000 samples with images x ∈ R28x28x1 and labels y that represent numbers from 0 to 9. For our dataset, we randomly select a subset of 10000 samples from MNIST. In order to assess the channel importance, we extend the MNIST im- ages with two additional noise channels. Therefore, we draw two noise matrices with shape 28x28 from a uniform distribution defined on [0, 255]. Subsequently, we add the previously generated noise channels to the input image such that we obtain a three channel input image x ∈ R28x28x3 where the first denotes the most important channel. For training, we split the data into a 70 percent training and a 30 percent hold-out set. The training set is further split into a 80 percent training and 20 percent validation set, respectively. Models. In order to demonstrate the effectiveness of our approach, we bench- mark DCMIX against a plain LCNet050, LassoNet [22] as well as on an attention- based [29,25] LCNet050. Quantitative Evaluation. Channel Importance In this experiment, we evaluate the channel importance on the validation set, and the results are re- ported in Table 1. As we can observe in the channel importance ranking, DCMIX can effectively learn the most important channel one and is in line with the more complex LassoNet and attention-based LCNet050. At the same time, DCMIX requires only a fraction of GFLOPS and model parameters. More specifically, DCMIX requires solely 5.9271 GFLOPS compared to 17.809 GFLOPS for the Attention-LCNet050. In addition, DCMIX need three times less parameters (0.2789 million) in contrast to Attention-LCNet050 (0.9281 million) and requires only the same amount of GFLOPS and parameters as the plain LCNet050. Quantitative Evaluation. Model Performance Despite the fact, that the aim of this method is not to improve the model performance but rather learnDCMIX 7 Table 1. Results of the MNIST channel importance and model size. Channel impor- tance ranking denotes the rank of the weights depicted in the second column. The model size is evaluated on GFLOPS and the number of model parameters where lower is better. Method Channel impor- tance ranking Channel weights GFLOPS # Parameters (million) LCNet050 - - 5.9269 0.2789 LassoNet [22] 1,3,2 120259, 51003, 52318 - - Attention[29,25]- LCNet050 1,3,2 1,3.24 × 10−11, 2.33 × 10−6 17.809 0.9281 DCMIX- LCNet050 1,3,2 0.82,0.21,0.22 5.9271 0.2789 the most important channel to gain biological insights for a drug discovery ex- periment, we want to ensure that DCMIX archives competitive performance to state-of-the art approaches. To do so, we compared DCMIX to a plain LCNet050, LassoNet and Attention-LCNet050 in Table 2. Here, we observe that DCMIX ob- tains competitive results compared to both LCNet050 and Attention-LCNet050 and outperforms LassoNet on accuracy, precision, recall and f1-score measures. Table 2. Results of model performance for the MNIST dataset on the hold-out dataset. We assess the model performance on four different metrics: accuracy, precision, recall and f1-score where higher is better. Values in brackets denote the standard deviation. Method Accuracy Precision Recall F1-Score LCNet050 0.992 (0.0008) 0.991 (0.002) 0.991 (0.002) 0.991 (0.002) LassoNet [22] 0.963 (0.012) 0.888 (0.002) 0.888 (0.002) 0.887 (0.002) Attention[29,25]- LCNet050 0.992 (0.002) 0.991 (0.001) 0.991 (0.001) 0.991 (0.001) DCMIX-LCNet050 0.991 (0.002) 0.990 (0.002) 0.990 (0.002) 0.990 (0.002) 4.2 RXRX1 Dataset. For our real world experiment, we employ the RXRX1 dataset[42] which consists of 125510 512x512 px fluorescence microscopy images (6 channels) of four different human cell lines that are perturbed with 1138 genetic pertur- bations (including 30 different positive control perturbations). In this study, we used as the training data 30 positive control siRNAs plus the non-active control which lead to 31 classes in total. All images were normalized using the 1 and 99 percent percentile and after, we extract image patches with a size of 192x192 px and an offset of 96 px. This step leads to 32776 image patches. For training,8 D. Siegismund, M. Wieser et al. we split the data into a 70 percent training and a 30 percent hold-out set. The training set is further split into a 80 percent training and 20 percent validation set, respectively. Models For the real-world RXRX1 experiment, we compare DCMIX to Las- soNet [22] and the attention-based [29,25] LCNet050. Table 3. RXRX1 channel importance evaluation for the HepG2 cell line. The impor- tance ranking illustrates the most important channels form left to right based on the weights depicted in the second column. In addition, model statistics are measured in GFLOPS and the number of model parameters (lower is better). Method Importance ranking Channel weights (in Chan- nel order) GFLOPS # param- eters (mil- lions) ViT-B16-Imagenet21k [11] + LassoNet [22] 6,4,1,5,2,3 73084, 52526, 31138, 87881, 55612, 107733 - - Attention-LCNet050 4,2,5,1,3,6 0.15, 0.17, 0.008, 0.48, 0.16, 0.007 35.61 1.75 DCMIX-LCNet050 4,2,3,5,1,6 0.30, 0.69, 0.38, 1.06, 0.36, 0.21 5.95 0.27 Quantitative Evaluation. Channel Importance Here, we describe the eval- uation results on channel importance for the RXRX1 dataset which is illustrated in Table 3. To do so, we compare the results to the ground truth introduced in [42]. The experiment was manually designed by a scientist in the laboratory such that both channels four and two hold the most important biological information and channel 6 contains no important information for the phenotype. Keeping this information in mind, we assess the channel importance of DCMIX, LassoNet and Attention-LCNet050. Here, we can confirm that DCMIX learns the two most im- portant channels four and two and the least important channel 6. These findings are also supported by Attention-LCNet050 which learned equivalent importance values. In contrast, LassoNet fails to uncover the correct channel importance by selecting the least important channel as the most important one. Despite find- ing the same important channels, DCMIX possess a 6-8 times higher speed and requires 6 times less parameters compared to the attention based networks and can be used in an end-to-end fashion which is not feasible for LassoNet. Quantitative Evaluation. Model Performance In this experiment, we eval- uate the model performance of DCMIX to LassoNet and Attention-LCNet050 and illustrate the results in Table 4. Here, we observe that DCMIX outperforms both LassoNet and Attention-LCNet050 in terms of accuracy by five and sevenDCMIX 9 percent, respectively. Furthermore, these finding are confirmed by precision, re- call and f1-scores where DCMIX outperforms both competitors by approximately five and seven percent. Table 4. Results of model performance for the RXRX1 dataset on the hold-out dataset. We asses the model performance on four different metrics: accuracy, precision, recall and f1-score where higher is better. Values in brackets denote the standard deviation. Method Accuracy Precision Recall F1-Score ViT-B16-Imagenet21k [11] + LassoNet [22] 0.695 (0.004) 0.705 (0.005) 0.705 (0.004) 0.704 (0.005) Attention[29,25]- LCNet050 0.744 (0.019) 0.753 (0.014) 0.747 (0.014) 0.747 (0.013) DCMIX-LCNet050 0.765 (0.004) 0.77 (0.037) 0.77 (0.042) 0.764 (0.043) 5 Discussion DCMIX demonstrates state-of-the-art channel importance scores in fluorescence cellular imaging DCMIX employs image blending to estimate the importance of each image channel. In Figure 2, we provide an overview of the Spearman rank correlation of the channel importance estimates for all tested methods. The results are comparable for all methods (except of LassoNet) with a Spearman ρ always larger than 0.83. Especially the correlations of DCMIX and Attention-LCNet050 to the ground truth shapley values from [42] are evident with a Spearman ρ of 0.89. Both methods estimate the channel 2 and 4 as most important and channel 6 as least important which was the intentionally experimental design and furthermore shown via shapley values [42]. The authors explained their finding with a very large spectral overlap of the fluorescence signal from channel 2 and 4 to any other channel rendering them more important [42]. In contrast, LassoNet does not show any overlap with the rankings selected by all other methods (Figure 2) with a maximal Spearman ρ value of -0.08. DCMIX achieves state-of-the-art classification performance with lower model complexity Across all classification metrics DCMIX archives competi- tive results on MNIST and state-of-the-art performances on real-world RXRX1 compared to its competitors. Intuitively, we attribute the competitive results on MINST to the problem simplicity which is further supported by the high classifi- cation scores of 99% (Table 2). Concurrently, DCMIX requires merely a fraction of model parameters in all experiments (Tables 1,3) compared to the baselines. Practical runtimes for DCMIX are 6-8 times faster than Attention- based approaches While DCMIX requires only 5.9271 GFLOPS and 5.9510 D. Siegismund, M. Wieser et al. Fig. 2. Visualization of Spearman’s rank correlation coefficient of the channel impor- tance estimates for all different methods from Table 3. A value of -1 indicates maximal ranking difference between the channel importance estimates, 1 indicates no difference. Matrix has been sorted using average linkage hierarchical clustering with euclidean distance. GFLOPS on RXRX1, achieving the same computational performance as plain LCNet050, Attention-LCNet050 needs 17.809 GFLOPS on MNIST and 35.614 GFLOPS on RXRX1, respectively (Table 1 and Table 3). Moreover, even post- hoc approaches such Shapely values that are trained on a black-box model re- quire often more significant computation time. For example, the training time required for the Shapley value explanation are in the range of several minutes for the smaller CIFAR-10 dataset [16]. This demonstrates that the speed of DCMIX outperforms not only interpretable competitors but also explainable post-hoc approaches on a large scale. DCMIX is applicable in real-world settings beyond biomedical imag- ing From an application standpoint we see an advantage of DCMIX over theDCMIX 11 other tested methods, as the high scalability of DCMIX allows a model training workflow were channel importance is – per default – applied, such that a scien- tist gets immediate feedback about where the classification relevant information is coming from, and whether it correlates with the known understanding of the underlying biology. DCMIX scales very well with the number of channels due to the addition of only one additional parameter per additional channel. This is particularly interesting for hyper spectral applications where hundreds of channels exist (e.g. in remote sensing) – a highly interesting application area for subsequent studies. In addition, DCMIX allows for any arbitrary downstream network which can be fine-tuned / designed for other applications than fluorescence imaging. DCMIX applies currently a simple addition channel mixing strategy to estimate channel importance without losing any classification performance (see model performance in Table 2 and 4). In principle several other channel blending meth- ods exist, e.g. difference, multiplication or luminosity. Due to the flexibility of DCMIX these other mixing strategies can be easily integrated. Several studies already show the applicability of complex multi-spectral channel blending for visualization and classification in remote sensing [1,18]. 6 Conclusion In this work, we present a novel lightweight framework, DCMIX, which estimates channel importance of fluoresce images based on image blending. This empowers us to estimate phenotype-focused interpretations in a simple yet effective man- ner. Our experimental results demonstrate that the channel importance scores uncovered by DCMIX are both biologically supported and in line with competi- tive state-of-the-art approaches on MNIST and RXRX1 datasets. Concurrently, DCMIX is more effective in terms of runtime and scaleable to an arbitrary num- ber of channels without scarifying the model performance. Limitations. We discuss the limitations of our approach in the following two aspects. (1) The weights of DCMIX which determine the channel importance are solely a proxy and do not explain the absolute importance between channels. (2) DCMIX is based on image blending and hence only supporting image-based datasets. For future work, we plan to investigate how DCMIX can be extended to other data modalities. References 1. Why Not a Single Image? Combining Visualizations to Facilitate Fieldwork and On-Screen Mapping. Remote Sensing (2019) 2. Alaa, A.M., van der Schaar, M.: Demystifying black-box models with symbolic metamodels. In: Advances in Neural Information Processing Systems (2019) 3. Bray, M.A., Singh, S., Han, H., Davis, C.T., Borgeson, B., Hartland, C., Kost- Alimova, M., Gustafsdottir, S.M., Gibson, C.C., Carpenter, A.E.: Cell Painting,12 D. Siegismund, M. Wieser et al. a high-content image-based assay for morphological profiling using multiplexed fluorescent dyes. Nature protocols 11(9) (2016) 4. Cai, Y., Liu, X., Cai, Z.: BS-Nets: An End-to-End Framework for Band Selection of Hyperspectral Image. IEEE Transactions on Geoscience and Remote Sensing (2020) 5. Carpenter, A.E., Jones, T.R., Lamprecht, M.R., Clarke, C., Kang, I.H., Friman, O., Guertin, D.A., Chang, J.H., Lindquist, R.A., Moffat, J., others: CellProfiler: image analysis software for identifying and quantifying cell phenotypes. Genome biology (2006) 6. Carrillo, A., Cantú, L.F., Noriega, A.: Individual explanations in machine learning models: A survey for practitioners. CoRR abs/2104.04144 (2021) 7. Castelvecchi, D.: Can we open the black box of AI? Nature News (2016) 8. Choi, E., Bahadori, M.T., Kulas, J.A., Schuetz, A., Stewart, W.F., Sun, J.: Re- tain: An interpretable predictive model for healthcare using reverse time attention mechanism. In: Advances in Neural Information Processing Systems (2016) 9. Cohen, W.W.: Fast effective rule induction. In: International Conference on Ma- chine Learning (1995) 10. Deng, L.: The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine (2012) 11. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., others: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In: International Con- ference on Learning Representations (2020) 12. Feng, J., Simon, N.: Sparse-input neural networks for high-dimensional nonpara- metric regression and classification (2019) 13. Friedman, J., Hastie, T., Tibshirani, R.: Sparse inverse covariance estimation with the graphical lasso. Biostatistics (2008) 14. Godinez, W.J., Hossain, I., Lazic, S.E., Davies, J.W., Zhang, X.: A multi-scale convolutional neural network for phenotyping high-content cellular images. Bioin- formatics (Oxford, England) (2017) 15. He, K., Sun, W., Yang, G., Meng, X., Ren, K., Peng, J., Du, Q.: A Dual Global–Local Attention Network for Hyperspectral Band Selection. IEEE Trans- actions on Geoscience and Remote Sensing (2022) 16. Jethani, N., Sudarshan, M., Covert, I., Lee, S.I., Ranganath, R.: FastSHAP: Real- Time Shapley Value Estimation. In: International Conference on Learning Repre- sentations (2022) 17. Jolliffe, I.: Principal component analysis. Springer Verlag (1986) 18. Jordanova, G., Verbovšek, T.: Improved Automatic Classification of Litho- Geomorphological Units by Using Raster Image Blending, Vipava Valley (SW Slovenia). Remote Sensing (2023) 19. Jullum, M., Redelmeier, A., Aas, K.: groupShapley: Efficient prediction explana- tion with Shapley values for feature groups (2021), arXiv:2106.12228 20. Kim, B., Wattenberg, M., Gilmer, J., Cai, C., Wexler, J., Viegas, F., sayres, R.: Interpretability beyond feature attribution: Quantitative testing with concept acti- vation vectors (TCAV). In: International Conference on Machine Learning (2018) 21. Kruger, S., Ilmer, M., Kobold, S., Loureiro Cadilha, B., Endres, S., Ormanns, S., Schuebbe, G., Renz, B., D’Haese, J., Schlößer, H., Heinemann, V., Subklewe, M., Boeck, S., Werner, J., von Bergwelt, M.: Advances in cancer immunotherapy 2019 - latest trends. Journal of Experimental and Clinical Cancer Research (2019) 22. Lemhadri, I., Ruan, F., Abraham, L., Tibshirani, R.: Lassonet: A neural network with feature sparsity. Journal of Machine Learning Research (2021)DCMIX 13 23. Levenson, R.M., Mansfield, J.R.: Multispectral imaging in biology and medicine: Slices of life. Cytometry Part A (2006) 24. Li, W., Chen, H., Liu, Q., Liu, H., Wang, Y., Gui, G.: Attention Mechanism and Depthwise Separable Convolution Aided 3DCNN for Hyperspectral Remote Sens- ing Image Classification. Remote Sensing (2022) 25. Lin, Z., Feng, M., Santos, C.N.d., Yu, M., Xiang, B., Zhou, B., Bengio, Y.: A Structured Self-attentive Sentence Embedding. In: International Conference on Learning Representations (2017) 26. Louizos, C., Welling, M., Kingma, D.P.: Learning sparse neural networks through l0 regularization. In: International Conference on Learning Representations (2018) 27. Lundberg, S.M., Erion, G., Chen, H., DeGrave, A., Prutkin, J.M., Nair, B., Katz, R., Himmelfarb, J., Bansal, N., Lee, S.I.: From local explanations to global under- standing with explainable ai for trees. Nature Machine Intelligence (2020) 28. Lundberg, S.M., Lee, S.I.: A unified approach to interpreting model predictions 29. Luong, T., Pham, H., Manning, C.D.: Effective approaches to attention-based neu- ral machine translation. In: Conference on Empirical Methods in Natural Language Processing (2015) 30. Marcinkevičs, R., Vogt, J.E.: Interpretable and explainable machine learning: A methods-centric overview with concrete examples. WIREs Data Mining and Knowledge Discovery (2023) 31. Nalepa, J.: Recent Advances in Multi- and Hyperspectral Image Analysis. Sensors (2021) 32. Nikzad, M., Gao, Y., Zhou, J.: An Attention-Based Lattice Network for Hyperspec- tral Image Classification. IEEE Transactions on Geoscience and Remote Sensing (2022) 33. Park, T., Casella, G.: The Bayesian Lasso. Journal of the American Statistical Association (2008) 34. Prabhakaran, S., Metzner, K.J., Böhm, A., Roth, V.: Recovering networks from distance data. In: Asian Conference on Machine Learning (2012) 35. Ribeiro, M.T., Singh, S., Guestrin, C.: "why should i trust you?": Explaining the predictions of any classifier. In: ACM SIGKDD International Conference on Knowl- edge Discovery and Data Mining (2016) 36. Rozemberczki, B., Watson, L., Bayer, P., Yang, H.T., Kiss, O., Nilsson, S., Sarkar, R.: The Shapley Value in Machine Learning. In: International Joint Conference on Artificial Intelligence (2022) 37. Schwab, P., Miladinovic, D., Karlen, W.: Granger-causal Attentive Mixtures of Ex- perts: Learning Important Features With Neural Networks. In: AAAI Conference on Artificial Intelligence (2019) 38. Shrikumar, A., Greenside, P., Kundaje, A.: Learning important features through propagating activation differences. In: International Conference on Machine Learn- ing (2017) 39. Siegismund, D., Wieser, M., Heyse, S., Steigele, S.: Self-supervised representation learning for high-content screening. In: International Conference on Medical Imag- ing with Deep Learning (2022) 40. Steigele, S., Siegismund, D., Fassler, M., Kustec, M., Kappler, B., Hasaka, T., Yee, A., Brodte, A., Heyse, S.: Deep Learning-Based HCS Image Analysis for the Enterprise. SLAS DISCOVERY: Advancing the Science of Drug Discovery (2020) 41. Sundararajan, M., Taly, A., Yan, Q.: Axiomatic attribution for deep networks. In: International Conference on Machine Learning (2017)14 D. Siegismund, M. Wieser et al. 42. Sypetkowski, M., Rezanejad, M., Saberian, S., Kraus, O., Urbanik, J., Taylor, J., Mabey, B., Victors, M., Yosinski, J., Sereshkeh, A.R., Haque, I., Earnshaw, B.: RxRx1: A Dataset for Evaluating Experimental Batch Correction Methods. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (2023) 43. Tibshirani, R.: Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society (Series B) (1996) 44. Ustun, B., Rudin, C.: Supersparse linear integer models for optimized medical scoring systems. Machine Learning (2015) 45. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L.u., Polosukhin, I.: Attention is all you need. In: Advances in Neural Information Processing Systems (2017) 46. Wachter, S., Mittelstadt, B.D., Russell, C.: Counterfactual explanations without opening the black box: Automated decisions and the gdpr. Cybersecurity (2017) 47. Wieczorek, A., Wieser, M., Murezzan, D., Roth, V.: Learning sparse latent repre- sentations with the deep copula information bottleneck. In: International Confer- ence on Learning Representations (2018) 48. Wieser, M., Siegismund, D., Heyse, S., Steigele, S.: Vision transformers show im- proved robustness in high-content image analysis. In: Swiss Conference on Data Science (2022) 49. Yuan, M., Lin, Y.: Model Selection and Estimation in Regression with Grouped Variables. Journal of the Royal Statistical Society Series B: Statistical Methodology (2005) 50. Zhang, L., Wen, T., Shi, J.: Deep Image Blending. In: IEEE/CVF Winter Confer- ence on Applications of Computer Vision (2020)
HLD 2023: 1st Workshop on High-dimensional Learning Dynamics On the Equivalence between Implicit and Explicit Neural Networks: A High-dimensional Viewpoint Zenan Ling LINGZENAN@HUST.EDU.CN Zhenyu Liao ZHENYU LIAO@HUST.EDU.CN Robert C. Qiu CAIMING@HUST.EDU.CN Huazhong University of Science and Technology, Wuhan, China Abstract Implicit neural networks have demonstrated remarkable success in various tasks. However, there is a lack of theoretical analysis of the connections and differences between implicit and explicit networks. In this paper, we study high-dimensional implicit neural networks and provide the high dimensional equivalents for the corresponding conjugate kernels and neural tangent kernels. Built upon this, we establish the equivalence between implicit and explicit networks in high dimensions. 1. Introduction Implicit neural networks (NNs) [2] have recently emerged as a new paradigm in neural network design. An implicit NN is equivalent to an infinite-depth weight-shared explicit NN with input- injection. Unlike explicit NNs, implicit NNs generate features by directly solving for the fixed point, rather than through layer-by-layer forward propagation. Moreover, implicit NNs have the remarkable advantage that gradients can be computed analytically only through the fixed point with implicit differentiation. Therefore, training implicit NNs only requires constant memory. Despite the empirical success achieved by implicit NNs [3, 11], our theoretical understanding of these models is still limited. In particular, there is a lack of theoretical analysis of the training dynamics and generalization performance of implicit NNs, and possibly more importantly, whether these properties can be connected to those of explicit NNs. [2] demonstrates that any deep NN can be reformulated as a special implicit NN. However, it remains unknown whether general implicit NNs have advantages over explicit NNs. [6] extends previous neural tangent kernel (NTK) studies to implicit NNs and give the exact expression of the NTK of the ReLU implicit NNs. However, the differences between implicit and explicit NTKs are not analyzed. Moreover, previous works [9, 10] have proved the global convergence of gradient descent for training implicit NNs. However, it is still unclear what distinguishes the training dynamic of implicit NNs and that of explicit NNs. In this paper, we investigate implicit NNs from a high-dimensional view. Specifically, we per- form a fine-grained asymptotic analysis on the eigenspectra of conjugate kernel (CKs) and NTKs of implicit NNs, which play a fundamental role in the convergence and generalization high dimen- sional NNs [8]. By considering input data uniformly drawn from the unit sphere, we derive, with recent advances in random matrix theory, high-dimensional (spectral) equivalents for the CKs and NTKs of implicit NNs, and establish the equivalence between implicit and explicit NNs by match- ing the coefficients of the corresponding asymptotic spectral equivalents. Surprisingly, our results reveal that a single-layer explicit NN with carefully designed activations has the same CK or NTK eigenspectra as a ReLU implicit NN, whose depth is essentially infinite. © Z. Ling, Z. Liao & R.C. Qiu. arXiv:2308.16425v1 [cs.LG] 31 Aug 2023ON THE EQUIVALENCE BETWEEN IMPLICIT AND EXPLICIT NNS 2. Preliminaries 2.1. Implicit and Explicit NNs Implicit NNs. In this paper, we study a typical implicit neural network, the deep equilibrium model (DEQ) [2]. Let X = [x1, · · · , xn] ∈ Rd×n denote the input data. We define a vanilla DEQ with the transform at the l-th layer as h(l) i = � σ2a m Az(l−1) i + � σ2 b m Bxi, z(l) i = ϕ(h(l) i ) (1) where A ∈ Rm×m and B ∈ Rm×d are weight matrices, σa, σb ∈ R are constants, ϕ is an element- wise activation, h(l) i is the pre-activation and z(l) i ∈ Rm is the output feature of the l-th hidden layer corresponding to the input data xi. The output of the last hidden layer is defined by z∗ i ≜ liml→∞ z(l) i and we denote the corresponding pre-activation by h∗ i . Note that z∗ i can be calculated by directly solving for the equilibrium point of the following equation z∗ i = ϕ   � σ2a m Az∗ i + � σ2 b m Bxi   . (2) We are interested in the conjugate kernel and neural tangent kernel (Implicit-CK and Implicit-NTK, for short) of implicit neural networks defined in Eq. (2). Following [6], we denote the corresponding Implicit-CK by G∗ = liml→∞ G(l) where the (i, j)-th entry of G(l) is defined recursively as G(0) ij = x⊤ i xj, Λ(l) ij = � G(l−1) ii G(l−1) ij G(l−1) ji G(l−1) jj � , G(l) ij = σ2 aE(u,v)∼N(0,Λ(l) ij )[ϕ(u)ϕ(v)] + σ2 bx⊤ i xj, ˙G(l) ij = σ2 aE(u,v)∼N(0,Λ(l) ij )[ϕ′(u)ϕ′(v)]. (3) And the Implicit-NTK is defined as K∗ = liml→∞ K(l) whose the (i, j)-th entry is defined as K(l) ij = l+1 � h=1 � G(h−1) ij l+1 � h′=h ˙G(h′) ij � . (4) Explicit Neural Networks. We consider a single-layer fully-connected NN model defined as Y = � 1 pσ(W X) where W ∈ Rp×d is the weight matrix and σ is an element-wise activation function. Let w ∼ N(0, Id), the corresponding Explicit-CK matrix Σ and Explicit-NTK matrix Θ are defined as follows: Σ = Ew[σ(w⊤X)⊤σ(w⊤X)], Θ = Σ + � X⊤X � ⊙ Ew[σ′(w⊤X)⊤σ′(w⊤X)]. (5) 2.2. CKs and NTKs of ReLU Implicit NNs We make the following assumptions on the random initialization, the input data, and activations. 2ON THE EQUIVALENCE BETWEEN IMPLICIT AND EXPLICIT NNS Assumption 1 (i) As n → ∞, d/n → c ∈ (0, ∞). All data points xi, i ∈ [n], are independent and uniformly sampled from Sd−1. (ii) A, B, and W are independent and have i.i.d entries of zero mean, unit variance, and finite fourth kurtosis. Moreover, we require σ2 a + σ2 b = 1. (iii) The activation ϕ of the implicit NN is the normalized ReLU, i.e., ϕ(x) = √ 2 max(x, 0). The activation σ of the explicit NN is a C3 function. Remark 1 (i) Despite derived here for uniform distribution on the unit sphere, we conjecture that our results extend the result to more general distributions by using the technique developed in [5, 7]. (ii) The additional requirement on the variance is to ensure the existence and uniqueness of the fixed point of the NTK and to keep the diagonal entries of the CK matrix at 1, see examples in [6]. (iii) It is possible to extend our results to implicit NNs with general activations by using the technique proposed in [10]. We defer the extension to more general data distributions and activation functions to future work. Under Assumptions 1, the limits of Implicit-CK and Implicit-NTK exist, and one can have precise expressions of G∗ and K∗ as follows [6, 9]. Lemma 1 Let f(x) = √ 1−x2+(π−arccos x)x π . Under Assumptions 1, the fixed point of Implicit-CK G∗ ij is the root of G∗ ij = σ2 af(G∗ ij) + (1 − σ2 a)x⊤ i xj. (6) The limit of Implicit-NTK is K∗ ij = h(G∗ ij) ≜ G∗ ij 1 − ˙G∗ ij where ˙G∗ ij ≜ σ2 aπ−1(π − arccos � G∗ ij � ). (7) 3. Main Results In this section, we prove the high-dimensional equivalents for CKs and NTKs of implicit and explicit NNs. As a result, by matching the coefficients of the asymptotic spectral equivalents, we establish the equivalence between implicit and explicit NNs in high dimensions. 3.1. Asymptotic Approximations CKs. We begin by defining several quantities that are crucial to our results. Note that the unique fixed point of Eq. (6) exists as long as σ2 a < 1. We define the implicit map induced from Eq. (6) as G∗ ij ≜ g(x⊤ i xj). Let ∠∗ = g(0) be the solution of ∠∗ = σ2 af(∠∗) when x⊤ i xj = 0. Using implicit differentiation, one can obtain that g′(0) = 1 − σ2 a 1 − σ2af′(∠∗), g′′(0) = σ2 a(1 − σ2 a)2f′′(∠∗) (1 − σ2af′(∠∗))3 . Now we are ready to present the asymptotic equivalent of the Implicit-CK matrix. Theorem 1 (Asymptotic approximation of Implicit-CKs) Let Assumptions 1 hold. As n, d → ∞, the Implicit-CK matrix G∗ defined in Eq. (6) can be approximated consistently in operator norm, by the matrix G, that is ∥G∗ − G∥2 → 0, where G = α11⊤ + βX⊤X + µIn, with α = g(0) + g′′(0) 2d , β = g′(0), and µ = g(1) − g(0) − g′(0). 3ON THE EQUIVALENCE BETWEEN IMPLICIT AND EXPLICIT NNS Theorem 2 (Asymptotic approximation for Explicit-CKs) Let Assumptions 1 hold. As n, d → ∞, the Explicit-CK matrix Σ defined in Eq. (5) can be approximated consistently in operator norm, by the matrix Σ, that is ∥Σ − Σ∥2 → 0, where Σ = α111⊤ + β1X⊤X + µ1In, with α1 = E[σ(z)]2 + E[σ′′(z)]2 2d , β1 = E[σ′(z)]2, and µ1 = E[σ2(z)] − E[σ(z)]2 − E[σ′(z)]2, for z ∼ N(0, 1). NTKs. For the Implicit-NTK, we define K∗ ij = k(x⊤ i xj), i.e., k(x⊤ i xj) = h(g(x⊤ i xj)), for i, j ∈ [n]. It is easy to check that k(0) = h(∠∗) and k(1) = h(g(1)). Using implicit differentiation again, we have k′(0) = (1 − σ2 a)h′(∠∗) σ2af′(∠∗) − 1 , k′′(0) = (1 − σ2 a)2(h′′(∠∗) − σ2 af′(∠∗)h′′(∠∗) + σ2 ah′(∠∗)f′′(∠∗)) (1 − σ2af′(∠∗))3 . Now we are ready to present the asymptotic equivalent of the Implicit-NTK matrix. Theorem 3 (Asymptotic approximation for Implicit-NTKs) Let Assumptions 1 hold. As n, d → ∞, the Implicit-NTK matrix K∗ defined Eq. (7) in can be approximated consistently in operator norm, by the matrix K, that is ∥K∗ − K∥2 → 0, where K = ˙α11⊤ + ˙βX⊤X + ˙µIn, with ˙α = k(0) + k′′(0) 2d , ˙β = k′(0), and ˙µ = k(1) − k(0) − k′(0). Theorem 4 (Asymptotic approximation for Explicit-NTKs) Let Assumptions 1 hold. As n, d → ∞, the Explicit-NTK matrix Θ defined in Eq. (5) can be approximated consistently in operator norm, by the matrix Θ, that is ∥Θ∗ − Θ∥2 → 0, where Θ = ˙α111⊤ + ˙β1X⊤X + ˙µ1In, with ˙α1 = E[σ(z)]2 + 3E[σ′′(z)]2 2d , ˙β1 = 2E[σ′(z)]2, and ˙µ1 = E[σ2(z)] + E[σ′(z)2] − E[σ(z)]2 − 2E[σ′(z)]2 for z ∼ N(0, 1). Remark 2 (i) Due to the homogeneity of the ReLU function, the Implicit-CK and the Implicit-NTK are essentially inner product kernel random matrices. Consequently, Theorem 1 and 3 can be built upon the results in [4]. We postpone the study on general activations to future work. (ii) The results in Theorem 2 and 4 generalize those of [1, 7] to the cases of “non-centred” activations, i.e., we do not require E[σ(z)] = 0 for z ∼ N(0, 1). 3.2. The Equivalence between Implicit and Explicit NNs In the following corollary, we show a concrete case of a single-layer explicit NN with an quadratic activation, that matches the CK or NTK eigenspectra of a ReLU implicit NN. The idea is to utilize the results of Theorems 1-4 to match the coefficients of the asymptotic equivalents such that α1 = α, β1 = β, µ1 = µ, or ˙α1 = ˙α, ˙β1 = β, ˙µ1 = µ. We implement numerical simulations to verify our theory. The numerical results are shown in Figure 1. 4ON THE EQUIVALENCE BETWEEN IMPLICIT AND EXPLICIT NNS CK NTK (a) Implicit (b) Explicit (c) Quadratic activation Figure 1: We independently generate n = 1 000 data points from the d = 1 200-dimensional unit sphere. We use Gaussian initialization and σ2 a is set as 0.2. Upper: the CK results. Bottom: the NTK results. (a) spectral densities of implicit kernels, (b) spectral densities of explicit kernels, (c) quadratic activations. Corollary 1 We consider a quadratic polynomial activation σ(t) = a2t2 + a1t + a0. Let As- sumptions 1 hold. As n, d → ∞, the Implicit-CK matrix G∗ defined in Eq. (6) can be approximated consistently in operator norm, by the Explicit-CK matrix Σ defined in Eq. (5), i.e., ∥G∗ −Σ∥2 → 0, as long as a2 = ± �µ 2 a1 = ± � β, a0 = ± � α − µ d − a2, and the Implicit-NTK matrix K∗ defined in Eq. (7) can be approximated consistently in operator norm, by the Explicit-NTK matrix Θ defined in Eq. (5), i.e., ∥K∗ − Θ∥2 → 0, as long as a2 = ± � ˙µ 6 , a1 = ± � ˙β 2 , a0 = ± � ˙α − ˙µ d − a2. 4. Conclusion In this paper, we study the CKs and NTKs of high-dimensional ReLU implicit NNs. We prove the asymptotic spectral equivalents for Implicit-CKs and Implicit-NTKs. Moreover, we establish the equivalence between implicit and explicit NNs by matching the coefficients of the asymptotic spectral equivalents. In particular, we show that a single-layer explicit NN with carefully designed activations has the same CK or NTK eigenspectra as a ReLU implicit NN. For future work, it would be interesting to extend our analysis to more general data distributions and activation functions. 5ON THE EQUIVALENCE BETWEEN IMPLICIT AND EXPLICIT NNS Acknowledgements Z. Liao would like to acknowledge the National Natural Science Founda- tion of China (via fund NSFC-62206101) and the Fundamental Research Funds for the Central Universities of China (2021XXJS110) for providing partial support. R. C. Qiu and Z. Liao would like to acknowledge the National Natural Science Foundation of China (via fund NSFC-12141107), the Key Research and Development Program of Hubei (2021BAA037) and of Guangxi (GuiKe- AB21196034). References [1] Hafiz Tiomoko Ali, Zhenyu Liao, and Romain Couillet. Random matrices in service of ml footprint: ternary random features with no performance loss. ICLR, 2022. [2] Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. [3] Shaojie Bai, Vladlen Koltun, and J Zico Kolter. Multiscale deep equilibrium models. Advances in Neural Information Processing Systems, 2020. [4] Noureddine El Karoui. The spectrum of kernel random matrices. The Annuals of Statistics, 2010. [5] Zhou Fan and Zhichao Wang. Spectra of the conjugate kernel and neural tangent kernel for linear-width neural networks. Advances in neural information processing systems, 33:7710– 7721, 2020. [6] Zhili Feng and J Zico Kolter. On the neural tangent kernel of equilibrium models. arxiv, 2020. [7] Lingyu Gu, Yongqi Du, Zhang Yuan, Di Xie, Shiliang Pu, Robert Qiu, and Zhenyu Liao. ” lossless” compression of deep neural networks: A high-dimensional neural tangent kernel approach. Advances in Neural Information Processing Systems, 35:3774–3787, 2022. [8] Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018. [9] Zenan Ling, Xingyu Xie, Qiuhao Wang, Zongpeng Zhang, and Zhouchen Lin. Global con- vergence of over-parameterized deep equilibrium models. In International Conference on Artificial Intelligence and Statistics, pages 767–787. PMLR, 2023. [10] Lan V Truong. Global convergence rate of deep equilibrium models with general activations. arXiv preprint arXiv:2302.05797, 2023. [11] Xingyu Xie, Qiuhao Wang, Zenan Ling, Xia Li, Guangcan Liu, and Zhouchen Lin. Optimiza- tion induced equilibrium networks: An explicit optimization perspective for understanding equilibrium models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022. 6
Multiple Augmented Reduced Rank Regression for Pan-Cancer Analysis Jiuzhou Wang, Eric F. Lock Division of Biostatistics, University of Minnesota, Minneapolis, MN, USA September 1, 2023 Abstract Statistical approaches that successfully combine multiple datasets are more powerful, efficient, and scientifically informative than separate analyses. To address variation archi- tectures correctly and comprehensively for high-dimensional data across multiple sample sets (i.e., cohorts), we propose multiple augmented reduced rank regression (maRRR), a flexible matrix regression and factorization method to concurrently learn both covariate- driven and auxiliary structured variation. We consider a structured nuclear norm objective that is motivated by random matrix theory, in which the regression or factorization terms may be shared or specific to any number of cohorts. Our framework subsumes several existing methods, such as reduced rank regression and unsupervised multi-matrix factor- ization approaches, and includes a promising novel approach to regression and factoriza- tion of a single dataset (aRRR) as a special case. Simulations demonstrate substantial gains in power from combining multiple datasets, and from parsimoniously accounting for all structured variation. We apply maRRR to gene expression data from multiple cancer types (i.e., pan-cancer) from TCGA, with somatic mutations as covariates. The method performs well with respect to prediction and imputation of held-out data, and provides new insights into mutation-driven and auxiliary variation that is shared or specific to cer- tain cancer types. Keywords: cancer genomics, data integration, low rank matrix factorization, missing data imputation, nuclear norm, reduced rank regression 1 arXiv:2308.16333v1 [stat.ME] 30 Aug 20231 Introduction The proliferation of omics data in biomedicine and genomics has allowed for increasingly comprehensive investigations that span multiple sample sets and multiple molecular facets. Statistical approaches that successfully combine multiple datasets within a single analytical framework are more powerful, efficient, and scientifically informative than separate analyses. This has spurred advances in methodology for high-dimensional data integration, however, there remain unmet needs especially for multi-cohort data in which the same features are measured for different sample groups. Our motivating example is gene expression and somatic mutation data from the Cancer Genome Atlas (TCGA) Pan-Cancer Project (Hoadley et al., 2018; Hutter and Zenklusen, 2018), for 6581 tumor samples from 30 cohorts corresponding to different cancer types. Given the importance of gene expression in the behavior of cancer, and the related etiology of distinct cancer types through somatic mutations, we are interested in distinguishing variation due to somatic mutations from auxiliary structured variation in cancer gene expression and whether these effects are shared across cancer types. Several unsupervised multi-matrix factorization methods provide low-rank representations of underlying structure. The singular value decomposition (SVD), principle component anal- ysis (PCA) and other well-known approaches allow a rank r approximation of a single matrix Xp×n ≈ Up×rVT n×r, r < min (p, n). Loadings U and scores V explain variation in the rows or columns, respectively. The joint and individual variation explained (JIVE) method extends PCA to multiple datasets with shared columns {X1, . . . , XJ} via Xi ≈ UiVT + WiVT i . Here the joint scores V capture shared structure among the datasets, and the individual scores Vi capture structure specific to dataset i. Numerous related approaches, such as AJIVE (Feng et al., 2018) and SLIDE (Gaynanova and Li, 2019) have been proposed to factorize multiple data from other perspectives. Moreover, BIDIFAC+ (Lock et al., 2022) enables a more flexi- ble way to identify multiple shared and specific modules of variation, which may be partially shared over row subset or column subsets. However, these unsupervised methods suffer from neglecting covariate information. Other supervised techniques (Wang and Safo, 2021; Zhang and Gaynanova, 2022) identify structures across multiple datasets relevant to predicting an outcome, but they do not capture both covariate-driven and auxiliary structures. To impose low-rank covariate effects, different types of penalties have been introduced in the the multivariate least square regression framework. Reduced rank regression (RRR) (Izenman, 1975) is a popular approach to predict X : p × n from Y : q × n via least squares in which the coefficients have low-rank, X ≈ BY with rank(B) < min (p, q). Rank penalized (RSC) (Bunea et al., 2011) and nuclear-norm penalized (NNP) least square criteria (Yuan et al., 2007) are widely used alternatives with penalties that enforce low-rank coefficients. Combining RRR with adaptive NNP (Chen et al., 2013) shows a better performance than RSC. Integrative RRR (Li et al., 2019) extends the estimation to multiple covariate sets all at once. Nonetheless, those regression methods have two limitations: (1) they do not allow for potentially unique covariate-driven signals across multiple sample cohorts and (2) they do not account for additional low-rank structure unrelated to the covariates. Missing values occur in genomics and other fields due to cost limitations or other technical issues. The data may have three types of missingness: entry-wise, column-wise or row-wise. To impute missing values matrix factorization based approaches, such as SVDImpute (Troy- anskaya et al., 2001) and SoftImpute (Mazumder et al., 2010) are popular since they are effective and straightforward, and many of the the aforementioned methods can be modified for imputation. However, they will suffer from the same limitations described above. Unifying reduced rank regression and unsupervised low-rank factorization using the nu- clear norm penalty, we develop the multiple augmented reduced rank regression (maRRR) method for multi-cohort data that enables a very flexible approach for the simultaneous iden- 2tification of covariate-driven effects and auxiliary structured variation. These covariate effects and augmented structures may be shared across any cohorts via a general objective function. This novel low-rank regression and factorization method can be used to impute various types of missing data, accurately capture the relationship between covariates and high-dimensional outcomes, and explore covariate-related and covariate-unrelated patterns of variation that are shared across or specific to different cohorts. 2 Proposed Model Let Xj : p × nj denote data matrices with accompanying covariates Yj : q × nj for j sam- ple cohorts j = 1, ..., J. Concatenations across all cohorts are denoted by ·, e.g., X· = [X1, X2, ..., XJ] and Y· = [Y1, Y2, ..., YJ]. Both X· and Y· are the only observed data in the model. For our application, we consider gene expression data X· and somatic mutations Y· for several patients across J = 30 cancer types. We are interested in decomposing X· into ‘modules’ of low-rank covariate-driven or auxiliary structures, where each module is shared on a different subset of the cohorts. We estimate low-rank coefficient matrices Bk : p × q for k = 1, ..., K modules of covariate-driven variation and we concurrently estimate low-rank auxiliary variation structures S(l) · : p × n for l = 1, ..., L modules. Acknowledging the errors Ej : p × nj, j = 1, ..., J for each cohort, the full model is X· = K � k=1 BkY(k) · + L � l=1 S(l) · + E· (1) where Y(k) · = [Y(k) 1 , Y(k) 2 , ..., Y(k) J ], S(l) · = [S(l) 1 , S(l) 2 , ..., S(l) J ], E· = [E1, E2, ..., EJ]. The presence of each Y(k) j or S(l) j across the cohorts are determined by binary indicator matrices CY : J × K and CS : J × L respectively: Y(k) j = � 0q×nj if CY [j, k] = 0 Yj if CY [j, k] = 1, S(l) j = �0p×nj if CS[j, l] = 0 U(l) S V(l)T Sj if CS[j, l] = 1. U(l) S represents shared loadings and V(l) Sj sample scores for cohort j in module l. Both indicator matrices may be determined either by pre-existing knowledge or via a data-driven algorithm, which we will detail in Section 7.2 and Appendix A.2. They are fixed in the model estimation process. There should be no identical columns within CY , so that each BkY(k) · is present on a distinct subset of the cohorts. Similarly, no duplicate columns within CS. We refer to each BkY(k) · and S(l) · as a module. Each S(l) · gives a low-rank module that explains covariate-unrelated structured variability within the cohorts (e.g., cancer types) identified by CS[:, l]. Each BkY(k) · gives another low-rank module for covariate-driven structure for the cancer type identified by CY [:, l]. Each module is assumed to be low-rank, meaning it can be factorized as the product of a small number of row and column vectors, Bk = U(k) B V(k)T B and S(l) · = U(l) S V(l)T S . We provide a schematic of our model in Fig. 1 and a table of notation details in Appendix A.1. 3Figure 1: A schematic of our proposed model maRRR with 3 cohorts as an example. All ma- trices in grey are observed, i.e. outcomes X· = [X1, X2, X3] and covariates Y· = [Y1, Y2, Y3]. Two binary indicator matrices for auxiliary structures CS = [[1, 0, 1]T , [0, 0, 1]T ] and for co- variate effects CY = [[1, 1, 1]T , [0, 1, 0]T ] are pre-specified. Then, the structures of S(1) · = [U(1) S V(1)T S1 , 0, U(1) S V(1)T S3 ], S(2) · = [0, 0, U(2) S V(2)T S3 ] and Y(1) · = [Y1, Y2, Y3], Y(2) · = [0, Y2, 0] are determined. All matrices in color are to estimate, i.e., auxiliary structures S(1) · , S(2) · , co- variate effect coefficients B1, B2 and random errors E· = [E1, E2, E3]. 43 Objective Function To estimate model (1) and impose low-rank structure, we minimize the following least squares criterion with a structured nuclear norm penalty: min {Bk}K k=1,{S(l) · }L l=1 {1 2||X· − K � k=1 BkY(k) · − L � l=1 S(l) · ||2 F + K � k=1 λ(k) B ||Bk||∗ + L � l=1 λ(l) S ||S(l) · ||∗} (2) Here || · ||∗ denotes the nuclear norm, i.e., the sum of the singular values of the matrix, a convex penalty which encourages a low-rank solution. There are three special cases of the general objective functions worth noting. The first two are novel and the third is previously described, listed as follows: 1. Augmented reduced rank regression (aRRR), our proposed approach minimizing (2) for K = L = J = 1. The nuclear-norm penalized reduced rank regression model for a single matrix is “augmented” to account for auxiliary structured variation S simultaneously. 2. Multi-cohort reduced rank regression (mRRR), our proposed approach minimizing (2) for L = 0, i.e. no auxiliary terms S. The reduced rank regression is extended to recover multiple (shared or individual) covariate effects at once. 3. Optimizing this objective with no covariate-driven structure (K = 0) corresponds to the BIDIFAC+ method (Lock et al., 2022) with horizontal structures only. Mazumder et al. (2010) and others have noted the equivalence of the nuclear norm penalty and an additive L2 penalty on the terms in the low-rank factorization, and this leads to an alternative form of our objective (Eq. (2)), min {U(k) B ,V(k) B }K k=1,{U(l) S ,V(l) S }L l=1 1 2{||X· − K � k=1 U(k) B V(k)T B Y(k) · − L � l=1 U(l) S V(l)T S ||2 F + K � k=1 λ(k) B (||U(k) B ||2 F + ||V(k) B ||2 F ) + L � l=1 λ(l) S (||U(l) S ||2 F + ||V(l) S ||2 F )} (3) where we only need to set a general upper bound for the estimated rank of each Bk and S(l) · , i.e. rB,upper and rS,upper. These upper bounds serve as the number of columns for each U(k) B , V(k) B , U(l) S , and V(l) S . The actual ranks of the solution may be smaller due to the rank sparsity encouraged by the nuclear norm penalty, and if the upper bounds are large enough the solution will correspond to that in equation (2). We state this formally in Theorem 1; the proof of this result, and all other novel results in this manuscript, are given in Appendix B. Theorem 1. If both (2) and (3) have the same penalty terms λ(k) B > 0, k = 1, ..., K and λ(l) S > 0, l = 1, ..., L, the solutions to the objective functions coincide. In what follows in Section 4 we describe a random matrix theory approach to automatically select the nuclear norm penalty weights λ for the different modules. 4 Theoretical results We describe conditions on the penalty to avoid degenerate cases in which certain modules are guaranteed to be zero in the solution (regardless of the data X· and Y·) in Proposition 1. 5Proposition 1. The following conditions on penalty parameters are needed to allow for non- zero estimation of each {Bk}K k=1, {S(l)}L l=1: 1. Let Ik ⊂ {1, ..., k − 1, k + 1, ..., K} be any subset of Y modules for which the non-zero blocks of {Y(i) · }i∈Ik cover exactly those of Y(k), i.e. � i∈Ik CY [·, i] = cy · Cy[·, k] for some positive integer cy. Then, λ(k) B < 1 cy � i∈Ik λ(i) B . 2. Let Ik ⊂ {1, 2, ..., L} be any subset of S modules for which the non-zero blocks of {S(i) · }i∈Ik cover exactly those of Yk, i.e. � i∈Ik CS[·, i] = csy · CY [·, k] for some positive integer csy. Then, λ(k) B < 1 csy � i∈Ik λ(i) S ||Y(k) · ||∗. 3. For l ̸= l′, if a module S(l′) · is contained in another module S(l) · , i.e. CS[j, l] ≥ CS[j, l′], ∀j, then λ(l′) S < λ(l) S . 4. Let Il ⊂ {1, ..., l − 1, l + 1, ..., L} be any subset of S modules that the non-zero blocks of {S(i) · }i∈Il cover exactly those of S(l) · , i.e. � i∈Il CS[·, i] = cs · CS[·, l] for some positive integer cs. Then, λ(l) S < 1 cs � i∈Il λ(i) S . To motivate a random matrix theory approach to select the tuning parameters, we present two results establishing the connection between the nuclear norm penalty and singular value thresholding. Lemma 1 is a well-known result for the unsupervised case (Cai et al., 2010), and in Proposition 2 we extend it to the regression context. Lemma 1. Let UDVT be the SVD of a matrix X. The solution to minS{1 2||X−S||2 F +λ||S||∗} is S = U �DVT , where �D is diagonal with entries �D[i, i] = max(D[i, i] − λ, 0). Proposition 2. Let Y be a semi-orthogonal matrix such that YYT = I and UDVT be the SVD of a matrix XYT . The solution to both of the following objectives: min B {1 2||X − BY||2 F + λ||B||∗} and min B {1 2||X − BY||2 F + λ||BY||∗}, is B = U �DVT , where �D is diagonal with entries �D[i, i] = max(D[i, i] − λ, 0). While the relative merits of penalizing ||B||∗ or ||BY||∗ has been debated (Yuan et al., 2007; Chen et al., 2013), Proposition 2 shows they are identical if Y is semi-orthogonal. In practice, we orthogonalize the columns of Y prior to estimation. However, this requires that the number of features in Y is less than the sample size (e.g., q < n); if q ≥ n then Y will be semi-orthogonal in the opposite direction YT Y = I, causing the solution to degenerate to the unsupervised case, which we establish in Proposition 5 in Appendix B. The following propositions describe the distribution of the singular values of a random matrix under general assumptions, which can then be used to motivate tuning parameters. Proposition 3. Let λmax be the largest singular value of a matrix E : m × n of independent Guassian entries with mean 0 and variance σ2. We have E(λmax) ≤ σ(√m + √n). Proposition 4. Let Yq×n be semi-orthogonal such that YYT = I. For integers m, q ≥ 1 defined in a way that m q → c > 0 as q → ∞, Let Xm×n, Bm×q, Em×n be three matrices such that X = BYq×n + 1 √qE, where entries of E are independent Guassian with mean 0 and variance σ2. Assume rank(B) = r. Denote the singular values of B and XYT are σ1(B) ≥ ... ≥ σr(B) > 0 and σ1(XYT ) ≥ ... ≥ σr(XYT ) > 0 respectively. As n → ∞, σj(XYT ) P−→ � s(σj(B)) > 1 + √c, if σj(B) > 4√c 1 + √c, if σj(B) ≤ 4√c , ∀1 ≤ j ≤ r, 6where s(·) is a known function. In particular, when Y is an identity matrix (q = n) and X = B + 1 √nE, it follows that σj(X) P−→ � s(σj(B)) > 1 + √c, if σj(B) > 4√c 1 + √c, if σj(B) ≤ 4√c. Proposition 3 comes directly from (Rudelson and Vershynin, 2010), and Proposition 4 is closely related to the result in (Shabalin and Nobel, 2013). Consider the reasonable penalty for S in Lemma 1, i.e. Xm×n = Sm×n + Em×n. A set of reasonable tuning parameters will (1) detect the low-rank signals and (2) not capture components that are solely due to noise. Considering Propositions 3 and 4, setting λ = σ(√m + √n) is reasonable because it only keeps the signals (top r components) whose singular values are expected to be greater than those of independent random noise. Consider the reasonable penalty for B in Proposition 2, i.e. Xm×n = Bm×qYq×n + Em×n. Following a similar argument, we set λ = σ(√m + √q). In practice, after normalizing raw data as described in Appendix C, the noise variance for X· is 1 (σ = 1) and each Y(k) are semi-orthogonal. Thus, in order to distinguish true signals {Bk}K k=1, {S(l) · }L l=1 from Gaussian noise in the objective (2), we fix λ(k) B = √p + √q for any module Bk, k = 1, ..., K and λ(l) S = √p + ��J j=1 njCS[j, l] for any module S(l) · , l = 1, ..., L. This directly extends our choices for a single matrix, as estimating any given module B(k) or S(l) · with the others fixed reduces to the setting of the previous propositions. 5 Estimation In practice we scale X (Gavish and Donoho, 2017) and orthogonalize Y prior to optimization. The details of this procedure are provided in Appendix C, and a simulation study illustrating its advantages is provided in Appendix D.3. 5.1 Optimization We estimate all regression coefficients B and auxiliary variation sources S simultaneously, via alternating optimization approaches for either formulation (2) or (3) of our objective. For objective (3), the introduction of U and V can make the optimization algorithm more efficient because the objective function has a closed-form gradient. Given all other estimates, we update every single U(k) B , V(k) B , U(l) S , V(l) S by setting its corresponding gradient to be zero. The details are provided in Algorithm 1. The symbol � means Kronecker product. Note that Algorithm 1 does not require the columns of Y(k) · to be orthogonal. When Y(k) · is semi-orthogonal, in light of Lemma 1 and Proposition 2, we develop an alternative approach based on iterative soft-singular value thresholding estimators for (2) in Algorithm 2. 7Algorithm 1 Alternating Least Square with Matrix Decomposition Input: Covariates Y and corresponding multivariate outcomes X; penalizing terms λB, λS; binary indicator matrices CY , CS Output: B, S 1: Initialization Construct {Y(k) · }K k=1 based on CY . Assign initialized numbers for each entry of {U(k) B , V(k) B }K k=1, {U(l) S , V(l) S }L l=1 2: while convergence criterion does not meet do 3: for k = 1, ..., K do 4: Compute the residual matrix X(k) · = X· − �K k′=1,k′̸=k U(k′) B V(k′)T B Y(k′) · − �L l=1 U(l) S V(l)T S 5: Update U(k) B = X(k) · Y(k)T · V(k) B (V(k)T B Y(k) · Y(k)T · V(k) B + λ(k) B IrB)−1 6: Update vec(V(k) B ) = [(U(k)T B U(k) B ) �(Y(k) · Y(k)T · ) + λ(k) B Iq∗rB]−1vec[Y(k) · (X(k)T · )U(k) B ] 7: Transform vec(V(k) B ) to V(k) B 8: end for 9: for l = 1, .., L do 10: Compute the residual matrix X(l) · = X·−�K k=1 U(k) B V(k)T B Y(k) · −�L l′=1,l′̸=l U(l′) S V(l′)T S 11: Set X(l) j = 0 where Cs[j, l] = 0 for j = 1, ..., J 12: Update U(l) S = X(l) · V(l) S (V(l)T S V(l) S + λ(l) S IrS)−1 13: Update V(l) S = X(l)T · U(l) S (U(l)T S U(l) S + λ(l) S IrS)−1 14: end for 15: end while 16: Set Bk = U(k) B V(k)T B for all k = 1, .., K, and S(l) · = U(l) S V(l)T S for all l = 1, .., L Algorithm 2 Alternating Least Square with Soft-threshold Estimators Input: Orthogonal covariates Y and corresponding multivariate outcomes X; penalizing terms λB, λS; binary indicator matrices CY , CS Output: B, S 1: Initialization Construct {Y(k) · }K k=1 based on CY . Assign initialized numbers for each entry of {Bk}K k=1, {S(l) · }L l=1 2: while convergence criterion does not meet do 3: for k = 1, .., K do 4: Compute the residual matrix X(k) · = X· − �K k′=1,k′̸=k Bk′Y(k′) · − �L l=1 S(l) · 5: Compute the SVD of X(k) · Y(k)T · , i.e. X(k) · Y(k)T · = L(k) B D(k) B R(k) B 6: Update Bk = L(k) B �D(k) B R(k) B where �D(k) B is a diagonal matrix with �D(k) B [r, r] = max(D(k) B [r, r] − λ(k) B , 0) for r = 1, 2, ... on its diagonal entries and zero otherwise 7: end for 8: for l = 1, .., L do 9: Compute the residual matrix X(l) · = X· − �K k=1 Bk′Y(k′) · − �L l=1,l′̸=l S(l′) · 10: Set X(l) j = 0 where Cs[j, l] = 0 for j = 1, ..., J 11: Compute the SVD of X(l) · , i.e. X(l) · = L(l) S D(l) S R(l) S 12: Update S(l) · = L(l) S �D(l) S R(l) S where �D(l) S is a diagonal matrix with �D(l) S [r, r] = max(D(l) S [r, r] − λ(l) S , 0) for r = 1, 2, ... on its diagonal entries and zero otherwise 13: end for 14: end while 8For both algorithms, we use the same convergence criteria to decide whether to stop the optimization process: �K k=1 ||�Bk − �Bk||2 F + �L l=1 ||�S(l) · − �S(l) · ||2 F < ϵ, where�denotes the estimation in the current epoch and � denotes the estimation in the previous epoch. It is also reasonable to use convergence of the loss function as the criteria. In practice both algorithms have different strengths and weaknesses. Theoretically, Al- gorithm 2 can be used only when we orthogonalize the original Y, because otherwise soft- thresholding to update B is not possible. In general, we find that the algorithms require similar computation time to achieve the same convergence criterion: Algorithm 1 tends to re- quire less time if the true ranks (and accompanying maximum ranks specified, i.e. rB,upper and rS,upper) are small, while Algorithm 2 is quicker and consumes less computational resources when the true rank and maximum ranks specific for Algorithm 1 are large. 5.2 Missing data imputation One of the main uses of our proposed method is to impute various types of missing data. Based on the assumption that the abundance of existing entries provides sufficient information to uncover the global structures (both covariate and auxiliary effects) and therefore, to estimate the values of absent entries. Denote the set of indexes of all missing entries as M. Our iterative imputation process is as follows: (1) Initialize �X· by �X·[m, n] = X·[m, n] if [m, n] /∈ M, otherwise 0; (2) Estimate {Bk}K k=1, {S(l) · }L l=1 by Algorithm 1 or 2 with current �X·; (3) Update �X· by setting �X·[m, n] = (�K k=1 BkY(k) · + �L l=1 S(l) · )[m, n] for all [m, n] ∈ M; (4) Back to (2) unless convergence; the final �X· is the imputation result. This can be considered a modified EM-algorithm, and is similar to the approach used for softImpute (Mazumder et al., 2010) for nuclear-norm penalized imputation of a single matrix with no covariates. 6 Simulations 6.1 Recovery of true structure for special cases Here, we present simulations as proof-of-concept for two novel scenarios within our approach: (i) simultaneous modeling of covariate effects and auxiliary low-rank variation and (ii) simul- taneous modeling of shared or specific covariate effects across multiple cohorts. For (i), we consider a single data matrix X : 100 × 100 and single set of covariates Y : 10 × 100 and generate data via X = BY + S + E, where BY is covariate-driven variation, S is auxiliary structured variation, and E is error. The coefficient array B has rank Ry via B = aUBVT B where UB : 100×Ry and VB : 10×Ry, and S has rank 5 via S = bUSVS where US : 100 × 5 and VS : 5 × 100. The entries of E, Y, UB, VB, US and VS are all generated independently from a standard normal distribution. We consider Ry = 1 or Ry = 5, and consider three conditions with different signal strength for each term by adjusting a and b: sd(BY) = 0.5 and sd(S) = 5 (||BY||/||S|| = 0.1, sd(BY) = sd(S) = 1 (||BY||/||S|| = 1), and sd(BY) = 5 and sd(S) = 0.5 (||BY||/||S|| = 10). For each set of conditions, we estimate B and S using four approaches: (1) Augmented reduced rank regression (aRRR), our proposed approach as described in Section 3, given 10 as the rank upper bound for B and S. (2) Supervised singular value decomposition (SupSVD) (Li et al., 2016), a related model of the form X = YBVT + FVT + E for one cohort, where F is the matrix of latent variables that correspond to auxiliary variation not related to the covariates, estimated using maximum likelihood and given the true rank of BVT and FVT . (3) Two-stage least squares, in which the coefficients B is determined by ordinary least squares regression and S is determined by an SVD approximation with the true rank (R = 5) on the residuals X − ˆBY. (4) Two-stage nuclear norm (NN), in which B is determined by an NN-penalized reduced rank regression 9and S by a NN-penalized matrix approximation to the residuals X − ˆBY. For each method, we compute the relative mean squared error (MSE) for B and S, e.g., ||B − ˆB||2 F /||B||2 F . Average relative MSEs for each condition, over 100 replications, are shown in Table 1A. This demonstrates clear advantages of a nuclear norm penalty on B, and the dramatic advantage of aRRR when the auxiliary signal S is strong. The latter point is critical, because molecular data typically have a large amount of structured variation that is driven by coordinated biological processes or other latent effects; it is common for such variation to be stronger than the signal of interest (i.e., BY), yet it is not systematically adjusted for in practice. For scenario (ii), we generate data {Xj : 100×100, Yj : 10×100} via Xj = (B+Bj)Y+E for two cohorts j ∈ {1, 2}. Here, Bj are covariate effects specific to cohort j and B are shared effects. The coefficient arrays are generated via B = aUBVT B, B1 = bUB1VT B1, and B2 = bUB2VT B2 where {UB, UB1, UB2} are each 100×Ry and {V, VB1, VB2} are each Ry×10. The entries of {E, Y, UB, UB1, UB2, VB, VB1, VB2} are each generated independently from a standard normal distribution. We consider Ry = 1 or 5, and three conditions with different signal strength for each term by adjusting a and b: a = 2 and b = 0.2 (||B||/||Bi|| = 10), a = b = 1 ((||B||/||Bi|| = 1), and a = 0.2 and b = 2 (||B||/||Bi|| = 0.1). For each set of conditions, we estimate B, B1 and B2 for J = 2 via maRRR with no auxiliary terms S, termed multi-cohort reduced rank regression (mRRR). Table 1B shows average relative MSEs of B and the Bi’s for mRRR in comparison to two-stage approaches analogous to those described previously. The mRRR approach can effectively recover shared and cohort specific effects, with dramatic improvement over ad-hoc multi-step approaches. Table 1: Relative MSE for scenarios assessing aRRR (A) and mRRR (B). Values that smaller than 0.01 are round to 0.01. The bold number represents the lowest value in a row. A aRRR SupSVD Two-stage LS Two-stage NN ||BY|| ||S|| Ry B S B S B S B S 10 1 0.01 0.61 0.01 0.46 0.01 0.60 0.01 0.61 1 1 0.04 0.22 0.13 0.19 0.22 0.21 0.05 0.25 0.1 1 0.17 0.01 10.97 0.10 11.44 0.11 7.45 0.08 10 5 0.01 0.63 0.01 0.48 0.01 0.60 0.01 0.63 1 5 0.14 0.24 0.17 0.19 0.23 0.21 0.15 0.26 0.1 5 0.40 0.01 11.31 0.10 11.66 0.11 7.83 0.08 B mRRR Two-stage LS Two-stage NN ||B|| ||Bi|| Ry B Bi B Bi B Bi 10 1 0.01 0.11 0.01 0.77 0.01 0.42 1 1 0.01 0.01 0.75 0.60 0.67 0.52 0.1 1 0.07 0.01 80.57 0.55 76.17 0.52 10 5 0.01 0.28 0.01 0.57 0.01 0.50 1 5 0.08 0.08 0.49 0.54 0.45 0.50 0.1 5 0.49 0.01 54.79 0.56 52.41 0.54 6.2 Missing data imputation In this simulation we assess the maRRR framework more broadly, with a focus on missing data imputation. Our general simulation procedure follows these steps: 1) complete data generation; 2) missingness assignment; 3) imputation analysis. In reality the true main signals may come from covariate effects or auxiliary structures and can be individual-level or shared across multiple cohorts. So we consider four fundamental scenarios: (a) large B, main signals from one global auxiliary structure which is shared by all cohorts; (b) large S, main signals 10from one global covariate effect which is shared by all cohorts; (c) large Bi, main signals from individual covariate effect of each cohort; (d) large Si, main signals from individual auxiliary structure in each individual cohort. In order to mimic the real situation, the number of samples and dimensions of the data is set to be the same as the TCGA data analyzed in Section 7. That is, X consists of 1000 features and 6581 samples from 30 study cohorts and Y consists of 50 predictors. Therefore, the ground truth can be written as Xj = BYj + BjYj + Sshared,j + Sj + Ej, j = 1, ..., 30. In each simulation, the standard deviation for the main signals is set to be √ 10 while that of the remaining signals and random errors are set to be 1. The complete data generation process is described in Appendix D.1. To mimic the various types of missingness that are encountered in reality, we conduct simulations in which four types of missingness are considered: (i) missing entries, (ii) missing columns, (iii) missing rows, and (iv) a balanced mix of these three types of missingness as the average of the results of those three types. Missingness is set to be 5% of the original data for the assumption that adequate information is provided for revealing global structures. All missing indices are randomly selected. Denote �X· as the estimate for true observation X·, based on non-missing entries. We define the relative squared error (RSE) for missing data imputation as RSE = � (m,n)∈M(X·[(m,n)]− �X·[(m,n)])2 � (m,n)∈M(X·[(m,n)])2 . We compare our method (maRRR with true 31 modules) with the following approaches: (1) BIDIFAC+ with 31 modules, i.e. only auxiliary variation structure S; (2) mRRR with 31 modules, i.e. only covariate-related structure BY; (3) aRRR with only one module for all cancer types’ cohorts together; (4) aRRR separately on each cancer type’s cohort; (5) nuclear norm regression (without S) of X· on Y·, i.e. mRRR with one all-shared module; (6) nuclear norm regression (without S) of Xj on Yj separate for each cancer type, i.e. mRRR with 30 individual modules; (7) nuclear norm approximation (without BY) for all cancer types together (8) nuclear norm approximation (without BY) for each cancer types separately. Based on the simulation results shown in Table 2, in terms of the average performance, our proposed method maRRR has the lowest RSE. In particular, maRRR has a very close RSE to the best result in the case of missing columns or rows under large individual covariate signals, and it performs the best in all other cases. BIDIFAC+ performs slightly worse than our proposed method while there are only missing entries or rows, but it cannot utilize any covariate information to impute in the case of missing columns. The models only considering covariate effects (mRRR and nuclear norm regression) cannot predict accurately when the auxiliary variation is large, no matter global or individual. On the contrary, the models with- out considering covariates (BIDIFAC+, nuclear norm approximation) can perform reasonably well in the cases of missing entries or rows since the variation from covariates may be counted into that of auxiliary structures. The special case of our proposed method, aRRR (for one cohort only), though worse than maRRR, has lower MSE than many other existing methods. 6.3 Computation Our proposed method is computationally efficient. For a matrix of size 1000 × 6581, the aver- age computational cost per epoch is 45 seconds for Algorithm 1 and 50 seconds for Algorithm 2. A comprehensive comparison of computation times for all methods utilized in this study is provided in Appendix D.2. 11Table 2: Imputation relative squared error(RSE) under different methods and different types of missingness, simulated data is set to be large at only one type of modules. Missingness is set to be 5% of the original X. The number of epochs for each method is set as 30. Each result is a mean of 10 replications. The standard error is less than 0.01 for all of the means shown. The bold number represent the lowest value in a column. large B Method missing entries missing columns missing rows mean maRRR 0.082 0.228 0.216 0.175 BIDIFAC+ 0.085 1 0.231 0.439 mRRR 0.202 0.241 0.285 0.243 aRRR, one all-shared 0.125 0.288 0.227 0.213 aRRR, 30 separate 0.093 0.287 1.014 0.465 NN reg, one all-shared 0.283 0.287 0.288 0.286 NN reg, 30 separate 0.212 0.255 1 0.489 NN approx, one all-shared 0.127 1 0.229 0.452 NN approx, 30 separate 0.096 1 1 0.699 large S Method missing entries missing columns missing rows mean maRRR 0.082 0.877 0.218 0.392 BIDIFAC+ 0.085 1 0.225 0.437 mRRR 0.759 1.066 0.927 0.917 aRRR, one all-shared 0.125 0.929 0.228 0.427 aRRR, 30 separate 0.093 0.884 1.004 0.66 NN reg, one all-shared 0.928 0.931 0.933 0.93 NN reg, 30 separate 0.783 1.072 1 0.952 NN approx, one all-shared 0.127 1 0.23 0.452 NN approx, 30 separate 0.096 1 1 0.699 large Bi Method missing entries missing columns missing rows mean maRRR 0.083 0.262 0.901 0.415 BIDIFAC+ 0.086 1 0.867 0.651 mRRR 0.204 0.246 0.928 0.459 aRRR, one all-shared 0.149 0.913 0.902 0.655 aRRR, 30 separate 0.093 0.287 1.013 0.464 NN reg, one all-shared 0.896 0.899 0.964 0.92 NN reg, 30 separate 0.212 0.252 1 0.488 NN approx, one all-shared 0.151 1 0.907 0.686 NN approx, 30 separate 0.096 1 1 0.699 large Si Method missing entries missing columns missing rows mean maRRR 0.083 0.873 0.861 0.606 BIDIFAC+ 0.086 1 0.866 0.651 mRRR 0.76 1.066 0.928 0.918 aRRR, one all-shared 0.148 0.93 0.906 0.661 aRRR, 30 separate 0.093 0.885 1.003 0.661 NN reg, one all-shared 0.929 0.931 0.935 0.932 NN reg, 30 separate 0.784 1.072 1 0.952 NN approx, one all-shared 0.15 1 0.91 0.687 NN approx, 30 separate 0.096 1 1 0.699 127 Real Data Analysis 7.1 Data description We consider data from the Cancer Genome Atlas (TCGA) Pan-Cancer Project (Hoadley et al., 2018). TCGA is an NIH-sponsored initiative to molecularly characterize cancer tissue samples obtained from hundreds of sites worldwide. We used data for 6581 tumor samples from distinct individuals representing 30 different cancer types (i.e., 30 cohorts). The number of samples per cancer type ranges from 57 samples for uterine carcinosarcoma (UCS) to 976 for breast carcinoma (BRCA). As outcomes, we consider gene expression data obtained from Illumina RNASeq platforms and normalized as described in Hoadley et al. (2018). We filter to the 1000 genes that have the highest standard deviation, yielding X· : 1000 × 6581. We filter to the 50 somatic most common somatic mutations (1=mutated and 0=not mutated) as covariates Y· : 50 × 6581. Data are standardized and orthogonalized as in Appendix C. Further details on the data are available in Appendix E.1 and E.2. 7.2 Decomposition results We first apply the optimization with dynamic modules for BIDIFAC+ (Lock et al., 2022), to uncover 50 low-rank modules in X·. This stepwise procedure iteratively determines modules of shared structure without consideration of any covariates (i.e., CS). Fifty was chosen as the upper bound because the variance explained by more modules than 50 was relatively inconsequential. To distinguish how much variance is attributed to mutation effects, we set covariate-related module indicators CY equal to CS. We then apply maRRR to these 50 modules to infer both mutation-driven and auxiliary structured variation, (K = L = 50) with penalties determined as in Section 4. We order the modules by total variance explained by descending by maRRR estimates, i.e. ||�BiY(i) · + �S(i) · ||2 2, i = 1, ..., 50. The ordered result is shown in Table 3. The top 3 modules by total variance explained are those with one or two cancers: BRCA, THCA and a combination of GBM and LGG (both neurological cancers), respectively. In general, auxiliary structures explained more variation than mutation-related structures, but their relative contribution varied widely across modules. For example, Modules 6 and 7 have fairly comparable amount of variation explained by both mutation-related and -unrelated parts. Other modules have negligible mutation-driven variation, such as Module 12 which is shared by all but LAML. The large amount of low-rank variation unrelated to covariates demonstrates the importance of accounting for this auxiliary structure. Moreover, the large amount of covariate-related and -unrelated variation that is specific to one or a small number of cancer types demonstrates the importance of accounting for individual and partially-shared structures. We present a comparison of the BIDIFAC+ and mRRR estimates with those of our pro- posed maRRR in Table 3. The total signal detected by maRRR closely aligns with that of BIDIFAC+, illustrating maRRR’s ability to discern covariate effects while accounting for similar total variance. The covariate effects identified by mRRR resemble those by maRRR, particularly when the sample size is large. However, mRRR tends to estimate larger covariate effects, especially for smaller sample sizes. This makes sense because the maRRR estimates are less prone to over-fitting by adjusting for unrelated structure. As the sample size de- creases, the relative square errors (RSE) between estimates from the two methods increase. However, both methods generally agree when the mutation effects are negligible (≤ 10000). For instance, both mRRR and maRRR reveal virtually no global mutation effects for Module 12 (even though the RSE is large due to the relative standardization). Principal components plots of the first three modules (BRCA, THCA, GBM and LGG) are shown in Figure 2. Figure 2A displays mutation-related variation for the BRCA module, 13Table 3: Cancer types and sources for the first 15 modules, ordered by variation explained by maRRR. Variance of S(i) · /BiY(i) · /signal refers to total variance explained by S(i) · /BiY(i) · /BiY(i) · + S(i) · estimated in maRRR. RSE of mRRR refers to relative square difference between BiY(i) · estimated by maRRR and mRRR. RSE of BIDIFAC+ refers to relative square difference between signal (BiY(i) · + S(i) · ) estimated by maRRR and BIDIFAC+. Sample size refers to the number of samples used in the current module. All the number smaller than 0.01 are round to 0.01. For reference, the RSE for mRRR and maRRR is defined as ||Bi,mRRR−Bi,maRRR||2 F ||Bi,mRRR+Bi,maRRR||2 F = ||Bi,mRRRY(i) · −Bi,maRRRY(i) · ||2 F ||Bi,mRRRY(i) · +Bi,maRRRY(i) · ||2 F since Y(i) · is semi-orthogonal; the RSE for mRRR and maRRR is defined as ||S(i) ·,BIDIF AC+−S(i) ·,maRRR||2 F ||S(i) ·,BIDIF AC++S(i) ·,maRRR||2 F for any i = 1, 2, ..., 50. Module Sample Variance RSE of Variance Variance RSE of Cancer (i) size of BiY(i) · mRRR of S(i) · of signal BIDIFAC+ types 1 976 129652.09 0.22 1085292.83 1524117.23 0.01 BRCA 2 400 167384.83 0.15 551269.19 992693.82 0.01 THCA 3 433 67242.88 0.20 645839.92 975136.09 0.01 GBM, LGG 4 331 49.52 0.97 730462.99 734935.73 0.01 PRAD 5 195 7231.61 0.68 633034.84 724960.62 0.01 LIHC 6 1029 137934.04 0.08 316746.84 656053.3 0.03 BLCA, CESC, ESCA, HNSC, KICH, LUSC 7 820 137765.92 0.14 298195.14 629007.34 0.02 COAD, ESCA, PAAD, READ, STAD 8 179 20.61 1.00 396671.22 396882.04 0.01 PCPG 9 170 53.04 0.98 347389.29 348257.22 0.01 LAML 10 576 13530.5 0.33 231107.59 316968.61 0.04 KIRC, KIRP 11 422 30138.96 0.21 192612.5 300435.91 0.03 SKCM, UVM 12 6411 0.14 0.74 264645.95 264654.13 0.23 All but *LAML* 13 211 63836.59 0.16 87452.29 251028.58 0.01 COAD, READ 14 149 495.86 0.87 237614.82 250811.31 0.01 TGCT 15 119 97.81 0.97 242978.25 243471.26 0.01 THYM 14Figure 2: A: scores for the first two principle components of covariate-related variation (BY) from Module 1 (BRCA); B: scores for the first two principle components of covariate- unrelated auxiliary variation (S) from Module 1 (BRCA), with symbols and colors showing TP53 mutation and 5 subtypes of BRCA respectively. C: scores for the first two principle components of covariate-related variation (BY) from Module 2 (THCA), with symbols show- ing BRAF mutation. Plot A&B contain 976 samples in the BRCA cohort respectively; Plot C contains 400 samples in the BRCA cohort respectively. D: scores for the first two principle components of covariate-related variation (BY) from Module 3 (GBM&LGG); E: scores for the first two principle components of covariate-unrelated auxiliary variation (S) from Module 3 (GBM&LGG), with symbols and colors showing TP53 mutation and 3 IDH/codel subtypes of GBM&LGG respectively. F: scatterplot for the regression coefficients for TP53 in module 1 and module 3. Each point in Plot D&E represents one sample (150 for GBM and 283 for LGG); each point in Plot F represents one gene (1000 in total). 15and samples are distinguished by whether they have a mutation in the TP53 gene or not. This makes sense, as TP53 is known to play a critical role in genomic activity for cancer and it is the most frequently mutated gene in breast cancer (Olivier et al., 2010). From Figure 2B,we see that the mutation-unrelated structure is driven by the 5 intrinsic BRCA subtypes (Cancer Genome Atlas Research Network, 2012): Normal-like, Luminal A (LumA), Luminal B (LumB), HER2-enriched (HER2), and Basal-enriched (Basal) tumors. This makes sense, as the BRCA subtypes are known to be genomically distinct, but (as is apparent) do not have a direct correspondence to TP53 or other common mutations. From Figure 2C, we observe that BRAF and non-BRAF groups are elegantly separated on the first principle component of mutation-driven variation for THCA, which explains much more variation than other components. This concurs with prior research indicating that the BRAF mutation defines a unique genomic and clinical subgroup within THCA patients (Dolezal et al., 2021). Figure 2D&E reveal substantial variation in both the GBM and LGG samples, as evidenced by the spread and intermingling of the “+” and “o” symbols. This observation indicates that the top components identified by Module 3 explain substantial variation in both the GBM and LGG cancer cohorts, suggesting that it is indeed shared by the two cancer types. The TP53 mutation drives separation of the mutation-driven structure, which makes sense as TP53 status is closely related to GBM and LGG aggressiveness (Ham et al., 2019). This was also observed in BRCA, however, from Figure 2F we see that the TP53 regression coefficients from module 3 (GBM&LGG) has little correlation to those from module 1 (BRCA). This demonstrates that while TP53 is an important somatic mutation related to different types of cancer, its effect on gene expression can differ dramatically depending on the cancer type. In general, we find that the same somatic mutation plays different roles for different cohorts and modules. This embodies the necessity of the flexible modeling for different (combinations of) cohorts. In fact, one interesting finding of our analysis is that the effect of somatic mutations on gene expression are almost not entirely shared across different types of cancer. For example, for the module that is shared across almost all cancers (module 12) the mutation- driven component is negligible. This observation is further supported by our analysis in Appendix E.4 and Section 7.3. 7.3 Missing data imputation Similar to Section 6.2, we compare our proposed maRRR with other relevant methods under four types of missingness for these data. Beyond the aforementioned eight methods in Sec- tion 6.2, we added (9) linear least squares regression to predict X from Y for all cancer types together and (10) linear least-squares regression for each cancer types separately. Note that maRRR, BIDIFAC+ and mRRR are based on the detected 50 modules. Results are provided in Table 4. In the scenario of missing entries, both maRRR and BIDIFAC+ have the lowest RSE, with similar values; both methods allow for an efficient decomposition of joint and indi- vidual structures. In the case of missing columns (some samples’ entire outcomes are missing), the methods that do not consider mutations and only consider S (BIDIFAC+, NN approx) have no predictive power, which is expected because the mutation data is needed to inform predictions if no gene expression is available for a sample. Here the methods that consider both B and S (maRRR, aRRR) are suboptimal compared to those methods that only consider mutation-driven variation (BY), indicating that for these data allowing for auxiliary varia- tion does not improve column-wise predictions. Moreover, methods that allow for separate mutation-driven structure across the cohorts perform substantially better, which is consistent with the fact that there were more individual modules in our analysis and mutation-driven variation was generally not shared (Table 3). In the event of missing rows (each cohort misses random features), methods that consider individual structure only do not perform well, as 16Table 4: Imputation relative squared error(RSE) under different methods and different types of missingness. Missingness is set to be 5% of the original X. “one all shared” means data for 30 groups are stacked together to form one matrix to analyze; “30 separate” means each group has its only model. “Missing entries” refers to missingness is entrywise; “missing columns” means some samples’ entire observation are missing; “missing rows” means each group has several features entirely missing. “N/A” means some specific method is not applicable. The bold number represents the lowest value in a column. Methods Missing entries Missing columns Missing rows Average maRRR 0.233 0.813 0.600 0.548 BIDIFAC+ 0.233 0.999 0.613 0.615 mRRR 0.603 0.711 0.998 0.770 aRRR, one all-shared 0.261 0.930 0.487 0.559 aRRR, 30 separate 0.376 0.780 1.001 0.719 LS reg, one all-shared 0.908 0.906 0.899 0.904 LS reg, 30 separate 0.560 N/A N/A N/A NN reg, one all-shared 0.912 0.913 1.032 0.953 NN reg, 30 separate 0.599 0.727 1.000 0.775 NN approx, one all-shared 0.273 1.000 0.454 0.576 NN approx, 30 separate 0.252 1.000 1.009 0.754 they cannot leverage shared structure when a gene is entirely missing within a cohort. On the contrary, methods with only one all-shared module (NN approx and aRRR) perform well. Here, maRRR also performs reasonable well, as including several individual modules does not limit its performance. In this case methods considering covariate effects only (mRRR, NN reg) do not perform well, as they tend toward estimates of zero (i.e., no predictions) to minimize squared error loss. NN approx is slightly better than aRRR., perhaps because it does not consider covariate-driven variation. Under the circumstances of a balanced mix of missingness for different conditions, maRRR has the best average recovering ability. This is largely because it is the most robust and flexible. Other comparable methods (BIDIFAC+, aRRR, NN approx) will have limited pe- formnance for at least one form of missingness. In reality, maRRR will be the most suitable for imputation since missingness is unpredictable and complex. 8 Discussion Two strengths of our proposed maRRR approach are its flexibility and versatility. It is flexible because it accounts for various types of signals - covariate-driven, shared or unshared - without prior assumptions on the size or rank of these signals. It is versatile because it is capable of performing many tasks at once: e.g., dimension reduction, prediction and missing data imputation. These advantages are well-illustrated by our pan-cancer application, in which adequate amounts of variation are explained by different components and the patterns detected are both insightful and consistent with existing scientific research on cancer. We focus on multi-cohort integration rather than multi-view (data on the same subjects from different sources) integration, in part because shared or unshared covariate effects are straightforward to interpret across multiple cohorts. But one can still argue that in a multi- view (e.g., multi-omics) context each sample will have intrinsic underlying signals that will affect variables from different sources. Without loss of generality, this method can be adapted to analyze multi-view data as well. This is achieved by simply switching the way we integrate 17matrices: horizontally across shared rows or vertically across shared columns. A promising future direction is to extend maRRR to the bidimensional integration context, where the data are both multi-cohort and multi-view. Another direction of future work is alternative empirical approaches to determine the module indicator matrices CY and CS, such as via an iterative stepwise selection procedure extending that in (Lock et al., 2022). While we have fixed the selection of penalty parameters λ(k) B , λ(l) S by employing random matrix theory, the parameters or the ranks of the underlying structures may be estimated empirically by a cross-validation procedure combined with a grid search. Further theoretical developments are also a pertinent future direction, such as proving the convergence of our optimization algorithms to a global optimum and establishing sufficient conditions for the uniqueness of the solution. There is empirical evidence for both conjectures, as we find that the converged solution is the same with different initializations and for the two optimization algorithms considered. Moreover, Theorem 1 of Lock et al. (2022) provides sufficient conditions for conditional uniqueness of the {S(l) · }L l=1 given {Bk}K k=1, and vice-versa. Data Availability Statement The data that support the findings in this paper are provided via this RData file. The user- friendly R package maRRR at https://github.com/JiuzhouW/maRRR performs all functions described herein, such as fitting models by the two algorithms in Section 5.1, imputing missing values as in Section 6.2, generating penalties as in Section 4, and generating data as in Appendix D.1. For real data analysis, we provide all the model estimates as Rdata file with detailed notation explanations and heatmaps for all module estimates in an online file. Acknowledgements We acknowledge support from NIH grant R01-GM130622 and helpful feedback from the Ed- itors and two referees. 18Appendix A Additional methodological details A.1 Notation details Detailed explanations for our proposed model 1 in the main manuscript are listed in Table 5. Table 5: Notation for the proposed model. J Observed Number of cohorts K Pre-specified Number of covariate effects L Pre-specified Number of auxiliary structures Xj Observed Outcome matrix for jth cohort Yj Observed Covariate matrix for jth cohort Y(k) · Pre-specified Design matrix for kth covariate effect concatenated from all J cohorts Bk Estimated coefficients for kth covariate effect S(l) · Estimated lth auxiliary structure concatenated from all J cohorts Ej Estimated Random error matrix for the jth cohort CY Pre-specified Binary indicator matrix where its [j, k]th entry determines whether jth cohort is considered in kth covariate effect CS Pre-specified Binary indicator matrix where its [j, l]th entry determines whether jth cohort is considered in lth auxiliary structure U(l) S Estimated Loading matrix of lth auxiliary structure V(l) Sj Estimated Score matrix of lth auxiliary structure for jth cohort U(k) B Estimated Loading matrix of kth covariate coefficients V(k) B Estimated Score matrix of kth covariate coefficients A.2 Construction of module indicator matrices The construction of module indicator matrices CY and CS can be accomplished in various ways depending on the specific context and available prior knowledge. If there exists prior knowledge indicating shared effects among certain cohorts, it would be straightforward to define modules accordingly. For example, defining a global module and individual modules for each cohort, as illustrated in Section 6.2. In absence of such information, there are other practical methods that can be applied. In scenarios where the number of cohorts is small, one can begin by enumerating all possible combinations of cohorts in CY and CS. As the objective function encourages rank sparsity, modules with no true shared structure may be estimated as zero (i.e., no structure) even if they are included in the algorithm. Moreover, after obtaining initial estimates, modules that explain a relatively higher amount of variance can be retained to form updated CY and CS. When dealing with a large number of cohorts, an alternative approach would be to adopt a data-driven strategy like the “Optimization algorithm: dynamic modules” section from Lock et al. (2022). This method is designed to select CS based on the amount of variance explained, after which CY can be set to match CS, thus partitioning the variance related to covariates. This is illustrated in Section 7.2. Optionally as a second step, one could keep the modules that explain a high amount of variance to reformulate CY and CS. This can help identify the most significant components in the covariate-related effects and auxiliary structures. 19Appendix B Proofs B.1 Proof of Theorem 1 Proof. Consider the following lemma, the proof of which is provided in Mazumder et al. (2010): Lemma 2. (Mazumder et al., 2010) For any matrix Z : m × n with rank(Z) = k, ∀r ≥ k, the following holds: ||Z||∗ = min U,V: Z=Um×rVT n×r 1 2(||U||2 F + ||V||2 F ) Applying Lemma 2 to each Bk, k = 1, ..., K and S(l) · , l = 1, ..., L: min {Bk}K k=1,{S(l) · }L l=1 {1 2||X· − K � k=1 BkY(k) · − L � l=1 S(l) · ||2 F + K � k=1 λ(k) B ||Bk||∗ + L � l=1 λ(l) S ||S(l) · ||∗} = min {Bk}K k=1,{S(l) · }L l=1 {1 2||X· − K � k=1 BkY(k) · − L � l=1 S(l) · ||2 F + K � k=1 λ(k) B min U(k) B ,V(k) B :Bk=U(k) B V(k)T B 1 2(||U(k) B ||2 F + ||V(k) B ||2 F )+ L � l=1 λ(l) S min U(l) S ,V(l) S :S(l) · =U(l) S V(l)T S 1 2(||U(l) S ||2 F + ||V(l) S ||2 F )} = min {U(k) B ,V(k) B :Bk=U(k) B V(k)T B }K k=1, {U(l) S ,V(l) S :S(l) · =U(l) S V(l)T S }L l=1 1 2{||X· − K � k=1 U(k) B V(k)T B Y(k) · − L � l=1 U(l) S V(l)T S ||2 F + K � k=1 λ(k) B min U(k) B ,V(k) B :Bk=U(k) B V(k)T B (||U(k) B ||2 F + ||V(k) B ||2 F )+ L � l=1 λ(l) S min U(l) S ,V(l) S :S(l) · =U(l) S V(l)T S (||U(l) S ||2 F + ||V(l) S ||2 F )} = min {U(k) B ,V(k) B }K k=1,{U(l) S ,V(l) S }L l=1 1 2{||X· − K � k=1 U(k) B V(k)T B Y(k) · − L � l=1 U(l) S V(l)T S ||2 F + K � k=1 λ(k) B (||U(k) B ||2 F + ||V(k) B ||2 F ) + L � l=1 λ(l) S (||U(l) S ||2 F + ||V(l) S ||2 F ).} B.2 Proof of Proposition 1 Proof. Assume a violation of condition 1, wherein λ(k) B ≥ 1 cy � i∈Ik λ(i) B . Let �BkY(k) · = � i∈Ik �B′ iY(i) · , where �B′ iY(i) · contains the blocks of �BkY(k) · corresponding to CY [·, i] and 0 otherwise. The choice of �B′ i is unique that �B′ i = 1 cy �Bk. For all i ∈ Ik, we have ||�B′ iY(i) · ||∗ = || 1 cy �BkY(i) · ||∗ ≤ || 1 cy �BkY(k) · ||∗ ≤ ||�BkY(k) · ||∗ since cy ≥ 1. Consider a minimizer {�Bk}K k=1, {�S(l) · }L l=1, 20where {�S(l) · }L l=1 = {�S(l) · }L l=1, �Bk = 0, �Bi = �Bi + �B′ i, ∀i ∈ Ik, and all other B estimates are equal. Then, by the triangle inequality, f({�Bk}K k=1, {�S(l) · }L l=1) − f({�Bk}K k=1, {�S(l) · }L l=1) = λ(k) B ||�Bk||∗ + � i∈Ik λ(i) B ||�Bi||∗ − � i∈Ik λ(i) B ||�Bi + �B′ i||∗ ≥ λ(k) B ||�Bk||∗ + � i∈Ik λ(i) B ||�Bi||∗ − � i∈Ik λ(i) B (||�Bi||∗ + ||�B′ i||∗) = λ(k) B ||�Bk||∗ − � i∈Ik λ(i) B ||�B′ i||∗ = (λ(k) B − 1 cy � i∈Ik λ(i) B )||�Bk||∗ ≥ 0 Now assume a violation of condition 2, wherein λ(k) B ≥ 1 csy � i∈Ik λ(i) S ||Y(k) · ||∗. Let �BkY(k) · = � i∈Ik �S(i)′ · , where �S(i)′ · contains the blocks of �BkY(k) · corresponding to CY [·, i] and 0 other- wise. The choice of �S(i)′ · is unique that �S(i)′ · = 1 csy �BkY(i) S· , where Y(i) S· = [Y(i) S1, Y(i) S2, ..., Y(i) SJ] with Y(i) Sj = � 0q×nj if CS[j, i] = 0 Yj if CS[j, i] = 1 for all i ∈ Ik. Since Y(i) S· is gained by setting some blocks of Y(k) · to be zero, ||�S(i)′ · ||∗ = || 1 csy �BkY(i) S· ||∗ ≤ || 1 csy �BkY(k) · ||∗. Consider a minimizer {�Bk}K k=1, {�S(l) · }L l=1, where �Bk = 0, �Si = �S(i) · + �S(i)′ · , ∀i ∈ Ik, and all other B, S estimates are equal. Then, by the triangle inequality, f({�Bk}K k=1, {�S(l) · }L l=1) − f({�Bk}K k=1, {�S(l) · }L l=1) = λ(k) B ||�Bk||∗ + � i∈Ik λ(i) S ||�S(i) · ||∗ − � i∈Ik λ(i) S ||�S(i) · + �S(i)′ · ||∗ ≥ λ(k) B ||�Bk||∗ + � i∈Ik λ(i) S ||�S(i) · ||∗ − � i∈Ik λ(i) S (||�S(i) · ||∗ + ||�S(i)′ · ||∗) = λ(k) B ||�Bk||∗ − � i∈Ik λ(i) S ||�S(i)′ · ||∗ ≥ λ(k) B ||�Bk||∗ − � i∈Ik λ(i) S || 1 csy �BkY(k) · ||∗ ≥ λ(k) B ||�Bk||∗ − � i∈Ik λ(i) S || 1 csy �Bk||∗||Y(k) · ||∗ = (λ(k) B − 1 csy � i∈Ik λ(i) S ||Y(k) · ||∗)||�Bk||∗ ≥ 0 The proof for condition 3 and 4 is similar to arguments made in (Lock et al., 2022). B.3 Proof of Proposition 2 Proof. Since the rows of Y are linear independent, the projection onto the space spanned by rows of Y, R(Y), is YT (YYT )−1Y = YT Y. By decomposing X onto R(Y) and R(I − Y), 21we have ||X − AY||2 F = ||X(I − YT Y) + XYT Y − AY||2 F = ||X(I − YT Y)||2 F + ||XYT Y − AY||2 F + tr[2(I − YT Y)XT (XYT − A)Y] = ||X(I − YT Y)||2 F + ||XYT Y − AY||2 F + tr[2Y(I − YT Y)XT (XYT − A)] = ||X(I − YT Y)||2 F + ||XYT Y − AY||2 F The second equation comes from the cyclic property of trace. Construct an orthogonal matrix Q = [Y, Y∗] where the rows of Y∗ give an orthonormal basis for R(I − Y), i.e. YY∗T = 0. Therefore, ||XYT Y − AY||2 F = tr[(XYT Y − AY)T (XYT Y − AY)] = tr[(XYT Y − AY)T (XYT Y − AY)QT Q] = ||XYT YQT − AYQT ||2 F = ||XYT Y � YT Y∗T � − AY � YT Y∗T � ||2 F = ||XYT �I 0 � − A �I 0 � ||2 F = ||XYT − A||2 F Combining these results, we rewrite min A {1 2||X − AY||2 F + λ||A||∗} = min A {1 2||X(I − YT Y)||2 F + 1 2||XYT − A||2 F + λ||A||∗} = min A {1 2||XYT − A||2 F + λ||A||∗} + 1 2||X(I − YT Y)||2 F Apply Lemma 1 and we get the first desired result. The second desired result follows imme- diately due to ||A||∗ = ||AY||∗ for YYT = I. Let SVD of A be UADAVT A and V′ A = YT VA. Since V′T A V′ A = VT AYYT VA = VT AVA = I, UADAV′T A is the SVD of AY. A and AY share the same singular values so that their nuclear norms are the same. B.4 Proof of Proposition 4 Proof. We start with the special case when Y is an identity matrix with q = n and X = B + 1 √nE. It follows directly from (Shabalin and Nobel, 2013) that σj(X) P−→      � 1 + σ2 j (B) + c + c σ2 j (B), if σj(B) > 4√c 1 + √c, if σj(B) ≤ 4√c . Note s(σj(B)) = � 1 + σ2 j (B) + c + c σ2 j (B) is a monotonic increasing function when σj(B) > 4√c. Therefore, min{s(σj(B))} > s( 4√c) = 1 + √c. Now consider the more general case that Xm×n = Bm×qYq×n + Em×n. Due to semi- orthogonality of Y, XYT = B+EYT . Since E is of matrix normal distribution MN m,n(0m×n, Im×m, In×n) and YT n×q is a linear transformation of full rank q ≤ n, we have EYT ∼ MN m,q(0YT , Im×m, YIYT ) = MN m,q(0m×q, Im×m, Iq×q). 22Recognize that entries of EYT are still independent normal. Denote ¯E = EYT and ¯X = XYT . The original question becomes ¯Xm×q = Bm×q + ¯Em×q. Applying the result of the special case, σj(XYT ) = σj( ¯X) = P−→      � 1 + σ2 j (B) + c + c σ2 j (B), if σj(B) > 4√c 1 + √c, if σj(B) ≤ 4√c. Following the aforementioned reasoning, we have the conclusion that σj(XYT ) will converge to a number larger than 1 + √c as σj(B) > 4√c. B.5 Proposition 5 and its proof Proposition 5. For any semi-orthogonal matrix Y such that YT Y = I, if the optimization problems minB{1 2||X − BY||2 F + λ||B||∗} and minS{1 2||X − S||2 F + λ||S||∗} have their optimal solutions as �B and �S respectively, then �S = �BY. Proof. Since orthogonal transformation preserves Frobenius norm, we have ||X − BY||F = ||XYT − B||F . Then, min B {1 2||X − BY||2 F + λ||B||∗} = min B {1 2||XYT − B||2 F + λ||B||∗}. Let SVD of X to be UDVT . Since (YV)T (YV) = VT YT YV = VT V = I, we have UD(YV)T = UDVT YT = XYT serving as the SVD of XYT . Applying Lemma 1, the solu- tion for minB{1 2||XYT −B||2 F +λ||B||∗} is �B = U �D(YV)T and the solution for minS{1 2||X− S||2 F + λ||S||∗} is �S = U �DVT . Therefore, �S = �BY. If Y(k) · is semi-orthogonal with Y(k)T · Y(k) · = In(q ≥ n) and contains information from exactly the same cohorts as S(l) · (i.e., CY [·, k] = CS[·, l]), the estimation process cannot differentiate �BkY(k) · from �S(l) · . Even without overlapping modules, the newly proposed model reverts to the unsupervised model 3 described in Section 3, which only comprises {�S·(l)}L l=1. Thus, the loss with semi-orthogonal regressors Y : q × n is only valid if q < n. This finding aligns with our real-world problem of interest: in reality, we have only a few somatic mutations, the number of which is less than the number of patients in the study. Appendix C Scaling and orthogonalization In reality the original Y is not orthogonal. In practice we scale X and orthogonalize Y prior to optimization. We first center each row of X· to have mean 0. In order to satisfy the standard normal noise requirement, we estimate the error variance for X· by using the median absolute deviation estimator from (Gavish and Donoho, 2017). The estimated variance of X· is denoted as ˆσ2. Then, we use X·/ˆσ as the final data matrix for optimization, which has residual variance approximately 1. We further orthogonalize the columns of Y via SVD prior to optimization, and transform the solution back to the original covariate space afterward. Here, we describe two scaling approaches for Y: one standardizing the original covariates with out orthogonalization, and another rotating the covariate space so that it is orthogonal. In the next section, we show simulation results that illustrate the difference between these two approaches. Instead of estimating B based on original Y(k) · , we first center each covariate in Y(k) · and scale each covariate to have variance 1 and then do the optimization. For each k = 1, ..., K, the detailed steps are: 231. Center each covariate in Y(k) · and calculate the square root of row sums of squared entry of Y(k) · , denoted as t(k) 1 , t(k) 2 , ..., t(k) q . 2. Construct a diagonal matrix Σnk = diag(t(k) 1 , ..., t(k) q ). 3. Use Y(k) ·scaled = Σn−1 k Y(k) · in optimization. 4. Denote the estimation with respect to Y(k) ·scaled as �Bk,scaled. Then, our final estimation for Bk on its original scale is �Bk = �Bk,scaledΣnk. In order to further reduce collinearity among covariates, after centerization we may instead choose to apply orthogonalization to Y(k) · rather than standardization. For each k = 1, ..., K, the detailed steps are: 1. Obtain the SVD of Y(k) · , i.e. Y(k) · = UkDkVT k . 2. Use Y(k) ·orth = VT k in optimization. 3. Denote the estimation with respect to Y(k) ·orth as �Bk,orth. Then, our final estimation for Bk on its original scale is �Bk = �Bk,orthUkDk. There are other ways to orthogonalize, such as Gram-Schmidt method. No matter standard- ization or orthogonalization, we record the transforming matrix from the original Y(k) · to new Y(k) ·scaled/orth for correction of estimated Bk at the final stage. Appendix D More details on the simulations D.1 Complete data generation Here we describe the complete process to generate data in Section 6.2. 1. Every entry in each Yj, j = 1, ..., J = 30 is drawn independently from a standard normal distribution. By default, the variance of each feature is 1. Construct one global shared Y(1) · = [Y1, ..., Y30] and 30 individual Y(k) · = [0, ..., Yk−1, ..., 0], k = 2, ..., 31 (only the k − 1th submatrix is non-zero). 2. For the K = 31 modules of Y(k) · , generate Bk = U(k) B V(k)T B /sd(U(k) B V(k)T B Y(k) · )∗ � nk/n, where nk is the number of samples in module k. Each entry of U(k) B : p × r, V(k) B : q × r comes from standard Normal distribution. 3. For a number of L = 31 modules of S(l) · involved, draw one global score matrix V(1) S : n×r and 30 individual score matrices V(l) S = [0, ..., Vl−1, ..., 0], l = 2, ..., 31, where each entry of V1, ..., V30 comes from standard Normal. Generate S(l) · = U(l) S V(l)T S /sd(U(l) S V(l)T S ) ∗ � nl/n, where each entry of U(l) S : p × r comes from standard Normal distribution and nl is the number of samples in module l. 4. Draw each entry of E· from a standard normal distribution. 5. Generate X· = a ∗ B1Y(1) · + b ∗ S(1) · + c ∗ �31 k=2 BkY(k) · + d ∗ �31 l=2 S(l) · + E·. The letters a, b, c, d are constant for signal size. For instance, scenario (a), a = √ 10 and the remaining equal 1. 24D.2 Computation time Table 6 demonstrates the computation time for all methods in missing data imputation for the TCGA data. The computation time of non-missing optimization and missing data imputation are similar. The proposed maRRR method consumes the most time since it considers both covariate effects and auxiliary modules. Generally, the computation time is proportional to the number of modules. But it will vary because of different number of cohorts within one module. Algorithm 1 requests more computation time when the true rank in some module is large. In general, both algorithms output similar RSE so that the one takes less computation time is used in the simulation. The case of missing columns takes slightly less computation time than the other two cases. This results from that it is uninformative when missing certain subjects. It may lead to all-zero imputation and this explains why aRRR for 30 separate modules takes significantly less time than the other cases. 25Table 6: Computation time (in seconds) for our proposed methods and all other comparison methods in missing data imputation (based on algorithm 1 and 2 respectively) for the TCGA real data application. Each other method is based on 30 epochs. NA represents that missing data imputation based on Algorithm 2 does not work for “NNreg” and “NNapprox”. “50 modules” means that the method is based on 50 detected modules. “1 all-shared + 30 separate” means that the method is based on one all-shared module and 30 separate modules (in total 31 modules). For method name representations, please refer to Section 6.2 in the main context. Algorithm 1 Algorithm 2 Method missing entries missing columns missing rows average missing entries missing columns missing rows average maRRR 50 modules 6251.0 5345.2 6442.4 6012.9 3725.3 3264.7 3754.8 3581.6 BIDIFAC+, 50 modules 5000.7 4283.4 5198.2 4827.4 1140.7 1058.2 1161.3 1120.1 mRRR, 50 modules 1566.7 1327.9 1576.9 1490.5 2905.9 2519.9 2873.3 2766.4 maRRR, 1 all-shared + 30 separate 1925.6 1623.3 1962.9 1837.3 2276.5 2085.1 2303.6 2221.7 BIDIFAC+, 1 all-shared + 30 separate 1109.9 967.1 1124.9 1067.3 705.2 684.8 714.9 701.6 mRRR, 1 all-shared + 30 separate 938.2 823.7 939.5 900.4 1711.2 1556.3 1729.0 1665.5 aRRR, one all-shared 617.9 524.8 637.8 593.5 75.1 67.6 74.7 72.5 aRRR, 30 separate 1246.3 68.5 1249.0 854.6 2196.8 123.4 2195.6 1505.3 NNreg, one all-shared 2.2 2.2 5.5 3.3 NA NA NA NA NNapprox, one all-shared 116.7 49.6 585.0 250.5 NA NA NA NA NNreg, 30 separate 4.5 18.5 21.5 14.9 NA NA NA NA NNapprox, 30 separate 60.4 22.6 256.2 113.1 NA NA NA NA D.3 Additional simulation to assess aRRR and maRRR Here we discuss an additional simulation study to assess aspects of aRRR and maRRR, including the effects of orthogonalizing Y and the recovery of the underlying ranks of the true structure. The simulation covers scenarios in which the original explanatory data matrices Yj, j = 1, ..., J are orthogonal or not. The process, used to generate the complete data for this section, is as follows: 1. Every entry in each Yj, j = 1, ..., J is drawn independently from a standard normal distribution. By default, the variance of each feature is 1. Denote the standard deviation for each Yj, j = 1, ..., J as sd(Yj), j = 1, ..., J. 262. If we aim to orthogonalize some Yj, j = 1, ..., J, denote the transpose of the right orthog- onal matrix from singular value decomposition of Yj as VYj. Calculate the orthogonal version of Yj, j = 1, ..., J by VYj/sd(Yj), j = 1, ..., J in order to keep the original scale of variation. 3. For a number of K modules of covariate effects involved, construct Y(k) · , k = 1, ..., K. Generate Bk = U(k) B V(k)T B , where each entry of U(k) B : q × r, V(k) B : nk × r comes from standard Normal distribution. 4. For a number of L modules of S(l) · involved, generate S(l) · = U(l) S V(l)T S , where each entry of U(l) S : p × r and each non-zero entry of V(l) S : q × r comes from standard Normal distribution. 5. Draw each entry of E· from a standard normal distribution. 6. Generate X· = �K k=1 BkY(k) · + �L l=1 S(l) · + E·. Besides mean squared error (mse) as a metric to assess accuracy, we want to understand how low-rank structures for B, S are uncovered is estimated under different orthogonality. Therefore, we define the ”rank sum ratio” as follows: �rB i=1 λi(�B) �rB,upper i=rB+1 λi(�B) , �rS i=1 λi(�S) �rS,upper i=rS+1 λi(�S) where rB is the true rank of B and rB,upper is the upper bound of B specified in estimation, both similarly defined for S. The smaller the rank sum ratio is, the lower the rank structure achieves. Table 7 and 8 shows the MSE and rank sum ratio for aRRR and maRRR simulations respectively. We consider one cohort for aRRR and two cohorts for maRRR. In general, orthogonaliztion before optimization and in data generation achieve similar results. Compared with only standardization of Y, orthogonaliztion of Y leads to less MSE and more accurate rank estimations. After orthogonaliztion, the proposed methods still overestimate the true rank of covariate-related signal, but it is not severe as the rank sum ratio is below 0.01. Therefore, orthogoalization of Y is helpful for optimization. 27Table 7: Mean squared error(MSE) for aRRR under different scenarios of orthogonality, true rank and signal size. Row names: “r y” refers to true rank of Y in the data generation process. “sd YB” and “sd S” refer to the standard deviation for matrix BY and S in generation respectively. “epochs” refers to the number of iterations to converge. “ratio B” refers to the rank sum ratio defined before. “est rank B” counts the number of singular values that large than 0.1. Column names: “no orth” means that the original Y is not orthogonal and we only standardize it before optimization. “orth opt” means that we only orthogonalize original “Y” before optimization. “orth gen” means that the original Y is orthogonal. r y sd YB sd S epochs mse B mse S est rank B ratio B no orth 1 5 0.5 65.82 0.001 0.607 1.3 0.002 no orth 1 1 1 31.27 0.038 0.225 1.39 0.012 no orth 1 0.5 5 44.82 0.172 0.012 1.48 0.037 no orth 5 5 0.5 52.36 0.007 0.629 5.04 0 no orth 5 1 1 34.38 0.141 0.238 4.95 0.001 no orth 5 0.5 5 41.5 0.404 0.013 4.42 0 orth opt 1 5 0.5 63.66 0.001 0.606 1.06 0 orth opt 1 1 1 33.69 0.033 0.224 1.12 0.004 orth opt 1 0.5 5 47.4 0.157 0.012 1.16 0.011 orth opt 5 5 0.5 45.41 0.006 0.626 5 0 orth opt 5 1 1 27.62 0.125 0.238 4.94 0 orth opt 5 0.5 5 39.7 0.372 0.013 4.42 0 orth gen 1 5 0.5 63.31 0.001 0.606 1.08 0.001 orth gen 1 1 1 33.47 0.034 0.224 1.1 0.004 orth gen 1 0.5 5 47.82 0.161 0.012 1.14 0.012 orth gen 5 5 0.5 45.42 0.006 0.624 5 0 orth gen 5 1 1 27.72 0.122 0.237 4.96 0 orth gen 5 0.5 5 41.2 0.369 0.013 4.34 0.001 28Table 8: Mean squared error(MSE) for maRRR under different scenarios of orthogonality, true rank and signal size. Row names: “r y” refers to true rank of Y in the data generation process. “sd YB” and “sd S” refer to the standard deviation for matrix BY and S in generation respectively. “epochs” refers to the number of iterations to converge. “ratio B” refers to the rank sum ratio defined before. “est rank B” counts the number of singular values that large than 0.1. Column names: “no orth” means that the original Y is not orthogonal and we only standardize it before optimization. “orth opt” means that we only orthogonalize original “Y” before optimization. “orth gen” means that the original Y is orthogonal. All results are the average of all B or S. r y sd YB sd S epochs mse B mse S est rank B ratio B no orth 1 2 0.2 84.56 0.008 0.953 1.493 0.008 no orth 1 1 1 77.72 0.037 0.175 1.873 0.024 no orth 1 0.2 2 88.6 0.4 0.065 1.76 0.141 no orth 5 2 0.2 123.45 0.115 0.962 6.617 0.033 no orth 5 1 1 87.78 0.202 0.195 6.06 0.023 no orth 5 0.2 2 91.41 0.735 0.065 3.273 0 orth opt 1 2 0.2 82.34 0.007 0.952 1.393 0.007 orth opt 1 1 1 76.96 0.034 0.174 1.66 0.019 orth opt 1 0.2 2 90.04 0.366 0.065 1.543 0.089 orth opt 5 2 0.2 119.61 0.114 0.955 6.727 0.036 orth opt 5 1 1 86.66 0.194 0.195 6.18 0.026 orth opt 5 0.2 2 93.63 0.702 0.065 3.313 0 orth gen 1 2 0.2 82.64 0.007 0.952 1.333 0.006 orth gen 1 1 1 78.4 0.034 0.175 1.64 0.018 orth gen 1 0.2 2 89.62 0.367 0.065 1.52 0.09 orth gen 5 2 0.2 116.33 0.109 0.955 6.743 0.035 orth gen 5 1 1 86.79 0.186 0.195 6.2 0.027 orth gen 5 0.2 2 93.07 0.692 0.065 3.437 0 Appendix E More details on the real data analysis E.1 Cancer type and mutation details We provide Table 9 as the summary of all the 30 cancer types and Table 10 as the summary of all the 50 somatic mutations considered in our real data analysis (Section 7). 29Table 9: The cancer study abbreviations, sample sizes and study names, sourc- ing from National Cancer Institute https://gdc.cancer.gov/resources-tcga-users/ tcga-code-tables/tcga-study-abbreviations. Abbreviation Sample Size Study Name ACC 77 Adrenocortical carcinoma BLCA 129 Bladder Urothelial Carcinoma BRCA 976 Breast invasive carcinoma CESC 193 Cervical squamous cell carcinoma and endocervical adenocarcinoma COAD 147 Colon adenocarcinoma ESCA 184 Esophageal carcinoma GBM 150 Glioblastoma multiforme HNSC 279 Head and Neck squamous cell carcinoma KICH 66 Kidney Chromophobe KIRC 415 Kidney renal clear cell carcinoma KIRP 161 Kidney renal papillary cell carcinoma LAML 170 Acute Myeloid Leukemia LGG 283 Brain Lower Grade Glioma LIHC 195 Liver hepatocellular carcinoma LUAD 230 Lung adenocarcinoma LUSC 178 Lung squamous cell carcinoma OV 115 Ovarian serous cystadenocarcinoma PAAD 150 Pancreatic adenocarcinoma PCPG 179 Pheochromocytoma and Paraganglioma PRAD 331 Prostate adenocarcinoma READ 64 Rectum adenocarcinoma SARC 245 Sarcoma SKCM 342 Skin Cutaneous Melanoma STAD 275 Stomach adenocarcinoma TGCT 149 Testicular Germ Cell Tumors THCA 400 Thyroid carcinoma THYM 119 Thymoma UCEC 242 Uterine Corpus Endometrial Carcinoma UCS 57 Uterine Carcinosarcoma UVM 80 Uveal Melanoma 30Table 10: Gene labels for 50 somatic mutation data in order. Index Mutation Index Mutation Index Mutation 1 TP53 18 FAT4 35 PKHD1L1 2 TTN 19 HMCN1 36 RYR1 3 MUC16 20 CSMD1 37 RYR3 4 PIK3CA 21 MUC5B 38 NEB 5 CSMD3 22 ZFHX4 39 PCDH15 6 LRP1B 23 FAT3 40 DST 7 KRAS 24 SPTA1 41 MLL3 8 RYR2 25 GPR98 42 MLL2 9 MUC4 26 PTEN 43 MACF1 10 FLG 27 FRG1B 44 DNAH9 11 SYNE1 28 AHNAK2 45 BRAF 12 USH2A 29 APOB 46 DNAH11 13 PCLO 30 ARID1A 47 DNAH8 14 APC 31 LRP2 48 CSMD2 15 DNAH5 32 XIRP2 49 MUC2 16 OBSCN 33 Unknown 50 ABCA13 17 MUC17 34 DMD E.2 Data processing and distributions The pan-cancer RNASeq data, described in Hoadley et al. (2018), were downloaded as the file ‘EBPlusPlusAdjustPANCAN IlluminaHiSeq RNASeqV2.geneExp.tsv’ from https://gdc.cancer. gov/about-data/publications/PanCan-CellOfOrigin [accessed June 23, 2021]. This dataset had undergone preprocessing steps described in Hoadley et al. (2018), including batch correc- tion using an empirical Bayes approach and upper-quartile normalization. These data were further log-transformed (via a log(1 + x) transformation), and filtered to the 1000 genes with the highest standard deviation after log-transformation. The log-transformed and filtered data were then gene-centered by subtracting the overall mean (across all cancer types) for each gene. The distribution of processed expression values for each cancer type are shown in Figure 3; the distributions are roughly similar and approximately bell-shaped across the different cancer types. The somatic mutation data were binary, prior to the the scaling described in Section C, where ‘1’ implies there is a somatic mutation in the gene for the given sample and ‘0’ implies there is no somatic mutation. Figure 4 gives the proportion of samples that have a mutation across the 50 genes considered, for each cancer type. Some cancer types have several genes that are frequently mutated, while others have a sparser profile with no frequently mutated genes among those considered. 31Figure 3: Violin plot of normalized expression values for each cancer type. 32Figure 4: Violin plot of proportion of samples with a somatic mutation across the 50 genes, for each cancer type. E.3 Selection of model parameters As mentioned in Section 7.2 of the main article, we first apply the optimization with dynamic modules for BIDIFAC+ (Lock et al., 2022), to uncover 50 low-rank modules in X·. BIDIFAC+, noted in Section 3, is the unsupervised version of our proposed model maRRR. The statistical model of BIDIFAC+ is X· = L � l=1 S(l) · + E· 33where S(l) · = [S(l) 1 , S(l) 2 , ..., S(l) J ], E· = [E1, E2, ..., EJ] and S(l) j = �0p×nj if CS[j, l] = 0 U(l) S V(l)T Sj if CS[j, l] = 1. The loss objective is min {S(l) · }L l=1 {1 2||X· − L � l=1 S(l) · ||2 F + L � l=1 λ(l) S ||S(l) · ||∗}. The model aggregates all signals as �L l=1 S·(l), yet it doesn’t differentiate whether these signals are related to covariates or not. The forward selection process to determine CS of BIDIFAC+ initiates with CS[:, l] = 0 for all l = 1, ..., L, and progressively includes cohorts j (CS[j, l] = 1) to minimize the objective function; see Section 6.3 in (Lock et al., 2022) for complete details. This update occurs iteratively through gradient descent, similar to our Algorithm 2 in Section 5.1 but with dynamic module memberships. This iterative process continues until convergence of loss, at which point the current CS is considered the final set of modules. After thorough analysis, we opt for L = 50 since including more modules does not significantly enhance our ability to explain variance. A few modules, selected at various L values, contribute substantially to the final model’s variance. Consequently, we select the corresponding module indicator matrix CS for L = 50, thus setting CY = CS to effectively partition the variance linked to covariate effects. A comprehensive visualization of final estimated module information is presented in the subsequent heatmap in Figure 5. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 ACC BLCA BRCA CESC COAD ESCA GBM HNSC KICH KIRC KIRP LAML LGG LIHC LUAD LUSC OV PAAD PCPG PRAD READ SARC SKCM STAD TGCT THCA THYM UCEC UCS UVM Figure 5: Heatmap for 50 modules detected by BIDIFAC+ model (the chosen CS) in the TCGA real data application. The row names represent the 30 cohorts while the column names represent the 50 modules. Black grids mean existence of cohorts in the current module. 34As outlined in Appendix C, we apply scaling to the standardized X· using the median absolute deviation estimator ((Gavish and Donoho, 2017)) to ensure that the residual variance is approximately 1, i.e., V ar(E·) = 1. Drawing from random matrix theorems with a variance of 1 in Section 4, we assign λ(i) B = √ 1000 + √ 50, i = 1, ..., 50 since all Bi share the same matrix size. Additionally, we set λ(1) S = √ 1000 + √ 976, λ(2) S = √ 1000 + √ 400, ..., and λ(50) S = √ 1000 + √ 742, with the second term representing the square root of the number of samples in the respective module. In Algorithm 1, we establish a general upper bound for the estimated rank of each Bi and S(i) · at 20, denoted as rB,upper = 20 and rS,upper = 20 respectively. Through experimentation, we determined that 20 is the minimum value for Algorithm 1 to match the performance of Algorithm 2. This rank upper bound of 20 is deemed reasonable for low-rank approximations. While the true ranks of most estimates hover around 10, opting for smaller maximum rank val- ues dismisses significant signals. Conversely, larger values don’t provide substantial additional information for improving predictions, but rather increase computational complexity. E.4 Additional pan-cancer analysis With the identification of 50 modules and a maximum rank set at 20, maRRR attains an impressive Relative Squared Error (RSE) of 0.184, signifying the substantial capture of vari- ation. Following Section 7.2, the subsequent scatterplot Figure 6 depicts the extent to which mutations account for variance within each module. While the impact of mutations is not overwhelmingly dominant, it is nevertheless significant, aligning with our initial expectations. This outcome is consistent with the extensive genetic information present in gene expressions that is unrelated to mutations. When considered alongside the heatmap of CS (Figure 5 in Appendix E.3), it becomes evident that modules such as numbers 2, 6, 7, 13, 22, 23, 41, and 50 exhibit substantial influence from mutations. Remarkably, nearly all of the 30 cancer types make varying appearances within those modules, with the exceptions of ACC, KIRC, and PRAD. 354.0 4.5 5.0 5.5 6.0 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Scatterplot of mutation ratios and log of signals Log (base 10) of the variance of the signal Ratio between variance of covariate effects and signal 1 2 3 5 7 8 9 10 11 13 15 16 18 19 21 22 23 24 26 28 29 30 32 33 34 35 37 38 39 40 41 42 43 44 45 47 48 49 50 4 6 1714 20 25 36 31 27 46 12 Figure 6: Scatterplot of The x-axis is the log (base 10) of the variance of the signal; the y-axis is the ratio between variance of covariate effects BiY(i) · and signal. The number near each dot is the module identification as in CS. Besides the individual and partially shared shared structures, we are able to detect global shared effects as well. Module 12 has little covariate effects, which implies that all 50 can- didate mutations do not have significant global effects on 29 cancer types. However, based on Figure 7, we are able to observe several clusters that share across all the cancer types (horizontally). 36Figure 7: Heatmap for Module 12 auxiliary structure. Columns represent individual samples, while rows represent distinct gene expressions. This column-row representation is consistent across all other heatmaps in the online application results spreadsheet. Extreme values out- side of 3 standard deviations are set to be threshold values. The graphics is based on the relative scale. Red colors represent high gene expressions and blue colors represents low gene expressions. References Bunea, F., She, Y., and Wegkamp, M. H. (2011). Optimal selection of reduced rank estimators of high-dimensional matrices. The Annals of Statistics 39, 1282–1309. Cai, J.-F., Cand`es, E. J., and Shen, Z. (2010). A singular value thresholding algorithm for matrix completion. SIAM Journal on optimization 20, 1956–1982. Cancer Genome Atlas Research Network (2012). Comprehensive molecular portraits of human breast tumours. Nature 490, 61–70. Chen, K., Dong, H., and Chan, K.-S. (2013). Reduced rank regression via adaptive nuclear norm penalization. Biometrika 100, 901–920. Dolezal, J. M., Trzcinska, A., Liao, C.-Y., Kochanny, S., Blair, E., Agrawal, N., Keutgen, X. M., Angelos, P., Cipriani, N. A., and Pearson, A. T. (2021). Deep learning prediction of braf-ras gene expression signature identifies noninvasive follicular thyroid neoplasms with papillary-like nuclear features. Modern Pathology 34, 862–874. Feng, Q., Jiang, M., Hannig, J., and Marron, J. (2018). Angle-based joint and individual variation explained. Journal of multivariate analysis 166, 241–265. 37Gavish, M. and Donoho, D. L. (2017). Optimal shrinkage of singular values. IEEE Transactions on Information Theory 63, 2137–2152. Gaynanova, I. and Li, G. (2019). Structural learning and integrative decomposition of multi- view data. Biometrics 75, 1121–1132. Ham, S. W., Jeon, H.-Y., Jin, X., Kim, E.-J., Kim, J.-K., Shin, Y. J., Lee, Y., Kim, S. H., Lee, S. Y., Seo, S., et al. (2019). Tp53 gain-of-function mutation promotes inflammation in glioblastoma. Cell Death & Differentiation 26, 409–425. Hoadley, K. A., Yau, C., Hinoue, T., Wolf, D. M., Lazar, A. J., Drill, E., Shen, R., Taylor, A. M., Cherniack, A. D., Thorsson, V., et al. (2018). Cell-of-origin patterns dominate the molecular classification of 10,000 tumors from 33 types of cancer. Cell 173, 291–304. Hutter, C. and Zenklusen, J. C. (2018). The cancer genome atlas: creating lasting value beyond its data. Cell 173, 283–285. Izenman, A. J. (1975). Reduced-rank regression for the multivariate linear model. Journal of multivariate analysis 5, 248–264. Li, G., Liu, X., and Chen, K. (2019). Integrative multi-view regression: Bridging group-sparse and low-rank models. Biometrics 75, 593–602. Li, G., Yang, D., Nobel, A. B., and Shen, H. (2016). Supervised singular value decomposition and its asymptotic properties. Journal of Multivariate Analysis 146, 7–17. Lock, E. F., Park, J. Y., and Hoadley, K. A. (2022). Bidimensional linked matrix factorization for pan-omics pan-cancer analysis. The annals of applied statistics 16, 193. Mazumder, R., Hastie, T., and Tibshirani, R. (2010). Spectral regularization algorithms for learning large incomplete matrices. The Journal of Machine Learning Research 11, 2287–2322. Olivier, M., Hollstein, M., and Hainaut, P. (2010). Tp53 mutations in human cancers: origins, consequences, and clinical use. Cold Spring Harbor perspectives in biology 2, a001008. Rudelson, M. and Vershynin, R. (2010). Non-asymptotic theory of random matrices: extreme singular values. In Proceedings of the ICM 2010, pages 1576–1602. World Scientific. Shabalin, A. A. and Nobel, A. B. (2013). Reconstruction of a low-rank matrix in the presence of gaussian noise. Journal of Multivariate Analysis 118, 67–76. Troyanskaya, O., Cantor, M., Sherlock, G., Brown, P., Hastie, T., Tibshirani, R., Botstein, D., and Altman, R. B. (2001). Missing value estimation methods for dna microarrays. Bioinformatics 17, 520–525. Wang, J. and Safo, S. E. (2021). Deep ida: A deep learning method for integrative discriminant analysis of multi-view data with feature ranking–an application to covid-19 severity. ArXiv page 2111.09964. Yuan, M., Ekici, A., Lu, Z., and Monteiro, R. (2007). Dimension reduction and coefficient estimation in multivariate linear regression. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 69, 329–346. Zhang, Y. and Gaynanova, I. (2022). Joint association and classification analysis of multi-view data. Biometrics 78, 1614–1625. 38
INTERPRETATION OF HIGH-DIMENSIONAL LINEAR REGRESSION: EFFECTS OF NULLSPACE AND REGULARIZATION DEMONSTRATED ON BATTERY DATA Joachim Schaeffer Technical University of Darmstadt Control and Cyber-Physical Systems Laboratory Karolinenpl. 5 Darmstadt, 64289, Germany Eric Lenz Technical University of Darmstadt Control and Cyber-Physical Systems Laboratory Karolinenpl. 5 Darmstadt, 64289, Germany William C. Chueh Stanford University 450 Jane Stanford Way Stanford, 94305, CA, USA Martin Z. Bazant Massachusetts Institute of Technology 77 Massachusetts Avenue Cambridge, 02139, MA, USA Rolf Findeisen Technical University of Darmstadt Control and Cyber-Physical Systems Laboratory Karolinenpl. 5 Darmstadt, 64289, Germany Richard D. Braatz Massachusetts Institute of Technology 77 Massachusetts Avenue Cambridge, 02139, MA, USA braatz@mit.edu ABSTRACT High-dimensional linear regression is important in many scientific fields. This article considers discrete measured data of underlying smooth latent processes, as is often obtained from chemical or biological systems. Interpretation in high dimensions is challenging because the nullspace and its interplay with regularization shapes regression coefficients. The data’s nullspace contains all coefficients that satisfy Xw = 0, thus allowing very different coefficients to yield identical predic- tions. We developed an optimization formulation to compare regression coefficients and coefficients obtained by physical engineering knowledge to understand which part of the coefficient differences are close to the nullspace. This nullspace method is tested on a synthetic example and lithium-ion battery data. The case studies show that regularization and z-scoring are design choices that, if chosen corresponding to prior physical knowledge, lead to interpretable regression results. Otherwise, the combination of the nullspace and regularization hinders interpretability and can make it impossible to obtain regression coefficients close to the true coefficients when there is a true underlying linear model. Furthermore, we demonstrate that regression methods that do not produce coefficients orthog- onal to the nullspace, such as fused lasso, can improve interpretability. In conclusion, the insights gained from the nullspace perspective help to make informed design choices for building regression models on high-dimensional data and reasoning about potential underlying linear models, which are important for system optimization and improving scientific understanding. Keywords Interpretable Machine Learning · Linear Regression · High Dimensions · Nullspace · Functional Data · Regression Coefficients · Lithium-Ion Batteries arXiv:2309.00564v1 [stat.ML] 1 Sep 2023Nullspace Perspective on Regression Coefficients 1 Introduction Many important regression problems have the dimensionality of the data p much larger than the sample size n [1]–[4]. Consequently, p ≫ n and the matrix X ∈ Rn×p of predictors is “wide”. This case arises, for example, in most spectroscopies, lithium-ion batteries [5], [6], brain imaging, and computational biology [1], [3], [7]. Classical literature on linear regression [8]–[10] focuses mainly on the case where p < n and mostly assumes full column rank; however, many linear regression methods work well with wide predictor matrices. While Ordinary Least Squares (OLS) is not defined for wide data matrices because X⊤X is singular, the related minimum norm solution [11] can be used instead. Ridge Regression (RR) and other shrinkage-based regression methods (e.g., Least Absolute Shrinkage and Selection Operator (lasso), Elastic Net (EN)) do not suffer from this problem due to the penalty term that is added to the main diagonal of X⊤X. The fused lasso, a generalization of the lasso, adds an L1-norm penalty of adjacent regression coefficient differences to the objective function [12]. This additional penalty encourages piecewise constant regression coefficients, i.e., sparsity in regression coefficient differences. Thus it is required that the predictors can be ordered in some meaningful way. Latent variable methods such as Partial Least Squares (PLS) and Principal Component Regression (PCR) are popular choices for high-dimensional regression in the chemometrics community. A key question is how to interpret high-dimensional linear regression results and the corresponding regression coefficients. In particular, how to reason about an underlying (linear) model for scientific insights and system optimization? Technically, regression coefficients for a linear model can be analyzed and compared to engineering or scientific expectations in terms of shape (e.g., peaks, plateaus, slopes), which is often done implicitly by engineers when looking at regression coefficients. However, as shown in this article, such an interpretation can lead to misleading conclusions. This article develops a method, based on the nullspace of the predictor matrix N(X), for comparing coefficients obtained by different methods with each other for the case of high-dimensional data that was generated by a smooth latent process, also called functional data [13].1 We use the fact that N(X) consists of all solutions to Xw = 0 and thus the predictions do not change when adding a vector of the nullspace to the regression coefficients X(β + w) = Xβ. The nullspace and its interplay with regularization significantly influence the shape of the regression coefficients. Therefore, an understanding of the effect of the nullspace is needed for interpretation and scientific understanding. Our objective is to support such an understanding with this article. The next section briefly introduces the key linear regression methods used in this article. Then the nullspace approach is derived. Subsequently, case studies are presented on fully synthetic data, lithium-ion battery data with two different synthetic linear responses, and the measured nonlinear cycle life response [15]. The conclusion section summarizes the key learnings. All code and data used in this article are open-source and open-access, allowing the reproduction of results. 2 Motivation and Linear Regression Linear, static models, assuming mean-centered data, have the general form y = Xβ∗ + ϵ (1) where the input data matrix X ∈ Rn×p, n is the number of observations, p is the number of predictors, and we assume that p ≫ n. Our work is motivated by measurements of chemical or biochemical systems, i.e., discrete, noisy measurements of an assumed smooth underlying process. Consequently, we assume a latent model structure, i.e., X (independently of y) can be approximated in a lower dimensional space, and X is not sparse. Most of the analysis in this article is technically not limited to this assumption. However, the nullspace perspective is motivated by a latent model structure and the high multicollinearity of columns that arises from functional data. The coefficients β ∈ Rp contain the relation between X and y. The errors ϵ ∈ Rp are assumed to be homoscedastic, to have zero means, and to be uncorrelated. Linear regression denotes statistical methods to determine ˆβ from data X and y minimizing the error ˆϵ concerning a defined measure of the error, y = ˆy + ˆϵ = X ˆβ + ˆϵ. (2) The objective of regression methods is to find a ˆβ that yields predictions that are reasonably close to the predictions of β∗ when applied to independent data, i.e., were not available during training. When a true underlying linear model 1Measured data from chemical and other systems often exhibit a certain degree of smoothness and can be considered to originate from discretized functions (similar to the assumptions made in [14]). The term smoothness, as used in this article, refers to data in which neighboring values are linked to each other to some extent, are not too different from one another, and there exists an underlying function that is differentiable once or multiple times. 2Nullspace Perspective on Regression Coefficients exists, interpretation and scientific insights would be supported by achieving a different goal, which is to reconstruct the true coefficients, i.e., ˆβ = β∗, where β∗ are the true coefficients of the model. As shown in [6], often columns in high-dimensional functional data are correlated, and regularized regression will find a solution that is optimal for its objective function; however, the resulting regression coefficients can be visually very different from β∗ due to the interplay of the regularization and the nullspace, N(X). Furthermore, in practice, β∗ is not known and the true underlying system might be nonlinear, requiring a thorough understanding of the interplay of regularization and the nullspace to draw reasonable conclusions about the underlying model. Generally, the regression coefficients associated with ˆβModel are random variables because X and y are realizations from a system that contains randomness (e.g., measurement errors, random system processes, etc.). One approach to model the regression coefficients probabilistically is Bayesian linear regression which places a prior on the regression coefficients and yields their posterior distribution, conditioned on data, which can then be analyzed (e.g., see [16] for more information on Bayesian linear regression for high-dimensional data). While probabilistic modeling of regression coefficients is important, we focus on analyzing linear regression methods that do not model regression coefficients probabilistically because chemical engineers commonly use non-probabilistic models. We use βModel to denote that it is a realization of the random variable by the “Model” and specific training data. From here, we drop the “hat” notation because it is clear from the model name in the superscript that the coefficients were obtained by regression from data. OLS regression estimates with the closed-form solution βOLS = (X⊤X)−1X⊤y for the case p < n have low bias and are optimal under the assumption of the Gauss-Markov theorem. However, the regression coefficients β have a very large variance if the condition number of X⊤X is large, as is the case for many real-world data analytics problems, resulting in low prediction accuracy on unseen data. RR addresses this problem by adding the squared L2-norm of the weights as a penalty to the least-squares objective [17]: min β ∥y − Xβ∥2 2 + λ∥β∥2 2, (3) yielding the closed-form solution βRR = (X⊤X + λI)−1X⊤y. The regularization penalty adds to the main diagonal of X⊤X and ensures that the resulting matrix is also invertible in the case p > n. RR improves the model’s generalization by introducing a bias that reduces variance in the estimated parameters [18]. For p < n and λ → 0, RR converges to OLS. In the more general case, without making assumptions about the dimensionality and rank of the real matrix X, Singular Value Decomposition (SVD), X = UΣV⊤, can be used to show that β0 = lim λ→0 βλ = X†y. (4) The full derivation and further information can be found in the Supplementary Information (SI), Sec. S1 and [4]. For the case p > n, the Moore-Penrose-Inverse X† can be written as β0 = X⊤(XX⊤) −1y. (5) This expression is known as the minimum norm solution (e.g., [11]), β0 = arg min β � ∥β∥2 2 �� ∥y − Xβ∥2 2 = 0 � . (6) For any ˜β that fulfills X ˜β = y (i.e., regression coefficients that fit the data X and y perfectly including the noise), (5) can be used to show that X( ˜β − β0) = 0, and that ( ˜β − β0)⊤β0 = ( ˜β − β0)⊤X⊤(XX⊤)−1y = (X( ˜β − β0))⊤(XX⊤)−1y = 0 (7) and consequently ( ˜β − β0) ⊥ β0 which is equivalent to N(X) ⊥ β0 [19]. Thus there exists a set of regression coefficients S that all fulfill X ˜β = y with ˜β ∈ S. Most regularized regression methods solve an optimization of the form min β ∥y − Xβ∥2 2 + F(β). (8) Regularized methods trade the perfect fit to the training data against the objective of keeping regression coefficients small. This trade-off is seen in the objective used to define the regularization methods. The orthogonality of the regression coefficients to N(X) does not hold for arbitrary regularization terms F(β). Orthogonality holds for RR, 3Nullspace Perspective on Regression Coefficients because the regularization term in (3) is always smaller for coefficients orthogonal to N(X). The PCR coefficients are orthogonal to the N(X) because all eigenvectors of X⊤X that correspond to nonzero eigenvalues are orthogonal to each other and to the nullspace. Similarly, the PLS coefficients are also orthogonal to the nullspace by construction. The pathological case of y being the nullvector must be excluded and is not relevant. Proofs for orthogonality between regression coefficients and nullspace for RR, PCR, and PLS are included in the SI, Sec. S2. However, regression coefficients obtained by the lasso and EN are not orthogonal to N(X) because of the L1-norm. For more information on regularized high-dimensional regression, see [6], [20]. Depending on the function F : Rp → R, regression coefficients obtain different shapes. Usually, methods such as RR, PCR, and PLS yield solutions that are not sparse, which can make interpretation difficult. An alternative method is the lasso, however, sparsity is often not a reasonable assumption for functional data. A generalization of the lasso is min β 1 2∥y − Xβ∥2 2 + λ∥Dβ∥1 (9) where choosing D as the identity matrix recovers the lasso. For D1 =   1 −1 0 · · · 0 0 1 −1 ... ... ... ... ... ... 0 0 · · · 0 1 −1   , (10) the resulting model is called the one-dimensional fused lasso which penalizes the L1-norm of the regression coefficients as well as their differences, but thus requires predictors that can be ordered [12], a characteristic of functional data [13]. The choice of D can incorporate expectations about the underlying model structure [21], and can thus yield models that should be interesting for many chemical engineering problems for its flexibility to incorporate assumptions, and its potential to yield easier-to-interpret regression coefficients. However, the authors are not aware that the fused lasso is currently being applied in the chemical engineering community, despite the popularity of the fused lasso in the statistics community. In the next section, we derive the nullspace method to compare the regression coefficients of different regularized models thoroughly. 3 Nullspace Method The addition of a vector v ∈ N(X), i.e., a vector in the nullspace, to any β yields coefficients with unchanged predictions. The vectors in the nullspace affect only the regularization term in the objective function. We are interested in a method for understanding the effects of the nullspace when comparing different coefficients and how such a comparison can be used to reason about underlying relationships. Consider a regularized regression model called A, ˆyA = XβA, (11) where βA is associated with method A. For any vector v in the nullspace, this equation implies that ˆyA = X(βA + v). (12) We want to compare the regression coefficients βA with other coefficients βB. The coefficients βB can either be another estimator obtained by another regression method or instead be chosen for engineering or scientific reasons (e.g., constant regression coefficients). Thus we propose finding coefficients v∗ ∈ N(X) that are closest to the difference between the coefficients under comparison β∆ = βA − βB. This approach can be formalized by min v ∥β∆ + v∥2 2 (13a) subject to Xv = 0, (13b) This optimization is a convex quadratic program with linear constraints. The solution is the projection of β∆ onto the nullspace, v∗ = (X⊤(XX⊤)−1X − I)β∆, (14) where XX⊤ is assumed to be invertible. The derivation is included in the SI, Sec. S3. The expression can be simplified by inserting the singular value decomposition X = UΣV⊤, v∗ = (VΣ⊤(ΣΣ⊤)−1ΣV⊤ − I)β∆, (15) which can be used to improve the numerical efficiency. Simplifying (15) leads to v∗ = � V � In 0 0 0 � V⊤ − I � β∆. (16) 4Nullspace Perspective on Regression Coefficients The property that V is an orthogonal matrix leads to v∗ = −V � 0 0 0 Ip−n � V⊤β∆. (17) The projection onto the nullspace can be a hard requirement that might yield a vector v∗ that is dominated by noise and difficult to interpret, in particular, if XX⊤ is ill-conditioned as is often the case for many real-world chemical engineering problems. Furthermore, regularization shapes regression coefficients by trading their variance against a bias towards zero to improve generalization. However, regularized regression coefficients usually differ from the true coefficients (if they exist), and their difference is not expected to lie exactly within the nullspace but might be close to it, motivating the relaxed optimization. We propose to reformulate the optimization in (13) to allow deviations from the nullspace, min v ∥β∆ + v∥2 2 + γ∥Xv∥2 2, (18) where γ is a nonnegative scalar. Setting the derivative of (18) with respect to v to zero gives vγ = −(γX⊤X + I)−1β∆. (19) For γ = 0, the nullspace is not considered and β∆ = v0. For γ → ∞, the optimization converges to (17), as seen by lim γ→∞ −(γX⊤X + I)−1β∆ = lim γ→∞ −V(γΣ⊤Σ + I)−1V⊤β∆ = −V � 0 0 0 Ip−n � V⊤β∆. (20) Analyzing the nullspace, i.e., comparing the coefficients βA and βA + vγ with βB, allows to identify which differences can be removed with a vector that is close to the nullspace and which differences would require significant deviations from the nullspace and are thus mainly responsible for the differences of the associated predictions. We propose to select γ, i.e., the penalization strength for deviations from the nullspace, based on a change in prediction accuracy to make it easier to interpret the result. That is, we define γ based on the Normalized-Root-Mean-Square Error (NRMSE) defined by s = max i {yi} − min i {yi} (21) L(ˆy, y) = 1 s√n∥ˆy − y∥2, (22) leading to the heuristic: max γ γ (23) subject to ��L(X(βA + vγ), y) − L(X(βA), y) ��≤ c vγ = −(γX⊤X + I)−1β∆ where c defines the maximum loss function change introduced by the nullspace approach that is considered acceptable. The optimization (23) is not convex for most practical examples but is easy to solve because it only has one degree of freedom, γ.2 4 Case Studies This section demonstrates the nullspace method on several example cases to derive insights for interpretation of regression coefficients. The data X and y are generated synthetically for the first example. The second and third examples are on data from lithium-ion batteries [15], where we use constructed response variables by assuming different linear relationships to showcase the differences between regression coefficients and true coefficients. The last example uses the measured cycle life response where the true relationship between X and y is unknown. 2For example, the optimization can be solved by plotting the left-hand side of the inequality with respect to γ. 5Nullspace Perspective on Regression Coefficients 4.1 Synthetic Parabolic Data The parabolic example is inspired by measurements of some quantity over a continuous domain (e.g., time, concentration, voltage) to keep the data and relationships simple. The data are drawn from xi = aid ⊙ d, i ∈ {1, 2, 3, · · · , 50}, (24) d = [1.0, 1.01, 1.02, · · · , 3.0], (25) X∗ = [x1, x2, · · · xn]⊤ , where d is the vector of discretizations on the underlying domain, with a constant spacing of 0.01 and a length of p = 201, and ⊙ is the element-wise product. The parameters ai ∼ N(µ, σ2) with µ = 0.3 and σ = 0.3. Consequently, X ∈ R50×201. We define the response as y∗ = X∗β∗, with β∗ = 1 pI. (26) The true coefficients are thus equal. Subsequently, we add white Gaussian noise to the data and response xi = x∗ i + ϵxi (27) y = y∗ + ϵy (28) yielding the matrices X and y for use in regression. The added noise ϵxi ∈ R201 is chosen such that the average Signal- to-Noise Ratio (SNR) of each sample (xi) is 50 and the SNR of y is 50 as well. Figure 1a shows the mean-centered 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 3.00 d 4 2 0 2 4 6 X a) Data Training 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 3.00 d 0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008 Coefficients b) Nullspace Perspective PLS 1 , NRMSE: 0.105% PLS 1 +v , 10.0, NRMSE : 0.104% * , NRMSE: 0.105% Figure 1: a) Mean centered parabolic data, with white Gaussian noise corresponding to an SNR of 50 added to X and y prior to mean centering, b) true coefficients in black and regression coefficients in green and nullspace-modified PLS coefficients in magenta. data, where each line corresponds to a matrix row. The 201 individual data points of each row are connected with a line, which is a reasonable visualization because of the underlying functional structure. We picked a PLS model with one component to learn the relationship between X and y. The PLS method is popular among chemical engineers, and its regularization parameter, the number of components, is discrete and simple to choose. Figure 1b shows that the true coefficients and the PLS coefficients have very different shapes. However, their predictions and prediction accuracies are almost identical (cf. SI, Sec. S4). The noise leads to a prediction error even when the true coefficients are used (i.e., 0.105% NRMSE). Using the proposed nullspace method with hand-selected γ = 10 to compare the true coefficients with the PLS coefficients shows that the resulting vector v10 is very close to the nullspace, i.e., does not significantly change the prediction accuracy and yields the adjusted coefficients βPLS 1 + v10 that are very similar to the true coefficients β∗. While βPLS 1 is orthogonal to the nullspace, βPLS 1 + v10 is not orthogonal to the nullspace. Due to the simple underlying structure of the data and the model, the PLS coefficients yield a similar prediction accuracy. However, the PLS coefficients have a smaller L2-norm, i.e., ∥βPLS 1 ∥2 2 < ∥β∗∥2 2, due to the implicit regularization of PLS. Assume that the coefficients are expected to be piecewise constant for physical reasons. We can then reformulate the regression as a generalized lasso problem with the matrix D in (9) set to D1. Figure 2 shows the regression coefficients 6Nullspace Perspective on Regression Coefficients 1.75 2.00 2.25 2.50 2.75 3.00 d Data 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 3.00 d 0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008 Coefficients Nullspace Perspective RR CV1SE, NRMSE: 0.117% RR CV1SE+v , 0.1, NRMSE : 0.116% FL CV1SE, NRMSE: 0.104% Figure 2: Ridge coefficients in green and fused lasso coefficients in black based on CV and the one-standard-error rule. Nullspace- modified ridge coefficients in magenta. associated with ridge regression and the fused lasso. The regularization parameter was chosen by Cross-Validation (CV) and the one-standard-error rule [3]. Figure 2 looks remarkably similar to Fig. 1. The fused lasso coefficients are nearly identical to the true coefficients, and the ridge coefficients are similar to the PLS coefficients with one component but slightly noisier. From the data alone, it is not possible to state whether y was constructed from constant or parabolic coefficients. Furthermore, regression coefficients obtained from methods that are orthogonal to the nullspace can yield coefficients that appear to disagree with prior knowledge at first sight. As this example shows, methods that are not orthogonal to the nullspace such as the fused lasso can be advantageous for interpretation and conclusions if selected based on prior knowledge. 4.2 Lithium-Ion Battery Data As a real-world measurement data example, we consider a Lithium Iron Phosphate (LFP) battery dataset, which contains cycling data for 124 batteries [15]. Each battery has a fixed charging and discharging protocol. The charging protocols vary widely between the cells, whereas the discharge is constant and identical for all cells. The objective of the original paper was the prediction of the cycle life, i.e., the number of cycles until the battery’s capacity drops below 80% of its nominal capacity. Features based on the difference between the discharge capacity of voltage curves for two cycles, subsequently called ∆Qa−b, were shown to linearly correlate well with the logarithm of the cycle life. For this case study, we use the cycle pair a = 100 and b = 10, as done in [15]. Furthermore, we denote by a tilde (∆ �Q100−10) that the columns are mean centered. The dimensionality ∆Q100−10 ∈ R41×1000 due to the high resolution of the discharge capacity over the voltage domain. More information about the data set and reasoning about the modeling objective can be found in [15]. Figure 3a shows the LFP data set, partitioned into training, primary, and secondary test data as suggested in [15]. Figure 3b shows the mean subtracted training data. The data of the shortest-lived battery is clearly separated from the remainder of the data set. However, we keep this battery in the data set, as its influence on the training is benign. Figure 3c shows the z-scored training data (i.e., standardized data, yielding unit variance columns). The unit (Ah) is lost by z-scoring the data. Usually, z-scoring is recommended for data with features that have different units and thus might vary by orders of magnitude. However, for functional high-dimensional data, the unit of all columns is the same. Nevertheless, the measured values can vary by order of magnitude. Figure 3c shows that the noise in the voltage region 3.2–3.5 V is amplified by rescaling because of a lower signal-to-noise ratio in this voltage region. A more detailed analysis of the SNR can be found in the SI, Sec. S5. However, whether z-scoring is useful does not only depend on the data matrix X but also on its underlying relationship with y, which we explore next on synthetic responses y before moving to the cycle life response. 7Nullspace Perspective on Regression Coefficients 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 Voltage (V) 0.14 0.12 0.10 0.08 0.06 0.04 0.02 0.00 Q100 10 (Ah) a) Data Training Test Test 2 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 Voltage (V) 0.10 0.08 0.06 0.04 0.02 0.00 0.02 0.04 Q100 10 (Ah) b) Mean Centered Training Data Training 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 Voltage (V) 5 4 3 2 1 0 1 2 QSTD 100 10 c) Z-Scored Training Data Training Figure 3: a) LFP Discharge capacity difference between cycle 100 and 10, data split into training, primary and secondary test set, b) mean centered training data, c) z-scored training data. 4.2.1 Synthetic Response Constant Coefficients. The response for this example is the sample mean defined in (26), with p = 1000 to match the dimensionality of the LFP data set with added white Gaussian noise corresponding to an SNR of 50. Figure 4a shows 3.0 3.2 3.4 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 Voltage (V) 0.0000 0.0002 0.0004 0.0006 0.0008 0.0010 0.0012 0.0014 Coefficients a) Nullspace Perspective PLS 2 , NRMSE: 0.159% PLS 2 +v , 1.45e+03, NRMSE : 0.149% * , NRMSE: 0.127% 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 d 5 4 3 2 1 0 1 2 Q100 10 Data Train 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 Voltage (V) 0.0 0.5 1.0 1.5 2.0 Coefficients 1e 5 b) Nullspace Perspective, Z-Scored Data PLS 4 , NRMSE: 0.108% PLS 4 +v , 2.7e-04, NRMSE : 0.118% * , NRMSE: 0.127% Figure 4: a) True coefficients in black, PLS coefficients based on CV and the one-standard-error deviation rule in green, nullspace- modified coefficients in magenta, b) nullspace perspective similar to a, with PLS coefficients estimated with the one-standard-error rule corresponding to z-scored data. the nullspace perspective for the constant coefficient response with the data on the original scale, and Fig. 4b based on z-scored data (i.e., columns of X are scaled to have a unit standard deviation). The number of PLS components is determined by CV and the one-standard-error rule. The PLS model associated with the z-scored data needs more components. The nullspace penalization parameter γ was chosen in both cases to yield c = 0.01% NRMSE prediction error change. Figure 4a shows the differences between the true coefficients and the PLS model’s regression coefficients in the section from 2.0–3.1 V are relatively close to one another; however, some differences remain. The differences in the voltage region from 3.2 to 3.5 V only have a minor effect on the prediction results. Most of the difference between the regression coefficients in this area is associated with the nullspace, indicated by the large difference between the nullspace-modified PLS coefficients in magenta and the original PLS coefficients in green. Thus, the differences in the region 3.2 to 3.5 V do not change the prediction results on the training data significantly and arise due to the interplay of the regularization objective with the nullspace. When the data are z-scored, most of the visible differences between the PLS coefficients in green and the true coefficients in black are contained in the enlarged nullspace (Fig. 4b). The modified coefficients match the true coefficients very well. The prediction error difference between the PLS coefficients in green and the nullspace-modified coefficients in magenta is 0.01% NRMSE. The remaining differences between the true coefficients in black and the modified coefficient in magenta are barely visible but are responsible for another 0.01% NRMSE prediction error change, highlighting the effect of the nullspace. Comparing Figs. 4a and 4b shows that, in case the true coefficients are constant (i.e., all columns are equally important), z-scoring can help regression to yield coefficients that are more similar to the true coefficients. 8Nullspace Perspective on Regression Coefficients Column Mean Coefficients. The true coefficient vector β∗ for the next synthetic example is the column mean of the data X prior to column centering β∗ j = 1 n n � i=1 xi,j, (29) β∗ = �β∗ 1, β∗ 2, · · · , β∗ p �⊤. The PLS model with 6 components associated with the z-scored data picks up a high amount of noise in the voltage regions from 3.3 to 3.5 V (Fig. 5b). In contrast, the PLS model with 3 components associated with the data on the original scale converges well to the true coefficients over the entire voltage region (Fig. 5a). The small differences are very closely associated with the nullspace. Here, z-scoring amplifies and feeds noise into the model, manifesting as the spiky regression coefficients, with the most extreme spikes in the voltage regions from 3.2 to 3.5 V (Fig. 5b). Still, the PLS model associated with the z-scored data has approximately the same prediction accuracy as the PLS model associated with the original data. 3.0 3.2 3.4 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 Voltage (V) 0.04 0.03 0.02 0.01 0.00 Coefficients a) Nullspace Perspective PLS 3 , NRMSE: 0.101% PLS 3 +v , 8.4, NRMSE : 0.106% * , NRMSE: 0.112% 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 d 5 4 3 2 1 0 1 2 Q100 10 Data Train 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 Voltage (V) 0.0008 0.0006 0.0004 0.0002 0.0000 Coefficients b) Nullspace Perspective, Z-Scored Data PLS 6 , NRMSE: 0.101% PLS 6 +v , 4.5e-04, NRMSE : 0.106% * , NRMSE: 0.112% Figure 5: a) True coefficients in black, PLS coefficients based on CV and the one-standard-error rule in green, nullspace-modified coefficients in magenta, b) nullspace perspective similar to a, with PLS coefficients estimated with the one-standard-error rule corresponding to z-scored data. Suppose there was some prior evidence or physical intuition that the true coefficients are constant or at least of similar magnitude. Then, z-scoring feeds the assumption that all the columns’ importance is in the same order of magnitude to the model. However, if the coefficients are expected to vary by an order of magnitude (e.g., as is the case for the true coefficients being the column mean of the data), then not z-scoring the data accounts for the assumption that the scale of the columns is correlated with the assumed underlying true coefficients. The two examples show that the potential effects of z-scoring on the regression coefficients should be considered carefully for functional data. When data are z-scored, the model can become better at learning the underlying relationship, but noise may be amplified, depending on the noise structure. 4.2.2 Measured Cycle Life Response The measured response associated with the LFP battery data is the cycle life. We train the models by using the logarithm of the cycle life and use the same training, primary test, and secondary test set as suggested in [15]. We determine the regularization parameter based on the minimum CV error and do not employ the one-standard-error rule.3 The PLS coefficients with five components have a similar shape as the fused lasso coefficients (cf. Figs. 6ab). However, the PLS coefficients have high-frequency perturbations, in particular, in the voltage range from 2.9–3.2 V, which is likely due to noise, making the PLS coefficients harder to interpret. The fused lasso coefficients (Fig. 6b) clearly indicate three regions of importance, enabling a physical interpretation. The range around 2.0–2.1 V is associated with the capacity change of the cell between cycles 10 and 100. Around 2.4 V, a different pattern can be seen in the data (Fig. 3), corresponding to the negative peak in the regression coefficients, which may correspond to LFP cathode degradation associated with iron anti-site defects, as the free energy of reaction (overpotential times charge) exceeds 3The standard deviation of the CV error is large due to the long-living cells that heavily influence the prediction performance, which would lead to overly conservative regularization estimates. 9Nullspace Perspective on Regression Coefficients 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 Voltage (V) 3 2 1 0 1 Coefficients a) PLS 5 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 Voltage (V) b) FL D1 Figure 6: Cross-validated regression coefficients, original data (cf. Fig. 3b): a) PLS coefficients, b) fused lasso coefficients. their formation energy ∼0.55 eV [22]. This interpretation is also consistent with experiments showing that chemical reduction of LFP by citric acid is able to heal iron anti-site defects with a similar free energy of reaction of 0.58 eV [23]. The voltage range around 2.9–3.3 V contains most of the regression coefficient peaks. The two dominant plateaus of the Open-Circuit Voltage (OCV), which result from the single broad plateau of LFP superimposed with two more narrow plateaus of graphite, are located here, and most of the cell’s capacity is discharged in this voltage range. These voltage plateaus correspond to phase transformations of the porous electrodes [24], specifically between the low and high-density stable phases of LFP, as well as between stages 1, 2, and 3 of lithiated graphite [25]. The fused-lasso coefficients showcase three distinct negative and positive peaks, corresponding to changes in the rate-dependent tilt of the voltage plateaus, which may result from changes in particle-size-dependent nucleation barriers and population dynamics of reaction-controlled phase transformations [25], [26]. The peak width and height can be interpreted as a weighted sum of the average slopes of the data between the respective peaks. On low-rate data, the position and magnitude of peaks in the incremental capacity analysis correspond to different degradation modes [27]. The peaks and peak shifts of the incremental capacity analysis blur out at higher C-rates, as expected from the suppression of phase separation by driven auto-inhibitory reactions [28]. In particular, the decreasing reaction rate with increasing lithium concentration in the LFP cathode, which has been predicted theoretically [29] and confirmed experimentally [30], erases the voltage plateaus at high rates and causes more homogeneous reactions that are likely favorable for battery lifetime [24]–[26]. However, the obtained regression coefficients indicate that there is degradation information in this region even in ∆Q100−10 (i.e., the discharge capacity difference of cycle 100 and 10, both at 4C) that is important for capturing past degradation and forecasting future degradation. On the other hand, if the 4C current is well into the regime of suppressed phase separation, then we would expect a negative correlation between lifetime and internal resistance of the intercalation reaction, which in turn is correlated with larger ∆Q100−10. 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 Voltage (V) 0.02 0.00 0.02 Coefficients a) PLS 9 z-scored 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 Voltage (V) b) FL D1 z-scored Figure 7: Cross-validated regression coefficients, z-scored data (cf. Fig. 3c): a) PLS coefficients, b) fused lasso coefficients. The coefficients regressed on the z-scored data have similar peaks and characteristics as the coefficients regressed on the original data (cf. Figs. 6ab, and 7ab). The z-scoring of columns introduces a linear transformation that significantly changes the regression coefficients in the range from 3.2 and 3.4 V. The fused lasso based on the z-scored columns yields high prediction accuracy and interpretable coefficients (cf. Tab. 1 and Fig. 7b). In the higher voltage region, an additional peak appears around 3.35 V, which could not be learned from the original data because of the very small variance of the data prior to rescaling in combination with regularization. Moreover, the coefficients estimated on the z-scored data have the highest prediction performance on the training, primary, and secondary test sets (Tab. 1), showcasing that there is valuable information in the higher voltage 10Nullspace Perspective on Regression Coefficients Table 1: Root-Mean-Square Error (RMSE) prediction accuracies associated with the coefficients in Figs. 6–7. Low Cycle Life (CL): yi ≤ 1200 cycles; high CL: yi > 1400 cycles. All models were trained on the entire training data. Original Scale Z-Scored Feature Set FL D1 PLS 5 Comp. FL D1 PLS 9 Comp. [31] Variance Model [15] Training (41) 68 83 62 57 104 Test 1 (42) 115 116 105 102 138 Test 2 (40) 198 217 192 174 196 Training Low CL (39) 62 82 53 50 103 Test 1 Low CL (39) 96 101 76 80 96 Test 2 Low CL (34) 135 202 115 132 119 Training High CL (2) 138 106 150 139 115 Test 1 High CL (3) 258 231 280 252 385 Test 2 High CL (6) 395 285 412 322 419 region above 3.2 V. Furthermore, both models on the z-scored data outperform the variance model suggested in [15]. The PLS model with nine components, suggested first in [31], slightly outperforms the fused lasso model when all cells are considered. However, the fused lasso yields the lowest RMSE error for both test sets when only evaluated on the shorter-lived cells. The higher performance of the PLS model with nine components on the secondary test set is thus mainly associated with the longest-living cells that are more difficult to predict (cf. [15], [31]). But, the coefficients associated with the PLS model are challenging to interpret because their sign changes frequently. What is more, the secondary test set was impacted by a longer calendar aging due to an extended storing period before the cycling started (cf. SI of [32]), making it tough to understand the higher prediction accuracy of the PLS model on the secondary test set. In particular, the PLS coefficients show further peaks in the voltage region above 3.4 V influenced at least partially by noise because the SNR in this region is very low (cf. SI Sec. S5). Similarly to the parabolic data set, we observe that the fused lasso coefficients are more interpretable than the PLS coefficients. Not requiring the coefficients orthogonal to the nullspace improves interpretability (Fig. 8ab). The 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 Voltage (V) 2 0 2 Coefficients a) FL D1 FL D1 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 Voltage (V) 0.01 0.00 0.01 0.02 0.03 b) FL D1 z-scored FL D1 z-scored Figure 8: Comparison of fused lasso coefficients (blue) and their component orthogonal to the nullspace (red): a) original data, b) z-scored data. component of the regression coefficients orthogonal to the nullspace (cf. red coefficients in Fig. 8ab) are less interpretable while making identical predictions on the training data. 5 Conclusion The article proposes a nullspace perspective for gaining insights to help make informed design choices for building regression models on high-dimensional data and for reasoning about potential underlying linear models. We demonstrate the nullspace method on a fully synthetic dataset and lithium-ion battery data with a designed linear response. Applying the nullspace insights for predicting the cycle life led to further insights into how degradation manifests itself for LFP batteries during discharge at 4C. The nullspace allows different-looking regression coefficients to yield similar predictions (Fig. 1). While z-scoring for high-dimensional functional data can be beneficial, it should be an active design choice because it can increase noise 11Nullspace Perspective on Regression Coefficients by scaling up columns with low SNR (Fig. 5). Appropriate regularization can mitigate increased noise after z-scoring. Furthermore, regularization and z-scoring must be carefully considered and correspond to prior physical knowledge to obtain interpretable regression results (Figs. 6, 7). Otherwise, the combination of the nullspace and regularization can hinder interpretability and potentially make it impossible to obtain regression coefficients close to the true coefficients. Regression methods which yield coefficients orthogonal to the nullspace, such as RR, PCR, and PLS, can be challenging to interpret. Methods that yield regression coefficients not orthogonal to the nullspace, such as the fused lasso, can be advantageous for interpretability (Fig. 8). The learnings from the nullspace analysis help to build and interpret linear regression models for high-dimensional functional data. The case studies show how to reason about underlying linear relationships between X and y, which is important for system optimization and to improve scientific understanding. Code and Data Availability The code for this article is available in the corresponding GitHub repository, HDRegAnalytics, https://github. com/JoachimSchaeffer/HDRegAnalytics. The repository contains the source code and notebooks to visualize the results. The repository contains a small subset of the LFP dataset that was published with [15] and is available at https://data.matr.io/1/. Author Contributions Joachim Schaeffer: Conceptualization, Methodology, Software, Validation, Formal Analysis, Investigation, Data Curation, Writing – original draft, Writing – review & editing, Visualization, Funding Acquisition; Eric Lenz: Formal Analysis, Writing – review & editing; William C. Chueh: Writing – review & editing; Martin Z. Bazant: Writing – review & editing; Rolf Findeisen: Resources, Writing – review & editing, Funding Acquisition; Richard D. Braatz: Conceptualization, Methodology, Resources, Writing - original draft, Writing – review & editing, Supervision, Project Administration, Funding Acquisition. Acknowledgements and Funding Initial ideas for this work were conceptualized during Joachim Schaeffer’s time at ETH Zürich, for which we acknowl- edge financial support from the German Academic Exchange Service (DAAD) within the scholarship program for Master studies abroad. The main work was carried out by Joachim Schaeffer at the Technical University of Darmstadt. The work was refined and extended during Joachim Schaeffer’s time at the Massachusetts Institute of Technology, for which we acknowledge financial support by a fellowship within the IFI program of the German Academic Exchange Service (DAAD), funded by the Federal Ministry of Education and Research (BMBF). Furthermore, financial support is acknowledged from the Toyota Research Institute through the D3BATT Center on Data-Driven-Design of Rechargeable Batteries. 12Nullspace Perspective on Regression Coefficients References [1] P. Bühlmann and S. Van De Geer, Statistics for High-Dimensional Data: Methods, Theory and Applications. Heidelberg: Springer Berlin, 2011. [2] I. M. Johnstone and D. M. Titterington, “Statistical challenges of high-dimensional data,” Philosophical Transac- tions of the Royal Society A: Mathematical, Physical and Engineering Sciences, vol. 367, no. 1906, pp. 4237– 4253, 2009. DOI: 10.1098/rsta.2009.0159. [3] T. Hastie, R. Tibshirani, and J. H. Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second. New York: Springer, 2009. [4] D. Kobak, J. Lomond, and B. Sanchez, “The optimal ridge penalty for real-world high-dimensional data can be zero or negative due to the implicit ridge regularization,” Journal of Machine Learning Research, vol. 21, no. 169, pp. 1–16, 2020. [5] N. M. Ralbovsky and I. K. Lednev, “Towards development of a novel universal medical diagnostic method: Raman spectroscopy and machine learning,” Chemical Society Reviews, vol. 49, no. 20, pp. 7428–7453, 2020. [6] J. Schaeffer and R. D. Braatz, “Latent variable method demonstrator – Software for understanding multivariate data analytics algorithms,” Computers & Chemical Engineering, vol. 167, p. 108 014, 2022. DOI: 10.1016/j. compchemeng.2022.108014. [7] A.-L. Boulesteix and K. Strimmer, “Partial least squares: A versatile tool for the analysis of high-dimensional genomic data,” Briefings in Bioinformatics, vol. 8, no. 1, pp. 32–44, 2007. DOI: 10.1093/bib/bbl016. [8] J. Groß, Linear Regression, 175 vols. Berlin Heidelberg: Springer Verlag, 2003. [9] D. C. Montgomery, E. A. Peck, and G. G. Vining, Introduction to Linear Regression Analysis, Fifth. Hoboken, N.J.: John Wiley & Sons, 2012. [10] G. A. F. Seber and A. J. Lee, Linear Regression Analysis. Hoboken, NJ: John Wiley & Sons, 2003. [11] A. Monticelli, “Least-squares and minimum norm problems,” in State Estimation in Electric Power Systems: A Generalized Approach. Boston: Springer US, 1999, pp. 15–37. [12] R. Tibshirani, M. Saunders, S. Rosset, J. Zhu, and K. Knight, “Sparsity and smoothness via the fused lasso,” Journal of the Royal Statistical Society: Series B (Statistical Methodology), vol. 67, no. 1, pp. 91–108, 2005. DOI: 10.1111/j.1467-9868.2005.00490.x. [13] J. O. Ramsay and B. W. Silverman, Functional Data Analysis, Second. New York: Springer, 2005, p. 38. [14] H. Dette and J. Tang, Statistical inference for function-on-function linear regression, arXiv preprint, https://arxiv.org/abs/2109.13603, 2021. DOI: 10.48550/ARXIV.2109.13603. [15] K. A. Severson, P. M. Attia, N. Jin, N. Perkins, B. Jiang, et al., “Data-driven prediction of battery cycle life before capacity degradation,” Nature Energy, vol. 4, no. 5, pp. 383–391, 2019. [16] E. Makalic and D. F. Schmidt, High-dimensional bayesian regularised regression with the bayesreg package, arXiv preprint, https://arxiv.org/abs/1611.06649, 2016. [17] A. E. Hoerl and R. W. Kennard, “Ridge regression: Biased estimation for nonorthogonal problems,” Technomet- rics, vol. 12, no. 1, pp. 55–67, 1970. [18] H. Zou and T. Hastie, “Regularization and variable selection via the elastic net,” Journal of the Royal Statistical Society. Series B: Statistical Methodology, vol. 67, no. 2, pp. 301–320, 2005. DOI: 10 . 1111 / j . 1467 - 9868.2005.00503.x. [19] S. Boyd and S. Lal, EE263: Introduction to Linear Dynamical Systems, Lecture 16: Least-norm Solutions of Underdetermined Equations, Department of Electrical Engineering, Stanford University, Stanford, California, 2022. [Online]. Available: http://ee263.stanford.edu/lectures.html. [20] R. Tibshirani, “Regression shrinkage and selection via the lasso,” Journal of the Royal Statistical Society: Series B (Methodological), vol. 58, no. 1, pp. 267–288, 1996. DOI: 10.1111/j.2517-6161.1996.tb02080.x. [21] R. J. Tibshirani and J. Taylor, “The solution path of the generalized lasso,” The Annals of Statistics, vol. 39, no. 3, pp. 1335–1371, 2011. DOI: 10.1214/11-AOS878. [22] R. Malik, D. Burch, M. Bazant, and G. Ceder, “Particle size dependence of the ionic diffusivity,” Nano Letters, vol. 10, no. 10, pp. 4123–4127, 2010, PMID: 20795627. DOI: 10.1021/nl1023595. [23] P. Xu, Q. Dai, H. Gao, H. Liu, M. Zhang, et al., “Efficient direct recycling of lithium-ion battery cathodes by targeted healing,” Joule, vol. 4, no. 12, pp. 2609–2626, 2020. DOI: 10.1016/j.joule.2020.10.008. [24] T. R. Ferguson and M. Z. Bazant, “Nonequilibrium thermodynamics of porous electrodes,” Journal of The Electrochemical Society, vol. 159, no. 12, A1967–A1985, 2012. DOI: 10.1149/2.048212jes. [25] T. R. Ferguson and M. Z. Bazant, “Phase transformation dynamics in porous battery electrodes,” Electrochimica Acta, vol. 146, pp. 89–97, 2014. DOI: 10.1016/j.electacta.2014.08.083. 13Nullspace Perspective on Regression Coefficients [26] Y. Li, F. El Gabaly, T. R. Ferguson, R. B. Smith, N. C. Bartelt, et al., “Current-induced transition from particle- by-particle to concurrent intercalation in phase-separating battery electrodes,” Nature Materials, vol. 13, no. 12, pp. 1149–1156, 2014. DOI: 10.1038/nmat4084. [27] M. Dubarry, C. Truchot, and B. Y. Liaw, “Synthesize battery degradation modes via a diagnostic and prognostic model,” Journal of Power Sources, vol. 219, pp. 204–216, 2012. DOI: 10.1016/j.jpowsour.2012.07.016. [28] M. Z. Bazant, “Thermodynamic stability of driven open systems and control of phase separation by electro- autocatalysis,” Faraday Discuss., vol. 199, pp. 423–463, 2017. DOI: 10.1039/C7FD00037E. [29] D. Fraggedakis, M. McEldrew, R. B. Smith, Y. Krishnan, Y. Zhang, et al., “Theory of coupled ion-electron transfer kinetics,” Electrochimica Acta, vol. 367, p. 137 432, 2021. DOI: 10.1016/j.electacta.2020.137432. [30] H. Zhao, H. D. Deng, A. E. Cohen, J. Lim, Y. Li, et al., Learning heterogeneous reaction kinetics from x-ray movies pixel-by-pixel, Preprint (Version 1) available at Research Square, 2022. DOI: 10.21203/rs.3.rs- 2320040/v1. [31] P. M. Attia, K. A. Severson, and J. D. Witmer, “Statistical learning for accurate and interpretable battery lifetime prediction,” Journal of The Electrochemical Society, vol. 168, no. 9, p. 090 547, 2021. DOI: 10.1149/1945- 7111/ac2704. [32] P. M. Attia, A. Grover, N. Jin, K. A. Severson, T. M. Markov, et al., “Closed-loop optimization of fast- charging protocols for batteries with machine learning,” Nature, vol. 578, no. 7795, pp. 397–402, 2020. DOI: 10.1038/s41586-020-1994-5. 14Nullspace Perspective on Regression Coefficients Supplementary Information for “Interpretation of High-Dimensional Linear Regression: Effects of Nullspace and Regularization Demonstrated on Battery Data” S1 Ordinary Least Squares and Minimum Norm Solution Ordinary Least Squares Regression finds a solution βOLS that minimizes the L2-norm of the regression error ˆϵ [1], min β ∥y − Xβ∥2 2. (S.1) If p < n and rank(X) = p, it follows that X⊤X is invertible, and (S.1) can be solved analytically to give βOLS = (X⊤X)−1X⊤y. (S.2) OLS estimates have low bias and are optimal under the assumption of the Gauss-Markov theorem. However, they have a very large variance if the condition number for inversion of X⊤X is large, as it the case for many real-world data analytics problems, resulting in low prediction accuracy on unseen data. For p < n and λ → 0, RR converges to OLS. In the more general case, without making assumptions about the dimensionality and rank of the real matrix X, the SVD, X = UΣV⊤, can be used to show that (e.g., [2]) β0 = lim λ→0 βλ = lim λ→0(X⊤X + λI)−1X⊤y = lim λ→0(VΣ⊤ΣV⊤ + λVV⊤)−1VΣ⊤U⊤y = VΣ†U⊤y = X†y. (S.3) S2 Orthogonality of Coefficient Vector and Predictor Nullspace for RR, PCR, and PLS This section shows the orthogonality of regression coefficients and the nullspace for RR, PCR, and PLS. Please note that the notation differs slightly from the main section to improve readability. To show the orthogonality for PCR and PLS, the SVD of X is written in the partitioned form X = [U1 U0] � Σ1 0 0 0 � �V⊤ 1 V⊤ 0 � = U1Σ1V⊤ 1 (S.4) where Σ1 contains only the non-zero singular values. Thus the columns of V0 give an orthonormal basis of N(X). As V is an orthogonal matrix, V⊤ 0 vi = 0 holds for every column vi of V1, or simply V⊤ 0 V1 = 0. The statement that a vector β is orthogonal to N(X) is equivalent to V⊤ 0 β = 0. Ridge regression Ridge regression solves arg min ˆβ ∥y − X ˆβ∥2 2 + γ∥ ˆβ∥2 2 . (S.5) Any β can be written as β = β1 + β0 with β0 ∈ N(X) (i. e. Xβ0 = 0) and β1 orthogonal to N(X). Then ∥y − X(β1 + β0)∥2 2 + γ∥β1 + β0∥2 2 = ∥y − Xβ1∥2 2 + γ∥β1∥2 2 + γ∥β0∥2 2 ≥ ∥y − Xβ1∥2 2 + γ∥β1∥2 2 (S.6) with equality if and only if β0 = 0. Thus the optimal ˆβ is always orthogonal to the nullspace of X. Principal Components Regression The PCR coefficient vector can be calculated by βPCR M = M � m=1 ˆθmvm (S.7) where vm are the first M right singular vectors of X, thus the first M columns of V1 [3]. The actual values of the coefficients ˆθm given by PCR are not necessary to show the orthogonality of βPCR M to N(X): V⊤ 0 βPCR M = M � m=1 ˆθmV⊤ 0 vm = 0 . (S.8) S1Nullspace Perspective on Regression Coefficients Partial Least Squares This analysis is based on the recursive PLS algorithm from [3]. The algorithm starts with X0 := X and ˆy(0) := 0, where the data were assumed to be centered. In contrast to [3], we are using matrix notation instead of explicit sums. One recursion step m ∈ {1, . . . , M} consists of calculating a regression input zm = Xm−1X⊤ m−1y (S.9) and performing a univariate regression of y onto zm, giving ˆθm (the actual value is again not needed in this analysis), leading to an update for the estimated output ˆy(m) = ˆy(m−1) + ˆθmzm . A recursion step ends by orthogonalizing the columns of Xm−1 with respect to zm, Xm = � I − 1 z⊤ mzm zmz⊤ m � Xm−1 . (S.10) The PLS solution after M steps is ˆy(M) = M � m=1 ˆθmzm . (S.11) We show below that each zm can be expressed in the form zm = XX⊤ζm with some ζm. Using this, ˆy(M) = X M � m=1 ˆθmX⊤ζm � �� � ˆβm =: XβPLS M (S.12) defines the regression coefficient βPLS M and furthermore, V⊤ 0 βPLS M = M � m=1 V⊤ 0 ˆβm = M � m=1 ˆθmV⊤ 0 X⊤ζm = M � m=1 ˆθmV⊤ 0 V1Σ1U⊤ 1 ζm = 0 ; (S.13) thus βPLS M is orthogonal to N(X). To show that zm = XX⊤ζm, insert (S.9) in (S.10) to give Xm = � I − 1 z⊤ mzm Xm−1X⊤ m−1yy⊤Xm−1X⊤ m−1 � Xm−1 = Xm−1 � I − 1 z⊤ mzm X⊤ m−1yy⊤Xm−1X⊤ m−1Xm−1 � , (S.14) which implies that the product XmX⊤ m can be written with Xm−1X⊤ m−1 as a left factor, XmX⊤ m = Xm−1X⊤ m−1(· · ·) . (S.15) This expression can be applied recursively, XmX⊤ m = Xm−1X⊤ m−1(· · ·) = Xm−2X⊤ m−2(· · ·) = X0X⊤ 0 (· · ·) , (S.16) and, as X0 = X, zm+1 can always be written as zm+1 = XX⊤ζm+1 . (S.17) S3 Derivation of the Nullspace Projection The optimization min v ∥β∆ + v∥2 2 (S.18a) subject to Xv = 0, (S.18b) is a convex quadratic program with linear constraints. Its solution can be derived by introducing Lagrange multipliers to give the equivalent unconstrained optimization min v,λ 1 2(β∆ + v)⊤(β∆ + v) + λ⊤Xv. (S.19) Set the derivatives of the objective function to zero to give v∗ = −β∆ − X⊤λ Xv∗ = −Xβ∆ − XX⊤λ = 0 λ = −(XX⊤)−1Xβ∆ v∗ = (X⊤(XX⊤)−1X − I)β∆ (S.20) S2Nullspace Perspective on Regression Coefficients S4 Parabolic Data Example Predictions X( PLS 1comp+v ), 10.0, NRMSE : 0.104% X PLS 1comp, NRMSE: 0.105% X * , NRMSE: 0.105% 1 0 1 2 3 4 y 1 0 1 2 3 4 y Predictions Training Data Xv , 10.0 0.008 0.006 0.004 0.002 0.000 0.002 y (a) Predictions associated with the coefficients in Fig. 1b. X( RR CV1SE+v ), 0.1, NRMSE : 0.110% X RR CV1SE, NRMSE: 0.111% X FL CV1SE, NRMSE: 0.104% 1 0 1 2 3 4 y 1 0 1 2 3 4 y Predictions Training Data Xv , 0.1 0.003 0.002 0.001 0.000 0.001 y (b) Predictions associated with the coefficients in Fig. 2. Figure S1: Scatter plots of predictions associated with the parabolic data example. S5 Signal-to-Noise Ratio Approximation 0 20 40 60 SNR [dB] a) 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 Voltage (V) 0.00 0.01 0.02 0.03 0.04 X b) 0 2 4 6 SNR 1e5 0.0 0.2 0.4 0.6 0.8 1.0 Noise Power 1e 7 0.000 0.005 0.010 0.015 0.020 LFP Training Data SNR Analysis Figure S2: a) Approximated SNR of the LFP data set with SNR in dB, SNR ratio, and the noise power. b) Mean and standard deviation of the data. Figure S2a shows the approximated SNR of the LFP data. The signal is estimated by fitting a spline, using scipy.interpolate.splrep with a smoothing parameter s = 10−6 and the polynomial degree k = 3, to the original data. The deviation to the spline is considered noise. The spline parameter choice depends on the expected degree of smoothness of the latent function. Figure S2b shows the associated mean and standard deviation. The SNR decreases strongly in the region 3.2–3.5 V; however, in this region, the standard deviation of the data is also very low. Rescaling the data matrix columns to unit variance thus amplifies the noise in this section. S3Nullspace Perspective on Regression Coefficients The above analysis is only based on the data X without considering y. Although standardization can amplify noise, whether a model based on z-scoring the data yields a higher prediction accuracy also depends on the relationship between X and y. An alternative to mitigate the amplification of noise would be to rescale the data such that the variance of the column matches the normalized SNR ratio. References [1] G. Strang, Introduction to Linear Algebra, Fifth. Wellesley, Massachusetts: Cambridge Press, 2016. [2] D. Kobak, J. Lomond, and B. Sanchez, “The optimal ridge penalty for real-world high-dimensional data can be zero or negative due to the implicit ridge regularization,” Journal of Machine Learning Research, vol. 21, no. 169, pp. 1–16, 2020. [3] T. Hastie, R. Tibshirani, and J. H. Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second. New York: Springer, 2009. S4
A stochastic block model for community detection in attributed networks Xiao Wang a, Fang Dai a, *, Wenyan Guo a, Junfeng Wang a a School of Science, Xi’an University of Technology, Xi’an, China E-mail addresses: daifang@xaut.edu.cn (Dai Fang) 2210920031@stu.xaut.edu.cn (Xiao Wang) Abstract—Community detection is an important content in complex network analysis. The existing community detection methods in attributed networks mostly focus on only using network structure, while the methods of integrating node attributes is mainly for the traditional community structures, and cannot detect multipartite structures and mixture structures in network. In addition, the model-based community detection methods currently proposed for attributed networks do not fully consider unique topology information of nodes, such as betweenness centrality and clustering coefficient. Therefore, a stochastic block model that integrates betweenness centrality and clustering coefficient of nodes for community detection in attributed networks, named BCSBM, is proposed in this paper. Different from other generative models for attributed networks, the generation process of links and attributes of nodes in BCSBM model follows the Poisson distribution, and the probability between community is considered based on the stochastic block model. Moreover, the betweenness centrality and clustering coefficient of nodes are introduced into the process of links and attributes of nodes generation. Finally, the expectation maximization algorithm is employed to estimate the parameters of the BCSBM model, and the node-community memberships in network is obtained through the hard division process, so the community detection is completed. By experimenting on six real-work networks containing different network structures, and comparing with the community detection results of five algorithms, the experimental results show that the BCSBM model not only inherits the advantages of the stochastic block model and can detect various network structures, but also has good data fitting ability due to introducing the betweenness centrality and clustering coefficient of nodes. Overall, the performance of this model is superior to other five compared algorithms. Keyworks stochastic block model; attributed network; community detection; betweenness centrality; clustering coefficient1. Introduction An attributed network refers to a network in which the nodes or edges contain attribute information. [1].Unlike non-attributed networks that only consider the links between nodes, attributed network can model the characteristics or attributes of nodes in complex systems and provide much richer and heterogeneous information [2]. For example, in social networks, attributes can provide information about individuals' age, gender, interests, occupation, and location. In academic citation networks, attributes contain important information such as titles, authors, abstract, keywords, etc. Therefore, the studying of attributed networks is very significant to the theoretical research and application of complex systems in the real world. Recently, the community detection in attributed networks has attracted widespread attention from researchers. Zhou et al. [4] proposed the SA-Cluster algorithm, which uses a unified distance measure to combine topology structures and attributes to construct a weighted network, and employed a clustering algorithm based on K- Medoids [5] to mine the community structure in the weighted network. This algorithms is similarity-based for community detection in attributed networks, the relevant algorithms are SAGL [6], PWMN [7], ANCA [8] and SAS-LP [9]. In addition, Wang et al. [10] proposed the SCI algorithm, which uses a two-factor non-matrix factorization method to extract the node memberships matrix from the attribute matrix and adjacency matrix to achieve community detection. The algorithm is based on non-negative matrix factorization for community detection in attributed networks, the related algorithms are PICS [11], SCD [12], MDNMF [13] and TENE [14]. These above algorithms are widely applied to detect communities with assortative structures shown as Fig. 1(a), i.e., tight intra-community node links and relatively sparse inter-community node links [15]. However, in the real world, there are not only assortative structures but also disassortative structures [16], such as multipartite structures shown as Fig. 1(b) (node links within communities are sparse, and node links between communities are reversely densely), and mixture structures such as Fig. 1(c) (It contains both community structure and multipartite structures). Currently, the main approach that can deal with both assortative networks and disassortative networks is stochastic block model (SBM) [18]. The SBM is a common model for characterizing networks with complex structural patterns, such as community, multipartite and mixture structures. Chai et al.[19] inherited the advantages of general stochastic block model (GSB) [20] and popularity and productivity link model (PPL) [21], introduced the popularity and productivity of nodes to simulate the scale-free properties of real networks and proposed PPSB_DC model. A two-stage projection algorithm is used to estimate model parameters for PPSB_DC, but a recent study found [22]: this two-stage algorithm does not guarantee convergence. Chen et al. [23] proposed BNPA model based on Newman's mixture models (NMM) [24] and Bayesian nonparametric theory. This model not only makes full use of the links between nodes and attributes of nodes to divide communities by sharing hidden variables, but also employs Bayesian nonparametric theory to determine the number of communities automatically, whichFig.1 Examples of assortative and disassortative networks [17] (a) assortative network with community structures; (b) disassortative network with multipartite structures; (c) disassortative network with mixture structures solves the problem of other methods that need to define the number of communities in advance, but it may be inaccurate to infer the number of communities, which will affect the accuracy of community detection. He et al. [25] proposed NEMBP model, which combines degree-corrected stochastic block model (DSBM) [26] and multinomial distributions to model the generation process of links and attributes to better fit the real network. The model parameters are inferred by using the nested Expectation-Maximum (EM) algorithm [27] and belief propagation (BP) [28]. Compared to the EM algorithm alone, the solution complexity of the NEMBP model is higher. Chen et al. [29] proposed subspace stochastic block model (SSB), which not only incorporates the attributes of nodes into the GSB model in the form of probabilities, but also constructs a hidden network by integrating the topology information and attribute information in the process of generating links between nodes. In this hidden network, it is possible to generate links between all nodes, avoiding the consideration of unobserved edges, thus avoiding the negative sampling strategy. Chang et al. [22] proposed PSB_PG model, which constructs a generative model based on the potential relationship between links and attributes of nodes. However, since real networks have scale-free property, the degree of nodes follows power-law distribution, Zheng et al. [30] introduced the degree of nodes on the basis of PSB_PG and proposed the degree-corrected stochastic block model for attributed networks (DPSB_PG). However, these two models do not fully consider the unique topological information of nodes, such as betweenness centrality and clustering coefficient, etc. According to the assumption of consistency between node attributes and topology structure, it can be seen that topological information of nodes often has a great correlation with the topology structures and can affect the links between nodes. Based on the DPSB_PG, an attributed network stochastic block model BCSBM that integrates betweenness centrality and clustering coefficient of nodes is presented in this paper, which comprehensively considers the importance of nodes and the property of node neighborhood structure. The betweenness centrality characterizes the importance of nodes by the number of shortest paths passing through a node and also describes the influence of nodes on the flow of information on the network. The clustering coefficient describes the likelihood that neighboring nodes of individuals inthe network are also neighbors of each other and is used to measure the extent of node clustering. In the BCSBM, the generation of network structures and node attributes follow the Poisson distribution and are independent of each other. It is worth noting that the BCSBM model in this paper is an extension of the DPSB_PG model, and its main difference is that the two observation variables of betweenness centrality and clustering coefficient of nodes are introduced, and the performance of the model is analyzed during the experiments. The paper is organized as follows. In Section 2, we introduce the BCSBM model. In Section 3, the parameters estimation of the BCSBM model by the EM algorithm is described. The process of community detection based on the BCSBM model is presented in Section 4. In Section 5, we give the analysis of the community detection results. Finally, we conclude our work and discuss future in Section 6. 2. BCSBM Model Let 𝐷���(𝑉, 𝐷���, 𝑉���) denotes an undirected and unweighted attributed network, where 𝑉 = {1, 2, ⋯ , 𝑖���} denotes the set of 𝑖��� nodes in the network and 𝐷��� = {𝑎���1, 𝑎���2, ⋯ , 𝑎���𝑖���} denotes the set of 𝑖��� edges in the network. If the attribute of each node is denoted by a 𝐾 dimension vector, the attribute matrix of all nodes can be expressed as 𝑉��� = (𝑥𝑖𝑖���)𝑖���×𝐾, where 𝑥𝑖𝑖��� = 1 denotes node 𝑖 has 𝑖��� th attribute, otherwise 𝑥𝑖𝑖��� = 0 . Typically, the adjacency matrix of an undirected unweighted network is denoted by 𝐴 = (𝑎𝑖𝑖���)𝑖���×𝑖��� , where 𝑎𝑖𝑖��� = 1 denotes node 𝑖 links to node 𝑖��� , otherwise 𝑎𝑖𝑖��� = 0. Suppose a network 𝐷��� has 𝑎��� different communities 𝑉1, 𝑉2, ⋯ , 𝑉𝑟 and 𝑉 = ⋃ 𝑉𝑟 𝑎��� 𝑟=1 . 2.1 A generative model for integrating node topology information In a standard stochastic block model [18], the stochastic block probability matrix Θ = (𝜃𝑟𝑟���)𝑎���×𝑎��� controls the probability of generating links in network, where 𝜃𝑟𝑟��� is the connecting probability of two nodes 𝑖 ∈ 𝑉𝑟 and 𝑖��� ∈ 𝑉𝑟��� and is only related to the communities to which 𝑖 and 𝑖��� belong. In the PSB_PG model [22], Chang et al. relaxed this restriction by introducing a node-community memberships matrix 𝐷 = (𝑎���𝑖𝑟)𝑖���×𝑎���, where 𝑎���𝑖𝑟 is the probability that a node 𝑖 belongs to the 𝑞���th community 𝑉𝑟. In the DPSB_PG model [30], Zheng et al. introduced the degree of nodes into the PSB_PG model to influence the generation of network links. In the BCSBM model of this paper, the generation of links in network is strengthened by introducing betweenness centrality and clustering coefficient of nodes to affect the distribution of community. A link between pair of nodes in network is not only related to the node-community memberships matrix 𝐷, the inter-community probability matrix Θ and the degree of nodes Γ = (𝑖���𝑖)𝑖���×1 , but also affected by the betweenness centrality of nodes Β = (𝑎���𝑖)𝑖���×1 and clustering coefficient of nodes Μ = (𝑎���𝑖)𝑖���×1 . So, we introduce betweenness centrality and clustering coefficient of nodes to control the network generation process, the real network can be better fitted. Assuming that the generation of links between pairs of nodes (𝑥, 𝑥���) is independent and follows the Poisson distribution, the expected number of links that nodes 𝑖 and node 𝑖��� lies incommunities 𝑉𝑟 and 𝑉𝑟��� is 𝑖���̂𝑖𝑖��� 𝑟𝑟��� = 𝛾���𝑖𝑎���𝑖𝑟𝜃𝑟𝑟���𝛾���𝑖���𝑎���𝑖���𝑟��� where 𝛾���𝑖 = 𝑖���𝑖 + 𝑎���𝑖 + 𝑎���𝑖 , 𝑎���𝑖 = 2𝑖���𝑖 𝑖���𝑖(𝑖���𝑖−1) is the clustering coefficient of node 𝑖 [31] , 𝑎���𝑖 = ∑ 𝑖���𝑟���𝑟��� 𝑖 𝑟���,𝑟���≠𝑖 𝑔𝑟���𝑟��� is the betweenness centrality of node 𝑖 [32], 𝑖���𝑖 is the degree of node 𝑖, 𝑖���𝑖 is the total real links number of node 𝑖’s neighbors, 𝑔𝑟���𝑟��� is the number of shortest paths between nodes 𝑞��� and 𝑞���, 𝑖���𝑟���𝑟��� 𝑖 is the number of those shortest paths that include node 𝑖. Considering all communities, the expected total number of links between nodes 𝑖 and 𝑖��� is 𝑖���̂𝑖𝑖��� = ∑ 𝛾���𝑖𝑎���𝑖𝑟𝜃𝑟𝑟���𝛾���𝑖���𝑎���𝑖���𝑟��� 𝑎��� 𝑟,𝑟���=1 where Θ is symmetrical, ∑ 𝛾���𝑖𝑎���𝑖𝑟 𝑖��� 𝑖=1 =1 and ∑ 𝜃𝑟𝑟��� 𝑎��� 𝑟,𝑟���=1 = 1 satisfy the normalization constraints. Suppose the generation of links is independent and the number of links follows the Poisson distribution with mean value 𝑖���̂𝑖𝑖���, given the parameters 𝐷, Θ and observed variables 𝑅���, the probability of generating a network is 𝑃(𝐴|𝐷, Θ, 𝑅���) = ∏ (∑ 𝛾���𝑖𝑎���𝑖𝑟𝜃𝑟𝑟���𝛾���𝑖���𝑎���𝑖���𝑟��� 𝑎��� 𝑟,𝑟���=1 ) 𝑎𝑖𝑖��� 𝑎𝑖𝑖���! exp (− ∑ 𝛾���𝑖𝑎���𝑖𝑟𝜃𝑟𝑟���𝛾���𝑖���𝑎���𝑖���𝑟��� 𝑎��� 𝑟,𝑟���=1 ) 𝑖��� 𝑖,𝑖���=1,𝑖<𝑖��� × ∏ (1 2 𝛾���𝑖𝑎���𝑖𝑟𝜃𝑟𝑟𝛾���𝑖𝑎���𝑖𝑟)𝑎𝑖𝑖/2 (𝑎𝑖𝑖/2)! 𝑖��� 𝑖=1 exp (− 1 2 ∑ 𝛾���𝑖𝑎���𝑖𝑟𝜃𝑟𝑟𝛾���𝑖𝑎���𝑖𝑟 𝑎��� 𝑟=1 ) (1) where 𝑅��� = ⋃ {𝑖���𝑖, 𝑎���𝑖, 𝑎���𝑖} 𝑖��� 𝑖=1 is the set of degree 𝑖���𝑖 , clustering coefficient 𝑎���𝑖 and betweenness centrality 𝑎���𝑖 of the nodes. 2.2 A generative model for integrating node attribute information Generally, the attributes corresponding to each node in attributed networks are high-dimensional, and whether the nodes in the community have common attributes is sparse. If the attributes of nodes in a community are highly correlated, they will also be consistent or complementary to the network topology structures, promoting the formation of community. Therefore, the generation of node attributes follows the Poisson distribution according to Poisson’ theorem [39]. Let 𝜙𝑟𝑖��� denote the probability that a community 𝑉𝑟 has the 𝑖��� th attribute, and Φ = (𝜙𝑟𝑖���)𝑎���×𝐾 denote community-related attributes matrix. Similarly, nodes 𝑖 in the community 𝑉𝑟 possessing 𝑖��� th attribute related to degree 𝑖���𝑖 , betweenness centrality 𝑎���𝑖 , clustering coefficient 𝑎���𝑖 , node-community memberships 𝑎���𝑖𝑟 and community-related attributes 𝜙𝑟𝑖���. And for this reason, the propensity of node 𝑖 in the community 𝑉𝑟 possessing the 𝑖���th attribute is 𝑉���̂𝑖𝑖��� 𝑟 = 𝛾���𝑖𝑎���𝑖𝑟𝜙𝑟𝑖���Summing over all communities 𝑉𝑟, the mean propensity of node 𝑖 possessing the 𝑖���th attribute is 𝑉���̂𝑖𝑖��� = ∑ 𝛾���𝑖𝑎���𝑖𝑟𝜙𝑟𝑖��� 𝑎��� 𝑟=1 where ∑ 𝑎���𝑖𝑟 𝑖��� 𝑖=1 = 1 and ∑ 𝜙𝑟𝑖��� = 1 𝐾 𝑖���=1 satisfy the normalization constraints. According to the Poisson distribution process, given the parameter matrix 𝐷, Φ and the observed variables 𝑅��� = ⋃ {𝑖���𝑖, 𝑎���𝑖, 𝑎���𝑖} 𝑖��� 𝑖=1 , the probability 𝑃(𝑉���|𝐷, Θ, 𝑅���) of generating node attributes in network is 𝑃(𝑉���|𝐷, Θ, 𝑅���) = ∏ ∏ (∑ 𝛾���𝑖𝑎���𝑖𝑟𝜙𝑟𝑖��� 𝑎��� 𝑟=1 )𝑥𝑖𝑖��� 𝑥𝑖𝑖���! exp (− ∑ 𝛾���𝑖𝑎���𝑖𝑟𝜙𝑟𝑖��� 𝑎��� 𝑟=1 ) 𝐾 𝑖���=1 𝑖��� 𝑖=1 (2) 2.3 Integrating node topology and attribute information Assuming that the generative process of the adjacency matrix 𝐴 and attribute matrix 𝑉��� in network are independent of each other, the joint probability is 𝑃(𝐴, 𝑉���|𝐷, Θ, Φ, 𝑅���) = 𝑃(𝐴|𝐷, Θ, 𝑅���) × 𝑃(𝑉���|𝐷, Φ, 𝑅���) = ∏ (∑ 𝛾���𝑖𝑎���𝑖𝑟𝜃𝑟𝑟���𝛾���𝑖���𝑎���𝑖���𝑟��� 𝑎��� 𝑟,𝑟���=1 ) 𝑎𝑖𝑖��� 𝑎𝑖𝑖���! exp 𝑖��� 𝑖,𝑖���=1,𝑖<𝑖��� (− ∑ 𝛾���𝑖𝑎���𝑖𝑟𝜃𝑟𝑟���𝛾���𝑖���𝑎���𝑖���𝑟��� 𝑎��� 𝑟,𝑟���=1 ) × ∏ (1 2 ∑ 𝛾���𝑖𝑎���𝑖𝑟𝜃𝑟𝑟𝛾���𝑖𝑎���𝑖𝑟 𝑎��� 𝑟=1 ) 𝑎𝑖𝑖 2 (𝑎𝑖𝑖 2 ) ! exp (− 1 2 ∑ 𝛾���𝑖𝑎���𝑖𝑟𝜃𝑟𝑟𝛾���𝑖𝑎���𝑖𝑟 𝑎��� 𝑟=1 ) 𝑖��� 𝑖=1 × ∏ ∏ (∑ 𝛾���𝑖𝑎���𝑖𝑟𝜙𝑟𝑖��� 𝑎��� 𝑟=1 )𝑥𝑖𝑖��� 𝑥𝑖𝑖���! exp (− ∑ 𝛾���𝑖𝑎���𝑖𝑟𝜙𝑟𝑖���) 𝑎��� 𝑟=1 ) 𝐾 𝑖���=1 𝑖��� 𝑖=1 (3) The generative process of integrating topology information and attribute information of nodes in network can be summarized as follows. 1) Extracting nodes 𝑖 and 𝑖��� from the communities 𝑉𝑟 and 𝑉𝑟��� with probability 𝑎���𝑖𝑟 and 𝑎���𝑖���𝑟���, respectively. 2) Forming a link between node 𝑖 and node 𝑖��� with probability 𝑖���𝑖𝑖��� , where 𝑖���𝑖𝑖���~𝑃𝑖���𝑖𝑞���𝑞���𝑖���𝑖���(𝑖���̂𝑖𝑖���). 3) Selecting an attribute 𝑖��� in the community 𝑉𝑟 with probability 𝜙𝑟𝑖���. 4) Selecting an attribute 𝑖��� for the node 𝑖 with probability 𝑉���𝑖𝑖��� , where 𝑉���𝑖𝑖���~𝑃𝑖���𝑖𝑞���𝑞���𝑖���𝑖���(𝑉���̂𝑖𝑖���). The corresponding probabilistic graph model is shown in Fig 2.Fig.2 The probabilistic graph model for BCSBM Fig.2 The probabilistic graph model for BCSBM 3. Estimating parameters of BCSBM model using the EM algorithm The model BCSBM contains observed variables 𝐴, 𝑉���, 𝑅���, hidden variables Q = (𝑞𝑖𝑖��� 𝑟𝑟���)𝑖���×𝑖��� ,Υ = ( 𝛾𝑖𝑖��� 𝑟 )𝑖���×𝑎��� , and model parameters 𝐷 = (𝑎���𝑖𝑟)𝑖���×𝑎��� , Θ = (𝜃𝑟𝑟���)𝑎���×𝑎��� , Φ = (𝜙𝑟𝑖���)𝑎���×K. Due to the existence of hidden variables in the model, the likelihood function cannot be solved directly, and the EM algorithm [27] is the most common hidden variable estimation method, which can handle such problems well. Therefore, in this paper, the EM algorithm is employed to estimate the parameters of the BCSBM model. The inference process is as follows. Considering the logarithm of the Eq. (3), neglecting the constants and terms independent of model parameters, we have 𝐾���(𝐷, Θ, Φ) = ∑ [1 2 𝑎𝑖𝑖��� ln ( ∑ 𝛾���𝑖𝑎���𝑖𝑟𝜃𝑟𝑟���𝛾���𝑖���𝑎���𝑖���𝑟��� 𝑎��� 𝑟,𝑟���=1 ) − 1 2 ∑ 𝛾���𝑖𝑎���𝑖𝑟𝜃𝑟𝑟���𝛾���𝑖���𝑎���𝑖���𝑟��� 𝑎��� 𝑟,𝑟���=1 ] 𝑖��� 𝑖,𝑖���=1 + ∑ ∑ ∑ [𝑥𝑖𝑖��� ln (∑ 𝛾���𝑖𝑎���𝑖𝑟𝜙𝑟𝑖��� 𝑎��� 𝑟=1 ) − ∑ 𝛾���𝑖𝑎���𝑖𝑟𝜙𝑟𝑖��� 𝑎��� 𝑟=1 ] 𝑎��� 𝑟=1 𝐾 𝑖���=1 𝑖��� 𝑖=1 (4) In E-step, given the parameters 𝐷, Θ and Φ , and the lower bound of the log- likelihood obtained by Jensen's inequality is 𝐾���̅(𝐷, Θ, Φ) = 1 2 ∑ ∑ [𝑎𝑖𝑖���𝑞𝑖𝑖��� 𝑟𝑟��� ln (𝛾���𝑖𝑎���𝑖𝑟𝜃𝑟𝑟���𝛾���𝑖���𝑎���𝑖���𝑟��� 𝑞𝑖𝑖��� 𝑟𝑟��� ) − 𝛾���𝑖𝑎���𝑖𝑟𝜃𝑟𝑟���𝛾���𝑖���𝑎���𝑖���𝑟���] 𝑎��� 𝑟,𝑟���=1 𝑖��� 𝑖,𝑖���=1 + ∑ ∑ ∑ [𝑥𝑖𝑖���𝛾𝑖𝑖��� 𝑟 ln (𝛾���𝑖𝑎���𝑖𝑟𝜙𝑟𝑖��� 𝛾𝑖𝑖��� 𝑟 ) − 𝛾���𝑖𝑎���𝑖𝑟𝜙𝑟𝑖���] 𝑎��� 𝑟=1 𝐾 𝑖���=1 𝑖��� 𝑖=1 (5) where𝑞𝑖𝑖��� 𝑟𝑟��� = (𝑖���𝑖 + 𝑎���𝑖 + 𝑎���𝑖)𝑎���𝑖𝑟𝜃𝑟𝑟���(𝑖���𝑖��� + 𝑎���𝑖��� + 𝑎���𝑖���)𝑎���𝑖���𝑟��� ∑ (𝑖���𝑖 + 𝑎���𝑖 + 𝑎���𝑖)𝑎���𝑖𝑟𝜃𝑟𝑟���(𝑖���𝑖��� + 𝑎���𝑖��� + 𝑎���𝑖���)𝑎���𝑖���𝑟��� 𝑎��� 𝑟,𝑟���=1 (6) 𝛾𝑖𝑖��� 𝑟 = (𝑖���𝑖 + 𝑎���𝑖 + 𝑎���𝑖)𝑎���𝑖𝑟𝜙𝑟𝑖��� ∑ (𝑖���𝑖 + 𝑎���𝑖 + 𝑎���𝑖)𝑎���𝑖𝑟𝜙𝑟𝑖��� 𝑎��� 𝑟=1 (7) 𝑞𝑖𝑖��� 𝑟𝑟��� denotes the probability that nodes 𝑖 and 𝑖��� lie in communities 𝑉𝑟 and 𝑉𝑟��� , respectively, and there is a link between nodes 𝑖 and 𝑖���; 𝛾𝑖𝑖��� 𝑟 denotes the probability that node 𝑖 in the community 𝑉𝑟 and has the 𝑖���th attribute. In M-step, given the hidden variables 𝑞𝑖𝑖��� 𝑟𝑟��� and 𝛾𝑖𝑖��� 𝑟 , and we can obtain the three parameter estimates 𝑎���𝑖𝑟, 𝜃𝑟𝑟���, 𝜙𝑟𝑖��� according to the Lagrange multiplier method as follows 𝑎���𝑖𝑟 = ∑ ∑ 𝑎𝑖𝑖���𝑞𝑖𝑖��� 𝑟𝑟��� + 2 × ∑ 𝑥𝑖𝑖���𝛾𝑖𝑖��� 𝑟 𝐾 𝑖���=1 𝑎��� 𝑟���=1 𝑖��� 𝑖���=1 (𝑖���𝑖 + 𝑎���𝑖 + 𝑎���𝑖) × [∑ ∑ 𝑎𝑖𝑖���𝑞𝑖𝑖��� 𝑟𝑟��� + 2 × ∑ ∑ 𝑥𝑖𝑖���𝛾𝑖𝑖��� 𝑟 𝐾 𝑖���=1 𝑖��� 𝑖=1 𝑎��� 𝑟���=1 𝑖��� 𝑖,𝑖���=1 ] (8) 𝜃𝑟𝑟��� = ∑ 𝑎𝑖𝑖���𝑞𝑖𝑖��� 𝑟𝑟��� 𝑖��� 𝑖,𝑖���=1 ∑ ∑ 𝑎𝑖𝑖���𝑞𝑖𝑖��� 𝑟𝑟��� 𝑎��� 𝑟,𝑟���=1 𝑖��� 𝑖,𝑖���=1 𝜙𝑟𝑖��� = ∑ 𝑥𝑖𝑖���𝛾𝑖𝑖��� 𝑟 𝑖��� 𝑖=1 ∑ ∑ 𝑥𝑖𝑖���𝛾𝑖𝑖��� 𝑟 𝐾 𝑖���=1 𝑖��� 𝑖=1 (9) The derivation of the parameters 𝑎���𝑖𝑟, 𝜃𝑟𝑟���, 𝜙𝑟𝑖��� can be found in Appendix A. The specific steps of the parameter estimation are shown in Algorithm 1. Algorithm 1 Parameter Inference Algorithm for BCSBM Input: the adjacency matrix 𝐴, the attribute matrix 𝑉���, the number of communities 𝑎���, the maximum iteration 𝐼𝑇 and the threshold 𝜕���. Output: the model parameters 𝐷, Θ, Φ 1: According to the adjacency matrix 𝐴, calculate the degree of node 𝑖���𝑖, betweenness centrality of node 𝑎���𝑖 , and clustering coefficient of node 𝑎���𝑖, 𝑖 = 1,2, ⋯ , 𝑖���. 2: Initialize 𝐷(0), Θ(0), Φ(0). 3: Compute the objective function 𝐾���(0) = (𝐷(0), Θ(0), Φ(0)) by Eq. (4). 4: for 𝑞��� = 1:𝐼𝑇 do 5: E-step: Compute 𝑞𝑖𝑖��� 𝑟𝑟���, 𝛾𝑖𝑖��� 𝑟 by Eq. (6) ~ (7). 𝑖, 𝑖��� = 1,2, ⋯ , 𝑖���; 𝑞���, 𝑞��� = 1,2, ⋯ , 𝑎���. 6: M-step: Compute 𝐷(𝑟���), Θ(𝑟���), Φ(𝑟���) by Eq. (8) ~ (9). 7: Compute the objective function 𝐾���(𝑟���) = (𝐷(𝑟���), Θ(𝑟���), Φ(𝑟���)) by Eq. (4). 8: if |𝐾���(𝑟���)(𝐷(𝑟���), Θ(𝑟���), Φ(𝑟���)) − 𝐾���(𝑟���−1)(𝐷(𝑟���−1), Θ(𝑟���−1), Φ(𝑟���−1))| < 𝜕��� or 𝑞��� = 𝐼𝑇 then 9: 𝐷 = 𝐷(𝑟���), Θ = Θ(𝑟���), Φ = Φ(𝑟���); STOP 10: end if 11: end forInitialized scheme of 𝚯 . In the algorithm BCSBM, the initialization of the probability matrix Θ has a great influence on the convergence speed of the algorithm. When the network structure generated by the initialization of the stochastic block probability matrix Θ is consistent with the real network structure, the algorithm will converge quickly. However, when the initial network structure (i.e., the initial values of Θ ) is inconsistent with the real network structure, the algorithm will converge very slowly. For the algorithm to achieve stability with as few iterations as possible, we apply maximum entropy distribution [33] and maximum likelihood to make appropriate choices for the stochastic block probability matrix Θ . The specific approach is as follows [22]. The initialization of the stochastic block probability matrix Θ is divided into three schemes: (1) when the diagonal elements are larger than the non-diagonal elements, it corresponds to the assortative structures in network; (2) when the diagonal elements are smaller than the non-diagonal elements, it corresponds to disassortative structures in network; and (3) when the elements in Θ are floating around a certain value (e.g., 0.5), it corresponds to the other structures in network. The BCSBM algorithm is executed a number of times (e.g., 10 times) for each of these three cases, and then the average of the maximum likelihoods is calculated for each scheme. The initialization form of Θ corresponding to the scheme with the largest average is used as the initialization of the BCSBM algorithm. The time complexity of BCSBM algorithm mainly depends on E-step and M-step of the EM algorithm for parameters estimation. In each iteration process, the time complexity of E-step is Ο(𝑖���𝑎���2 + 𝑖���𝐾𝑎���), and the time complexity of M-step is Ο(𝑖���𝑎��� + 𝑎���2 + 𝑖���𝐾𝑎���). Since the number of communities 𝑎��� is much smaller than the number of nodes 𝑖��� , i.e., 𝑎��� ≪ 𝑖��� . Therefore, the time complexity of M-step can be written as Ο(𝑖���𝐾𝑎���) , and due to the maximum number of iterations of the algorithm is 𝐼𝑇 , the overall time complexity of the algorithm is Ο(𝐼𝑇(𝑖���𝑎���2 + 𝑖���𝐾𝑎���)). 4. Community detection based on BCSBM model Since the node-community memberships matrix 𝐷 characterizes the distribution of each node over all communities, our main goal is to infer the node-community memberships matrix 𝐷 = (𝑎���𝑖𝑟)𝑖���×𝑎��� , i.e., the probability that a node belongs to any community 𝑉𝑟(𝑞��� = 1, 2, ⋯ , 𝑎���). We infer the node-community memberships in network by hard division, that is, using 𝑎���∗ = argmax 𝑟 {𝑎���𝑖𝑟} to limit that a node can only belong to a community. Since hidden variables are introduced when the model parameters are inferred by the EM algorithm, 𝐷 = (𝑎���𝑖𝑟)𝑖���×𝑎��� cannot be processed directly. In order to get the hard partition, we set an operation on the parameter 𝐷 = (𝑎���𝑖𝑟)𝑖���×𝑎���, Θ = (𝜃𝑟𝑟���)𝑎���×𝑎���, that is 𝐼𝑖𝑟 = ∑ 𝜃𝑟𝑟���𝑎���𝑖𝑟 𝑎��� 𝑟���=1 ∑ 𝜃𝑟𝑟���𝑎���𝑖𝑟 𝑎��� 𝑟,𝑟���=1 (10) The BCSBM algorithm can use 𝑞���∗ = argmax 𝑟 {𝐼𝑖𝑟} to find which community the nodeultimately belongs to. 5. Experimental results and analysis 5.1 Datasets In this paper, six real-world attributed networks are selected to examine the community detection performance of BCSBM model, including WebKB (Cornell, Texas, Washington, Wisconsin), Cora and Citeceer. The basic characteristics of the attributed networks are shown in Table 1, where 𝑖��� and 𝑖��� are the number of nodes and links, respectively; 𝐾 is the attribute type; and 𝑎��� is the number of communities. Table 1 Features of the attributed Networks Datasets n m K c Structure WebKB Cornell 195 304 1703 5 disassortative Texas 187 328 1703 5 disassortative Washington 230 446 1703 5 disassortative Wisconsin 265 530 1703 5 disassortative Cora — 2708 5429 1433 7 assortative Citeseer — 3312 4723 3703 6 assortative 1) WebKB dataset [34] is a citation network consisting of web pages and links between web pages of four American universities, Cornell, Texas, Washington, and Wisconsin, with a total of 877 nodes representing all web pages, and 1,608 links representing hyperlinks between web pages. Web pages (i.e., nodes) in network are classified into the following five types, i.e., course, faculty, student, project, and staff. Each node consists of a 1703-dimensional attribute vector. 2) Cora dataset [35] is a citation network of scientific and technical literature with 2708 nodes representing all scientific publications. 5429 links representing the citation relationships from a publication to another. All the scientific publications (i.e., nodes) in network are classified into the following seven types, i.e., case-based reasoning, genetic algorithms, neural networks, probabilistic methods, reinforcement learning, rule learning, and theory. Each node consists of a 1433- dimensional attribute vector. 3) Citeseer dataset [36] is an academic citation network containing 3312 nodes representing all academic papers. 4723 links representing citation relationships between papers. All papers (i.e., nodes) in network are classified into the following six types, i.e., agents, artificial intelligence, databases, human-computer interaction, information retrieval, and machine learning. Each node consists of a 3703- dimensional attribute vector. 5.2 Evaluation criteria To evaluate the community detection performance of BCSBM model on the realnetworks, two evaluation indexes, Normalized Mutual Information (NMI) and Pairwise F-measure (PWF), are adopted in this paper. (1) 𝐾���𝐾���𝐼 The 𝐾���𝐾���𝐼 proposed in [37] is based on the confusion matrix to judge the completeness of information retention after community division. Its definition is shown in Eq. (11). 𝐾���𝐾���𝐼(𝐴, 𝐴���) = −2 ∑ ∑ 𝐾���𝑟𝑟���log (𝐾���𝑟𝑟���𝐾��� 𝐾���𝑟𝐾���𝑟���) 𝑎���2 𝑟���=1 𝑎���1 𝑟=1 ∑ 𝐾���𝑟 𝑎���1 𝑟=1 log (𝐾���𝑟 𝐾��� ) + ∑ 𝐾���𝑟��� 𝑎���2 𝑟���=1 log (𝐾���𝑟��� 𝐾��� ) (11) Where 𝐴 is real community, 𝐴��� is the community divided by the community detection algorithm, 𝑎���1 denotes the number of real community 𝐴, 𝑎���2 denotes the number of community 𝐴��� divided by the community detection algorithm, 𝐾��� is the total number of nodes in network 𝐷���, 𝐾���𝑟, 𝐾���𝑟��� denote the number of nodes in communities 𝑞��� and 𝑞��� respectively, 𝐾���𝑟𝑟��� denotes the number of nodes that should belong to community 𝑞��� but are wrongly assigned to community 𝑞���. The range of 𝐾���𝐾���𝐼 is [0,1] , the larger the value of 𝐾���𝐾���𝐼 is, the better the community detection performance. If 𝐴 = 𝐴���, 𝐾���𝐾���𝐼(𝐴, 𝐴���) = 1 . If 𝐴 and 𝐴��� are completely different, 𝐾���𝐾���𝐼(𝐴, 𝐴���) = 0. (2) 𝑃𝑉���𝐷��� The 𝑃𝑉���𝐷��� proposed in [38] integrates the concepts of precision and recall into a single evaluation, and its definition is shown in Eq. (12). 𝑃𝑉���𝐷��� = 2 × 𝑃𝑞���𝑎���𝑎���𝑖𝑞���𝑖𝑖���𝑖��� × 𝑅𝑎���𝑎���𝑎𝑖���𝑖��� 𝑃𝑞���𝑎���𝑎���𝑖𝑞���𝑖𝑖���𝑖��� × 𝑅𝑎���𝑎���𝑎𝑖���𝑖��� (12) where 𝑃𝑞���𝑎���𝑎���𝑖𝑞���𝑖𝑖���𝑖��� = |𝑅��� ∩ 𝑅���|/|𝑅���| , 𝑅𝑎���𝑎���𝑎𝑖���𝑖��� = |𝑅��� ∩ 𝑅���|/|𝑅���| denote the precision and recall of the division results of the community detection algorithm, respectively, 𝑅��� denotes the set of nodes that are assigned to the same community, 𝑅��� denotes the set of nodes that have the same label, and | ∙ | denotes the number of elements in the set. The range of values of 𝑃𝑉���𝐷��� is also [0,1] . The larger the value of 𝑃𝑉���𝐷��� is, the better the partitioning effect of community algorithm division. 5.3 Experimental results and analysis To verify the validity of the BCSBM model, in this section, experiments are conducted on the six real-world attributed networks shown in Table 1 and compared with the existing generative models that integrate links and attributes of nodes, PPSB_DC [19], BNPA [23], NEMBP [25], PSB_PG [22], and DPSB_PG [30]. In order to maintain fairness, all algorithms keep the optimal parameter settings mentioned in the original paper, and the experimental results are shown in Table 2 and Table 3, Fig.3 and Fig.4. Table 2 and Fig.3 show the 𝐾���𝐾���𝐼 metrics of the six algorithms, and Table 3 and Fig.4 show the 𝑃𝑉���𝐷��� metrics of the six algorithms. Since the EM algorithm is particularly sensitive to initial values, we conducted 30 experiments on each model, and the mean and maximum values of 30 times are given for two indicators.Table 2 𝐾���𝐾���𝐼 of the BCSBM model and compared algorithms on attributed networks Datasets NMI Model Value PPSB_DC BNPA NEMBP PSB_PG DPSB_PG BCSBM Cornell mean 0.1128 0.0772 0.1510 0.3131 0.3246 0.3550 max 0.2503 0.0933 0.2793 0.3973 0.4460 0.4555 Texas mean 0.2085 0.2265 0.2965 0.2882 0.2926 0.3214 max 0.3663 0.2694 0.4202 0.3750 0.3933 0.4636 Washington mean 0.1726 0.2469 0.1938 0.3235 0.3222 0.3617 max 0.3690 0.2701 0.3107 0.3631 0.3437 0.4106 Wisconsin mean 0.1315 0.3212 0.2322 0.3736 0.3772 0.4219 max 0.2409 0.3413 0.4075 0.4230 0.4436 0.4787 Cora mean 0.1820 0.4391 0.4033 0.3012 0.3143 0.3360 max 0.5221 0.5022 0.4757 0.3699 0.3488 0.3593 Citeseer mean 0.1335 0.1700 0.2003 0.2507 0.2646 0.3045 max 0.3805 0.3196 0.2911 0.3318 0.3246 0.3862 Table 3 𝑃𝑉���𝐷��� of the BCSBM model and compared algorithms on attributed networks Datasets PWF Model Value PPSB_DC BNPA NEMBP PSB_PG DPSB_PG BCSBM Cornell mean 0.3789 0.3446 0.3646 0.4378 0.4498 0.4882 max 0.5025 0.3528 0.4722 0.5672 0.6179 0.6637 Texas mean 0.5610 0.5084 0.5622 0.4117 0.4250 0.4852 max 0.6753 0.5257 0.7002 0.5028 0.5408 0.5854 Washington mean 0.4751 0.3851 0.4034 0.4879 0.4829 0.5233 max 0.6118 0.3921 0.6064 0.5483 0.5123 0.6175 Wisconsin mean 0.3900 0.4818 0.3867 0.5290 0.5294 0.5916 max 0.4839 0.4967 0.5484 0.5880 0.5953 0.6501 Cora mean 0.2835 0.4809 0.4337 0.3554 0.3621 0.3846 max 0.3592 0.5423 0.5203 0.4228 0.3891 0.4073 Citeseer mean 0.2733 0.3548 0.3236 0.3561 0.3642 0.4014 max 0.4768 0.3984 0.3804 0.4323 0.3977 0.4318 Note 1: The red bolded values in Table 2 and Table 3 indicate the best, and the black bolded values indicate the next best of the six models. Note 2: The first 3 columns of data are from the literature [30].Fig.3 𝐾���𝐾���𝐼 of the six algorithms on attributed networks Fig.4 𝑃𝑉���𝐷��� of the six algorithms on attributed networks From the above experimental results, it can be seen that the BCSBM model proposed in this paper is suitable for a variety of network structures detection, and the detection effect is significantly improved in the real attribute networks (Cornell, Texas, Washington, Wisconsin) containing disassortative structures. Compared with the PSB_PG model without considering the degree of nodes, the DPSB_PG model has improved the detection effect on attributed networks Cornell, Texas, and Wisconsin, Cornell Texas Washington Wisconsin Cora Citeseer 0.0 0.1 0.2 0.3 0.4 0.5 NMI Datasets PPSB_DC BNPA NEMBP PSB_PG DPSB_PG BCSBM Cornell Texas Washington Wisconsin Cora Citeseer 0.0 0.1 0.2 0.3 0.4 0.5 0.6 PWF Datasets PPSB_DC BNPA NEMBP PSB_PG DPSB_PG BCSBMbut the performance on the Washington is slightly worse. However, the BCSBM model proposed in this paper takes into account the betweenness centrality and clustering coefficient of nodes at the same time, and performs best on the four real attribute networks with disassortative structures. The experiments show that the integration betweenness centrality and clustering coefficient of nodes have a positive impact on the community detection for attributed networks. The BNPA model performs best on the attributed network Cora containing assortative structure, this is due to the fact that it introduces a priori information, which needs to adjust priori parameters, and its detection precision depends on the accuracy of the number of communities estimated, which does not perform well in other networks. The model BCSBM proposed in this paper performs best on the attributed network Citeseer with assortative structure, which indicates that the BCSBM model has good performance in attributed networks containing disassortative and assortative structures. In summary, the comprehensive performance of the BCSBM model proposed in this paper is better than the other five related algorithms, and the experiments show that the stochastic block model integrating betweenness centrality and clustering coefficient of nodes, as well as the Poisson distribution can better identify the assortative and disassortative structures in attributed networks. 6. Conclusion and discussion Integrating the linking relationships between nodes and the inherent attribute information of the nodes to mine the potential structure in network, and utilizing the attribute information to enhance the interpretability of the identified community, and then revealing the function of the network system is gradually being paid attention to. Based on the DPSB_PG model, the stochastic block model BCSBM that integrating betweenness centrality and clustering coefficient of nodes in attributed networks is proposed in this paper. The BCSBM model combines the network topology information and attribute information, and improves the accuracy of community detection by fitting the real network from the perspectives of node importance and node neighborhood. The uniform form of Poisson distribution facilitates the estimation of model parameters, and the EM algorithm is used to realize the parameters inference to ensure the convergence of the model. By comparing with the existing model on real attribute networks, it can be seen that BCSBM model can discover a variety of structures in networks, and the community detection accuracy is better than the DPSB_PG model, and the performance is improved in different extent compared with other existing related algorithms, which further illustrates the importance of betweenness centrality and clustering coefficient of nodes to improve the accuracy of community detection algorithm. Since the EM algorithm may require a large amount of computation and high time complexity in the process of parameters estimation depending on the network size, it affects the efficiency of the model. Therefore, in future work, other parameter estimation methods can be considered to improve the computational efficiency of the algorithm while ensuring the accuracy of community detection.Acknowledgment This work is supported by the National Natural Science Foundation of China [grant number 61976176]. Appendix A From Eq. (5), we know that the lower bound of the log-likelihood function is 𝐾���̅(𝐷, Θ, Φ) = 1 2 ∑ ∑ [𝑎𝑖𝑖���𝑞𝑖𝑖��� 𝑟𝑟��� ln (𝛾���𝑖𝑎���𝑖𝑟𝜃𝑟𝑟���𝛾���𝑖���𝑎���𝑖���𝑟��� 𝑞𝑖𝑖��� 𝑟𝑟��� ) − 𝛾���𝑖𝑎���𝑖𝑟𝜃𝑟𝑟���𝛾���𝑖���𝑎���𝑖���𝑟���] 𝑎��� 𝑟,𝑟���=1 𝑖��� 𝑖,𝑖���=1 + ∑ ∑ ∑ [𝑥𝑖𝑖���𝛾𝑖𝑖��� 𝑟 ln (𝛾���𝑖𝑎���𝑖𝑟𝜙𝑟𝑖��� 𝛾𝑖𝑖��� 𝑟 ) − 𝛾���𝑖𝑎���𝑖𝑟𝜙𝑟𝑖���] 𝑎��� 𝑟=1 𝐾 𝑖���=1 𝑖��� 𝑖=1 (5) Under 𝑎���𝑖𝑟 = 𝛾���𝑖𝑎���𝑖𝑟 and ∑ 𝑎���𝑖𝑟 = ∑ 𝛾���𝑖𝑎���𝑖𝑟 = 1 𝑖��� 𝑖=1 𝑖��� 𝑖=1 , we have 𝐾���̃(𝐷) = 1 2 ∑ ∑ [𝑎𝑖𝑖���𝑞𝑖𝑖��� 𝑟𝑟��� ln (𝑎���𝑖𝑟𝜃𝑟𝑟���𝛾���𝑖���𝑎���𝑖���𝑟��� 𝑞𝑖𝑖��� 𝑟𝑟��� ) − 𝑎���𝑖𝑟𝜃𝑟𝑟���𝑎���𝑖���𝑟���] 𝑎��� 𝑟,𝑟���=1 𝑖��� 𝑖,𝑖���=1 + ∑ ∑ ∑ [𝑥𝑖𝑖���𝛾𝑖𝑖��� 𝑟 ln (𝑎���𝑖𝑟𝜙𝑟𝑖��� 𝛾𝑖𝑖��� 𝑟 ) − 𝑎���𝑖𝑟𝜙𝑟𝑖���] + ∑ 𝜁𝑟 𝑎��� 𝑟=1 𝑎��� 𝑟=1 𝐾 𝑖���=1 𝑖��� 𝑖=1 (1 − ∑ 𝑎���𝑖𝑟 𝑖��� 𝑖=1 ) (A1) Taking the partial derivative of 𝐾���̃(𝐷), we have 𝜕𝐾���̃(𝐷) 𝜕𝑎���𝑖𝑟 = 1 2 ∑ ∑ 𝑎𝑖𝑖���𝑞𝑖𝑖��� 𝑟𝑟��� 𝑎��� 𝑟���=1 𝑖��� 𝑖���=1 𝑎���𝑖𝑟 − 1 2 ∑ ∑ 𝜃𝑟𝑟���𝑎���𝑖���𝑟��� + ∑ 𝑥𝑖𝑖���𝛾𝑖𝑖��� 𝑟 𝐾 𝑖���=1 𝑎���𝑖𝑟 𝑎��� 𝑟���=1 𝑖��� 𝑖���=1 − ∑ 𝜙𝑟𝑖��� − 𝜁𝑟 𝐾 𝑖���=1 = 1 2 ∑ ∑ 𝑎𝑖𝑖���𝑞𝑖𝑖��� 𝑟𝑟��� 𝑎��� 𝑟���=1 𝑖��� 𝑖���=1 𝑎���𝑖𝑟 − 1 2 ∑ 𝜃𝑟𝑟��� 𝑎��� 𝑟,𝑟���=1 + ∑ 𝑥𝑖𝑖���𝛾𝑖𝑖��� 𝑟 𝐾 𝑖���=1 𝑎���𝑖𝑟 − 1 − 𝜁𝑟 (A2) Let 𝜕𝐾���̃(𝐷) 𝜕𝑎���𝑖𝑟 = 0, then 1 2 ∑ ∑ 𝑎𝑖𝑖���𝑞𝑖𝑖��� 𝑟𝑟��� 𝑎��� 𝑟���=1 𝑖��� 𝑖���=1 𝑎���𝑖𝑟 − 1 2 ∑ 𝜃𝑟𝑟��� 𝑎��� 𝑟,𝑟���=1 + ∑ 𝑥𝑖𝑖���𝛾𝑖𝑖��� 𝑟 𝐾 𝑖���=1 𝑎���𝑖𝑟 − 1 − 𝜁𝑟 = 0 (A3) 1 2 ∑ ∑ 𝑎𝑖𝑖���𝑞𝑖𝑖��� 𝑟𝑟��� 𝑎��� 𝑟���=1 𝑖��� 𝑖,𝑖���=1 − 1 2 ∑ 𝜃𝑟𝑟��� 𝑎��� 𝑟,𝑟���=1 + ∑ ∑ 𝑥𝑖𝑖���𝛾𝑖𝑖��� 𝑟 𝐾 𝑖���=1 𝑖��� 𝑖=1 − 1 − 𝜁𝑟 = 0 (A4) By Eq. (A3), Eq.(A4) and 𝑎���𝑖𝑟 = (𝑖���𝑖 + 𝑎���𝑖 + 𝑎���𝑖)𝑎���𝑖𝑟 , we have 𝑎���𝑖𝑟 in Eq. (8) as follows.𝑎���𝑖𝑟 = ∑ ∑ 𝑎𝑖𝑖���𝑞𝑖𝑖��� 𝑟𝑟��� + 2 × ∑ 𝑥𝑖𝑖���𝛾𝑖𝑖��� 𝑟 𝐾 𝑖���=1 𝑎��� 𝑟���=1 𝑖��� 𝑖���=1 (𝑖���𝑖 + 𝑎���𝑖 + 𝑎���𝑖) × [∑ ∑ 𝑎𝑖𝑖���𝑞𝑖𝑖��� 𝑟𝑟��� + 2 × ∑ ∑ 𝑥𝑖𝑖���𝛾𝑖𝑖��� 𝑟 𝐾 𝑖���=1 𝑖��� 𝑖=1 𝑎��� 𝑟���=1 𝑖��� 𝑖,𝑖���=1 ] Note the constraint ∑ 𝜃𝑟𝑟��� 𝑎��� 𝑟,𝑟���=1 = 1, we have 𝐾���̃(Θ) = 1 2 ∑ ∑ [𝑎𝑖𝑖���𝑞𝑖𝑖��� 𝑟𝑟��� ln (𝛾���𝑖𝑎���𝑖𝑟𝜃𝑟𝑟���𝛾���𝑖���𝑎���𝑖���𝑟��� 𝑞𝑖𝑖��� 𝑟𝑟��� ) − 𝛾���𝑖𝑎���𝑖𝑟𝜃𝑟𝑟���𝛾���𝑖���𝑎���𝑖���𝑟���] 𝑎��� 𝑟,𝑟���=1 𝑖��� 𝑖,𝑖���=1 + ∑ ∑ ∑ [𝑥𝑖𝑖���𝛾𝑖𝑖��� 𝑟 ln (𝛾���𝑖𝑎���𝑖𝑟𝜙𝑟𝑖��� 𝛾𝑖𝑖��� 𝑟 ) − 𝛾���𝑖𝑎���𝑖𝑟𝜙𝑟𝑖���] + 𝜇 𝑎��� 𝑟=1 𝐾 𝑖���=1 𝑖��� 𝑖=1 (1 − ∑ 𝜃𝑟𝑟��� 𝑎��� 𝑟,𝑟���=1 ) (A5) Taking the partial derivative of 𝐾���̃(Θ), we have 𝜕𝐾���̃(Θ) 𝜕𝜃𝑟𝑟��� = 1 2 ∑ 𝑎𝑖𝑖���𝑞𝑖𝑖��� 𝑟𝑟��� 𝑖��� 𝑖,𝑖���=1 𝜃𝑟𝑟��� − 1 2 ∑ 𝛾���𝑖𝑎���𝑖𝑟𝛾���𝑖���𝑎���𝑖���𝑟��� 𝑖��� 𝑖,𝑖���=1 − 𝜇 = 1 2 ∑ 𝑎𝑖𝑖���𝑞𝑖𝑖��� 𝑟𝑟��� 𝑖��� 𝑖,𝑖���=1 𝜃𝑟𝑟��� − 1 2 − 𝜇 (A6) Let 𝜕𝐾���̃(Θ) 𝜕𝜃𝑟𝑟��� = 0, then 1 2 ∑ 𝑎𝑖𝑖���𝑞𝑖𝑖��� 𝑟𝑟��� 𝑖��� 𝑖,𝑖���=1 𝜃𝑟𝑟��� − 1 2 − 𝜇 = 0 (A7) ∑ ∑ 𝑎𝑖𝑖���𝑞𝑖𝑖��� 𝑟𝑟��� 𝑎��� 𝑟,𝑟���=1 𝑖��� 𝑖.𝑖���=1 − 1 − 2𝜇 = 0 (A8) By Eq. (A7) and Eq.(A8), we can derive the equation 𝜃𝑟𝑟��� in Eq. (9) as follows. 𝜃𝑟𝑟��� = ∑ 𝑎𝑖𝑖���𝑞𝑖𝑖��� 𝑟𝑟��� 𝑖��� 𝑖,𝑖���=1 ∑ ∑ 𝑎𝑖𝑖���𝑞𝑖𝑖��� 𝑟𝑟��� 𝑎��� 𝑟,𝑟���=1 𝑖��� 𝑖,𝑖���=1 Similarly, for 𝜙𝑟𝑖���, we have 𝐾���̃(Φ) = 1 2 ∑ ∑ [𝑎𝑖𝑖���𝑞𝑖𝑖��� 𝑟𝑟��� ln (𝛾���𝑖𝑎���𝑖𝑟𝜃𝑟𝑟���𝛾���𝑖���𝑎���𝑖���𝑟��� 𝑞𝑖𝑖��� 𝑟𝑟��� ) − 𝛾���𝑖𝑎���𝑖𝑟𝜃𝑟𝑟���𝛾���𝑖���𝑎���𝑖���𝑟���] 𝑎��� 𝑟,𝑟���=1 𝑖��� 𝑖,𝑖���=1 + ∑ ∑ ∑ [𝑥𝑖𝑖���𝛾𝑖𝑖��� 𝑟 ln (𝛾���𝑖𝑎���𝑖𝑟𝜙𝑟𝑖��� 𝛾𝑖𝑖��� 𝑟 ) − 𝛾���𝑖𝑎���𝑖𝑟𝜙𝑟𝑖���] + ∑ 𝜉𝑟 𝑎��� 𝑟=1 𝑎��� 𝑟=1 𝐾 𝑖���=1 𝑖��� 𝑖=1 (1 − ∑ 𝜙𝑟𝑖��� 𝐾 𝑖���=1 ) (A9)𝜕𝐾���̃(Φ) 𝜕𝜙𝑟𝑖��� = ∑ 𝑥𝑖𝑖���𝛾𝑖𝑖��� 𝑟 𝑖��� 𝑖=1 𝜙𝑟𝑖��� − ∑ 𝛾���𝑖𝑎���𝑖𝑟 𝑖��� 𝑖=1 − 𝜉𝑟 = ∑ 𝑥𝑖𝑖���𝛾𝑖𝑖��� 𝑟 𝑖��� 𝑖=1 𝜙𝑟𝑖��� − 1 − 𝜉𝑟 (A10) Let 𝜕𝐾���̃(Φ) 𝜕∅𝑟𝑖��� = 0, then ∑ 𝑥𝑖𝑖���𝛾𝑖𝑖��� 𝑟 𝑖��� 𝑖=1 𝜙𝑟𝑖��� − 1 − 𝜉𝑟 = 0 (A11) ∑ ∑ 𝑥𝑖𝑖���𝛾𝑖𝑖��� 𝑟 𝐾 𝑖���=1 𝑖��� 𝑖=1 − 1 − 𝜉𝑟 = 0 (A12) By Eq. (A11) and Eq.(A12), we have 𝜙𝑟𝑖��� in Eq. (9) in the following. 𝜙𝑟𝑖��� = ∑ 𝑥𝑖𝑖���𝛾𝑖𝑖��� 𝑟 𝑖��� 𝑖=1 ∑ ∑ 𝑥𝑖𝑖���𝛾𝑖𝑖��� 𝑟 𝐾 𝑖���=1 𝑖��� 𝑖=1 References [1] Bothorel C, Cruz J D, Magnani M, et al. Clustering attributed graphs: models, measures and methods[J]. Network Science, 2015, 3(3): 408-444. [2] Khan K U, Nawaz W, Lee Y K. Set-based unified approach for summarization of a multi- attributed graph[J]. World Wide Web, 2017, 20: 543-570. [3] Wang Y, Li Y, Fan J, et al. A survey of typical attributed graph queries[J]. World Wide Web, 2021, 24: 297-346. [4] Zhou Y, Cheng H, Yu J X. Graph clustering based on structural/attribute similarities[J]. Proceedings of the VLDB Endowment, 2009, 2(1): 718-729. [5] Chen X, Peng H, Hu J. K-medoids substitution clustering method and a new clustering validity index method[C]. Sixth World Congress on Intelligent Control and Automation. IEEE, 2006, 2: 5896-5900. [6] Z. Wu, L. Zhao, S.-Y. Ho. Community Detection with Topological Structure and Attributesin Information Networks[J]. ACM Transactions on Intelligent Systems and Technology, 2017,8(2):1–17. [7] Alinezhad E, Teimourpour B, Sepehri M M, et al. Community detection in attributed networks considering both structural and attribute similarities: two mathematical programming approaches[J]. Neural Computing and Applications, 2020, 32: 3203-3220. [8] Falih I, Grozavu N, Kanawati R, et al. ANCA: Attributed network clustering algorithm[C]. International Conference on Complex Networks and their Applications. Springer, Cham, 2017: 241-252. [9] Berahmand K, Haghani S, Rostami M, et al. A new attributed graph clustering by using label propagation in complex networks[J]. Journal of King Saud University-Computer and Information Sciences, 2022, 34(5): 1869-1883. [10] Wang X, Jin D, Cao X, et al. Semantic community identification in large attribute networks[C].Proceedings of the AAAI Conference on Artificial Intelligence. 2016, 30(1). [11] Akoglu L, Tong H, Meeder B, et al. Pics: Parameter-free identification of cohesive subgroups in large attributed graphs[C]. Proceedings of the 2012 SIAM international conference on data mining. Society for Industrial and Applied Mathematics, 2012: 439-450. [12] Li Z, Pan Z, Hu G, et al. Detecting semantic communities in social networks[J]. IEICE TRANSACTIONS on Fundamentals of Electronics, Communications and Computer Sciences, 2017, 100(11): 2507-2512. [13] Huang J, Zhang T, Yu W, et al. Community Detection Based on Modularized Deep Nonnegative Matrix Factorization[J]. International Journal of Pattern Recognition and Artificial Intelligence, 2021, 35(2): 2159006. [14] Yang S, Yang B. Enhanced network embedding with text information[C].24th International Conference on Pattern Recognition (ICPR). IEEE, 2018: 326-331. [15] Newman M E J, Clauset A. Structure and inference in annotated networks[J]. Nature communications, 2016, 7(1): 11863. [16] Fortunato S, Hric D. Community detection in networks: A user guide[J]. Physics reports, 2016, 659: 1-44. [17] Liu X, Yang B, Song W, et al. A block-based generative model for attributed network embedding[J]. World Wide Web, 2021, 24: 1439-1464. [18] Holland P W, Laskey K B, Leinhardt S. Stochastic blockmodels: First steps[J]. Social Networks, 1983, 5(2): 109-137. [19] Chai B, Yu J, Jia C, et al. Combining a popularity-productivity stochastic block model with a discriminative-content model for general structure detection[J]. Physical review E, 2013, 88(1): 012807. [20] Shen H W, Cheng X Q, Guo J F. Exploring the structural regularities in networks[J]. Physical Review E, 2011, 84(5): 056111. [21] Yang T, Chi Y, Zhu S, et al. Directed network community detection: A popularity and productivity link model[C]. Proceedings of the 2010 SIAM international conference on data mining. Society for Industrial and Applied Mathematics, 2010: 742-753. [22] Chang Z, Jia C, Yin X, et al. A generative model for exploring structure regularities in attributed networks[J]. Information Sciences, 2019, 505: 252-264. [23] Chen Y, Wang X, Bu J, et al. Network structure exploration in networks with node attributes[J]. Physica A: Statistical Mechanics and its Applications, 2016, 449: 240-253. [24] Newman M E J, Leicht E A. Mixture models and exploratory analysis in networks[J]. Proceedings of the National Academy of Sciences, 2007, 104(23): 9564-9569. [25] He D, Feng Z, Jin D, et al. Joint identification of network communities and semantics via integrative modeling of network topologies and node contents[C]. Proceedings of the AAAI Conference on Artificial Intelligence. 2017, 31(1). [26] Karrer B, Newman M E J. Stochastic block models and community structure in networks[J]. Physical review E, 2011, 83(1): 016107. [27] Dempster A P, Laird N M, Rubin D B. Maximum likelihood from incomplete data via the EM algorithm[J]. Journal of the royal statistical society: series B (methodological), 1977, 39(1): 1- 22. [28] Decelle A, Krzakala F, Moore C, et al. Inference and phase transitions in the detection ofmodules in sparse networks[J]. Physical Review Letters, 2011, 107(6): 065701. [29] Chen H, Yu Z, Yang Q, et al. Attributed graph clustering with subspace stochastic block model[J]. Information Sciences, 2020, 535: 130-141. [30] Zheng Yimei, Jia Caiyan, Chang Zhenhai, Li Xuanya. A Degree Corrected Stochastic Block Model for Attributed Networks[J]. Journal of Computer Research and Development, 2020, 57(8): 1650-1662. doi: 10.7544/issn1000-1239.2020.20200158 [31] Zhou M, Han Q, Li M, et al. Nearest neighbor walk network embedding for link prediction in complex networks[J]. Physica A: Statistical Mechanics and its Applications, 2023, 620: 128757. [32] Wang Z Y, Han J T, Zhao J. Identifying node spreading influence for tunable clustering coefficient networks[J]. Physica A: Statistical Mechanics and its Applications, 2017, 486: 242- 250. [33] Xuan G, Shi Y Q, Chai P, et al. An enhanced EM algorithm using maximum entropy distribution as initial condition[C]. Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012). IEEE, 2012: 849-852. [34] Sen P, Namata G, Bilgic M, et al. Collective classification in network data[J]. AI magazine, 2008, 29(3): 93-93. [35] Rostami M, Oussalah M. A novel attributed community detection by integration of feature weighting and node centrality[J]. Online Social Networks and Media, 2022, 30: 100219. [36] Namata G, London B, Getoor L, et al. Query-driven active surveying for collective classification[C].10th international workshop on mining and learning with graphs. 2012, 8: 1. [37] Danon L, Diaz-Guilera A, Duch J, et al. Comparing community structure identification[J]. Journal of statistical mechanics: Theory and experiment, 2005(09): P09008. [38] Yang T, Jin R, Chi Y, et al. Combining link and content for community detection: a discriminative approach[C]. Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining. 2009: 927-936. [39] Liu W, Chang Z, Jia C, et al. A generative node-attribute network model for detecting generalized structure and semantics[J]. Physica A: Statistical Mechanics and its Applications, 2022, 588: 126557.
Calibrated Explanations for Regression Tuwe L¨ofstr¨om1*, Helena L¨ofstr¨om2,3, Ulf Johansson1, Cecilia S¨onstr¨od1, Rudy Matela1 1*Department of Computing, J¨onk¨oping University, Box 1026, J¨onk¨oping, 55111, Sweden. 2J¨onk¨oping International Business School, J¨onk¨oping University, Box 1026, J¨onk¨oping, 551 11, Sweden. 3Department of Information Technology, University of Bor˚as, Bor˚as, 501 90, Sweden. *Corresponding author(s). E-mail(s): tuwe.lofstrom@ju.se; Contributing authors: helena.lofstrom@ju.se; ulf.johansson@ju.se; cecilia.sonstrod@ju.se; rudy.matela@ju.se; Abstract Artificial Intelligence (AI) is often an integral part of modern decision support sys- tems (DSSs). The best-performing predictive models used in AI-based DSSs lack transparency. Explainable Artificial Intelligence (XAI) aims to create AI systems that can explain their rationale to human users. Local explanations in XAI can provide information about the causes of individual predictions in terms of feature importance. However, a critical drawback of existing local explanation methods is their inability to quantify the uncertainty associated with a feature’s impor- tance. This paper introduces an extension of a feature importance explanation method, Calibrated Explanations (CE), previously only supporting classifica- tion, with support for standard regression and probabilistic regression, i.e., the probability that the target is above an arbitrary threshold. The extension for regression keeps all the benefits of CE, such as calibration of the prediction from the underlying model with confidence intervals, uncertainty quantification of fea- ture importance, and allows both factual and counterfactual explanations. CE for standard regression provides fast, reliable, stable, and robust explanations. CE for probabilistic regression provides an entirely new way of creating probabilistic explanations from any ordinary regression model and with a dynamic selection of thresholds. The performance of CE for probabilistic regression regarding stabil- ity and speed is comparable to LIME. The method is model agnostic with easily understood conditional rules. An implementation in Python is freely available 1 arXiv:2308.16245v2 [cs.LG] 1 Sep 2023on GitHub and for installation using pip making the results in this paper easily replicable. Keywords: Explainable AI, Feature Importance, Calibrated Explanations, Conformal Predictive Systems, Uncertainty Quantification, Regression, Probabilistic Regression, Counterfactual Explanations 1 Introduction In recent times, Decision Support Systems (DSSs) in various domains such as retail, sport, or defence have been incorporating Artificial Intelligence (AI) extensively [1]. However, the predictive models used in AI-based DSSs generally lack transparency and only provide probable results [2, 3]. This can result in misuse (when users rely on it excessively) or disuse (when users do not rely on it enough) [4, 5]. The lack of transparency has led to the development of explainable artificial intel- ligence (XAI), which aims to create AI systems capable of explaining their reasoning to human users. The goal of explanations is to support users in identifying incorrect predictions, especially in critical areas such as medical diagnosis [6]. An explanation provided by XAI should highlight the underlying model’s strengths and weaknesses and predict how it will perform in the future [2, 7]. Regarding explanations in XAI, there are two types: local and global. Local explanations focus on the reasons behind individual predictions, while global expla- nations provide information about the entire model [8–10]. Despite the apparent strength stemming from the possibility of providing explanations for each instance, local explanations typically have some drawbacks. For example, they can lack robust- ness, meaning that minor differences in the instance can lead to significantly different explanations, or be instable, meaning that the same model and instance may result in different explanations [11, 12]. Lack of robustness and instability create issues when evaluating the quality of the explanations. Metrics like fidelity, which measure how well an explanation captures the behaviour of the underlying model, do not give an accurate picture of explanation quality since they depend heavily on the details of the explanation method [9, 11, 13–18]. Furthermore, even the best explanation tech- niques offer limited insight into model uncertainty and reliability. Recent research has emphasized uncertainty estimation’s role in enhancing the transparency of underlying models [11, 19]. Although achieving well-calibrated uncertainty has been underscored as a critical factor in fostering transparent decision-making, [19] points out the chal- lenges and complexities of obtaining accurately calibrated uncertainty estimates for complex problems. Moreover, as indicated by [11], the focus has predominantly leaned towards adopting a well-calibrated underlying model (such as Bayesian) rather than relying on calibration techniques. The probability estimate that most classifiers output is commonly used as an indi- cator of the likelihood of each class in local explanation methods for classification. However, it is widely recognized that these classifiers are often poorly calibrated, 2resulting in probability estimates that do not faithfully represent the actual proba- bility of correctness [20]. Specialized calibration techniques such as Platt Scaling [21] and Venn-Abers (VA) [22] have been proposed to tackle these shortcomings. The VA method generates a probability range associated with each prediction, which can be refined into a properly calibrated probability estimate utilizing regularisation. When employing the VA approach for decision-making, it is essential to recognize that the technique provides intervals for individual classes. These intervals quantify the uncertainty within the probability estimate, offering valuable insights from an explana- tory standpoint. The breadth of the interval directly corresponds to the model’s level of uncertainty, with a narrower interval signifying heightened confidence in the probabil- ity estimate. In comparison, a broader interval indicates more substantial uncertainty in said estimates. Furthermore, this uncertainty information can be extended to the features, given that the feature weights are informed by the prediction’s probability estimate. Being able to quantify the uncertainty can improve the quality and usefulness of explanations in XAI. Recently, a local explanation method, Calibrated Explana- tions (CE), utilizing the intervals provided by VA to estimate feature uncertainty was introduced for classification [23]. Existing explanation methods most commonly focus on explaining decisions from classifiers, despite the fact that regression is widely used in highly critical situations. Due to the lack of specialized explanation techniques for regression, applying methods designed for classification on regression problems is not unusual, highlighting the need for well-founded explanations methods for regression [24]. The aim of this study is to propose an explanation method - with the same pos- sibility of quantifying uncertainty of feature weights as VA, through CE, provides for classification - for a regression context. The conformal prediction framework [25] pro- vides several different techniques for quantifying uncertainty in a regression context. In this paper, the Conformal Predictive Systems (CPSs) technique [26] for uncertainty estimation is used in CE to allow creation of calibrated explanations with uncertainty estimation for regression. Using CPS is not only a very flexible technique, providing a rich toolbox to be used for uncertainty quantification, but it also allows for estimat- ing the probability that the target is above any user-defined threshold. Based on this, a new form of probabilistic explanation for regression is also proposed in this paper. These approaches are user-friendly and model-agnostic, making them easy to use and applicable to diverse underlying models. In summary, this paper introduces extensions of CE aimed at regression, with the following characteristics: • Fast, reliable, stable and robust feature importance explanations for regression. • Calibration of the predictions from the underlying model through the application of CPSs. • Arbitrary forms of uncertainty quantification of the predictions from the underly- ing model and the feature importance weights through querying of the conformal predictive distribution (CPD) derived from the CPS. • Possibility of creating explanations on the probability of the prediction exceeding a user-defined threshold. 3• Rules with straightforward interpretation in relation to the feature values and the target. • Possibility to generate counterfactual rules with uncertainty quantification of the expected predictions (or probability of exceeding a threshold). 2 Background 2.1 Post-Hoc Explanation Methods The research area of XAI research can be broadly categorised into two main types: developing inherently interpretable and transparent models and utilising post-hoc methods to explain opaque models. Post-hoc explanation techniques seek to construct simplified and interpretable models that reveal the relationship between feature values and the model’s predictions. These explanations, which can be either local or global, often leverage visual aids such as pixel representations, feature importance plots, or word clouds. These visuals emphasise the features, pixels, or words accountable for causing the model’s predictions [9, 27]. Two distinct approaches of explanations exist: factual explanations, where a fea- ture value directly influences the prediction outcome, and counterfactual explanations, which explore the potential impact on predictions when altering a feature’s values [28–30]. Importantly, counterfactual explanations are intrinsically local. They are particularly human-friendly, mirroring how human reasoning operates [27]. 2.2 Essential Characteristics of Explanations Creating high-quality explanations in XAI requires a multidisciplinary approach that draws knowledge from both Human-Computer Interaction (HCI) and Machine Learn- ing (ML) fields. The quality of an explanation method depends on the goals it addresses, which may vary. For instance, assessing how users appreciate the expla- nation interface differs from evaluating if the explanation accurately mirrors the underlying model [31]. However, specific characteristics are universally desirable for post-hoc explanation methods. It is crucial that an explanation method accurately reflects the underlying model, which is closely related to the concept that an explana- tion method should have a high level of fidelity to the underlying model [11]. Therefore, a reliable explanation must have feature weights that correspond accurately to the actual impact on the estimates to correctly reflect the model’s behaviour. In other words, it should be well-calibrated [19]. Stability and robustness are two additional critical features of explanation meth- ods [7, 18, 32]. Stability refers to the consistency of the explanations [11, 14]; the same instance and model should produce identical explanations across multiple runs. On the other hand, robustness refers to the ability of an explanation method to pro- duce consistent results even when an instance undergoes small perturbations [7] or other circumstances change. Therefore, the essential characteristics of an explanation method in XAI are that it should be reliable, stable, and robust. 42.3 Explanations for classification and regression Distinguishing between explanations for classification and regression lies in the nature of the insights they offer. In classification, the task involves predicting the specific class an instance belongs to from a set of predefined classes. The accompanying probabil- ity estimates reflect the model’s confidence level for each class. Various explanation techniques have been developed for classifiers to clarify the rationale behind the class predictions. Notable methods include SHAP [33], LIME [34], and Anchor [35]. These techniques delve into the factors that contribute to the assignment of a particular class label. Typically, the explanations leverage the concept of feature importance, e.g., words in textual data or pixels in images. In regression, the paradigm shifts as there are no predetermined classes or cate- gorical values. Instead, each instance is associated with a numerical value, and the prediction strives to approximate this value. Consequently, explanations for regression models cannot rely on the framework of predefined classes. Nevertheless, explana- tion techniques designed for classifiers, as mentioned above, can often be applied to regression problems, provided these methods concentrate on attributing features to the predicted instance’s output. 2.4 Calibrated Explanations for Classification (CEC) Below is an introduction to CEC [23], which provides the foundation upon which this paper is contributing1. In the following descriptions, a factual explanation is composed of a calibrated prediction from the underlying model accompanied by an uncertainty interval and a collection of factual feature rules, each composed of a feature weight with an uncertainty interval and a factual condition, covering that feature’s instance value. Counterfactual explanations only contain a collection of counterfactual feature rules, each composed of a prediction estimate with an uncertainty interval and a counter- factual condition, covering alternative instance values for the feature. The prediction estimate represents a probability estimate for classification, whereas for regression, the prediction estimate will be expressed as a potential prediction. 2.4.1 Venn-Abers predictors Probabilistic predictors offer class labels and associated probability distributions. Val- idating these predictions is challenging, but calibration focuses on aligning predicted and observed probabilities [25]. The goal is well-calibrated models where predicted probabilities match actual accuracy. Venn predictors [36] produce multi-probabilistic predictions, converted to confidence-based probability intervals. Inductive Venn prediction [37] involves a Venn taxonomy, categorizing calibration data for probability estimation. Within each category, the estimated probability for test instances falling into a category is the relative frequency of each class label among all calibration instances in that category. Venn-Abers predictors (VA) [22] offer automated taxonomy optimization via iso- tonic regression, thus introducing dynamic probability intervals. A two-class scoring 1The Python implementation can be accessed at github.com/Moffran/calibrated explanations or installed through: pip install calibrated-explanations 5classifier assigns a prediction score s(xi) to a test object xi. A higher score implies higher belief in the positive class. In order to calibrate a model, some data must be set aside and used as a calibration set. Consequently, split the training set {z1, . . . , zi, . . . , zn}, with objects xi and labels yi, into a proper training set ZT and a calibration set {z1, . . . , zl}. Train a scoring classifier on ZT to compute s for {x1, . . . , xl, xn+1}. Inductive VA prediction follows these steps: 1. Derive isotonic calibrators g0 and g1 using {{s1, y1}, . . . , {sl, yl}, {sn+1, yn+1 = 0}} and {{s1, y1}, . . . , {sl, yl}, {sn+1, yn+1 = 1}}, respectively. 2. The probability interval for yn+1 = 1 is [g0(sn+1), g1(sn+1)] (henceforth referred to as [Pl, Ph], representing the lower and higher bounds of the interval). 3. Obtain a regularized probability estimate for yn+1 = 1 using the recommendation by [22]: P = Ph 1 − Pl + Ph In summary, VA produce a calibrated (regularized) probability estimate P together with a probability interval with a lower and upper bound [Pl, Ph]. 2.4.2 Factual Explanations for Classification Assuming we have a scoring classifier trained with the appropriate training set ZT , and we want to generate a local explanation for a test instance xn+1. We classify the set of features F into two categories: categorical features C and numerical features N. Let Vf denote all feature values for a feature f ∈ F let and fv denote the index of value v for feature f. The feature value held by the test instance xn+1 for a particular feature is denoted f ′ v. Using VA as calibrator, producing a probability interval [Pl, Ph] and a calibrated probability estimate P for the test instance xn+1, the explanation process follows these steps: 1. Define a discretizer for numerical features that sets thresholds and conditions (≤ , >) for features in N. LIME discretizers or their sub-classes described below are used as discretizers. For categorical features, rules are based on identity conditions (=). 2. For each feature f ∈ F: • If f ∈ C a) Iterate through all possible categorical values v ∈ Vf. Create one per- turbed instance per feature value v except the original value by replacing the original value xf n+1, resulting in a perturbed instance xfv n+1 = v. Here, fv denotes the index of value v for feature f. b) Calculate and record the probability intervals [Pfv l , Pfv h ] and the cali- brated probability estimate Pfv for the perturbed instances. c) Form a factual condition covering the instance value v as f = v. 6• If f ∈ N a) Utilize the discretizer’s thresholds to identify the nearest lower or upper threshold around the feature value xf n+1. Divide all possible feature val- ues in the calibration set for feature f into two groups Vf, separated by the lower or upper condition thresholda. b) Extract the 25th, 50th, and 75th percentiles within each group to get percentile values pv. c) For each group, iterate over the percentile values pv and create a per- turbed instance by substituting the feature value xf n+1 with one value fpv at a time, yielding the perturbed instance xfpv n+1 = pv. Apply the calibrator to the perturbed instance and record the probability intervals [Pfpv l , Pfpv h ] and the calibrated probability estimate Pfpv. d) Before proceeding to the next group, calculate an average over all percentile values within the group. This yields a probability interval [Pfv l , Pfv h ] and a calibrated probability estimate Pfv for each group. e) Let f ′ v denote the index of the group containing the feature value xf n+1 from the test instance. f) Form a factual condition covering the instance value v based on the Discretizer used. The rule will be either f ≤ v or f > v. Finalize Step 2: Form a feature rule for feature f composed of a factual condition covering the instance value and the feature weight. Calculate the feature weight (and interval weights) for feature f as the difference between P and the average of all Pfv (and [Pfv l , Pfv h ]) except for index f ′ v. This is because P = Pf ′ v. The weights for the calibrated prediction and the lower and upper bounds are computed as follows: wf = P − 1 |Vf| − 1 � ∀fv\f ′v Pfv, (1) wf l = P − 1 |Vf| − 1 � ∀fv\f ′v Pfv l , (2) wf h = P − 1 |Vf| − 1 � ∀fv\f ′ v Pfv h . (3) aWhen creating counterfactual rules, both a lower and an upper condition threshold may be used to enable counterfactual rules representing the possibility of both smaller and larger values. Since feature weights are derived from calibrated probabilities, they would lose clarity if feature rules allowed interval formats like 0 < feature f ≤ 2. The reason is that probabilities for values below the interval (feature f < 0) may differ significantly from probabilities for values above the interval (feature f > 2), making averages hard to interpret. Hence, the discretizer is typically binary for normal use of CE. Two binary discretizers are implemented to complement the discretizers existing in LIME: 7• A simple binary discretizer (BinaryDiscretizer) that uses the median of all calibration set values for any numerical feature. It’s like QuartileDiscretizer and DecileDiscretizer in LIME, but uses only the median. • A binary entropy discretizer (BinaryEntropyDiscretizer) is similar to LIME’s EntropyDiscretizer, except it employs a decision tree with depth limited to 1, forcing a binary split based on a threshold from the calibration set. When using a binary discretizer on numeric features, two groups are formed with one representing f ′ v. This simplifies equation (1) to wf = P − P¬f ′ v, along with corresponding adjustments to equations (2) and (3). Each individual rule only conveys the contribution of an individual feature. To counteract this shortcoming, conjoined rules can be derived to estimate the joint contribution between combinations of features. This is done separately from the generation of the feature rules. 2.4.3 Counterfactual Explanations for Classification Using the CE definition above, generating counterfactual rules becomes straight- forward. When employing Counterfactual Calibrated Explanations for classification (CCEC), it is advisable to use non-binary discretizers for numeric features. Thus, for- mation of both ≤-rules and >-rules will be allowed. For categorical features, one rule per alternative categorical value will be formed. The EntropyDiscretizer in LIME is the recommended choice for CCEC. Each feature rule’s expected probability inter- val is already established as [Pfv l , Pfv h ], following the CE process in step 2, defining one feature rule for each alternative instance value. The condition will be similar as in Step 2, but for the alternative instance value v. Equation (1)’s feature weights are mainly employed to sort counterfactual rules by impact. The calibrated probability estimate Pfv is normally neglected in counterfactual rules for classification. 3 Calibrated Explanations for Regression The basic idea in CEC is that each factual and counterfactual explanation is derived using three calibrated values: The calibrated probability and the probability interval represented by the lower and upper bound. For regression, there are two natural use cases that are commonly occurring. The most obvious is predicting the continues target value directly, i.e., standard regression, and another common use case is predicting the probability of the target being below (or above) a given threshold, basically viewing the problem as a binary classification problem. CPSs produce CPDs, which are cumulative distribution functions. These distri- butions can be used for various purposes, such as deriving prediction intervals for specified confidence levels or obtaining the probability of the true target falling below or above any threshold. CPSs are extending conformal regression. 83.1 Conformal Regression Conformal predictors (CPs) [25] offer predictive confidence by generating prediction regions, which encompass the true target with a specified probability. These regions are sets of class labels for classification or prediction intervals for regression. Errors arise when the true target falls outside the region, yet CPs are automati- cally valid under exchangeability, yielding an error rate of ϵ over time. Thus, the key evaluation criterion is efficiency, gauged by the region’s size and sharpness for greater insight. Conformal regressors (CRs), specifically an inductive (split) CR, follows these steps: 1. Divide the data into a proper training set ZT and a calibration set {z1, . . . , zl}. 2. Fit an underlying regression model h to ZT . 3. Define nonconformity as the absolute error |yi − h(xi)|. 4. Compute nonconformity scores for {z1, . . . , zl} and sort them in descending order to obtain α1 ≤ α2 ≤ ... ≤ αl. 5. Assign an ϵ, e.g., 0.01, 0.05, or 0.1. 6. Calculate the (1 − ϵ)-percentile nonconformity score, αs, where index s = ⌊ϵ(l + 1)⌋. 7. For a new instance xi, the prediction interval is h(xi) ± αs. To individualize intervals, the normalized nonconformity function augments noncon- formity with σi and β. These adapt intervals based on predicted difficulty σi for each yi. Normalized nonconformity is |yi−h(xi)| σi+β , and the interval is h(xi) ± αs(σi + β). This approach yields individualized prediction intervals, accommodating prediction difficulty and enhancing region informativeness. 3.2 Conformal Predictive Systems The process of creating (normalized) inductive CPSs closely resembles the forma- tion of inductive conformal regressors. The primary distinction lies in calculating nonconformity scores using actual errors, defined as: f (zi) = yi − h (xi) , (4) or normalized errors: f (zi) = yi − h (xi) σi + β , (5) where σi, xi, and β retain their prior definitions. The prediction for a test instance xi (potentially with an estimated difficulty σi) then becomes the following CPD: Q(y) = � i+τ l+1 , if y ∈ � C(i), C(i+1) � , for i ∈ {0, ..., l} i′−1+(i′′−i′+2)τ l+1 , if y = C(i), for i ∈ {1, ..., l} (6) where C(1), . . . , C(l) are obtained from the calibration scores α1, . . . , αl, sorted in increasing order: C(i) = h (x) + αi or, when using normalization: C(i) = h (x) + σαi 9Fig. 1: A CPD with three different intervals representing 90% confidence are defined: Lower-bounded interval: more than the 10th percentile; Two-sided interval: between the 5th and the 95th percentiles; Upper-bounded interval: less than the 90th percentile. The black dotted lines indicate how to determine the probability of the true target being smaller than 0.5, which in this case would be approximately 80%. with C(0) = −∞ and C(l+1) = ∞. τ is sampled from the uniform distribution U(0, 1) and its role is to allow the P-values of target values to be uniformly distributed. i′′ is the highest index such that y = C(i′′), while i′ is the lowest index such that y = C(i′) (in case of ties). For a specific value y, the function returns the estimated probability P(Y ≤ y), where Y is a random variable corresponding to the true target. Given a CPD, a two-sided prediction interval for a chosen significance level ϵ can be obtained by [C⌊(ϵ/2)(l+1)⌋, C⌈(1−ϵ/2)(l+1)⌉]. One-sided prediction intervals can be obtained by [C⌊ϵ(l+1)⌋, ∞] for a lower-bounded interval, and by [−∞, C⌈(1−ϵ)(l+1)⌉] for an upper-bounded interval. Similarly, a point prediction corresponding to the median of the distribution can be obtained by (C⌈0.5(l+1)⌉ + C⌊0.5(l+1)⌋)/2. The median predic- tion can be seen as a calibration of the underlying models prediction. Unless the model is biased, the median will tend to be very close to the prediction of the underlying model. Figure 1 illustrates how the CPD can form one-sided and two-sided confidence intervals. It also illustrates how the probability of the true target falling below a given threshold can be determined. Or, conversely, it illustrates what threshold a specific probability does correspond to. 10Compared to a CR, also able to provide valid confidence intervals from the under- lying model, a CPS offers richer opportunities to define intervals and probabilities through querying of the CPD. There are several different ways that difficulty (σ) can be estimated, such as: • The (Euclidean) distances to the k nearest neighbors. • The standard deviation of the targets of the k nearest neighbors. • The absolute errors of the k nearest neighbors. • The variance of the predictions of the constituent models, in case the underlying model is an ensemble. 3.3 Factual and Counterfactual Explanations for Regression (CER and CCER) In order to get CER, the probability intervals [Pl, Ph] and a calibrated probability estimate P from VA are exchanged for a confidence interval and the median which are derived from the CPD. The confidence interval is defined by user-selected lower and upper percentiles and allows dynamic selection of arbitrary confidence intervals. Thus, for the algorithm to produce factual and counterfactual rules in the same way as for classification, the only thing that needs to be adjusted in the algorithm described in section 2.4.2 is to exchange the calibrator from VA to CPS. Since the confidence interval from CPS is based on the user-provided percentiles, the lower and upper percentiles are two necessary additional parameters. By default, the lower and upper percentiles are [5th, 95th], resulting in a two-sided 90% confidence interval derived from the CPD. One-sided intervals can in practice be handled as a two-sided interval with either −∞ or ∞ assigned as lower or upper percentiles. The calibrated probability estimate used in classification is exchanged for the median from the CPD, which in practice represents a calibration of the underlying model’s prediction, neutralizing any systematic bias in the underlying model. Consequently, using a CPS effectively enables CER with uncertainty quantification of both the prediction from the underlying model and each feature rule. More formally, the confidence interval and the median is derived as follows: 1. Use the calibration set to calculate the calibration residuals ri = yi − h(xi), i ∈ {1, . . . , l}. 2. Fit a ConformalPredictiveSystem model cps using the residuals. 3. Obtain the median and interval values [mn+1, ln+1, hn+1] = cps(h(xn+1), percentiles = [50th, Pth l , Pth h ]) using the 50th, the lower Pth l and the higher Pth h percentiles. 4. To create CER following the procedure described in section 2.4.2 above, substitute P and [Pl, Ph] from the VA calibrator with mn+1 and [ln+1, hn+1] from the CPS. 5. To make the weights have a natural interpretation for regression, with a positive weight indicating that the feature is positively contributing to the prediction, 11equations (1)-(3) are inverted: wf =   1 |Vf| − 1 � ∀fv\f ′ v Pfv   − mn+1, (7) wf l =   1 |Vf| − 1 � ∀fv\f ′v Pfv l   − mn+1, (8) wf h =   1 |Vf| − 1 � ∀fv\f ′v Pfv h   − mn+1. (9) It is important to realize that since the input to the CER differ from CEC, not being probability estimates but instead actual predicted values, the CER will result in feature weights indicating changes in prediction rather than changes in probabilities. If a difficulty estimator is used to get explanations based on normalized CPDs, σ is calculated using DifficultyEstimator in crepes.extras and passed along to cps both when fitting and obtaining median and interval values. A minor difference between classification and regression is related to the dis- cretizers that can be used. As both the BinaryEntropyDiscretizer and the EntropyDiscretizer require categorical target values for the calibration set, the BinaryDiscretizer and the DecileDiscretizer are recommended instead. These are automatically assigned based on the kind of problem and explanation that is extracted. 3.4 Factual and Counterfactual Probabilistic Calibrated Explanations for Regression (PCER and CPCER) The simplest approach when trying to predict the probability that a target value is below (or above) a threshold is to treat the problem as a binary classification problem, with the target defined as ˙yi = � 1 if yi ≤ t 0 if yi > t, (10) where y are the regression targets, t the threshold, and ˙y the binary classification target. To obtain the probability, some form of probabilistic classifier is used. If several different thresholds are of interest, or if each test instance needs a dynamic threshold based on some contextual information, this approach is infeasible. The CPS makes it possible to query any regular regression model for the probability of the target falling below any given threshold. This effectively eliminates the need to treat the problem as a classification problem. Utilizing this strength to create explanations is straightforward if it is only the probability that is of interest. However, achieving a calibrated explanation with uncer- tainty quantification for this scenario is not as straightforward as creating factual and counterfactual explanations for classification or regression. There is no obvious 12equivalent to the probability interval created by VA in classification or the confidence interval derived from a CPS in regression. The fact that probabilistic predictions for regression can be achieved by viewing it as a classification problem holds a key to a solution. VA need a score s for both the calibration and the test instances. By using a CPS as a probabilistic scoring function for both calibration and test instances, it becomes possible to use VA to calibrate the probability and provide a probability interval. The score used is the probability (from a CPD) of calibration and test instances being above the given threshold. The isotonic regressors used by VA also need a binary target for the calibration set, which is defined using equation (10). Since the CPS is defined using the calibration set, the probabilities achieved on the same calibration set will be biased and consequently not be entirely trustworthy. To counteract that, the probability for each calibration instance is achieved by defining a CPS with all other calibration instances. More formally, the scores are derived as follows: 1. Use the calibration set to calculate the calibration residuals R = {r1, . . . , rl} where ri = yi − h(xi), i ∈ {1, . . . , l}. 2. Fit a ConformalPredictiveSystem model cps using the residuals R. 3. Define the score for the test instance xn+1 as sn+1 = cps(xn+1, threshold = t). 4. For each calibration instance xi, for i ∈ {1, ..., l}: a) Use the residuals R\i, i.e., the residuals for all calibration instances except instance i, to fit a ConformalPredictiveSystem model cps\i. b) Calculate the score si = cps\i(xi, t) representing the probability of yi ≤ t. c) Let ˙yi = yi ≤ t represent the categorical target for calibration instance xi 5. Use s as scores and ˙y as targets to define a VA calibrator, producing probability intervals [Pl, Ph] and a calibrated probability estimate P for yn+1 ≤ t and create a calibrated explanation using the description in section 2.4.2. The solution presented above is preferable since it avoids bias when calculating the scores for the calibration set. However, it is also very computationally expensive. A much faster but somewhat biased solution would be to use the same cps to get the scores s for both the calibration and test instances. This solution is used for normalized PCER and CPCER, to achieve reasonable computational performance. The same discretizers as used for CER and CCER (see section 3.3) needs to be applied for PCER and CPCER, as it is motivated by the problem type. Furthermore, if normalized CPSs are to be used, σ is calculated using DifficultyEstimator in crepes.extras and passed along to cps and cps\i both when fitting and obtaining probability scores. 3.5 Summary of Calibrated Explanations With the two solutions proposed here, Calibrated Explanations provide a number of possible use cases, which are summarized in Table 1. The general structure of factual and counterfactual explanations composed of lists of feature rules with conditions and feature weights or feature prediction estimates with confidence intervals (as described in Section 2.4) is general. 13Probabilistic Explanation Characteristics Classification Regression Regression Regular Factual Only prediction CI TI 5TI + 5LI + 5UI 5TI Uncertainty Factual Rule + prediction CI TI 5TI 5TI Counterfactual Only rule CI TI 5TI + 5LI + 5UI 5TI Table 1: Summary of available ways of using CE. CI means confidence intervals, TI means two-sided intervals, LI means lower-bounded one-sided intervals, UI means upper-bounded one-sided intervals, and the multiplying factor (5) indicates the number of alternative ways to use normalization (no normalization + the four types of normalization listed in Section 3.2) 3.6 Quality of Calibrated Explanations for Regression The median from a CPD based on the calibration data can be seen as a form of cali- bration of the underlying model’s prediction, since it may adjust the prediction on the test instance to match what has previously been seen on the calibration set. The cal- ibration will primarily affect systematic bias in the underlying model. Consequently, since CE calibrates the underlying model, it will create calibrated predictions and explanations. In addition, VA provides uncertainty quantification of both the probabil- ity estimates from the underlying model and the feature importance weights through the intervals for PCER. By using equality rules for categorical features and binary rules for numerical features (as recommended above), interpreting the meaning of a rule with a corresponding feature weight in relation to the target and instance value is straightforward and unambiguous and follows the same logic as for classification. The explanations are reliable because the rules straightforwardly define the rela- tionship between the calibrated outcome and the feature weight (for CER and PCER) or feature prediction estimate (for CCER and CPCER). The explanations are robust, i.e., consistent, as long as the feature rules cover any perturbations in feature val- ues. Variation in predictions, e.g. when training using different training sets, can be expected to result in some variation in feature rules, corresponding to the variation in predictions. Obviously, the method does not guarantee robustness for perturbations violating a feature rule condition. The CER and CCER explanations are stable as long as the same calibration set and model are used. Finally, depending on the size of the calibration set which is used to define a CPS, the generation of CER is, in most cases, faster than or at least comparable to existing solutions such as LIME and SHAP. Generating a PCER will be slower than CEC since both require a VA to be trained. Compared to CEC, PCER will have some additional overhead from using a CPS on each calibration instance as well. Finally, the calibrated predictions and their confidence intervals, which are an inte- gral part of factual CE, provide the same guarantees as the calibration model used, i.e., the same guarantees as VA for classification and CPSs for regression (or a combination of both for probabilistic regression). However, even if the uncertainty quantification in the form of intervals for the feature rules are also derived from the same calibration model, these feature rule intervals do not necessarily provide the same guarantees. The reason is that the perturbed instances used in Step 2 in Section 2.4.2 are artificial and the combination of feature values may not always exist naturally in the problem domain. Whenever that happens, the underlying model and the calibration model will 14indicate that it is a strange instance but may not estimate the degree of strangeness correctly as there is no evidence in the data to base a correct estimate on. 4 Method The implementation of both the regression and the probabilistic regres- sion solutions is expanding the calibrated-explanations Python package [23] and relies on the ConformalPredictiveSystem from the crepes pack- age [38]. By default, ConformalPredictiveSystem is used without normaliza- tion but DifficultyEstimator provided by crepes.extras is fully supported by calibrated-explanations, with normalization options corresponding to the list given at the end of Section 3.2. 4.1 Presentation of Calibrated Explanations trough Plots In this paper, three different kinds of plots for CE are presented. The first two are used when visualizing CER. These plots are inspired by LIME, especially the rules in LIME have been seen as providing valuable information in the explanations. • Regular explanations, providing CE without any uncertainty information. These explanations are directly comparable to other feature importance explanation techniques like LIME. • Uncertainty explanations, providing CE including uncertainty intervals to high- light both the importance of a feature and the amount of uncertainty connected with its estimated importance. For the reasons given in previous sections, CE is meant to use binary rules with factual explanations (even if all discretizers used by LIME can also be used by CE). One noteworthy aspect of CE is that the feature weights only show how each feature separately affects the outcome. It is possible to see pairwise combined weights through conjoined conjunctions of features (combining two or three different rules into a con- junctive feature rule). It is important to clarify that the feature weights do not convey the same meaning as in attribution-based explanations, like SHAP. The third kind of plot is a counterfactual plot showing preliminary prediction estimates for each feature when alternative feature values are used. Features rules are always ordered based on feature weight, starting with the most impactful rules. When plotting CE explanations, the user can choose to limit the num- ber of rules to show. Factual explanations have one rule per feature. Counterfactual explanations, where CE creates as many counterfactual rules as possible, may result in a much larger number of rules, especially for categorical with many categories. Internally, CE uses the same representation for both classification and regression. However, the plots visualizing the explanations have been adapted to suit the CER and PCER. 4.1.1 Calibrated Explanations Plots The same kind of plots exists for regression as for classification. Compared to the plots used for classification, the regression plots differ in two essential aspects. 15A common difference for both CER and CCER is that the feature weights represent changes in actual target values. For CER, this means that a feature importance of +100 means that the actual feature value contributes with +100 to the prediction. For a CCER, showing the prediction estimates with uncertainty intervals, the plot shows what the prediction is estimated to have been if the counterfactual condition would be fulfilled. A difference that only applies to the factual plots is that the top of the plot omits the probabilities for the different classes and instead shows the median m and the confidence interval [l, h] as the prediction. 4.1.2 Probabilistic Calibrated Explanations Plots Since the PCER represents feature importances as probabilities, just like CEC. The only difference needed for the plots for PCER compared to classification is to change the probabilities for a class label into probabilities for being below (P(y ≤ t)) or above (P(y > t)) the given threshold. 4.2 Experimental Setup The evaluation is divided into an introduction to CER, CCER, PCER, and CPCER through plots and an evaluation of performance. All plots are from the California Housing data set [39]. The underlying model in all experiments is a RandomForestRegressor from the sklearn package. Our proposed algorithm is claimed to be fast, reliable, stable, and robust. These claims requires validation in an evaluation of performance. The explanations are reli- able due to the validity of the uncertainty estimates used, i.e., the results achieved by querying the CPD, and from the uncertainty quantification of the feature weights or feature prediction estimates. Speed, stability and robustness will be evaluated in an experiment using the California Housing data set on a fixed set of test instances. Each experiment is repeated 100 times using 500 instances as a calibration set and 10 test instances. The target values were normalized, i.e., y ∈ [0, 1]. The following setups are evaluated: • CER: Factual explanation without normalization. • CER Var.: Factual explanation, with normalization based on the variance of the predictions of the constituent models in the underlying random forest regressor. • CCER: Counterfactual explanation without normalization. • PCER: Probabilistic factual explanation without normalization. The threshold is 0.5 for all instances, i.e., the mid-point of the interval of possible target values. • LIME: LIME explanation. • LIME CPS: LIME explanation using the median from a CPD as prediction. The CPS was based on the underlying random forest regressor. • Tree SHAP: SHAP explanation. The TreeExplainer class is used, which is imple- mented in C++ and optimized for tree-based models, such as the underlying random forest regressor. • SHAP CPS: SHAP explanation using the median from a CPD as prediction. The CPS was based on the underlying random forest regressor. Here, the Explainer class was used. 16The evaluated metrics are: • Stability means that multiple runs on the same instance and model should pro- duce consistent results. Stability is evaluated by generating explanations for the same predicted instances a 100 times with different random seeds. The largest variance in feature weight (or feature prediction estimate) can be expected among the most important features (by definition having higher absolute weights). The top feature for each test instance is identified as the feature being most impor- tant most often in the 100 runs (i.e., the mode of the feature ranks defined by the absolute feature weight). The variance for the top feature is measured over the 100 runs and the mean variance among the test instances is reported. • Robustness means that small variations in the input should not result in large variations in the explanations. Robustness is measured in a similar way as stabil- ity, but with the training and calibration set being randomly drawn and a new model being fitted for each run, creating a natural variation in the predictions of the same instances without having to construct artificial instances. Again, the variance of the top feature is used to measure robustness. The same setups as for stability are used except that each run use a new model and calibration set and that the random seed was set to 42 in all experiments. • Computational speed is compared between the setups regarding explanation gen- eration times (in seconds per instance). It was only the method call resulting in an explanation that was measured. Any overhead in initiating the explainer class has not been considered (it is assumed to be negligible). The closest equivalent to PCER would be to apply LIME and SHAP for classification to a thresholded classification model, as described in section 3.4. Since VA is comparably slow and PCER combines both CPSs and VA, with fitting and calls to a CPS for each calibration instance, it can be expected to be slow. 5 Results The results are divided into two parts: 1) a presentation of CE through plots, explain- ing and showcasing a number of different available ways CE can be used and viewed; and 2) an evaluation of performance with comparisons to LIME and SHAP. 5.1 Presentation of Calibrated Explanations through Plots In the following subsections, a number of introductory examples of CE are given for regression. First, factual and counterfactual explanations for regression are shown, followed by factual and counterfactual explanations for probabilistic regression. 5.1.1 Factual Calibrated Explanations for Regression The regular CER plot in Fig. 2 illustrates the calibrated prediction of the underlying model as the solid red line at the top bar together with the 90% confidence interval in light red. As can be seen, the house price is predicted to be ≈$285K and with 90% confidence, the price can be expected to be between [$215K-$370K]. Turning to the feature rules, the solid black line represents the median in the top-bar. The rule condi- tion is shown to the left and the actual instance value is shown to the right of the lower 17plot area. The fact that this house is located more northbound (latitude > 34.26) has a large negative impact on the price (reducing it with ≈$95K). On the other hand, since the median income is a bit higher (median income > 3.52), the price is pressed upwards with about $60K. Housing median age and population are two more features that clearly impact the price negatively. 100000 200000 300000 400000 500000 Prediction interval with 90% confidence Median prediction 100000 80000 60000 40000 20000 0 20000 40000 60000 Feature weights ocean_proximity = <1H OCEAN total_bedrooms > 429.50 total_rooms > 2078.00 households > 409.50 longitude <= -118.53 population > 1138.50 housing_median_age <= 28.00 median_income > 3.52 latitude > 34.26 Rules <1H OCEAN 564.0 3075.0 543.0 -121.98 1633.0 25.0 5.25 37.27 Instance values Fig. 2: A regular CER plot for the California Housing data set. The top-bar illustrates the median (the red line) and a confidence interval (the light red area), defined by the 5th and the 95th percentiles. The subplot below visualizes the weights associated with each feature. The weights indicate how much that rule contributes to the prediction. Negative weights indicate a negative impact on the prediction whereas positive weights indicate a positive impact. When one-sided intervals are used instead, it is only the top-bar that is affected when using a regular CER plots. Figures 3a and 3b illustrate an upper bounded and a lower bounded explanation for the same instance, with the identical feature rule subplot omitted. As can be seen, the median (solid red line) is the same as before, while the confidence interval stretches one entire side of the bar. The upper bound (≈$330K in Fig. 3a) is lower and the lower bound (≈$240K in Fig. 3b) is higher compared to the two-sided CER plot in Fig. 2. Fig. 4 illustrates an uncertainty plot for the same instance as before2. When includ- ing uncertainty quantification in the CER plot, the feature importance has a light colored area corresponding to the span of possible contribution within the confidence used. The grey area surrounding the solid black line represents the same confidence interval as seen in the top bar. As can be seen, the northbound location still has a large negative impact but the span of uncertainty about exactly how large the impact 2Uncertainty plots are not available for one-sided explanations, as the visualization becomes obscured and hard to interpret. However, the one-sided uncertainty interval for each feature rule is calculated and can be accessed and used if needed. 18100000 200000 300000 400000 500000 Prediction interval with 90% confidence Median prediction 100000 80000 60000 40000 20000 0 20000 40000 60000 Feature weights latitude > 34.26 & population > 1138.50 latitude > 34.26 & housing_median_age <= 28.00 population > 1138.50 & median_income > 3.52 longitude <= -118.53 & population > 1138.50 housing_median_age <= 28.00 & median_income > 3.52 latitude > 34.26 & median_income > 3.52 population > 1138.50 housing_median_age <= 28.00 median_income > 3.52 latitude > 34.26 Rules 37.27 1633.0 37.27 25.0 1633.0 5.25 -121.98 1633.0 25.0 5.25 37.27 5.25 1633.0 25.0 5.25 37.27 Instance values (a) Upper bounded explanation 100000 200000 300000 400000 500000 Prediction interval with 90% confidence Median prediction 100000 80000 60000 40000 20000 0 20000 40000 60000 Feature weights ocean_proximity = <1H OCEAN total_bedrooms > 429.50 total_rooms > 2078.00 households > 409.50 longitude <= -118.53 population > 1138.50 housing_median_age <= 28.00 median_income > 3.52 latitude > 34.26 Rules <1H OCEAN 564.0 3075.0 543.0 -121.98 1633.0 25.0 5.25 37.27 Instance values (b) Lower bounded explanation Fig. 3: The top-bars of one-sided CER plot with confidence intervals bounded by the 90th upper percentile (Fig. 3a) and the 10th lower percentile (Fig. 3b). The red solid line represents the median. The weights (and consequently the entire subplot visualizing weights) are the same for these one-sided explanations as in Fig. 2. covers about $150K, falling approximately within the interval [-$180K, -$30K]. The fact that part of the line is solid in color indicates that we can expect this feature to impact the price at least with -$30K, given the selected confidence level. Looking at the other features, we can see that all of them include the median in the uncertainty interval, meaning that with 90% confidence, these features may impact the price in both directions. Obviously, both median income and in particular housing median age are more likely to have a positive and negative impact, respectively. Since no normalization have been used with this example, all the intervals are similar in width. 5.1.2 Counterfactual Calibrated Explanations for Regression Turning to CCER, Fig. 5 shows a CCER plot for the same instance as before. Here, the solid line and the very light area behind it represent the median and the confidence interval of the calibrated prediction of the underlying model (i.e., the same as in Fig. 2). This is the ground truth that all the counterfactual feature rules should be contrasted against. Contrary to CER, none of the rules cover the instance values in the CCER plot. Instead, there are several examples of the same feature being present in multiple rules. Here the interpretation is that the solid line and lighter red bar for each rule is the expected median and confidence interval achieved if the instance would have had values according to the rule. As an example, with everything else the same but median income > 6.28, then the expected price would be ≈$405K with a confidence interval of [$340K, $490K]. It is also clear that if the house would have been located further south (latitude < 36.7), the price would go up, and if it would have been even further north (latitude > 37.6), the price would have gone down even further. So far, all examples have used a standard CPS to construct the explanations (both CER and CCER), with the result that all confidence intervals are almost equal-sized. In Fig. 6, a difficulty estimator based on the standard deviation of the targets of the k nearest neighbors is used. The normalization will both affect the calibration of the underlying model, creating confidence intervals with varying sizes between instances, 19100000 200000 300000 400000 500000 Prediction interval with 90% confidence Median prediction 150000 100000 50000 0 50000 100000 Feature weights ocean_proximity = <1H OCEAN total_bedrooms > 429.50 total_rooms > 2078.00 households > 409.50 longitude <= -118.53 population > 1138.50 housing_median_age <= 28.00 median_income > 3.52 latitude > 34.26 Rules <1H OCEAN 564.0 3075.0 543.0 -121.98 1633.0 25.0 5.25 37.27 Instance values Fig. 4: An uncertainty CER plot for the California Housing data set. The top bar is the same as in Fig. 2, showing the median and the [5th, 95th] percentiles confidence interval. In the subplot below, the uncertainty of the weights is highlighted, using the [5th, 95th] percentiles confidence interval in light red or blue for each feature. The weights still indicate how much that rule contributes to the prediction but with a confidence interval highlighting the span of uncertainty for the impact of the feature value and rule combined. 100000 200000 300000 400000 500000 Prediction interval with 90% confidence housing_median_age < 20.0 ocean_proximity = ISLAND housing_median_age > 25.0 latitude > 37.59 population < 1541.7999999999997 median_income < 5.08154 ocean_proximity = INLAND latitude < 36.734 longitude < -122.02 median_income > 6.27628 Counterfactual rules 25.0 <1H OCEAN 25.0 37.27 1633.0 5.25 <1H OCEAN 37.27 -121.98 5.25 Instance values Fig. 5: A CCER plot for the California Housing data set. The large lightest red area in the background is the confidence interval defined by the 5th and the 95th percentiles. Each row represents a counterfactual rule with an interval in darker red indicating what confidence intervals a breach according to the rule condition would result in. The confidence intervals for the counterfactual rules are also defined by the 5th and the 95th percentiles. The solid lines represent the median values. 20and the feature intervals. A crude assumption regarding the width of the feature inter- vals is that when the calibration set contains fewer instances covering an alternative feature value, the feature intervals will tend to be larger due to less information, and vice versa. This does not have to be the whole truth, as difficulty in this example is defined based on the standard deviation of the neighboring instances target values. As can be seen in Fig. 6, normalized CCER may generate rules resulting in both smaller and wider confidence intervals then the non-normalized rules. 100000 200000 300000 400000 500000 Prediction interval with 90% confidence total_bedrooms > 579.3 ocean_proximity = ISLAND housing_median_age > 25.0 latitude > 37.59 population < 1541.7999999999997 median_income < 5.08154 ocean_proximity = INLAND latitude < 36.734 longitude < -122.02 median_income > 6.27628 Counterfactual rules 564.0 <1H OCEAN 25.0 37.27 1633.0 5.25 <1H OCEAN 37.27 -121.98 5.25 Instance values Fig. 6: A normalized CCER plot comparable to Fig 5, resulting in rules with varied interval widths as a consequence of the normalization. Difficulty is estimated as the standard deviation of the targets of the k nearest neighbors. Similarly to CER, CCER can also be one-sided. Fig. 7 shows an upper-bounded explanation with 90% confidence. The interpretation of the first rule is that, with everything else as before, but median income > 6.28 the price will be below ≈$450K with 90% certainty. Since the same CPS is used, the median is still the same as for a two-sided explanation. 21100000 200000 300000 400000 500000 Prediction interval with 90% confidence housing_median_age < 20.0 ocean_proximity = ISLAND housing_median_age > 25.0 latitude > 37.59 population < 1541.7999999999997 median_income < 5.08154 ocean_proximity = INLAND latitude < 36.734 longitude < -122.02 median_income > 6.27628 Counterfactual rules 25.0 <1H OCEAN 25.0 37.27 1633.0 5.25 <1H OCEAN 37.27 -121.98 5.25 Instance values Fig. 7: A one-sided CCER plot for the California Housing data set. Confidence inter- vals are defined by the 90th upper percentile only. The interpretation is that with 90% certainty, the true value of the original instance will fall within the lightest red area. If the counterfactual rule had been true for each feature individually, the true value will fall within that feature’s darker red area with approximately 90% certainty. 5.1.3 Factual Probabilistic Calibrated Explanations for Regression P(y<=250000.00) 0.0 0.2 0.4 0.6 0.8 1.0 Probability P(y>250000.00) 0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Feature weights total_bedrooms > 429.50 households > 409.50 population > 1138.50 housing_median_age <= 28.00 latitude > 34.26 longitude <= -118.53 ocean_proximity = <1H OCEAN total_rooms > 2078.00 median_income > 3.52 Rules 564.0 543.0 1633.0 25.0 37.27 -121.98 <1H OCEAN 3075.0 5.25 Instance values Fig. 8: A regular PCER plot for the California Housing data set. The plot shows the probability of the prediction for this instance being above the given threshold ($250K in this case). The explanation is similar to a regular CEC plot with the main difference being that it shows the probabilities of being below or above the threshold and that the probabilities are given by the CPD. 22Fig. 8 shows a regular PCER plot for the same instance as above. In this plot, the possibility of querying the CPD about the probability of being below or above a given threshold is utilized. In this case, the threshold is set to a house price of $250K. Here, median income > 3.52 contributes strongly to the probability that the target is above $250K. P(y<=250000.00) 0.0 0.2 0.4 0.6 0.8 1.0 Probability P(y>250000.00) 0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Feature weights total_bedrooms > 429.50 households > 409.50 population > 1138.50 housing_median_age <= 28.00 latitude > 34.26 longitude <= -118.53 ocean_proximity = <1H OCEAN total_rooms > 2078.00 median_income > 3.52 Rules 564.0 543.0 1633.0 25.0 37.27 -121.98 <1H OCEAN 3075.0 5.25 Instance values Fig. 9: An uncertainty PCER for the same explanation as in Fig. 8. The plot includes uncertainties for the feature weights. In Fig. 9, the same explanation is shown with uncertainties. As can be seen, the size of the uncertainty varies a lot between features, depending on the calibration of the VA calibrator. 5.1.4 Counterfactual Probabilistic Calibrated Explanations for Regression Fig. 10 shows a normalized CPCER plot for the same instance. In this case, the normalization used was based on the variance of the predictions of the trees in the random forest. The most influential rule relates to median income, with a lower income increasing the probability for a lower price. The normalization will affect the feature probability estimates and confidence intervals and may consequently also result in a different ordering of rules. The final example, shown in Fig. 11, illustrates both conjunctive rules, combining two feature conditions in one rule, and normalization using the variance of the pre- dictions of the trees in the random forest. Here, the number of rules to plot has been increased to 15. Here we see that conjunctive rules often result in more influential rules than single condition rules, illustrated by the majority of rules being conjunctive. 230.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Probability of target being below 250000.00 median_income > 6.27628 ocean_proximity = NEAR BAY population > 1847.2 total_rooms < 2790.5999999999995 latitude < 36.734 housing_median_age < 20.0 longitude > -121.326 latitude > 37.59 ocean_proximity = INLAND median_income < 5.08154 Counterfactual rules 5.25 <1H OCEAN 1633.0 3075.0 37.27 25.0 -121.98 37.27 <1H OCEAN 5.25 Instance values Fig. 10: A normalized CPCER plot for the same instance as before. . 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Probability of target being below 250000.00 longitude < -122.02 & median_income < 5.08154 latitude < 36.734 & housing_median_age < 20.0 latitude < 36.734 & median_income < 5.08154 longitude > -121.326 latitude < 36.734 & ocean_proximity = INLAND latitude > 37.59 longitude < -122.02 & ocean_proximity = INLAND total_bedrooms < 487.79999999999995 & median_income < 5.08154 ocean_proximity = INLAND housing_median_age < 20.0 & ocean_proximity = INLAND total_rooms < 2790.5999999999995 & ocean_proximity = INLAND median_income < 5.08154 total_rooms < 2790.5999999999995 & median_income < 5.08154 total_bedrooms < 487.79999999999995 & ocean_proximity = INLAND housing_median_age < 20.0 & median_income < 5.08154 Counterfactual rules -121.98 5.25 37.27 25.0 37.27 5.25 -121.98 37.27 <1H OCEAN 37.27 -121.98 <1H OCEAN 564.0 5.25 <1H OCEAN 25.0 <1H OCEAN 3075.0 <1H OCEAN 5.25 3075.0 5.25 564.0 <1H OCEAN 25.0 5.25 Instance values Fig. 11: A normalized CPCER plot with conjunctive rules for the same instance as before. . Factual or counterfactual rules can be generated without normalization or with any of the normalization options available in DifficultEstimator in crepes.extras. 24Conjunctive rules can be added at any time after the explanations are generated. All the examples shown here are from the same instance and the same underlying model, to showcase a subset of available ways the proposed solutions can be used. Further examples can be found in the code repository. 5.2 Performance Evaluation Table 2 shows the results achieved regarding stability, robustness, and computational speed. Stability is measured using the mean variance when constructing explanations on the same instance using different random seeds, with lower values representing more stability. It is evident that both SHAP setups and all CE setups but PCER must be considered stable, since the mean variance is 0 (i.e., less than 1e − 31). LIME and PCER, on the other hand, has a non-negligible mean variance, meaning that they are not, in comparison, as stable. The reason for why PCER is less stable is related to the sensibility of the probabilities derived from the CPD. The reason for the sensibility is that a relatively small change in prediction can easily result in a comparably much larger change in probability for exceeding the threshold, especially if the target is close to the threshold (which is set to 0.5, i.e., the mid-point in the interval of possible target values). Explanations using the median from a CPD and explanations using the underlying model result in similar stability levels. Robustness is measured in a similar way as stability, but with a new model trained using different distributions of training and calibration instances between each run. The results achieved on robustness should be seen in relation to the variance in pre- dictions from the underlying model on the same instances. The reason is that if the predictions that the explanations are based on fluctuate, then we can expect a some- what similar degree of fluctuation in the feature weights as well, since they are defined using the predictions. The mean prediction variance is 4.7e−2. Even if all the CE setups have higher mean variance compared to LIME and SHAP (i.e., are being less robust), it is still lower compared to the mean prediction variance. Furthermore, the explana- tions produced by the CE setups do not only rely on the crisp feature weight used to measure the mean variance but also include the uncertainty interval, highlighting the degree of uncertainty associated with each feature weight. CER CER CCER PCER LIME LIME Tree SHAP Metric Var. CPS SHAP CPS Stability 0 0 0 1.9e-5 3.9e-5 3.2e-5 0 0 Robustness 4.2e-4 3.7e-4 1.9e-3 3.9e-3 8.2e-5 8.6e-5 1.4e-4 1.4e-4 Speed 0.37 0.66 0.51 4.20 3.08 3.11 0.06 0.56 Table 2: Evaluation of stability, robustness and computational speed Regarding the computational speed, it should come as no surprise that Tree SHAP (TreeExplainer), implemented in C++, is fast when applied to a tree-based model like the random forest regressor. It is about 9 times faster than when SHAP (Explainer) is applied on the median from CPD. In comparison, CER is slightly faster than SHAP CPS but clearly slower than Tree SHAP. CER is in turn about 8 times faster than LIME which takes on average around 3 seconds per instance. CER Var. 25(i.e., with normalization) should be expected to be slightly slower than CER, as the difficulty estimation requires some calculation. Also CCER, normally creating more rules than CER, is somewhat slower than CER. The slowest solution is PCER, having to calculate probabilities for all calibration instances as well as training two isotonic calibrators for each test instance. It is worth pointing out that PCER with normal- ization (not evaluated) becomes very much slower, as each calibration instance must apply a CPS. However, in the current implementation, the biased but faster solution discussed above are used for normalized PCER and CPCER. 6 Concluding Discussion This paper extends Calibrated Explanations (CE), previously introduced for classi- fication, with support for regression. Two primary use cases are identified: standard regression and probabilistic regression, i.e., measuring the probability of exceeding a threshold. The proposed solution relies on Conformal Predictive Systems (CPS), mak- ing it possible to meet the different requirements of the two identified use cases. The proposed solutions provide access to factual and counterfactual explanations with the possibility of conveying uncertainty quantification for the feature rules, just like CE for classification. In the paper, the solutions have been demonstrated using several plots, showcas- ing some of the many ways that the proposed solutions can be used. Furthermore, the paper also includes a comparison with some of the best-known state-of-the-art explanation methods (LIME and SHAP). The results demonstrate that the proposed solution for standard regression is both stable and robust. Furthermore, it is reason- ably fast, even if it cannot compete with the SHAP implementation in C++, optimized for tree models. The suggested solution is considered reliable for two reasons: 1) The calibration of the underlying model and 2) the uncertainty quantification, highlighting the degree of uncertainty of both prediction and feature weights. The solution proposed to build probabilistic explanations for regression does not share all the benefits seen for standard regression. The solution has comparable per- formance as LIME, even if it is somewhat slower than LIME. The main strength of this solution is that it provides the possibility of getting probabilistic explanations in relation to an arbitrary threshold from any standard regression model without having to impose any restrictions on the regression model. A Python implementation of the CE solution described in this paper is freely available with a BSD3-style license from: • Code repository: https://github.com/Moffran/calibrated explanations • PyPi package: https://pypi.org/project/calibrated-explanations/ • Documentation: https://calibrated-explanations.readthedocs.io/ Since it is on PyPI, it can be installed with pip install calibrated-explanations. The GitHub repository includes Python scripts to run the examples in this paper, making the results here easily replicable. The repository also includes several note- books with additional examples. This paper details calibrated-explanations as of version 0.0.24. 266.1 Future Work There are several directions for future work. Incorporating support for Mondrian CPSs, already supported by the crepes package, would be a natural first endeavor. One moti- vation for this is that Mondrian CPSs have been shown to remedy heteroscedasticity in the underlying model (e.g., often happening with ensemble models due to averaging affecting the boundery cases differently from the main mass of instances). There are room for improvement regarding the plot layout and providing addi- tional ways of visualization is a natural development in the future. This involves implementing support for explanations within image and text prediction, even if these improvements are more closely connected to classification problems. An interesting area to look into is how this technique can be adapted to expla- nations of time-series problems. How to capture and convey the dependency between different time steps pose an interesting challenge. Finally, the computational speed can probably be increased if implementing the core in C++ or by relying on fast languages being able to run Python code more effi- ciently, e.g., Mojo. The computational speed of probabilistic calibrated explanations can be dramatically improved by allowing the probability estimates of the calibra- tion set to be biased. Further evaluation of the practical impact of such a change is warranted, to better understand the implications of making that trade-off. Acknowledgments. The authors acknowledge the Swedish Knowledge Foundation and industrial partners for financially supporting the research and education environ- ment on Knowledge Intensive Product Realization SPARK at J¨onk¨oping University, Sweden. Projects: AFAIR grant no. 20200223 and PREMACOP grant no. 20220187. Helena L¨ofstr¨om is a PhD student in the Industrial Graduate School in Digital Retailing (INSiDR) at the University of Bor˚as, funded by the Swedish Knowledge Foundation, grant no. 20160035. References [1] Zhou, J., Gandomi, A.H., Chen, F., Holzinger, A.: Evaluating the quality of machine learning explanations: A survey on methods and metrics. Electronics 10(5), 593 (2021) [2] David Gunning: Explainable Artificial Intelligence. Web. DARPA (2017). https: //www.darpa.mil/attachments/XAIProgramUpdate.pdf Accessed 2019-08-29 [3] Ribeiro, M.T., Singh, S., Guestrin, C.: ”Why Should I Trust You?”: Explaining the Predictions of Any Classifier. In: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. KDD ’16, pp. 1135–1144. Association for Computing Machinery, New York, NY, USA (2016). https://doi.org/10.1145/2939672.2939778 [4] Alvarado-Valencia, J.A., Barrero, L.H.: Reliance, trust and heuristics in judgmen- tal forecasting. Computers in human behavior 36, 102–113 (2014) 27[5] Bu¸cinca, Z., Lin, P., Gajos, K.Z., Glassman, E.L.: Proxy tasks and subjective measures can be misleading in evaluating explainable ai systems. In: Proceedings of the 25th International Conference on Intelligent User Interfaces, pp. 454–464 (2020) [6] Gunning, D., Aha, D.W.: Darpa’s explainable artificial intelligence program. AI Magazine 40(2), 44–58 (2019) [7] Dimanov, B., Bhatt, U., Jamnik, M., Weller, A.: You shouldn’t trust me: Learning models which conceal unfairness from multiple explanation methods. Frontiers in Artificial Intelligence and Applications: ECAI 2020 (2020) [8] Guidotti, R., Monreale, A., Ruggieri, S., Turini, F., Giannotti, F., Pedreschi, D.: A survey of methods for explaining black box models. ACM computing surveys (CSUR) 51(5), 1–42 (2018) [9] Moradi, M., Samwald, M.: Post-hoc explanation of black-box classifiers using confident itemsets. Expert Systems with Applications 165, 113941 (2021) [10] Martens, D., Foster, P.: Explaining data-driven document classifications. MIS Quaterly 38(1), 73–100 (2014) [11] Slack, D., Hilgard, A., Singh, S., Lakkaraju, H.: Reliable post hoc explanations: Modeling uncertainty in explainability. Advances in neural information processing systems 34, 9391–9404 (2021) [12] Rahnama, A.H.A., Bostr¨om, H.: A study of data and label shift in the lime framework. arXiv preprint arXiv:1910.14421 (2019) [13] Hoffman, R.R., Mueller, S.T., Klein, G., Litman, J.: Metrics for explainable ai: Challenges and prospects. Technical report, DARPA Explainable AI Program (2018) [14] Carvalho, D.V., Pereira, E.M., Cardoso, J.S.: Machine learning interpretability: A survey on methods and metrics. Electronics 8(8), 832 (2019) [15] Adadi, A., Berrada, M.: Peeking inside the black-box: A survey on explainable artificial intelligence (xai). IEEE Access 6, 52138–52160 (2018) [16] Wang, D., Yang, Q., Abdul, A., Lim, B.Y.: Designing theory-driven user-centric explainable ai. In: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. CHI ’19, pp. 1–15. Association for Computing Machinery, New York, NY, USA (2019). https://doi.org/10.1145/3290605.3300831 . https: //doi.org/10.1145/3290605.3300831 [17] Mueller, S.T., Hoffman, R.R., Clancey, W., Emrey, A., Klein, G.: Explanation in 28human-ai systems: A literature meta-review, synopsis of key ideas and publica- tions, and bibliography for explainable ai. Technical report, DARPA Explainable AI Program (2019) [18] Agarwal, C., Krishna, S., Saxena, E., Pawelczyk, M., Johnson, N., Puri, I., Zitnik, M., Lakkaraju, H.: Openxai: Towards a transparent evaluation of model expla- nations. Advances in Neural Information Processing Systems 35, 15784–15799 (2022) [19] Bhatt, U., Antor´an, J., Zhang, Y., Liao, Q.V., Sattigeri, P., Fogliato, R., Melan¸con, G., Krishnan, R., Stanley, J., Tickoo, O., et al.: Uncertainty as a form of transparency: Measuring, communicating, and using uncertainty. In: Proceed- ings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, pp. 401–413 (2021) [20] Vovk, V.: Cross-conformal predictors. Annals of Mathematics and Artificial Intelligence 74, 9–28 (2015) [21] Platt, J., et al.: Probabilistic outputs for support vector machines and compar- isons to regularized likelihood methods. Advances in large margin classifiers 10(3), 61–74 (1999) [22] Vovk, V., Petej, I.: Venn-Abers predictors. arXiv preprint arXiv:1211.0025 (2012) [23] L¨ofstr¨om, H., L¨ofstr¨om, T., Johansson, U., S¨onstr¨od, C.: Calibrated Explanations: with Uncertainty Information and Counterfactuals (2023) [24] Letzgus, S., Wagner, P., Lederer, J., Samek, W., M¨uller, K.-R., Montavon, G.: Toward explainable artificial intelligence for regression models: A methodological perspective. IEEE Signal Processing Magazine 39(4), 40–58 (2022) [25] Vovk, V., Gammerman, A., Shafer, G.: Algorithmic Learning in a Random World. Springer, Berlin, Heidelberg (2005) [26] Vovk, V., Shen, J., Manokhin, V., Xie, M.: Nonparametric predictive distributions based on conformal prediction. Mach. Learn. 108(3), 445–474 (2019) [27] Molnar, C.: Interpretable Machine Learning, 2nd edn. Leanpub, ??? (2022). https: //christophm.github.io/interpretable-ml-book [28] Mothilal, R.K., Sharma, A., Tan, C.: Explaining machine learning classifiers through diverse counterfactual explanations. In: Proceedings of the 2020 Confer- ence on Fairness, Accountability, and Transparency, pp. 607–617 (2020) [29] Guidotti, R.: Counterfactual explanations and how to find them: literature review and benchmarking. Data Mining and Knowledge Discovery, 1–55 (2022) [30] Wachter, S., Mittelstadt, B., Russell, C.: Counterfactual explanations without 29opening the black box: Automated decisions and the gdpr. Harv. JL & Tech. 31, 841 (2017) [31] L¨ofstr¨om, H., Hammar, K., Johansson, U.: A meta survey of quality evaluation cri- teria in explanation methods. In: De Weerdt, J., Polyvyanyy, A. (eds.) Intelligent Information Systems, pp. 55–63. Springer, Cham (2022) [32] Alvarez-Melis, D., Jaakkola, T.S.: On the robustness of interpretability methods. arXiv preprint arXiv:1806.08049 (2018) [33] Lundberg, S.M., Lee, S.-I.: A unified approach to interpreting model predic- tions. In: Proceedings of the 31st International Conference on Neural Information Processing Systems, pp. 4768–4777 (2017) [34] Ribeiro, M.T., Singh, S., Guestrin, C.: ”why should i trust you?” explaining the predictions of any classifier. In: Proceedings of the 22nd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining, pp. 1135–1144 (2016) [35] Ribeiro, M.T., Singh, S., Guestrin, C.: Anchors: High-precision model-agnostic explanations. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol. 32 (2018) [36] Vovk, V., Shafer, G., Nouretdinov, I.: Self-calibrating probability forecasting. In: Advances in Neural Information Processing Systems, pp. 1133–1140 (2004) [37] Lambrou, A., Nouretdinov, I., Papadopoulos, H.: Inductive venn prediction. Annals of Mathematics and Artificial Intelligence 74(1), 181–201 (2015) [38] Bostr¨om, H.: crepes: a python package for generating conformal regressors and predictive systems. In: Johansson, U., Bostr¨om, H., An Nguyen, K., Luo, Z., Carlsson, L. (eds.) Proceedings of the Eleventh Symposium on Conformal and Probabilistic Prediction and Applications. Proceedings of Machine Learning Research, vol. 179. PMLR, ??? (2022) [39] Pace, R.K., Barry, R.: Sparse spatial autoregressions. Statistics & Probability Letters 33(3), 291–297 (1997) 30
arXiv:2308.16192v1 [econ.EM] 27 Aug 2023 High Dimensional Time Series Regression Models: Applications to Statistical Learning Methods* Christis Katsouris Department of Economics, University of Southampton University of Exeter Business School September 1, 2023 Abstract These lecture notes provide an overview of existing methodologies and recent developments for estimation and inference with high dimensional time series regression models. First, we present main limit theory results for high dimensional dependent data which is relevant to covariance matrix structures as well as to dependent time series sequences. Second, we present main aspects of the asymptotic theory related to time series regression models with many covariates. Third, we discuss various applications of statistical learning methodologies for time series analysis purposes. *Part of these lecture notes were prepared during my Ph.D. studies at the Department of Economics, University of Southampton. Moreover, these notes were motivated by discussions and presentations during the weekly sessions of the Time Series and Machine Learning Reading Group at the School of Mathematical Sciences, Statistics Division, University of Southampton. The author is grateful to Zudi Lu and Chao Zheng for stimulating discussions. The author is also grateful for stimulating seminar talks and discussions at the Southampton Statistical Sciences Research Institute seminar series. This draft was prepared during the academic year 2022-2023 at the Department of Economics, University of Exeter Business School. Dr. Christis Katsouris is a Lecturer in Economics, University of Exeter Business School, Exeter EX4 4PU, United Kingdom. Email Address: christiskatsouris@gmail.com 1Contents 1. Introduction 5 2. Limit theory for High Dimensional Dependent Data 6 2.1. Limit theory of Covariance Matrices for Linear Processes . . . . . . . . . . . . . . . . . . . . . . 6 2.1.1. Covariance and Precision matrix estimation for high dimensional time series . . . . . . . 6 2.1.2. Main Results on Probability Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.1.3. Limiting Laws of Coherence of Random Matrices with Applications . . . . . . . . . . . . 13 2.1.4. Large deviations for quadratic forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.1.5. Asymmetry Helps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 2.2. Limit theory for Functional Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 2.2.1. Simultaneous Diagonalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 2.2.2. Common Functional Principal Components . . . . . . . . . . . . . . . . . . . . . . . . . 23 2.3. Limit theorems for Dependent Sequences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 2.3.1. Method of bounded martingale differences . . . . . . . . . . . . . . . . . . . . . . . . . 25 2.3.2. Concentration Inequalities for Dependent Random Variables . . . . . . . . . . . . . . . . 25 2.3.3. Central limit theorems for high dimensional dependent data . . . . . . . . . . . . . . . . 26 2.3.4. Sub-Weibull random vectors under β−mixing . . . . . . . . . . . . . . . . . . . . . . . . 29 2.3.5. Approximation Theorems for Strongly Mixing Random Variables . . . . . . . . . . . . . 29 2.4. Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 3. Time Series Regression Models with Many Covariates 31 3.1. A forecasting Application . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 3.1.1. Best Linear Predictor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 3.1.2. Forecast Combination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 3.2. Statistical Theory for Lasso Regression Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 3.2.1. Statistical Principles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 3.2.2. Dantzig Selector . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 3.2.3. Oracle inequalities for prediction loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 3.3. Parameter Estimation and Model Selection Consistency Properties . . . . . . . . . . . . . . . . . 40 3.3.1. Consistency Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 3.4. A Lasso-based Time Series Regression Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 3.4.1. Oracle Property . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 3.5. Lasso shrinkage with long memory regression errors . . . . . . . . . . . . . . . . . . . . . . . . 45 3.5.1. Results with ﬁnite sample . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 3.6. Residual empirical process based on the ALasso . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 3.6.1. Main results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 23.6.2. Sparsity in high dimensional estimation problems . . . . . . . . . . . . . . . . . . . . . . 50 3.7. Lasso shrinkage for Predictive Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 3.8. Lasso estimation with a structural break . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 3.9. Classical Shrinkage Type Estimation Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 3.9.1. Limiting Distributional Risk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 3.9.2. Asymptotic representation of estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 3.10. Lasso Inference for High Dimensional Time Series under NED . . . . . . . . . . . . . . . . . . . 56 3.10.1. Inference on low-dimensional parameters . . . . . . . . . . . . . . . . . . . . . . . . . . 59 3.10.2. Error Bound and the Consistency for the Lasso . . . . . . . . . . . . . . . . . . . . . . . 59 3.10.3. Uniformly Valid Inference via the Disparsiﬁed Lasso . . . . . . . . . . . . . . . . . . . . 60 4. High Dimensional Feature Selection Methods 61 4.1. Ultra-high dimensionality under dependence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 4.1.1. SIS with dependent observations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 4.1.2. Experimental design examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 4.2. Uniform Asymptotic Inference and Model Selection . . . . . . . . . . . . . . . . . . . . . . . . . 66 4.2.1. Uniform-in-Submodel Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 4.3. Non-nested Regressions and Variable Selection Speciﬁcation Testing . . . . . . . . . . . . . . . . 68 4.4. Divide and Conquer Variable Selection Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 70 4.5. Sample-Splitting and Variable Importance Algorithm . . . . . . . . . . . . . . . . . . . . . . . . 73 4.5.1. The projection parameter β�S . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 4.5.2. The LOCO parameters γ�S and φ�S . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 4.5.3. The prediction parameter ρ�S . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 4.5.4. Conﬁdence sets for the projection parameters: The bootstrap . . . . . . . . . . . . . . . . 76 4.6. Multiple Testing Procedure and Variable Selection . . . . . . . . . . . . . . . . . . . . . . . . . . 77 4.6.1. Rejection Principle of Familywise Error Control . . . . . . . . . . . . . . . . . . . . . . 78 4.6.2. Subgroup Selection Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80 4.7. Model Selection in Cointegrating Regressions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82 4.7.1. Model Selection Consistency and Oracle Property . . . . . . . . . . . . . . . . . . . . . 83 4.7.2. Model Selection and Rank of a Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 5. Statistical Learning Methods in Time Series Analysis 85 5.1. Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 5.2. Non-Asymptotic Probability Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86 5.3. Shallow Neural Network Estimate learned by Gradient Descent . . . . . . . . . . . . . . . . . . . 87 5.3.1. Learning of linear penalized least squares estimates by gradient descent . . . . . . . . . . 89 5.4. Deep Neural Network Estimate learned by Gradient Descent . . . . . . . . . . . . . . . . . . . . 92 5.5. Deep Neural Network Architecture Approximations . . . . . . . . . . . . . . . . . . . . . . . . . 96 35.5.1. Error Bounds of DNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96 5.5.2. Topological Space for DNN Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . 99 5.6. Statistical Inference with Stochastic Gradient Descent . . . . . . . . . . . . . . . . . . . . . . . . 100 5.6.1. Literature Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 5.6.2. Preliminary Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 5.6.3. Assumptions and error bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102 5.6.4. Estimators for asymptotic covariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103 5.6.5. Averaged SGD Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103 5.7. Sieve Estimation Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 5.7.1. Asymptotic Properties of Sieve Estimation . . . . . . . . . . . . . . . . . . . . . . . . . 104 5.7.2. Sieve Estimation for Panel Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 5.7.3. Sieve M inference on irregular parameters . . . . . . . . . . . . . . . . . . . . . . . . . . 106 5.8. Further Statistical Algorithms in Economic Applications . . . . . . . . . . . . . . . . . . . . . . 108 A Elements of Weak Convergence of Empirical Processes 109 A1. Sub-Gaussian processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109 A2. Clivenko-Cantelli theorems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110 A3. Donsker Theorems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111 B Elements of Stochastic Processes 112 B1. Asymptotic Equicontinuity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112 B2. LLNs for Hilbert Space-Valued Mixingales . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114 C Elements of Bayesian Statistics 115 References 129 41. Introduction These lecture notes series present a uniﬁed framework for recent developments of deep learning method- ologies in time series econometrics and statistics. Speciﬁcally, statistical machine learning methods and theory are now widespread in social sciences, economics1, ﬁnance and mathematical modelling. Neural networks can be considered as a type of linear sieve estimation where the basis functions them- selves are ﬂexibly learned from the data by optimizing over many combinations of simple functions (see, Farrell et al. (2021)). Deep reinforcement learning has gain attention in recent years especially when modelling dynamic interactions of economic agents. In particular deep neural nets contain many hidden layers of neurons between the input and output layers, which has been found to exhibit superior perfor- mance across a variety of contexts. Our goal is to consolidate the theoretical and practical underpinnings of machine learning methods when modelling time series data for estimation, inference and forecasting purposes. In order to express precise conditions for ergodicity, we turn to the Lyapounov exponent, a concept which is well-known by those studying stability of dynamical systems. Therefore, in the context of nonlinear time series, nonstability means explosive behaviour. Thus, the Lyapounov exponent, as we deﬁne it for the state space model {Xt} of a time series, is given by γ = lim inf n→∞ lim sup ∥x∥→∞ 1 n (1.1) which measures the drift of the process when the sample size is large. Deﬁne with ℓ1 = � (xn)∞ n=1 : ∑ n≥1 |xn| < ∞ � (1.2) ℓ2 = � (xn)∞ n=1 : ∑ n≥1 x2 n < ∞ � (1.3) The sets ℓ1,ℓ2 and ℓ∞ are all vector spaces and it holds that ∥(xn)∥1 = ∑n≥1 |xn|, ∥(xn)∥2 = ∑n≥1 � ∑n≥1 x2 n �1/2 deﬁne norms on ℓ1 and ℓ2 spaces respectively. For any x ∈ Rn, ∥x∥ = � ∑n i=1x2 i , ∥x∥ℓ1 = ∑n i=1 |xi| and ∥x∥∞ = max 1≤i≤n|xi|, denote ℓ2, ℓ1 and ℓ∞−norms respectively. We also denote with ∥x∥ℓ0 = ∑n i=11{xi ̸= 0} which is simply the number of non-zero entries of x. For any n×n matrix M, ∥M∥ℓ∞ = max1≤i≤n ∑n j=1|Mi, j| denotes the induced ℓ∞ matrix norm. Detailed statistical theory for high dimensional statistics applica- tions are presented in Wainwright (2019) (see also Vershynin (2018)). Let {Xt,n}n t=1, n ∈ N be a random array and {Ft}∞ t=−∞ a ﬁltration such that Xt,n is Ft−measurable for all t and n. Based on the aforemen- tioned notation we discuss important theoretical results from the statistics and econometrics literature. 1The idea of representing the dynamic interactions of economic agents similar to the structure of brain neural networks dates back to Ashby (1957). Furthermore the application of neural networks for modelling purposes dates back to the funda- mental works of Hebb (1949) and Rosenblatt (1958). 52. Limit theory for High Dimensional Dependent Data 2.1. Limit theory of Covariance Matrices for Linear Processes 2.1.1. Covariance and Precision matrix estimation for high dimensional time series Following the framework of Chen et al. (2013), suppose we have n temporally observed p−dimensional vectors (zi)n i=1 having mean zero and covariance matrix Σi = E(zi,zi) whose dimension is p × p. Our goal is to estimate the covariance matrices Σi and their inverses Ωi = Σ−1 i based on the data matrix Zp×n = (z1,...,zn). In the classical situation where p is ﬁxed, n → ∞ and zi are mean zero independent and identically distributed i.i.d random vectors, it is well known that the sample covariance matrix ˆΣn = 1 n n ∑ i=1 ziz⊤ i (2.1) is a consistent and well behaved estimator of Σ, and ˆΩn = ˆΣ−1 n is a natural and good estimator of Ω. However, when the dimensionality p grows with n, random matrix theory asserts that ˆΣn is no longer a consistent estimate of Σ in the sense that its eigenvalues do not converge to those of Σ, as the Marcenko- Pastur law states. Moreover, it is clear that ˆΩn is not deﬁned when ˆΣn is not invertible in the high- dimensional case with p >> n. • Let Tu � �Σu � = Q ˆΛQ⊤ = ∑p j=1 ˆλ jqjq⊤ j be its eigen-decomposition, where Q is an orthonormal matrix and ˆΛ is a diagonal matrix. For v > 0, consider ˜Sv = p ∑ j=1 � ˆλ j ∨v � qjq⊤ j , (2.2) where 0 < v ≤ √p ¯ω and ω2 is the rate of convergence. • Let µ1,...,µp be the diagonal elements of Q⊤ΣQ. Then, by Theorem 2.1 in Chen et al. (2013), it holds that ∑p j=1 � ˆλ j − µj �2 ≤ p2 ¯ω2, and consequently �� ˜Sv −Σ ��2 F ≤ 2 �� ˜Sv −Tu �ˆΣ ���2 F +2 ��Tu �ˆΣu � −Σ ��2 F ≤ 2 p ∑ j=1 � ˆλ j − � ˆλ j ∨v ��2 +2 ¯ω2p2 ≤ 2 p ∑ j=1 � 2ˆλ 2 j 1 � ˆλ j ≤ 0 � +2v2� +2 ¯ω2p2. Furthermore, if ˆλ j ≤ 0, since µi ≥ 0, we have that ���ˆλ j ��� ≤ ���ˆλ j − µi ���. Then, �� ˜Sv −Σ ��2 F ≤ 4v2p+6 ¯ω p2 ≤ 10 ¯ω2p2. (2.3) 6Notice that the eigenvalues of ˜Sv are bounded below by v, and thus it is positive deﬁnite such that v = � p−1 p ∑ j,k=1 u2 ×1 ��� ˆσjk �� ≥ u � �1/2 The same positive-deﬁnization procedure also applies to the spectral norm and its rate can be similarly preserved. Sparsity Conditions We begin by presenting the commonly used sparsity condition deﬁned in terms of the strong ℓq−ball such that Gr (M ) = � Σ ���� max j≤p σj j ≤ 1; max 1≤k≤p p ∑ j=1 ��σj j ��r ≤ ˜M � ,0 ≤ r ≤ 1. (2.4) In particular, when r = 0, the sparsity condition implies that max 1≤k≤p∑p j=1 1 � σjk ̸= 0 � ≤ ˜M. Example 1. (Stationary linear process). An important special class is the vector linear process as deﬁned zi = ∞ ∑ j=0 Ajei−j (2.5) where Aj are p × p matrices, and ei are i.i.d mean zero random vectors with ﬁnite covariance matrix Σe = E � eie⊤ i � . A linear process representation is a ﬁlter which ensures the dependence on the innovation sequences. Then, zi exists almost surely with covariance matrix Σ = ∑∞ j=0 AjΣeA⊤ j if the latter converges. Assume that the innovation vector ei = (e1i,...,epi)⊤, where eji are i.i.d with mean zero, variance 1 and eji ∈ L 2q,q > 2, and the coefﬁcient matrices Ai = � ai, jk � 1≤j,k≤p satisfy maxj≤p p ∑ k=1 a2 i, jk = O � i−2−2γ� ,γ > 0 (2.6) Take the AR(1) process, zi = Azi−1 +ei, where A is the real matrix with spectral norm ρ(A) < 1, it is of the form as in (2.5) with Aj = Aj. and the functional dependence measure θi,2q, j = O � ρ(A)i� . Example 2. Consider the estimation framework for the graphical Lasso estimator with off-diagonal en- tries peranilzed by the 1-norm. In particular, for i.i.d p−dimensional vectors with polynomial moment condition, it can be shown that p = O �� n d2 � q 2τ � for some τ > 2, where d is the maximum degree in the Gaussian graphical model, then 1 p2 �� ˆΩn −Ω �� = Op � s+ p p2 . p2τ/q n � , (2.7) where s is the number of non-zero off-diagonal entries in Ω. 7We now compare the results with the CLIME (constrained L1−minimization for inverse matrix estima- tion) method, a non-Lasso type estimator which is estimated as below minimize |Θ|1 subject to ��ˆΣnΘ−I �� ∞ ≤ λn. (2.8) Notice that we can also consider the slightly modiﬁed version of the graphical Lasso: LetV = diag � σ1/2 11 ,....,σ1/2 pp � and R be the correlation matrix with ˆV and ˆR be the sample counterparts. We estimate Ω = V −1KV −1 by ˆΩλ = ˆV −1 ˆKλ ˆV −1 (2.9) where ˆKλ = arg min Φ>0 � trace � Φ ˆR � −log det(Ψ)+λ ��Ψ−�� 1 � . (2.10) Furthermore, the structure of the proposed risk matrix has different properties in contrast to the covariance matrix. For instance, the estimation procedure is not invariant to variable permutations. In contrast the covariance matrix itself has the property of being invariant to variable permutations. Therefore, this property allows the implementation of high dimensional techniques in inference problems. For instance, in the case of the covariance matrix Ledoit and Wolf (2004) proposed a way to compute an optimal linear combination of the sample covariance with the identity matrix, which also results in shrinkage of eigenvalues. Furthermore, shrinkage estimators are invariant to variable permutations, but they do not affect the eigenvectors of the covariance, only the eigenvalues, and it has been shown that the sample eigenvectors are also not consistent when p is large. Therefore, developing also estimation methods which are invariant to variable permutations is important. Deﬁnition 1. For any sequence X, the φ−mixing coefﬁcient φk is deﬁned as follows: φk(X) = sup � P(B|A)−P(B)| : A ∈ σℓ,B ∈ σ′ ℓ+k,ℓ ≥ 1 � . (2.11) Remark 1. Notice that the sub-Gaussianity assumption can be still employed in a time series setting (e.g., see Wong et al. (2020)). To do this, we combine the Rini-mixing condition with the sub-Gaussianity assumption which ensures that there is enough weak dependence captured as well as that the vectors of the model have good properties for estimation and inference. Lemma 1. Let Zi be i.i.d N (0,Σp) and λmax (Σp) ≤ ¯k < ∞. Then if Σp = [σab], P ������ n ∑ i=1 � Zi jZik −σjk � ����� ≥ nν � ≤ c1exp � −c2nν2� for |ν| ≤ δ (2.12) where c1,c2 and δ depend on ¯k only. 82.1.2. Main Results on Probability Bounds Following the framework of Chen et al. (2013) we focus in how to establish the convergence theory for covariance matrix estimates, we shall use the functional dependence measure of Wu (2005). Recall that Zji = gj(Fi),1 ≤ j ≤ p, where gj(.) is the jth coordinate projection of the Rp−valued measurable function g. For w > 0, the functional dependence measure of Zji is deﬁned by θi,w, j = ��Zji −Z′ ji �� w = � E ��Zji −Z′ ji ��w�1/w , (2.13) where Z′ ji = gj(F ′ i ),F ′ i = � ...,e−1,e′ 0,e1,...,ei � and e′ 0 and e′ 0, eℓ, ℓ ∈ Z, are i.i.d. In other words, Z′ ji is a coupled version of Zji with e0 in the latter replaced by an i.i.d copy of e′ 0. Proof of Theorem 2.1 (Chen et al. (2013)) We ﬁrst assume that α > 1/2−1/q. Notice that E ��Tu �ˆΣu � −Σ ��2 F = p ∑ j,k=1 E � ˆσjk1 ��� ˆσjk �� ≥ u � −σjk �2 ≤ 2 p ∑ j,k=1 E � W 2 jk � +2B(u/2), Wjk = ˆσjk1 ��� ˆσjk �� ≥ u � −σjk1 ���σjk �� ≥ u/2 � B(u) = p ∑ j,k=1 σ2 jk1 ���σjk �� < u � . Since the functional dependence measure for the product process � ZjiZki � i has a bounded inequality, under the decay condition Θm,2q ≤ Cm−α,α > 1/2−1/q. we have by Theorem 2(ii) in Wu (2013) that P ���ξjk �� > v � ≤ C2n (nv)q +C3e−C4nv2 (2.14) holds for all v > 0. Using integration by parts we obtain E � ξ 2 jk1 ���ξjk �� > v �� = v2P ���ξjk �� > v � + � ∞ v2 P ���ξjk �� > √w � dw. (2.15) Remark 2. Note that sparsity assumptions in the literature of high dimensional covariance matrices are usually imposed with respect to the inverse of the covariance matrix. In other words, if the ( j,k)−th component of Σ−1 is zero, then the variables Zj and Zk are partially uncorrelated, given the other vari- ables. More speciﬁcally, the current way of deﬁning sparsity in high dimensional models based on an underline covariance structure is to consider the notion of partial correlation as a measure of conditional independence (or dependence) in graphical based models. On the other hand, Katsouris (2023), deﬁnes a novel tail dependency matrix in which case the absence of a link between two nodes on the network implies the conditional tail independence2 (see, also Engelke and Hitz (2020)). 2Associate Professor Sebastian Engelke (University of Geneva) gave a seminar with title: "Causality for extreme values" at the S3RI Departmental Seminar Series at the University of Southampton, on the 9th of December 2021. 9Moreover, some useful limit results are also presented in the paper of Xiao and Wu (2013). In particular, the sample covariance between columns xi and xj is deﬁned as ˆσi j = 1 n (xi − ¯xi)⊤(xi − ¯xi). (2.16) In high-dimensional covariance inference, a fundamental problem is to establish an asymptotic distribu- tional theory for the maximum deviation Mn = max 1≤i≤j≤m �� ˆσi j −σi j ��. (2.17) Proofs of (iii) and (iv) The ﬁrst step is the truncation step. Step 1. (Truncation Step) We truncate Xn,k,i by ˜Xn,k,i = Xn,k,iI ���Xn,k,i �� ≤ n1/4� log(n) � (2.18) Deﬁne ˜Mn similarly as Mn with Xn,k,i being replaced by its truncated version ˜Xn,k,i, P � ˜Mn ̸= Mn � ≤ nmMn(p)n−p/4 (log(n))p ≤ CMn(p)n−δ/4(log(n))p = o(1). (2.19) Furthermore, since the asymptotics are not affected for notational simplicity, we still use ˜Xn,k,i to denote its centered version with mean zero. Deﬁne ˜σn,i, j = E � ˜Xn,k,i ˜Xn,k, j � and ˜τn,i, j = Var � ˜Xn,k,i ˜Xn,k, j � . Furthermore, denote with Mn,1 = max 1≤i<j≤m 1 �˜τn,i, j ����� 1 n n ∑ k=1 ˜Xn,k,i ˜Xn,k, j − ˜σn,i, j ����� (2.20) Mn,2 = max 1≤i<j≤m 1 �˜τn,i, j ����� 1 n n ∑ k=1 ˜Xn,k,i ˜Xn,k, j −σn,i, j ����� (2.21) Simple calculations are given by max 1≤i≤j≤m �� ˜σn,i, j −σn,i, j �� ≤ Cn−(p−2)/4 (log(n))p, (2.22) max α,β∈In ��Cov � ˜Xn,α ˜Xn,β � −Cov � Xn,αXn,β ��� ≤ Cn−(p−2)/4 (log(n))p , (2.23) Step 2. (Effect of estimated means) Set ¯Xn,i = 1 n ∑n k=1 ˜Xn,k,i. Deﬁne with Mn,3 = max 1≤i≤j≤m 1 �˜τn,i, j ����� 1 n n ∑ k=1 � ˜Xn,k,i − ¯Xn,i �� ˜Xn,k, j − ¯Xn, j � −σn,i, j �����. (2.24) 10Furthermore, observe that ��Mn,3 −Mn,2 �� ≤ max 1≤i≤j≤m �� ¯Xn,i ¯Xn, j �� �˜τn,i, j ≤ max 1≤i≤m| ¯Xn,i|2 . � min 1≤i≤j≤m˜τn,i, j �−1/2 . (2.25) Furthermore, using Bernestein’s inequality we can show that max 1≤i≤m| ¯Xn,i| = Op �� log(n) n � , (2.26) which in together with the previous result implies that ��Mn,3 −Mn,2 �� = Op �log(n) n � . (2.27) Step 3. (Effect of estimated variances) Denote by ˇσn,i, j the estimate of ˜σn,i, j ˇσn,i, j = 1 n n ∑ k=1 � ˜Xn,k,i − ¯Xn,i �� ˜Xn,k, j − ¯Xn, j � . (2.28) Thus, since in the deﬁnition of ˜Mn, ˜τn,i, j is unknown, and is estimated by ˇτn,i, j = 1 n n ∑ k=1 �� ˜Xn,k,i − ¯Xn,i �� ˜Xn,k, j − ¯Xn, j � − ˇσn,i, j �2. (2.29) Therefore, in order to show that the exponential limit holds for ˜Mn, it sufﬁces to verify that max 1≤i≤m ��ˇτn,i, j − ˜τn,i, j �� = op(1/log(n)). (2.30) Set ˇτn,i, j,1 = 1 n n ∑ k=1 �� ˜Xn,k,i − ¯Xn,i �� ˜Xn,k, j − ¯Xn, j � − ˜σn,i, j �2, (2.31) ˇτn,i, j,2 = 1 n n ∑ k=1 � ˜Xn,k,i ˜Xn,k, j − ˜σn,i, j � . (2.32) Using the probability limit, we know that max 1≤i≤j≤m ��ˇτn,i, j,1 − ˇτn,i, j �� = Op(log(n)/n). (2.33) 11Since � ˜Xn,k,i ˜Xn,k, j − ˜σn,i, j �2 ≤ 64n/(log(n))4 . (2.34) We have that max 1≤i≤j≤mP ���ˇτn,i, j,2 − ˜τn,i, j �� ≥ (log(n))−2� ≤   Cn n(log(n))−2. � n(log(n))−3�q∧1   log(n) ≤ � C(log(n))5 nq∧1 �log(n) Then, it follows that max 1≤i≤j≤m ��ˇτn,i, j,2 − ˜τn,i, j �� = Op � (log(n))−2� . (2.35) Thus, it remains to prove that max 1≤i≤j≤m ��ˇτn,i, j,1 − ˇτn,i, j,2 �� (2.36) A normal comparison principle Suppose that for each n ≥ 1, (Xn,i)i∈In is a Gaussian random vector whose entries have mean zero and variance one, where In is an index set with cardinality |In| = sn. Let ±n � rn,i, j � i, j∈In be the covariance matrix of (Xn,i)i∈In. Assume that sn → ∞ as n → ∞. We impose either of the following two conditions. (B1) For any sequence (bn) such that bn → ∞, γn(n,bn) = o(1/log(bn)) and lim sup n→∞ γn < 1. (2.37) (B2) For any sequence (bn) such that bn → ∞, γn(n,bn) = o(1) and ∑ i̸=j∈In r2 n,i, j = O � s2−δ n � for some δ > 0 and lim sup n→∞ γn < 1 (2.38) where γ(n,bn) := sup i∈In sup A ⊂In,|A |=bn inf j∈In ��rn,i, j ��, γn := sup i, j∈In,i̸=j ��rn,i, j ��. (2.39) 122.1.3. Limiting Laws of Coherence of Random Matrices with Applications Furthermore, following the framework of Cai and Jiang (2011), we present some useful lemmas below. Lemma 2. Let hi = ∥xi − ¯xi∥/√n for each i. Then, ���nΓn −X⊤ n Xn ��� ≤ � b2 n,1 +2bn,1 � Wnb−2 n,3 +nb−2 n,3b2 n,4, (2.40) where bn,1 = max 1≤i≤p|hi −1|, Wn = max 1≤i<j≤p ���x⊤ i xj ���, bn,3 = min 1≤i≤phi, bn,4 = max 1≤i≤p|¯xi|. Lemma 3. Let ξ,1 ≤ i ≤ n, be independent random variables with Eξi = 0. Set s2 n = n ∑ i=1 Eξ 2 i , ρ2 n n ∑ i=1 E|ξi|3 , Sn = n ∑ i=1 ξ. (2.41) Assume that max 1≤i≤n|ξi| ≤ cnsn for some 0 < cn ≤ 1. Then, P(Sn > xsn) = eγ(x/sn) (1 = Φ(x)) � 1+θn,x(1+)s−3 n ρn � (2.42) for 0 < x ≤ 1/(18cn), where |γ(x)| ≤ 2x3ρn and |θn,x| ≤ 36. Therefore, we have that P(Wn ≤ an) ≤ e−λn +b1,n +b2,n, (2.43) Suppose we consider an Rd−valued time series process {Xt,t ∈ Z} with Xt = � X1,t,....,Xd,t �⊤, and we have data X1,...,Xn at hand to use for estimation and inference purposes. Corollary 1. The process {Xt,t ∈ Z} is assumed to be strictly stationary and its (d ×d) autocovariance matrix C(h) = � Ci j(h) � i, j=1,...,d at lag h ∈ Z is C(h) = E �� Xt+h − µ �� Xt+h − µ �⊤� , (2.44) where µ = E[Xt], and the sample autocovariance is denoted with ˆC(h) = � ˆCi j(h) � i, j=1,...,d at lag |h| < n and can be deﬁned accordingly. 132.1.4. Large deviations for quadratic forms Large deviations for quadratic forms of stationary processes have been extensively studied in the litera- ture. In particular, we need an upper bound for the tail probability under less restrictive conditions. In this section, we follow the framework proposed by Xiao and Wu (2012). More speciﬁcally, we prove a result on probabilities of large deviations of quadratic forms of stationary processes, which take the form QT = ∑ 1≤s≤t≤T as,tXsXt. (2.45) The coefﬁcients as,t = aT,s,t may depend on T. Throughout this section we assume that sups,t |as,t| ≤ 1, and as,t = 0 when |s−t| > BT, where BT → ∞, and BT = O(T γ) for some 0 < γ < 1. Remark 3. Notice that large deviations for quadratic forms of stationary processes have been extensively examined in the literature, which include: (i) large deviations principle for Gaussian processes, (ii) func- tional large deviation principle, (iii) moderate deviations principle, (iv) moderate deviation principles for quadratic forms of Gaussian processes and (v) moderate deviations of periodograms of linear processes as well as Cramer-type moderate deviation for spectral density estimates of Gaussian processes. Theorem 1. Assume that Xt ∈ L p, p > 4, EXt = 0, and Θp(m) = O(m−a). Set cp = (p+4)ep/4Θ2 4. For any M > 1, let xt = 2cp √TMBTlogBT. Assume that BT → ∞ and BT = O (T γ) for some 0 < γ < 1. Then, for any γ < β < 1, there exists a constant Cp,M,β > 0 such that the following holds P � |E0QT| ≥ xT � ≤ Cp,M,βx−p/2 T (logT) � (TBT)p/4T −αβ p/2 +TBp/2−1−αβ p/2 T +T � +Cp,M,βB−M T . Proof. Without loss of generality, assume BT ≤ T γ. For γ < β < 1, let mT = ⌊T √ β⌋, ˜XT = Ht−mT Xt and QT = ∑ 1≤s≤t≤T as,tXsXt. (2.46) We have that P ���E0 � QT − ˜QT ��� ≥ cpM1/2� TBT (logBT) � (2.47) Split [1,T] into blocks B1,...,BbT of size 2mT, and deﬁne QT = ∑ t∈Bk ∑ 1≤s≤t as,t ˜Xs ˜Xt. (2.48) 142.1.5. Asymmetry Helps In this section, we present the main results obtained by Chen et al. (2021b). We being by considering the low-rank matrix completion of Chen et al. (2021b). Low-rank matrix completion Suppose that M is generated using random partial entries of M⋆ as Mi j =    1 pM⋆ i j with probability p, 0 else, (2.49) where p denotes the fraction of the entries of M⋆ being revealed. We have that H = M−M⋆ is zero-mean and obeys ��Hi j �� ≤ µ np := B and Var � Hi j � ≤ µ2 pn2. For instance, in the case where p ≈ µ2log(n) n , then invoking Corollaries 1-3 of Chen et al. (2021b) yields |λ −λ ⋆| |λ ⋆| ≤ 1 √n � µ3log(n) pn , (2.50) which gives that min � ∥u−u∥∞ ,∥u+u∥∞ � ≤ 1 √n � µ3log(n) pn (2.51) with high probability, where a ∈ Rn is any ﬁxed unit vector. Empirically, eigen-decomposition outper- forms SVD in estimating both the leading eigenvalue and eigenvector of M⋆ Why asymmetry helps? According toChen et al. (2021b), if we consider their Theorem 3, focusing on the case with λ ⋆ = 1 for simplicity, then the key ingredient is the Neumann trick stated in Theorem 2. Speciﬁcally, in the rank-1 case we can expand u = 1 λ � u⋆⊤u � ∞ ∑ s=0 1 λ sHsu⋆. (2.52) We obtain that ����a⊤ � u− u⋆⊤u λ u⋆ ����� = ����� u⋆⊤u λ ∞ ∑ s=1 a⊤Hsu⋆ λ s ����� ≤ ∞ ∑ s=1 ���� a⊤Hsu⋆ λ s ���� (2.53) where the last inequality holds since (i) ��u⋆⊤u �� ≤ 1, and (ii) λ is real-valued and obeys λ ≈ 1 if ∥H∥ << 1. As a result, the perturbation can be well controlled as long as ��a⊤Hsu⋆�� is small for every s ≤ 1. 15• (Asymmetric case) When H is composed of independent zero-mean entries each with variance σ2 n , we obtain that E � a⊤H2u⋆� = a⊤E � H2� u⋆ = a⊤� σ2I � u⋆ = σ2a⊤u⋆ (2.54) • (Symmetric case) When H is symmetric and its upper triangular part consists of independent zero- mean entries with variance σ2 n , it holds that E � a⊤H2u⋆� = a⊤E � H2� u⋆ = a⊤� nσ2I � u⋆ = nσ2a⊤u⋆. (2.55) Pertrubation Analysis for the rank-r case Eigenvalue perturbation for the rank-r case The eigenvalue perturbation analysis can be extended to accommodate the case where M⋆ is symmetric and rank−r. As before, we assume that the r nonzero eigenvalues of M⋆ obey that |λ1| ≥ ... ≥ |λr| Theorem 2 (Chen et al. (2021b)). (Perturbation of linear forms of eigenvectors (rank-r)) Consider a rank−r symmetric matrix M⋆ ∈ Rn×n with incoherence parameter µ. Suppose that σ � n log(n),B log(n) λ ⋆max ≤ c1 κ (2.56) for some sufﬁciently small constants c1 > 0. Then for any ﬁxed unit vector a ∈ Rn and any 1 ≤ ℓ ≤ r, with probability at least 1−O � n−10� one has that �����a⊤ � uℓ − r ∑ j=1 λ ⋆ j u⋆⊤ j uℓ λ j u⋆ j ������ ≤ max � σ � nlog(n),Blog(n) � κ |λℓ| � µr n (2.57) ≤ max � σ � nlog(n),Blog(n) � λ ⋆max κ2 � µr n (2.58) The particular condition allows to control the perturbation of the linear form of eigenvectors. Thus, the perturbation upper bound grows as either the rank r or the condition number κ increases. Corollary 2 (Chen et al. (2021b)). Consider the ℓ−th eigenvalue λℓ for ℓ ∈ {1,...,r} of the matrix M. Under the assumptions of theorem 4, with probability at least 1−O � n−10� , there exists 1 ≤ j ≤ r ��λℓ −λ ⋆ j �� ≤ max � σ � nlog(n),Blog(n) � κr � µ n , max � σ � nlog(n),Blog(n) � λ ⋆max ≤ c1/κ2 (2.59) for sufﬁciently small constant c1 > 0. 16In comparison, the Bauer-Fike theorem (Lemma 2) together with Lemma 1 gives a perturbation bound ��λℓ −λ ⋆ j �� ≤ ∥H∥ ≤ max � σ � nlog(n),Blog(n) � for some 1 ≤ j ≤ r. (2.60) For the low-rank case where r << √n, the eigenvalue pertrubation bound derived in Corollary 5 can be much sharper than the Bauer-Fike theorem. Corollary 3 (Chen et al. (2021b)). Under the same setting of Theorem 4, with probability 1−O � n−9� , ���a⊤U ��� 2 ≤ κ√r ���a⊤U⋆��� 2 + max � σ � nlog(n),Blog(n) � λ ⋆max κ2r � µ n . (2.61) Consequently, by taking a = ei for 1 ≤ i ≤ n in Corollary 6, we arrive at the following statement regarding the alternative deﬁnition of the incoherence of the eigenvalue matrix U. Corollary 4. Under the same setting of Theorem 4, with probability 1−O � n−8� we have ∥U∥2,∞ kr � µ n . (2.62) Proof. Given that ∥U∥2,∞ = max1≤i≤n ��e⊤ i U �� 2 and recalling our assumption implies that the following condition holds ∥U∥2,∞ ≤ � µr/n, we can invoke Corollary 6 and the union bound to derive the advertised entrywise bounds. Proof of Theorem (2) Without loss of generality, we shall assume that λ ⋆ max = λ ⋆ 1 = 1 throughout the proof. To begin with, Lemma 2 implies that for all 1 ≤ ℓ ≤ r, |λℓ| ≤ |λ ⋆ min|−∥H∥ > 1/(2κ) > ∥H∥ (2.63) as long as ∥H∥ < 1/(2κ). In view of the Neumann trick (Theorem 2), we can derive that �����a⊤uℓ − r ∑ j=1 λ ⋆ j u⋆⊤ j uℓ λ j a⊤u⋆ j ����� = ����� r ∑ j=1 λ ⋆ j λ j � u⋆⊤ j uℓ �� ∞ ∑ s=1 1 λ s ℓ a⊤Hsu⋆ j ������ ≤   r ∑ j=1 ���λ ⋆ j ��� ��λ j �� ���u⋆⊤ j uℓ ���   � max 1≤j≤r ∞ ∑ j=1 1 |λℓ|s ���a⊤Hsu⋆ j ��� � ≤ � r r ∑ j=1 ���u⋆⊤ j uℓ ��� 2    max 1≤j≤r ���λ ⋆ j ��� ��λ j ��    � max 1≤j≤r ∞ ∑ j=1 1 |λℓ|s ���a⊤Hsu⋆ j ��� � ≤ √r. 1 |λℓ|. � max 1≤j≤r ∞ ∑ j=1 1 |λℓ|s ���a⊤Hsu⋆ j ��� � 17where the third line follows since ∑r j=1 ���u⋆ juℓ ��� 2 ≤ ∥uℓ∥2 2 = 1, and the last inequality makes use of (49). Apply Corollary 4 in Chen et al. (2021b) to obtain a bound as below ≤ √r |λℓ| ∞ ∑ j=1 � 2c2κ max � Blog(n), � nσ2log(n) ��s � µ n ≤ κ |λℓ|max � Blog(n), � nσ2log(n) �� µr n ≤ κ2max � Blog(n), � nσ2log(n) �� µr n with the provision that |λℓ| > 1/(2κ) and max � Blog(n), � nσ2log(n) � ≤ c1/κ for some sufﬁciently small constant c1 > 0. The condition |λℓ| > 1/(2κ) follows immediately by combining Lemma 2, Lemma 1 and the condition (34). Appendix We justify the existence and uniqueness of an eigenvalue in B(1,∥H∥). Denote with Λ(M) = {λ1,...,λn} and deﬁne a set of auxiliary matrices Mt = M⋆ +tM, 0 ≤ t ≤ 1. (2.64) As we can see the set of eigenvalues of Mt depends continuously on t, we can write as below Λ(Mt) = {λ1(t),λ2(t),...,λn(t)}, (2.65) with each λ j(t), 1 ≤ j ≤ n being a continuous function in t. Meanwhile as ∥H∥ < 1/2 and 0 ≤ t ≤ 1, the two disks B(1,t ∥H∥) and B(0,t ∥H∥) are always disjoint sets. Therefore, the continuity of the spectrum with respect to t requires that λ j(t) to always stay within the same disk where λ j(0) ∈ {0,1} lies, namely, such that λ j(t) ∈ B � λ j(0),t ∥H∥ � . (2.66) Given that M⋆ has n−1 eigenvalues equal to 0 and one eigenvalue equal to 1, we establish the lemma for the rank-1 case. Rank-r Follow the above argument for the case where we have a rank−1 matrix, we can immediately show that: if ∥H∥ < λ ⋆ r /2, then (i) there are exactly n − r eigenvalues lying within B(0,∥H∥), (ii) all other eigenvalues lie within ∪1≤j≤rB � λ ⋆ j ,∥H∥ � , which are exactly top−r leading eigenvalues of M. 18Proof of Theorem 2 [Chen et al. (2021b)] Following the deﬁnition of eigenvectors, we have that (M⋆ +H) = λℓuℓ, or equivalently, 1 λℓ M⋆uℓ = � I− 1 λℓ H � uℓ (2.67) When ∥H∥2 < |λℓ|, one can invert � I− 1 λℓH � and therefore we obtain that uℓ = � I− 1 λℓ H �−1 1 λℓ M⋆uℓ = 1 λℓ � I− 1 λℓ H �−1� r ∑ j=1 λ ⋆ j u⋆ ju⋆⊤ j � uj = r ∑ j=1 λ ⋆ j λ j � u⋆ ju⋆⊤ j �� I− 1 λℓ H �−1 u⋆ j, where the last term follows by rearranging terms. Finally, replacing � I− 1 λℓH �−1 with the Neumann series we obtain that ∑∞ s=0 1 λ sHs, we establish the theorem. Proof of Lemma 3 [Chen et al. (2021b)] We start with the rank-1 case. Towards this, we resort to the Neumann trick in Theorem 2, which in the rank-1 case gives the following expression u = 1 λ � u⋆⊤ j u⋆ j � ∞ ∑ s=0 � 1 λ H �s u⋆. (2.68) From Lemma 2, we know that λ is real-valued and that λ > 1−∥H∥ ≥ 3/4 > ∥H∥ under our assumption. This together yields that ����u− u⋆⊤u λ u⋆ ���� 2 ≤ 1 λ ∞ ∑ s=0 ���� 1 λ H ���� s ∥u⋆∥2 = 1 λ ∞ ∑ s=0 ���� 1 λ H ���� s = ∥H∥ λ (λ −∥H∥) ≤ 8 3 ∥H∥, (2.69) where the last inequality holds since λ ≥ 3/4 and λ −∥H∥ ≥ 1−2∥H∥ ≥ 1/2. Next, by decomposing u into two orthogonal components u = � u⋆⊤u � u⋆ + � u− � u⋆⊤u � u⋆� (2.70) we obtain that ���u⋆⊤u ��� = ��� � u⋆⊤u � u⋆��� 2 = � 1− ��u− � u⋆⊤u � u⋆��2 2 ≥ 1− ���u− � u⋆⊤u � u⋆��� 2 2 ≥ 1− ����u− u⋆⊤u λ u⋆ ���� 2 2 ≥ 1− 64 9 ∥H∥2 . the particular inequality holds since � u⋆⊤u � u⋆ is orthogonal projection of u onto the subspace spanned by u⋆, and hence ���u− � u⋆⊤u � u⋆��� 2 ≤ ����u− 1 λ � u⋆⊤u � u⋆ ���� 2 . (2.71) 19Moreover, since u is real-valued we obtain the following bound: min � ∥u−u⋆∥2,∥u+u⋆∥2 � = � ∥u∥2 2 +∥u⋆∥2 2 −2 ��u⋆⊤u �� ≤ 8 √ 2 3 ∥H∥. (2.72) Next, we consider the rank−r case, since for any 1 ≤ r ≤ r we have that r ∑ j=1 ���u⋆⊤ j uℓ ��� 2 = ����� r ∑ j=1 � u⋆⊤ j uℓ � u⋆ j ����� 2 2 = 1− �����uℓ − r ∑ j=1 � u⋆⊤ j uℓ � u⋆ j ����� 2 2 ≥ 1− �����uℓ − r ∑ j=1 λ ⋆ j u⋆⊤ j uℓ λ j u⊤ j ����� 2 2 where the inequality arises since ∑r j=1 � u⋆⊤ j uℓ � u⋆ j is the Euclidean projection of uℓ onto the space of {u⋆ 1,...,u⋆ r}. Furthermore, we observe that ����� r ∑ j=1 λ ⋆ j � u⋆⊤ j uℓ � u⋆ j ����� 2 ≤ � ∑ j � λ ⋆ j �2���u⋆⊤ j uℓ ��� 2 ≤ λ ⋆ max � ∑ j ���u⋆⊤ j uℓ ��� 2 = λ ⋆ max∥uℓ∥2 = 1. (2.73) This taken collectively with Theorem 2 leads to �����uℓ − r ∑ j=1 λ ⋆ j u⋆⊤ j uℓ λℓ u⋆ j ����� 2 = ����� r ∑ j=1 λ ⋆ j λℓ � u⋆⊤ j uℓ �� ∞ ∑ s=1 1 λ 2 ℓ Hsu⋆ j ������ 2 = ����� 1 λℓ r ∑ j=1 1 λ s ℓ Hs � r ∑ j=1 λ ⋆ j � u⋆⊤ j uℓ � u⋆ j ������ 2 ≤ 1 |λℓ| ∞ ∑ s=1 1 |λℓ|s ∥H∥s ����� r ∑ j=1 λ ⋆ j � u⋆⊤ j uℓ � u⋆ j ����� 2 ≤ 1 |λℓ|. ∥H∥ |λ j|−∥H∥ ≤ 8κ2 3 ∥H∥. The last two lines follow since, when ∥H∥ < 1/(4κ). We can show that |λℓ| > |λ ⋆ min| − ∥H∥ ≥ 3/(4κ) and (ii) |λℓ|−∥H∥ ≥ ��λ ⋆ min ��−2∥H∥ ≥ 1/(2κ). Proof of Lemma 5 on Edge and Vertices [Chen et al. (2021b)] To establish this lemma, we exploit entrywise independence of H and develop a combinatorial trick. To begin with, we expand the quantity of interest as below � a⊤Hsu⋆�k = ∑ 1≤i(b) t ≤n,0≤t≤s,1≤b≤k k ∏ b=1 ai(b) 0 � Hi(b) t−1i(b) t � u⋆ i(b) s (2.74) In this section, we use that I := � i(b) t |0 < t < s,1 < b < k � ∈ [n](s+1)k, to denote such a collection of (s+1)k indices. Therefore, one can write that E �� a⊤Hsu⋆�k� = ∑ I ∈[n](s+1)k E � k ∏ b=1 ai(b) 0 � s ∏ t=1 Hi(b) t−1i(b) t � u⋆ i(b) t−1 � (2.75) Further details of these proofs can be found in the Appendix of Chen et al. (2021b). 202.2. Limit theory for Functional Data 2.2.1. Simultaneous Diagonalization In this section we brieﬂy study the framework proposed by Yuan and Cai (2010). In particular, before studying the asymptotic properties of the regularized estimators, we ﬁrst investigate the relationship be- tween the eigen structures of the covariance operator for X(.) and the reproducing kernel of the functional space H . As observed in earlier studies, eigenvector structures play prominent roles in determining the nature of the estimation problem in functional linear regression. Recall that K is the reproducing kernel of H1. Because K is continuous and square integrable, it follows from Mercer’s theorem that K admits the following spectral decomposition K(s,t) = ∞ ∑ k=1 ρkψk(s)ψk(t). (2.76) Here ρ1 ≥ ρ2 ≥ ... are the eigenvalues of K, and {ψ1,ψ2,...} are the corresponding eigenfunctions, Kψk = ρkψk, k = 1,2,... (2.77) Moreover, ⟨ψi,ψj⟩L2 = δi j and ⟨ψi,ψj⟩L2 = δi j/ρ j, (2.78) where δi j is the Kronecker’s delta. Convergence rates We now turn to the asymptotic properties of the smoothness regularized estimators. To ﬁx ideas, in what follows, we shall focus on the squared error loss. Recall in this case we have that � ˆαnλ, ˆβnλ � = arg min α∈R,βinH � 1 n n ∑ i=1 � yi − � α + � I xi(t)β(t)dt ��2 +λJ(β) � . (2.79) Remark 4. Due to the fact that the risk matrix employed in the optimal portfolio problem is not a typical covariance operator, then in order to investigate the convergence rates and asymptotic properties of the elements of the risk matrix and its eigenvalues, we can simplify the problem by assuming that the entries of the matrix have a linear process representation. This simpliﬁcation allows to model the heavy tailed entries which represent the tail-risk measures without considering the distributional properties of the forecasts based on the quantile speciﬁcations mentioned above (see, Muller and Stadtmuller (2005)). 21Furthermore, we have that ℓn(β) = 1 n n ∑ i=1 � yi − � I xi(t)β(t)dt �2 . (2.80) Observe that ℓ∞ := E[ℓn(β)] = E � Y − � I X(t)β(t)dt �2 = σ2 + � I � I � β(s)−β0(s) � C(s,t) � β(s)−β0(s) � dsdt = σ2 +∥β −β0∥2 0 . Write ¯β∞λ = arg min β∈H � ℓ∞(β)+λJ(β) � . (2.81) For instance, we have the following decomposition ˆβnλ −β0 = � ˆβnλ − ¯β∞λ � + � ¯β∞λ −β0 � . (2.82) we refer to the terms above on the right hand side as the stochastic error and deterministic error respec- tively. Deterministic Error Write β0(.) = ∞ ∑ k=1 akωk(.) and β(.) = ∞ ∑ k=1 bkωk(.) (2.83) Then, Theorem 3 above implies that ℓ∞(β) = σ2 + ∞ ∑ k=1 (bk −ak)2 , J(β) = ∞ ∑ k=1 γ−1 k b2 k. (2.84) Therefore, ¯β∞λ(.) = ∞ ∑ k=1 ak 1+λγ−1 k ωk(.) =: ∞ ∑ k=1 ¯bkωk(.). (2.85) 222.2.2. Common Functional Principal Components According to Benko et al. (2009), the elements of the risk matrix Γ are functionals of the model estimates which can be estimated with asymptotically negligible bias and a parametric rate of convergence T −1/2 i . For example when the data are generated from a balanced, equidistance design, then it can be easily seen that for i ̸= j this rate of convergence is achieved by the following estimator. Notice that a bias corrected estimator may yield negative eigenvalues. However, in practise these values are small and can be interpreted close to zero. For instance, when evaluating eigenvalues and eigenfunctions of the empiri- cal covariance operator of nonparametrically estimated curves ˆXi, then for ﬁxed r ≤ r0 the above rate of convergence for the estimated eigenfunctions may well be achieved for a suitable choice of smoothing parameters. For example, when using standard methods it does not seem to be possible to obtain a corre- sponding rate of convergence, since any smoothing bias ��E � ˆXi � −Xi(t) �� will invariably affect the quality of the corresponding estimate of ˆλr. Main Results In the following, we have that ∥v∥ = �� 1 0 v(t)2dt �1/2 will denote the L2−norm for any square integrable function v. Consider that ˆΣm is the m×m matrix with elements given by � 1 n n ∑ i=1 � β ji − ¯β j �� βki − ¯βk � �m j,k=1 (2.86) Let λ1 �ˆΣm � ≥ λ2 �ˆΣm � ≥ ... ≥ λm �ˆΣm � and ˆζ1,m,..., ˆζm,n denote eigenvalues and corresponding eigen- vectors of ˆΣm. Therefore, some straightforward algebra shows that ˆλr,m = λr �ˆΣm � , ˆγ = gm(t)⊤ ˆζr,m. (2.87) We will use Σm to represent the m×m diagonal matrix with diagonal entries given by λ1 ≤ ... ≤ λm. Then, the corresponding eigevectors are given by the m−dimensional unit vectors denoted by e1,m,...,em,m. Then, by Lemma A of Kneip and Utikal we obtain that the differences between the eigenvalues and eigenvectors of Σm and ˆΣm can be bounded by �ˆλr,m −λr � = trace � er,me⊤ r,m �ˆΣm −Σ �� + ˜Rr,m, ˜Rr,m ≤ 6 sup ∥a∥=1 a⊤ �ˆΣm −Σ �2a min s |λs −λr| (2.88) Moreover, we have that ˆζr,m −er,m = −Sr,m �ˆΣm −Σ � er,m +R∗ r,m, ��R∗ r,m �� ≤ 6 sup ∥a∥=1 a⊤�ˆΣm −Σ �2a min s |λs −λr| , (2.89) where we denote with Sr,m = ∑s̸=r 1 λs−λr es,me⊤ s,m. 23Assumption 1 implies that E � ˆβr � = 0 and Var � ˆβr � = λr r and with δii = 1 as well as δi j = 0 for i ̸= j, we obtain that E � sup ∥a∥=1 a⊤ �ˆΣm −Σ � � ≤ E � trace ��ˆΣm −Σ �2�� = E � m ∑ j,k=1 �1 n � β ji − ¯β j �� βki − ¯βk � −δjkλ j �2� ≤ E � ∞ ∑ j,k=1 �1 n � β ji − ¯β j �� βki − ¯βk � −δjkλ j �2� = 1 n � ∑ j ∑ k E � β 2 jiβ 2 ki � � +o(n−1) = O � n−1� for all m. Furthermore, since trac � er,me⊤ r,m �ˆΣm −Σm �� = 1 n n ∑ i=1 � βri − ¯βr �2 −λr (2.90) Therefore after applying the central limit theorem we obtain that √n � ˆλr −λr � = 1 √n n ∑ i=1 � βri − ˆβr �2 −λr +Op � n−1/2� = 1 √n n ∑ i=1 � (βri)2 −E � (βri)2�� +Op � n−1/2� → N (0,Λr). Obviously the event ˆλr−1 > ˆλr > ˆλr+1 occurs with probability 1. Remark 5. Therefore, via the asymptotic analysis and the econometric model above we aim to verify that the position of the node in the network in terms of centrality, such as for example highly central nodes versus nodes which are on the periphery of the network can have an impact on the stability properties of a community of nodes. For example, if a node is quite central then the impact of the addition of a node to the particular node can dramatically change the dynamics in the network in the sense that that node as well as the structure of the remaining nodes can have higher inteconnectedness. Even though this outlier node is not highly interconnected affects the stability of the remaining of the nodes. 242.3. Limit theorems for Dependent Sequences In this Section, we consider some important results presented in the literature for dependent sequences. We consider a "high-dimensional" probability space (Ω,F,P). For functions of dependent random vari- ables (Xi)i∈N the main challenge is often to quantify and bound the dependence among random variables Xi, in terms of various types of mixing coefﬁcients (see, Kontorovich and Ramanan (2008)). To establish concentration bounds a sufﬁciently rapid decay of the mixing coefﬁcients is necessary. 2.3.1. Method of bounded martingale differences The particular methodology presented in the paper of Kontorovich and Ramanan (2008) provides a suit- able mechanism of establishing concentration inequalities (see also Doukhan et al. (1995)). Theorem 3. Suppose that S is a countable space, F is a set of all subsets and P is a probability measure on (S n,F) and ϕ : S n → R is a Lipschitz function (with respect to the Hamming metric) on S n for some c > 0. Then, for any t > 0, P(|ϕ −Eϕ| ≥ t) ≤ 2exp � − t2 2nc2 ∥∆n∥2 ∞ � . (2.91) Remark 6. Notice that the bound can be simpliﬁed further. More precisely, given any intial probability distribution p0(.) and stochastic transition kernels pi(.|.),1 ≤ i ≤ n−1, then the probability measure is P � (X1,...,Xi) = x � = p0(x1) i−1 ∏ j=1 pj � xj+1|xj � , ∀ 1 ≤ i ≤ n (2.92) (see also Finner (1992)). 2.3.2. Concentration Inequalities for Dependent Random Variables We consider the concept of pointwise inequalities, that is, inequalities that hold uniformly for any param- eter θ ∈ Θ. Deﬁne the function (see, van de Geer (2002)) ψα(x) := exp(xα)−1, for any x > 0. (2.93) For a real-valued random variable ξ, we deﬁne with ∥ξ∥ψα := inf � λ > 0 : E � ψα �|ξ| λ �� ≤ 1 � (2.94) Moreover, we write that ξ ∈ L q for some q > 0 if it holds that ∥ξ∥q := {E(|ξ|q)}1/q (2.95) 25Deﬁnition 2 (Orlicz-norm). For any convex function ψ : R+ → R+ such that ψ(0) = 0 and ψ(x) → ∞ as x → ∞ and (real-valued) random variable X, we denote with ∥x∥ψ the Orlicz-norm, which is deﬁned by ∥X∥ψ := inf � C > 0 : E � ψ �|X| C �� ≤ 1 � . (2.96) • Denote the ℓp Orlicz-norm of X by ∥X∥p for p ∈ [0,+∞) by setting ψ(x) = xp and ∥X∥eγ the exponential Oricz-norm for γ > 0 by setting ψ(x) = exp(xγ)−1 for some γ ≥ 1. • The function ψ(x) is the convex hull of x �→ exp(xγ) − 1 for some γ ∈ (0,1), which ensures con- vexity. • Moreover, when X is a random vector, we deﬁne its Orlicz-norm by ∥X∥ψ := sup∥u∥≤1∥u′X∥ψ. 2.3.3. Central limit theorems for high dimensional dependent data Following the framework proposed by Chang et al. (2021) , recall that we deﬁne with Sn,x = n−1/2 ∑n t=1 Xt. Let G ∼ N (0,Ξ) where Ξ := Cov � n−1/2 ∑n t=1 Xt � . Without loss of generality we assume that G is in- dependent of X = {X1,...,Xn}. We write with Xt = (Xt,1,...,Xt,p)′. Then, the long-run variance of the j−th coordinate marginal sequence � Xt, j �n t=1 is deﬁned as Vn, j = Var � 1 √n n ∑ t=1 Xt, j � . (2.97) Therefore, in order to determine the convergence rate of ρn for the α−mixing sequence {Xt}, we impose additional regularity conditions. The above condition assumes that the partial sum 1 √n ∑n t=1Xt, j is non- degenerated which is necessary to bound the probability of a Gaussian vector taking values in a small region. When � Xt, j � t≥1 is stationary, then it holds that Vn, j := Γj(0)+2 n−1 ∑ k=1 � 1− k n � Γj(k) (2.98) where Γk(k) = Cov � X1, j,Xk+1, j � is the autocovariance of � Xt, j � t≥1 at lag k. Assumption 1 (Subexponential moment). There exists a sequence of constants Bn ≥ 1 and a universal constant γ1 ≥ 1 such that ��Xt, j �� ψγ1 ≤ Bn for all t ∈ [n] and j ∈ [p]. Assumption 2 (Decay of α−mixing coefﬁcients). There exist some universal constants K1 > 1,K2 > 0 and γ2 > 0 such that αn(k) ≤ K1e(−K2kγ2) for any k ≥ 1. Assumption 3 (Non-degeneracy). There exists a universal constant K3 > 0 such that minj∈[p]Vn, j ≥ K3. 26Example 3. Consider the inverse of the covariance matrix such that ��� ��� ��� ����Σ −1 i ��� ��� ��� ��� ψ ≤ C. (2.99) For the polynomial case, applying the union bound followed by Markov’s inequality we conclude that max i ����Σ −1 i ��� ≤P n1/p and max i,t, j ���X(j) i,t ��� ≤P (nkT)1/p . (2.100) Lemma 4. Let X and Y be random elements deﬁned in the same probability space (Ω,F,P) taking values in the metric space (S,d). Then for measurable A and δ ≥ 0 Due to the fact that x �→ exp �� x � |||X|||eγ �γ� is non-decreasing then P(|X| ≥ x) = P � exp �� |X| � |||X|||eγ �γ� ≥ exp �� x � |||X|||eγ �γ�� ≤ exp � − � x � |||X|||eγ �γ� Eexp �� |X| � |||X|||eγ �γ� . It holds that, ψeγ(x) = Kγx1 � 0 ≤ x ≤ aγ � +[exp(xγ)−1]1 � x ≥ aγ � (2.101) where Kγ := (expaγ γ−1) aγ and aγ is deﬁned as below aγ := inf � x ∈ R+ : x ≥ �1−γ γ �1/γ� (2.102) Moreover, it holds that �1−γ γ �1/γ ≤ aγ ≤ �1 γ �1/γ . (2.103) Example 4. Consider the martingale sequence Sn = n ∑ i=1 Xi,n ≥ 1. (2.104) Consider the Fi−1measurable random variables Ki > 0, for i = 1,2,.... Deﬁne with B2 0 = 0 and for any n ≥ 1 such that B2 n = n ∑ i=1 K2 i � 1+E � ψ �|Xi| Ki ���Fi−1 �� (2.105) 27Theorem 4. Let ψ be an Orlicz function such that it holds that supx,y→∞ψ(x)ψ(y)/ψ(cxy) < ∞, for some constant c. Suppose that {Zθ : θ ∈ Θ} is a separable stochastic process indexed by θ in the pseudo-metric space (Θ,τ). Assume that ∥Zθ −Zϑ∥ψ ≤ C′ � diam(Θ) 0 ψ−1� D(δ) � dδ (2.106) where diam(Θ) is the diameter of Θ and D(δ) is the δ−packing number. Corollary 5. Let Wi be Fi−measurable and E(Wi|Fi−1) = 0 for i ≥ 1. Suppose that for some constant c < ∞ it holds that E � ψ �|Wi| c ���Fi−1 � ≤ 1, almost surely i = 1,2,... (2.107) Remark 7. Partitioning entropy could be applied to nonstationary time series? This could be the case when considering a discretenized method, such as block of nonstationary time series (i.e., m−dependence). Notice that this paper doesn’t have in depth explanation of the dependence structure. However, the Orlicz norm provides related moment condition for understanding the asymptotic behaviour. We deﬁne with φ(d) the following quantity φ(d) = � d 0 H1/2(δ,d)dδ ∨d := min �� d 0 H1/2(δ,d)dδ,d � , (2.108) • What type of dependence structure does the entropy integral φ(d) introduce? For example, what form this integral would have in the case of Garch processes or for the autoregressive model? Are there any related results to Hoeffding’s inequality for β−mixing sequences? To derive the proofs of main results presented in the paper we use that P(A) ≤ exp � −βα +2β 2b2� . • All probability bounds are derived with respect to Sn, which the sum of stationary martingale dif- ferences. Under the assumption of stationary sequences we assume that sub-Gaussianity condition holds in order to obtain probability bounds. Deﬁne with g(y1,...,yn) a measurable function of the data, which for example could be extended to sample moments of estimators. • An important related assumption is the Geometric ergodicity which along with β−mixing can facilitate the development of further the asymptotic theory in time series model. Moreover the par- titioning entropy condition clearly holds in the case of stationary sequences but the main challenge in the case of nonstationary time series with a LUR process representation is the presence of the nuisance parameter of persistence. • The theoretical framework presented in the paper shows that the theory can be also extended to the case of M estimators (such as quantile autoregression) using suitable smoothing conditions and deriving the corresponding probability bounds. 282.3.4. Sub-Weibull random vectors under β−mixing Most analysis on lasso assume that data that have sub-Gaussian or subexponential tails. These assump- tions ensure that the moment generating function exists, at least for some values of the free parameter. Nonexistence of the moment generating function is often taken as a deﬁnition of having a heavy tail. Deﬁnition 3 (Sub-Weibull random variables). A sub-Gaussian random variable X can be deﬁned as one for which E(|X|p)1/p ≤ K√p, ∀ p ≥ 1, where K is constant. (2.109) A natural realization which allows for heavier tails is as follows. Fix some γ > 0, and require ∥X∥p := (E|X|p)1/p ≤ Kp1/γ ∀ p ≥ 1∨γ. (2.110) Notice that the condition above requires that the tail is no heavier than that of a Weibull random variable with parameter γ. Deﬁnition 4 (Sub-Weibull Random Variables Properties). Let X be a random variable. Then, the follow- ing statements are equivalent for every γ > 0. The constants K1,K2,K3 differ from each other at most by a constant depending only on γ. (i) The tails of X satisﬁes P(|X| > t) ≤ 2exp � − � t K1 γ�γ� ∀ t ≥ 0. (2.111) 2.3.5. Approximation Theorems for Strongly Mixing Random Variables Theorem 5. Suppose that (Xk) is a strictly stationary sequence of real-valued Random Variables with E(Xk) = 0, E � X2 k � < ∞ and Var(Sn) → ∞ as n → ∞. Suppose that δ > 0 and that λ > 1 +3/δ are real numbers, such that α(n) = o � (log)−λ� as n → ∞, and sup n→∞ E|Sn|2+δ (VarSn) (2+δ) 2 < ∞. (2.112) Then, ∃ σ2, such that 0 < σ2 < ∞, such that lim n→∞ 1 nVar(Sn) = σ2. Thus, without changing its probabil- ity process � S(t),t ≥ 0 � can be redeﬁned on another probability space, together with a Wiener process � W(t),t ≥ 0 � P ���S(t)−W(σ2t) ��� = o � t1/2(loglogt)−1/2� as t → ∞ (2.113) 292.4. Discussion The various mixing properties of time series (either contemporaneously or temporally dependent) are important for understanding how these particular features affect the asymptotic theory for model estima- tors, test statistics and the relevant properties of model selection methods based on dependent data which exhibit these features. Some potential data speciﬁc features such as the presence of serial dependence among errors and regressors, heteroscedasticity and fat tails can lead to misspeciﬁed time series models which especially in a high dimensional setting require to establish validity via error bound determination (see, Adamek et al. (2023) and Wong et al. (2020) among others). In particular, shrinkage methodologies assume a certain structure on the unknown parameter vector of interest. Generally, the sparsity condition implies that a small but unknown subset of the high dimen- sional vector of covariates is considered to have "signiﬁcantly different than zero" coefﬁcients, while the remaining subset of covariates have negligible, or even exactly zero, coefﬁcients. In other words, for Lasso shrinkage methodologies to render meaningful inference the penalty function exploits the under- line sparsity condition. Speciﬁcally, Adamek et al. (2023) obtain novel theoretical results for both point estimation and inference via the desparsiﬁed lasso. Furthermore, they consider a general time series framework where the regressors and error terms are allowed to be non-Gaussian, serially correlated and heteroscedastic and the number of variables can grow faster than the time dimension. Example 5 (Nuclear Norm Regularized Estimation). Denote the Frobenius norm of an (N ×T) matrix A be a ∥A∥2 := � ∑N i=1 ∑T t=1A2 it �1/2 which implies that ∥A∥2 2 = � ∑N i=1∑T t=1 A2 it � . Then, the OLS estimator of β is given by �βOLS := min λ∈RN×R, f ∈RT×R 1 2NT ��Y −β.X −λ f ′��2 2 , (2.114) Relevant research questions of interest: • How does the limiting distribution of the OLS estimator in the above high-dimensional environment is affected under different mixing conditions and distributional assumptions on the error term? • What approach we need to follow in order to obtain error bounds on relevant statistical quantities? Example 6 (Prewhitening Estimators). The idea behind prewhitening goes like this: Suppose one is nonparametrically estimating a function f(λ) at λ0 by taking unbiased estimates of f(λ) at a number of points λ in a neighbourhood of λ0 and averaging them. Additionally, if the function f(λ) is ﬂat in this neighbourhood, then this procedure yields an unbiased estimator of f(λ0). If f(λ) is not ﬂat in this neighbourhood, then the procedure is biased and the magnitude of the bias depends on the degree of non-constancy of f(λ). In the time series literature, the idea of prewhitening has been applied to nonparametric estimators of the spectral density function. In this case, one tries to transform (ﬁlter) the data in such a way that the transformed data is uncorrelated, since an uncorrelated sequence has a ﬂat spectral density function. Understanding the technical tools of learning theory and mixing conditions are useful for this application as well. 303. Time Series Regression Models with Many Covariates A recent growing literature develops econometric frameworks for estimation and inference in regression models with many covariates. Relevant studies include among others Cattaneo et al. (2018), Karmakar et al. (2022) and Wei et al. (2023) (see, also Farrell (2015) although not in a time series setting). 3.1. A forecasting Application Let ˆyt be the forecast of yt based upon information up to t − 1. Notice that when the interested of the researcher is the one-period ahead forecast, (yt − ˆyt)2 is the cost to be minimized. However, there are two situations where the accumulated cost function, denoted with ∑t j=1 (yt − ˆyt)2 is more appropriate. For example, in the sequential forecast case, the forecaster are updated sequentially over many periods and therefore the accumulated cost function is the target to be minimized. Consider for example the one-period expected loss function E(yT+1 − ˆyT+1)2. For instance, for the AR(1) model, under the assumption that E � ε2 t |Ft−1 � = σ2 almost surely for all t, then it can be shown that under appropriate assumptions that 1 T ∑T t=1 (yT+1 − ˆyT+1)2 almost surely. Let ˆβt be the Least square estimate of β, then we have that ˆβt = � t ∑ j=1 Yj−1Y′ j−1 �−1 � t ∑ j=1 Y′ j−1yk � (3.1) where Yt = {y1,...,yt}′, then ˆyt = ˆβ ′ t−1Yt−1 is the least square prediction of yt at time t −1. Theorem 6 (Dhrymes (2013)). Assume that εt are i.i.d random variables with E(εt) = 0 such that it holds 0 < E � ε2 t � < σ2 < ∞. Moreover, let Xt = (xt−1,....,xt−p)′, ST = ∑T t=1 εt. Lemma 5 (Dhrymes (2013)). Assume that Xt+1 = AXt +εt, where εt = (εt,0,....,0)′ and the eigenvalues of A are all inside the unit circle. Then, we have that lim T→∞ T ∑ t=1 XtSt � T T ∑ t=1 S2t = 0, almost surely, (3.2) Proof. It is known that lim T→∞ 1 T T ∑ t=1 XtX′ t = Σ, almost surely, (3.3) where Σ is a positive deﬁnite matrix. Further details can be found in Dhrymes (2013). 313.1.1. Best Linear Predictor The main tool of prediction, or forecasting, in time series relies on the concept of the best linear pre- dictor which is often employed in order to obtain the limiting distribution of an arbitraty size vector of autocovariance estimators (see, Dhrymes (2013)). Deﬁnition 5. Let {Xt : t ∈ T } be a zero mean stationary time series indexed on the linear index set. The best linear predictor of Xt+h,h ≥ 1, given {X1,...,Xn}, is the function given by ˆXt+h = n ∑ j=1 αjXn+1−j (3.4) which minimizes S = E � Xn+h − n ∑ j=1 αjXn+1−j �2 (3.5) The ﬁrst order conditions are given by ∂S ∂α = −2E � Xn+h −α′X(n) � X′ (n) = 0, X(n) =   Xn Xn−1 ... X1   (3.6) Thus, by rearranging we get that E � Xn+hX′ (n) � = α′E � X(n)X′ (n) � (3.7) or α = Cn(i− j)cnh. Proposition 1 (Dhrymes (2013)). The limiting distribution of the coefﬁcient vector of the best linear predictor (BLP) obeys the following limit √n( ˆα −α) d→ N � 0,GVG′� , (3.8) where the matrix G is deﬁned as below: G = C−1 n � F − � α′ ⊗In � (BST,0) � . (3.9) 323.1.2. Forecast Combination Assume that we are interested to construct a forecast combination criterion based a speciﬁc econometric model of interest. Then, a simple approach would be to consider the construction of a criterion which uses OOS forecast combinations. Let ft be a sequence of out-of-sample forecasts for yt+1 derived using a set m different econometric models. The combination forecast is then deﬁned as f(w) = w ′f. Therefore, the OOS empirical MSFE is given by ˆσ2 = 1 P n ∑ t=n−P � yt+1 −w ′ft �2 (3.10) Furthermore, the Granger-Ramanathan forecast combination method implies to select w to minimize the OOS MSFE. However, minimization over w is equivalent to the least-squares regression over yt on the forecasts yt+1 = w ′f+εt+1. Then, the unrestricted least-squares gives the following vector of weights ˆw = � n ∑ t=n−P ftf ′ t �−1 n ∑ t=n−P ftyt+1 (3.11) Notice that the above unrestricted least-squares approach can produce weights which are far outside [0,1], thus an alternative representation can be constructed by subtracting yt+1 from each side, such that 0 = w ′f−yt+1 +εt+1, and by deﬁning et+1 = (ft −yt+1) to be the negative forecast errors. Then, it holds that 0 = w ′et+1 +εt+1, which is the regression of 0 on the forecast errors. Therefore, the constrained GR weights solve the following problem. min w � w ′Aw � subject to M ∑ m=1 w(m) = 1 and 0 ≤ w(m) ≤ 1 (3.12) where A = ∑ t et+1e ′ t+1, the M ×M matrix of forecast error empirical variances/covariances. Proposition 2. In linear models, the combination forecast is the same as the forecast based on the weighted average of the parameter estimates across the different models. That is, ˆyn+1(w) = M ∑ m=1 w(m)ˆyn+1(m) = M ∑ m=1 w(m)xn(m) ′�β(m) ≡ xn(m) ′�β(w) (3.13) where �β(w) = M ∑ m=1 w(m)�β(m). Relevant studies include Hansen (2007), Hansen (2008) and Cheng and Hansen (2015) while a recent approach related to forecasting can be found in Boot and Nibbering (2019) and Swanson et al. (2020). For nonstationary time series models relevant studies include Hansen (2010), Coroneo and Iacone (2020) and Kejriwal and Yu (2021) among others. 333.2. Statistical Theory for Lasso Regression Models 3.2.1. Statistical Principles Example 7. (Predicting inﬂation rates) • Minimization of the empirical risk for a sample {y1,...,yn} : �µ = arg min µ∈R 1 T n ∑ t=1 (yt − µ)2 . (3.14) • Taking the ﬁrst-order conditions give: ∂ ∂µ � 1 T n ∑ t=1 (yt − µ)2 ����� µ=�µ = −2 n n ∑ t=1 (yt − �µ) = 0 (3.15) Thus we obtain that, �µ := 1 T n ∑ t=1 yt ≡ ¯y. Corollary 6 (Shrinkage method). If we drop the requirement of unbiasedness, can we ﬁnd estimators �β with the following property: K ∑ j=1 MSE ��β j � ≤ K ∑ j=1 MSE ��β ols j � (3.16) where MSE ��β j � = E ���βk −βk �2� . • If we aim for (3.16) to hold for all possible β ∈ RK then, this is equivalent to imposing a condition that the OLS estimator is admissible. • If we aim for (3.16) to hold for some given ﬁxed β ∈ RK, then ﬁnding such a �β that improves on �β ols is relatively easy. Corollary 7 (Shrinkage: Ridge Regression). Suppose that there exists a given penalty parameter λ ≥ 0 the ridge regression estimators is given by the following expression �β ℓ2(λ) = arg min b∈RK � 1 n n ∑ t=1 (yt −xtb)2 +λ ∥b∥2 2 � ≡ � X′X +nIk �−1� X′y � (3.17) Recall that the Euclidean norm is given by: ∥b∥2 = √ b′b. In other words, the inverse of (X′X) is regularized by nλIk (called Tikhonov regularization for ill-posed problems). 34Special cases of Ridge Regression include: • When the design matrix X′X is singular then it corresponds to a large variance of the OLS estimator. • When n−1X′X = I p, then simply �β ℓ2(λ) = (1+λ)−1 �β ols. Corollary 8 (Bias of the Ridge Estimator when p < n). Consider the ridge estimator as below: �β ridge(λ) = � X′X +λI �−1 � X′Y � = (R+λI)−1 R � R−1X′Y � = � R � I +λR−1��−1R �� X′X �−1� X′Y �� = � I +λR−1�−1 R−1R�β ols = � I +λR−1�−1 �β ols. Therefore, it holds that E ��β Ridge(λ)|X � = E �� I +λR−1�−1 �β ols|X � = � I +λR−1�−1 β E � �β Ridge(λ) � = E �� I +λR−1�−1� β ̸= β. Based on the above results we can introduce the model selection idea with an ℓ0−penalty. Thus, in the literature various studies mention that one of the main advantages of the Lasso shrinkage is that it "bets on sparsity". Therefore, using an ℓ0 penalty term we have that ∥β∥0 = p ∑ j=1 1 � β j ̸= 0 � (3.18) which implies that �β ℓ0(λ) = arg min β∈Rp � 1 n n ∑ t=1 � Yt −β ′Xt �2 +λ p ∑ j=1 1 � β j ̸= 0 � � = arg min β∈Rp �1 n ∥Y −Xβ∥2 2 +λ ∥β∥0 � . where λ ≥ 0 is the tuning (penalty) parameter which needs to be chosen. Similarly, using an ℓ1 penalty �β ℓ1(λ) = arg min β∈Rp � 1 n n ∑ t=1 � Yt −β ′Xt �2 +λ p ∑ j=1 ��β j �� � = arg min β∈Rp �1 n ∥Y −Xβ∥2 2 +λ ∥β∥1 � where λ ≥ 0 is the tuning (penalty) parameter which needs to be chosen. The lasso shrinkage approach was proposed in the seminal study of Tibshirani (1996). Various studies examine its statistical properties such as Fan and Li (2001) and Zhang (2010) among others, while many variants of the lasso shrinkage have been proposed in the literature such as Zou and Hastie (2005), Zou (2006), Huang et al. (2008), Park and Casella (2008) and James et al. (2009). A relevant empirical study is present by Katsouris (2021). Recently, the literature focuses on applications of the lasso shrinkage approach with group structure (see, also Huang et al. (2012) and Bing et al. (2022)). 35In particular, Klau et al. (2018) propose the priority lasso, which considers the use of different penalty across a baseline group of covariates (see, Breheny and Huang (2015) for case with grouped predictors) and the remaining set of variables while Campbell and Allen (2017) propose a variable selection method using the exclusive Lasso. Now, the Lasso estimates of the slope coefﬁcients in a linear regression model solve the ℓ1−penalized least regression problem: min β n ∑ i=1 � Yi − p ∑ j=1 β jXi j �2 subject to p ∑ j=1 ��β j �� ≤ s, (3.19) or, equivalently we have that min β n ∑ i=1 � Yi − p ∑ j=1 β jXi j �2 +λ p ∑ j=1 ��β j �� (3.20) where β = (β1,...,βp)′, and s and λ are tuning parameters. The lasso uses a constraint in the form of ℓ1−norm: ∑p j=1 ��β j �� ≤ s. Therefore, by using the ℓ1−penalty, the Lasso achieves variable selection and shrinkage simulatneously, while λ controls the amount of regularization. Denote with β 0 the true vector of parameters. There are three types of errors of interest in LASSO regres- sion, that is, (i) the prediction error: ���X � �β −β 0���� 2 2, (ii) the parameter estimation error: ��� � �β −β 0���� 2 2, and (iii) the model selection error: P � supp ��β � = � β 0�� . In other words, the prediction problem im- plies that given a random sample (yi,xi),i = 1,...,n and the covariates xn+1 of an additional observation, we aim to predict the unobserved outcome yn+1. Choice of λ is achieved using Cross-Validation method: λ ∗ = arg min λ>0 n ∑ i=1 � yi −xi �β (−i) (λ) �2 , (3.21) where �β (−i)(λ) is the estimator �β ℓ1(λ) obtained after dropping observation i from the sample. Notice that cross-validation is usually used to choose the tuning parameter λ for in-sample model estimation and variable selection. However, there are theoretical results that show that LASSO performs well for prediction even for large number of covariates p, as long as the true parameter β is sparse. Under appropriate regularity conditions with high probability (for large n) we have that 1 n ���X �β ℓ1(λ ∗)−Xβ ��� 2 2 ≤ C∥β∥1 � log(p) n (3.22) The cross-validation is usually used to choose the tuning parameter λ for in-sample model estimation and variable selection. Furthermore, a key property for model selection implies that under appropriate regularity conditions with high probability (large n) one can show the following relation �S := � j ∈ {1,..., p} : �β ℓ1 j (λ) ̸= 0 � = � j ∈ {1,..., p} : β j ̸= 0 � . (3.23) 36Lemma 6 (Maximum Inequality for Gaussians). For a sample of n Gaussian random variables Zi, for i = 1,...,n, such that E(Zi) = 0 and E(Z2 i ) ≤ ¯σ2 z , ∀ i, then it holds that P � max i=1,...,n|Zi| ≥ z � ≤ 2ne − nz2 2 ¯σ2z . (3.24) Under our assumptions, U′X j T , j = 1,..., p, is Gaussian with mean zero and variance bounded by C σ2 n . Then, it holds that P ����� U′X n ���� ∞ ≥ z � ≤ 2pe− nz2 2C ¯σ2 . (3.25) The aim is to make (bound) the above probability as small as possible. Moreover, to obtain convergence rates and bounds we can use the following deﬁnitions z = σ � 2log( ep δ ) n , where δ > 0 and e := exp(1). Set also C = 1. Therefore, we obtain that P   ���� U′X n ���� ∞ ≥ � 2log �ep δ � n   ≤ 2e−log( ep δ )+log(p) = 2e−log(e)+log(δ) = 2 eδ < δ. Therefore, we get that 1 n ���X � �β −β 0���� 2 2 ≤ 4σ ���β 0��� 1 � 2log(ep/δ) n . (3.26) Another important aspect related to the statistical properties of the lasso shrinkage approach is the sign consistency. Speciﬁcally, Strong Sign Consistency: implies that one can use a pre-selected λ to achieve consistent model selection via the LASSO. On the other hand, a General Sign Consistency: means that for a random realization there exists a correct amount of regularization that selects the true model. Further discussion and related asymptotic results on the consistency and regularization property3 under different distributional assumptions can be found in Medeiros and Mendes (2016). Lemma 7 (Oracle Property). Let �β ols,S denote the OLS estimator of β 0 S. Suppose that a lower bound for β can be established such that βmin > � λ/n1−ξ/2�� s1/2/φmin � , then it holds that √nα′ ��β S −β 0 S � = √nα′ ��β ols,S −β 0 S � (3.27) for any s−dimensional vector α with Euclidean norm 1. 3Professor Marcelo C. Medeiros gave a seminar with title: "Bridging Factor and Sparse Models" (see, Fan et al. (2021b)) at the Department of Economics, University of Southampton on the 6th of October 2021. 37Example 8 (Modelling Unobserved Heterogeneity). Consider the linear regression model below: y = p ∑ j=1 xjβ 0 j +ε, y = Xβ 0 +ε, (3.28) where X = (x1,...,xp) is an (n× p) matrix, y = (y1,...,yn)′ is an (n ×1) vector and β 0 = � β 0 1 ,...,β 0 p � is the true parameter vector. Then, the homogeneity assumption implies that the regression coefﬁcients β j share the same value in their unknown clusters such that under the null hypothesis: H0 : β 0 j = β 0 A,k for all j ∈ Ak (3.29) which also demonstrates the variable selection consistency property. 3.2.2. Dantzig Selector A large stream of literature has focused on the ℓ1 penalized LASSO estimator of parameters in high- dimensional linear regression when the number of variables can be much larger than the sample size. We consider the linear regression with many covariates such as y = Xβ +ε (3.30) where X is the n×M deterministic design matrix, with M possibly much larger than n, and ε is a vector of i.i.d standard normal random variables. In particular, we are interested in the case of sparsity parameters, which implies that the high-dimensional vector β has coefﬁcients that are mostly 0. Denote with �S(β), the residual sum of squares such that �S(β) = 1 n n ∑ i=1 � Yi − fβ (Zi) �2 (3.31) for all β ∈ RM. Deﬁne the Lasso solution such that �βL = ��β1,L,...., �βM,L � by the following expression �βL = arg min β∈RM � �S(β)+2r M ∑ j=1 ��f j �� n ��β j �� � , (3.32) where r > 0 is some tuning constant. Then, the corresponding Lasso estimator �fL(x) = f�βL(x) = M ∑ j=1 �β j,L f j(z) (3.33) A necessary and sufﬁcient condition of the minimizer is that 0 belongs to the subdifferential of the convex function β �→ n−1 |y−Xβ|2 2 +2r ���D1/2β ��� 1. 38This implies that the Lasso selector �βL satisﬁes the constraint ���� 1 nD−1/2X⊤ � y−X �βL ����� ∞ ≤ r, D = diag � ∥ f1∥2 n ,...,∥fM∥2 n � . (3.34) Various studies in the literature present relevant applications and statistical theory of the Dantzig selec- tor such as Osborne et al. (2000), Candes and Tao (2007), Bickel et al. (2009), Koltchinskii (2009) and James et al. (2009) among others. We say that β ∈ RM satisﬁes the Dantzig constraint if β belongs to the set � β ∈ RM : ���� 1 nD1/2X⊤(y−XβL) ���� ∞ ≤ r � . (3.35) The Dantzig estimator is deﬁned by the following expression �fD(z) = f�βD = M ∑ j=1 �β j,D f j(z), (3.36) where �βD = ��β1,D,..., �βM,D � is the Dantzig selector. By the deﬁnition of Dantzig selector, we have that ����βD ��� 1 ≤ ����βL ��� 1. Notice that the Dantzig selector is computationally feasible, since it reduces to a linear programming problem. Finally, for any n ≥ 1, M ≥ 2, we consider the Gram matrix as below Ψn = 1 nX′X = � 1 n n ∑ i=1 f j(Zi) f j′(Zi) � 1≤j, j′≤M (3.37) and let φmax denote the maximal eigenvalue of ψn. 3.2.3. Oracle inequalities for prediction loss In this section, we prove sparsity oracle inequalities for the prediction loss of the Lasso and Dantzig estimators. These inequalities allow us to bound the difference between the prediction errors of the estimators and the best sparse approximation of the regression function (e.g., by an oracle that knows the truth but is constrained by sparsity). We demonstrate that the distance between the prediction losses of the Dantzig and Lasso estimators is of the same order as the distances between them and their oracle approximations. Recall that an estimator has the oracle property if it is variable selection consistent and the limiting distribution of its subvector corresponding to the non-zero coefﬁcients is the same as if their set were known prior to estimation (see, Giurcanu (2016)). Theorem 7. Let Wi be independent N � 0,σ2� random variables with σ2 > 0. Fix some ε > 0 and integers n ≥ 1, M ≥ 2, 1 ≤ s ≤ M. Let Assumption RE (s,3 + 4/ε) be satisﬁed. Consider the Lasso estimator �fL with r = Aσ � logM n , for some A > 2 √ 2. Then, with probability at least 1−M1−A2/8, it can be proved that ��� �fL − f ��� 2 n is bounded. 393.3. Parameter Estimation and Model Selection Consistency Properties Consider the linear regression model as below (see, Amann and Schneider (2018)) y = Xβ +ε (3.38) where y ∈ Rn is the response vector, X ∈ Rn×p the nonstochastic regressor matrix which is assumed to have full column rank, β ∈ Rp the unknown parameter vector, and ε ∈ Rn the unobserved stochastic error term consisting of i.i.d distributed components with mean zero and ﬁnite second moments, deﬁned on some probability space (Ω,F,P). Model consistency results can be found in Shibata (1986) and Potscher (1991). Moreover, to deﬁne the adaptive Lasso estimator, we consider the following expression Ln(b) = ||y−Xb||2 +2 p ∑ j=1 λ j |bj| |ˆbj| (3.39) where ˆβ denotes the OLS estimator. In particular, we assume that the event � ˆβ j = 0 � to have zero probability, for all j = 1,..., p and thus we do not consider this event occurring in the subsequent analysis. Lemma 8 (Equivalence to LS estimator, Amann and Schneider (2018)). If λ ∗ → 0, then ˆβAL and ˆβLS are asymptotically equivalent in the sense that √n � ˆβAL − ˆβLS � → 0, as n → ∞ for all ω ∈ Ω. (3.40) Remark 8. The above Lemma shows that in the case that λ ∗ → ∞, then the adaptive Lasso estimator is asymptotically equivalent to the LS estimator. Furthermore, in terms of consistency in parameter estimation it can be shown that the adaptive lasso estimator is both pointwise and uniformly consistent for the unknown parameter vector of the possibly high dimensional linear model. 3.3.1. Consistency Properties Consider any sequence (βn)n∈N ⊂ Rp converging to φ and let fn(β) = Pβ � β ∈ ˆβ − � λ ∗ n Md � . (3.41) Then, by the Portmanteau Theorem, we have that 0 ≤ lim inf n lim β∈Rp fn(β) ≤ lim sup n lim β∈Rp fn(β) ≤ lim sup n Pβn �� n λ ∗ � ˆβ −βn � ∈ Md � ≤ Pφ � arg min u Vφ(u) ∈ Md � = 1{m ∈ Md} = 0. 40Then, for any ω ∈ Ω, we have that M = � φ∈ ¯Rp arg min u∈Rp Vφ(u)(ω). (3.42) In other words, while the limit of � n λ ∗ � ˆβ −βn � will in general be random, the set M is not random. In particular, Proposition 8 shows that, for any ω, the union of limits over all possible sequences of unknown parameters is always given by the same compact set M . Speciﬁcally, this observation is central for the construction of conﬁdence regions in the following section. It also shows that while in general, a stochastic component will survive in the limit, it is always restricted to have bounded support that depends on the regressor matrix and the tuning parameter through the matrix C and the quantities ψ and λ 0. Interestingly, M only depends on ψ for the components where ψj = ∞, in which case the set M loses a dimension. In particular, this can be seen as a result of the j−th component being penalized much less than the maximal one, so that the scaling factor used in Theorem 7 is not large enough for this component to survive in the limit. Consider the following expression (see, Amann and Schneider (2018)) ����� �XX′ nk � ˆβAL − ˆβLS �� j ����� = λ j nk 1 ��� ˆβLS, j ��� (3.43) where ˆβAL ̸= 0. Notice that the left-hand side is bounded by L, whereas the right-hand side converges to c |β j| in probability. We therefore get Pβ � ˆβAL, j = 0 � → 1, for all β j ∈ R, satisfying the condition ��β j �� < c L. The set M = M1 acts as a benchmark for conﬁdence sets in the sense that if we take a "slightly larger" set, multiplied with the appropriate factor and centered at the adaptive Lasso estimator, we get a conﬁdence region with minimal asymptotic coverage probability equal to 1. But if we base the region on a "slightly larger" set than M , we end up with a conﬁdence set of asymptotic minimal coverage 0. Remark 9 (Amann and Schneider (2018)). In order to provide some further insights of the main pitfalls, we focus on the case where λ 0 ∈ (0,1]p, that is, the case where all components of λ 0 are nonzero (implying that ψ = 0). In other words, this implies that all components are penalized at the same rate, which is obviously fulﬁlled for uniform tuning. However, in the case of uniform tuning (i.e., parameter penalization with the same rate), then the asymptotic distribution is mere point-mass with no stochastic part surviving in the limit. The reason for this, is the fact that when controlling for the bias of the estimator, the stochastic part vanishes asymptotically. In other words, the appropriate scaling factor is simply not large enough to keep the random component in the limit (i.e., it is asymptotically negligible for large samples). This basically illustrates that the bias is of larger order than the stochastic component when viewed under uniform lens - a fact that is generally inherent to penalized estimators. Moreover, the aspect of omitted variable bias in high dimensional settings is discussed by Wüthrich and Zhu (2023). 41Example 9. Suppose that the data {yt,t = 1− p,...,n} is generated by the model yt = pn ∑ j=1 Φjyt−j +εt, t = 1,...,n (3.44) where yt = � yt,1,...,yt,k � , a (k ×1) vector of variables in the model, where εt is a sequence of i.i.d error terms with N (0,Σ) distribution. Furthermore, all roots of �����Ik − p ∑ j=1 Φjz j ����� are assumed to lie outside the unit disc. Deﬁnition 6 (Restricted Eigenvalue Condition). The restricted eigenvalue condition RE(r) is said to sat- isﬁed for some 1 ≤ r ≤ kp if k2 ψn(r) := min R⊂{1,...,kp} min δ∈Rkp\{0} δ ′Φnδ ∥δR∥2 > 0. (3.45) where R ⊂ {1,...,kp} and |R| is its cardinality. Then, one is interested to investigate the properties of the Lasso shrinkage norm when applied to each equation i ∈ {1,..,k} separately. The Lasso estimates are obtained by minimizing the objective function: L(βi) = 1 n ∥yi −Xβi∥2 +2λn∥βi∥ℓ1 (3.46) Let J( ˆβi) = � j : ˆβi, j ̸= 0 � be the indices of the parameters for which the estimator is non-zero. Example 10. Consider the time series vector Yt, which is an N−dimensional random vector generated by the VAR model such that Yt = A1Yt−1 +...+ApYt−p +ut, t ∈ {1,...,T}. Deﬁne the N(p+1) vector Xt = � Y ⊤ t−p,...,Y⊤ t−1,Y ⊤ t �⊤ and let Σx = Var(Xt) = E � XX⊤� and Γi = E � YtY ⊤ t−i � the autocovariance matrix. An equivalence relation between the matrix coefﬁcients of the two multivariate regression models Bi,kℓ = 0 ⇐⇒ Corr � Y k,t,Y ℓ,t−i ��� Xt\ � Y k,t,Y ℓ,t−1 ��� (3.47) for i ∈ {1,..., p} (see, Poignard and Asai (2023)). Thus, to derive the partial correlation coefﬁcient be- tween two variables in Xt, we rely on the inverse of Σx. Denoting the (k.ℓ)−th element of Σ−1 x by σkℓ x , the partial correlation coefﬁcient between the k−th and the ℓ−th elements of Xt is then ρkℓ x = − σkℓ x √ σkk x σℓℓ x for k ̸= ℓ. Also, we denote with σkℓ u the (k,ℓ)−th element of the matrix Σ−1 u . Then, it can be obtained that Bi,kℓ = ρrs x � Σx,ss(1−ρ2 s\r) Σx,rr(1−ρ2 r\s) �1/2 . (3.48) 423.4. A Lasso-based Time Series Regression Model Example 11. Consider the following Dickey-Fuller regression model ∆yt = ρ∗yt−1 + p ∑ j=1 ∆yt−j +εt. (3.49) Notice that in the case that the autoregression parameter ρ∗ = 0, then the model is said to have a unit root and is said to be nonstationary. According to Kock (2016) it can be shown that: (i) The adaptive Lasso possesses the oracle property in stationary and nonstationary autoregressions (see, Kwiatkowski et al. (1992), Muller (2008), Nielsen (2009)). Hence, the ALasso shrinkage approach can distinguish between stationary and nonstationary autoregressions which is extremely important when choosing the right model for forecasting; (ii) Show that choosing the tuning parameter by BIC results is consistent model selection; (iii) Analyze the asymptotic behaviour of the probability of classifying ρ∗ as 0 in the stationary, non- stationary and local to unity setting such that ρ∗ = c/T (see also Caner and Knight (2013)). In the nonstationary setting the problem due to nonuniformity in the asymptotics, can be alleviated if one is willing to tune the adaptive Lasso to perform conservative model selection instead of consistent model selection. We employ the following variant of the adaptive Lasso which is deﬁned as the minimized of ΨT (ρ,β) = T ∑ t=1 � ∆yt −ρyt−1 − p ∑ j=1 β j∆yt−j �2 +λTwγ1 1 |ρ|+λT p ∑ j=1 wγ2 1 ��β j ��, (3.50) where γ1,γ2 > 0 and w1 = 1/| ˆρI| and w2 = 1/| ˆβI, j| for ˆρI and ˆβI, j denote some initial estimators of the parameters of the model. Furthermore, notice that the objective function is modiﬁed compared to the usual adaptive Lasso since it penalized ρ, the coefﬁcient on the potentially nonstationary variable variable yt, different from the coefﬁcients on the stationary variables. We denote with θ = (ρ∗,β ∗′)′ the set of model parameters. The set of active variables is denoted with A = � 1 ≤ j ≤ p+1 : θ∗ j ̸= 0 � , (3.51) Moreover, denote with S = diag � T, √ T,..., √ T � denotes a (p+1× p+1). Denote with ˆθ = � ˆρ, ˆβ ′�′ denote the minimizer of the objective function. Furthermore, denoting M0 the true model and ˆ M the estimated model, then we can say that the shrinkage methodology is consistent if for all � ρ∗, ˆβ ∗� it holds that P � ˆ M = M0 � → 1. Thus, the shrinkage selection methodology is said to be conservative if for all � ρ∗, ˆβ ∗� it holds that P � ˆ M0 ⊂ ˆ M � → 0, which implies that the probability of excluding relevant variables tend to zero. 433.4.1. Oracle Property We discuss the oracle property of the adaptive Lasso for stationary and nonstationary autoregressions. Theorem 8. (Consistent model selection). Assume that εt is i.i.d with E(ε1) = 0 and E � ε4 1 � < ∞. (A) (Nonstationary Case) Then, if ρ∗ = 0, and it holds that λT T 1−γ1 → ∞, λT T 1/2−γ2/2 → ∞, and λT T 1/2 → 0, the following properties hold (i) Consistency: ����ST �� ˆρ, ˆβ ′�′ −(0,β ∗′)′ ����� ℓ2 ∈ Op(1). (ii) Oracle I: P( ˆρ = 0) → 1 and P � ˆβA c = 0 � → 1. (iii) Oracle II: √ T � ˆβT −βA � → N � 0,σ2[ΣA ]−1� . (B) (Stationary Case) If yt is stationary such that ρ∗ ̸= 0, λT T −1/2−γ2/2 → ∞, and λT T 1/2 → 0, (i) Consistency: ����ST �� ˆρ, ˆβ ′�′ −(ρ∗,β ∗′)′ ����� ℓ2 ∈ Op(1). (ii) Oracle I: P( ˆρ = 0) → 1 and P � ˆβA c = 0 � → 1. (iii) Oracle II: �√ T � ˆβT −βA �√ T � ˆβT −βA �� → N � 0,σ2[ΣA ]−1� . Example 12. Consider the framework proposed by Wong et al. (2020). The particular modelling environ- ment corresponds to (i) stationary Gaussian processes with suitably decaying α−mixing coefﬁcients, and (ii) stationary processes with sub-Weibull marginals and geometrically decaying β−mixing coefﬁcients. Moreover, it is well known that guarantees for lasso follow if one can establish the restricted eigenvalue (RE) conditions and provide deviation bounds for the correlation between noise and the regressors. Then, statistical estimation can be achieved as below: Θ⋆ = argmin Θ∈Rp×q E ���Yt −Θ′Xt ��2 2 � . (3.52) Then, the ℓ1penalized least squares estimator �Θ ∈ Rp×q is deﬁned as �Θ = argmin Θ∈Rp×q 1 T ∥vec(Y −XΘ)∥2 2 +λT ∥vec(Θ)∥1 (3.53) Remark 10. The β−mixing condition has been of interest in statistical learning theory for obtaining ﬁnite sample generalization error bounds for empirical risk minimization. Speciﬁcally, the usefulness of β−mixing lies in the fact that by using a simple blocking technique, (e.g., by Yu (1994)), one can reduce the situation to the i.i.d setting. However, there are no results showing that the RE and DB conditions holds under mixing conditions. 443.5. Lasso shrinkage with long memory regression errors In many problems of practical interest in which the lasso shrinkage is applied, such as for example when modelling and forecasting Realized Volatility measures, it is reasonable to consider the existence of long memory errors. We investigate the asymptotic behaviour of Lasso in regression models with long memory errors. Consider Xi = (xi1,....,xip)′ for i = 1,...,n to be the vector of design matrices and Yi to denote its response variable. Thus we have the model (see, Kaul (2014)) Yi = X′ i β +εi, for some β ∈ Rp, 1 ≤ i ≤ n. (3.54) The errors εi are assumed to be long memory moving average with i.i.d innovations, that is, εi = ∞ ∑ k=1 akζi−k = i ∑ k=−∞ ai−kζk, (3.55) where ak = c0k−1+d,∀k ≥ 1, 0 ≤ d ≤ 1 2 and some constant c0 > 0, and ak = 0 for k ≤ 0. Moreover, we have that ζj, j ∈ Z := {0,±1,±2,...}, are i.i.d RV’s with mean zero and variance σ2 ζ . Without loss of generality we assume that σ2 ζ = 1. We denote with X = � xi j � n×p as the design matrix, and ε := (ε1,...,εn)′. Moreover, {εi,i ∈ Z} is a stationary process with autocovariance function γε(k) = ∞ ∑ j=1 ajaj+k = k−1+2dB(d,1−2d) (3.56) Moreover, the Lasso estimate of β is deﬁned as follows ˆβ n(λ) = arg min β �1 n ��Y −X′β ��2 2 +λn∥β∥1 � , λ > 0, (3.57) where Y = (Y1,...,Yn)′ and ∥β∥1 := ∑p j=1 ��β j �� denotes ℓ1 norm of β = (β1,...,βp)′. Notice that the literature in the area of regularized estimation with dependence considerations is scarce. In this paper, we we investigate the asymptotic behaviour of Lasso under strong dependence structure and less restrictive model assumptions. In particular, we assign a long memory structure on the model errors ε, that is, ∑∞ k=1 |γε(k)| = ∞. We provide restrictions on the rate of increase of the design variables as well as the rate of increase of the dimension of p in order to obtain the corresponding ﬁnite sample error bounds. Furthermore, we allow the design variables to grow with the restriction ∑1≤i≤n x2 i j = O(n), and hence the results obtained can also easily be extended to the case of Gaussian random designs. Notice that the framework proposed by Babii et al. (2022) considers a machine learning application for high-frequency time series panel data. The particular time series ﬁlters are constructed based on mixed- frequency data and therefore assumptions on the dependence structure of the errors such as persistence, mixing and long-memory are indeed plausible in such economic and ﬁnance studies. 453.5.1. Results with ﬁnite sample In this Section we prove a ﬁnite sample oracle inequality for the Lasso solution when the design matrix is non-random. We deﬁne with Wnj = n−(1/2+d) n ∑ i=1 xi jεi = n−(1/2+d) n ∑ i=1 i ∑ s=−∞ xi jai−sζs = n ∑ s=−∞ cns, jζs, (3.58) where we have that cns, j := n−(1/2+d) n ∑ i=1 xi jai−s, s ∈ Z, j = 1,..., p, (3.59) cn, j := sup −∞<s≤n ��cns, j ��, cn = max 1≤j≤pcn, j. (3.60) Moreover, we denote with σ2 n, j := Var � Wnj � , σ2 n = max 1≤j≤pσ2 n, j. (3.61) Therefore, we shall prove that with an appropriate choice of λn, the Lasso solution obeys the following oracle inequality in the long memory case, for any n ≥ 1, 1 n ���X � ˆβ −β ���� 2 2 +λn ��� ˆβ −β ��� 1 ≤ 4λ 2 n s0 φ2 0 (3.62) where λn = (O(1))log(p)/n1/2−d, under some conditions on the design matrix. Moreover, s0 denotes the cardinality of the set of non-zero components of β and φ0 is a constant depending on the design matrix X. Therefore, in order to prove the result we need to obtain a probability bound for the set as below Λ = � max 1≤j≤p 2 n ����� n ∑ i=1 xi jεi ����� ≤ λ0n � , (3.63) for a proper choice of λ0n. Thus, once this probability bound is obtained, the oracle inequality follows by deterministic arguments. Remark 11. The presence of long memory errors can affect the convergence rates of estimators as well as the variable selection procedure. Moreover, the presence of serial correlation can be captures by modeling the error term such that εt = ρεt−1+ut. In particular, there are cases in which serial correlation can manifest as structural breaks in high dimensional models (see, Kapetanios and Zikes (2018)). On the other hand, the long memory property is a common feature of mean-revering processes, which implies the signiﬁcance of sample autocorrelations at large lags. In other words, long memory processes implies that when predicting future values this will depend persistently from the past observations. For instance, d = 0.5 is refereed to the fractional or long-memory parameter of the stochastic process. 46Theorem 9. For the long memory regression model suppose that the design variables satisfy the model assumptions. Further, suppose that the tuning parameter λn is such that λn → λ0 ≥ 0, then we have that ˆβn p→ argmin � Z(φ) � (3.64) Z(φ) = (φ −β)′C(φ −β)+λ0 p ∑ j=1 ��φj ��, φ ∈ Rp (3.65) Thus, if λn = o(1) then arg minφ (Z(φ)) = β and ˆβn(λn) is consistent for the unknown parameter β. Proof. To prove this theorem we consider the following objective function Zn(φ) = 1 n n ∑ i=1 � Yi −X′ i φ �2 +λn n ∑ j=1 ��φj ��, (3.66) then Zn(φ) is convex. Therefore, we need to show the pointwise convergence (in probability) of Zn(φ) to Z(φ)+k2 for some constant. Clearly, it holds that λn n ∑ j=1 ��φj �� → λ0 n ∑ j=1 ��φj ��. (3.67) Consider expanding the expression, 1 n n ∑ i=1 � Yi −X′ i φ �2 = 1 n n ∑ i=1 � εi −X′ i (φ −β) �2 = 1 n n ∑ i=1 ε2 i + 1 n n ∑ i=1 (φ −β)′ XiX′ i (φ −β)− 2 n (φ −β)′ n ∑ i=1 Xiεi, = 1 n n ∑ i=1 ε2 i + 1 n n ∑ i=1 (φ −β)′ XiX′ i (φ −β)− 2 n (φ −β)′ X′ε, Notice that the ﬁrst term in the above expression converges to k2 by the ergodic theorem, since the error sequence {εi} forms a stationary ergodic sequence. The second term converges to (φ −β)′C(φ −β) and the last term converges to zero in probability, since we have that 1 n0.5+d X′ε converges in distribution. Theorem 10. For the long memory regression model assume that the design variables satisfy the model assumptions. Suppose that n1/2−dλn → λ0 ≥ 0 as n → ∞, then n1/2−d � ˆβ n −β � →D arg min u V(u), (3.68) V(u) = −2u′W +u′Cu+λ0 p ∑ j=1 � ujsign � β j � I[β j̸=0] + ��uj ��I[β j̸=0] � (3.69) such that W is an N (0,Σ) random variable. 47Proof. We deﬁne the following expression Vn(u) = n1−2d � n ∑ i=1 1 n �� εi − X′ i u n 1 2−d �2 −ε2 i � +λn p ∑ j=1 �����β j + uj n 1 2−d ���� � − ��β j �� � . (3.70) Furthermore, we denote the ﬁrst term of the above expression by (I) and the second term by (II). Then, we obtain that (I) = n1−2d � 1 n n ∑ i=1 ε2 i −2∑n i=1X′ i uεi nn1−2d + u′∑n i=1 XiX′ i u nn 1 2−d − 1 n n ∑ i=1 ε2 i � = �u′∑n i=1 XiX′ i u n −2∑n i=1X′ i uεi n 1 2+d � → u′Cu−2u′W , as n → ∞, where W ∼ N (0,Σ). Moreover, we have that (II) = n 1 2−dλn p ∑ j=1 ����n 1 2−dβ j +uj ���−n 1 2−d ��β j �� � → λ0 p ∑ j=1 � ujsign � β j � I[β j̸=0] + ��uj ��I[β j̸=0] � . Notice that the notion of weak dependence for stationary time series is measured in terms of covariance functions. The following lemma given by Gupta (2012) is useful. Lemma 9. For each ﬁxed n, let A := ������ n ∑ i=−∞ Yni ����� > r � , Bm = ������ n ∑ i=−m Yni ����� > r −δ � , r > 0,δ > 0,m = 1,2,... B = lim inf m→∞ Bm. If ����� n ∑ i=−∞ Yni ����� < ∞, almost surely, then, for each ﬁxed n, A ⊂ B. For the proof see Gupta (2012). The argument is to let ω ∈ A, then it follows that ����� n ∑ i=−∞ Yni ����� > r, ����� n ∑ i=−∞ Yni ����� < ∞. Remark 12. Another relevant aspect for the lasso shrinkage approach regardless of the properties of the error terms is the cross-validation consistency (see, Chetverikov et al. (2021) and Yang (2007)). More- over, although we discuss various cases of the Lasso shrinkage algorithm, those correspond to a different penalization property as well as possibly different-type of thresholding, especially when one considers the pathwise selection approach against the cross-validation approach. 483.6. Residual empirical process based on the ALasso Moreover, the framework proposed by Chatterjee et al. (2015) provides regularity conditions and as- sumptions for deriving the asymptotic behaviour the adaptive Lasso estimator. Speciﬁcally, the Alasso estimator of β is deﬁned as the minimizer of the weighted ℓ1−penalized least squares criterion function �β n = arg min u∈Rp n ∑ i=1 � yi −x′ iu �2 +λn p ∑ j=1 |uj| ˜β j,n γ , (3.71) where, λn > 0 is a regularization parameter, γ > 0 and ˜β j,n is the jth component of �β n, a consistent preliminary estimator of β. 3.6.1. Main results Asymptotic uniform linearity in high dimensions For t ∈ [0,1], we deﬁne �Zn(t) = 1 √n n ∑ i=1 [1(F(ei) ≤ t)−t] (3.72) Zn(t) = 1 √n n ∑ i=1 [1(F(εi) ≤ t)−t] (3.73) We set J(t) = f � F−1(t) � . The ﬁrst result of this section proves the AUL property of the ALASSO based residual empirical distribution function for p. Theorem 11. sup t∈[0,1] ����Zn(t)− � Zn(t)− ¯x′ n √n ��β n −β � J(t) ���� = op(1). (3.74) Notice that Theorem 11 shows that the AUL property holds for the empirical distribution function of the residuals based on the ALASSO ﬁt even for p >> n, provided the regularity conditions hold and the regression model has enough sparsity. Functional oracle property of the ALASSO The AUL property has various important applications in the context of statistical inference in high dimensional regression. For instance, it allows to establish the asymptotic distribution of the residual edf which, in turn, can be used to carry the goodness of ﬁt tests on F and set conﬁdence bands for F. Under standard weak convergence theory we have that Z OR n = n1/2 � �FOR n −F � →w Z∞ on L∞ ([−∞,+∞]) (3.75) where �FOR n is the edf of the residuals based in OLS under the oracle property. 49Moreover, Z∞ is a zero mean Gaussian process with covariance function as below τ(x,y) = Cov(Z∞(x),Z∞(y)), x,y ∈ (−∞,+∞) (3.76) We also deﬁne the ALASSO based residual empirical process as below Zn(x) = √x � �Fn(x)−F(x) � , x ∈ (−∞,+∞). (3.77) Proof. We denote with Γn (t,u) = 1 √n n ∑ i=1 � 1 � F � εi −n1/2x′ iu � ≤ t �� , t ∈ [0,1], and u ∈ Rp. (3.78) Next, we deﬁne the random vector Wn = n−1/2 ∑n i=1 x′ iεi and write Wn = � W(1)′ n ,W(2)′ n � . 3.6.2. Sparsity in high dimensional estimation problems The Lasso estimator appears to be suitable for high dimensional inference problems such as the case when p is much larger than the time series observations n. Some useful results are as below: Regression in the iid case Under the usual assumption that the εi are iid and subGaussian, ∀ s,E � exp � sε2 i �� ≤ exp �s2σ2 2 � (3.79) for some known σ2, then we have that P ������ 1 n n ∑ i=1 W (j) i ����� ≥ t√n � ≤ ψ(t) = exp � − t2 2σ2 � . (3.80) 3.7. Lasso shrinkage for Predictive Regression In this model, the unknown coefﬁcients β ∗ n can be obtained from the data by running OLS �β OLS = arg min β ∥y−Xβ∥2 , (3.81) The asymptotic behaviour of the OLS estimator has been extensively examined in the time series econo- metrics literature (see, Koo et al. (2020), Lin and Tu (2020), Yousuf and Ng (2021), Lee et al. (2022)). A relevant framework for lasso time series regression is proposed by Adamek et al. (2023). 50In particular, the following functional central limit theorem holds 1 √n ⌊nr⌋ ∑ j=1 � e′ j. uj � ⇒ � Be(r) Bu(r) � ≡ BM(Σ). (3.82) To represent the asymptotic distribution of the OLS estimator, deﬁne u+ i = ui −Σ′ euΣeee′ i. By deﬁnition, Cov � ei j,u+ i � = 0 for all j so that X′u n ⇒ ζ := � 1 0 Be(r)dBu+(r)+ � 1 0 Be(r)Σ′ euΣ−1 ee dBe(r)′ (3.83) which is the sum of a mixed normal random vector and a non-standard random vector. The OLS limit distribution is then given by n � ˆβ OLS −β ∗ n � = �X′X n2 �−1 X′u n ⇒ Ω−1ζ, (3.84) where Ω := � 1 0 Be(r)Be(r)′dr. This implies that when we inﬂate �β OLS j by the factor nδ j so that its mag- nitude is comparable to the constant β 0∗ j , we attain consistency in that nδ j ��β OLS −β ∗ jn � = nδ j �β OLS −β 0∗ j = Op � nδ j−1� = op(1) for all j ≤ p. (3.85) Moreover, if β 0∗ j ̸= 0, when δj is close to 1, the signal of xj is weak, and therefore the rate of convergence is slow. However, notice that some true coefﬁcients β ∗ j could be exactly zero, where the associated predictors would be redundant (inactive) in the regression. Let M∗ = � j : β 0∗ j ̸= 0 � be the index set of regressors relevant to the regression, p∗ = |M∗|, and M∗c = {1,..., p}M∗ be the set of redundant regressors. For simplicity, we refer to M∗ as the active set, meaning that it plays an active role in the regression, and we call M∗c the inactive set. Deﬁne �β ora = ��β ora′ M∗ , �β ora′ M∗ � , where �β ora M∗ = arg min β �����y− ∑ j∈M∗ xjβ j ����� 2 (3.86) and the complement of the oracle estimator is just the remaining coefﬁcients which are identical equal, that is, �β ora M∗c = 0. Notice that the oracle estimator is the infeasible oracle information of M∗. The asymp- totic distribution of the estimator is given by n ��β ora −β ∗ n � M∗ ⇒ ΩM∗ζM∗, (3.87) where ΩM∗ is the p∗ × p∗ submatrix � Ω j, j′� j, j′∈M∗ and ζM∗ is the p∗ ×1 subvector � ζj � j∈M∗. 513.8. Lasso estimation with a structural break Relevant studies with frameworks for lasso estimation robust to the presence of structural breaks in the covariates include Chan et al. (2014), Cheng et al. (2016), Qian and Su (2016) and Smith et al. (2019). For any n−dimensional vector W = (W1,...,Wn)′, deﬁne the empirical norm as ∥W∥n := �1 n ∑n i=1W 2 i �1/2. f(α,τ)(x,q) := x′β +x′δ1 � q < τ � (3.88) f0(x,q) := x′β0 +x′δ01 � q < τ0 � (3.89) ˆf(α,τ)(x,q) := x′ ˆβ +x′ ˆδ1 � q < ˆτ � (3.90) Then, we deﬁne the prediction risk as below �� ˆf − f0 �� n := � 1 n n ∑ i=1 � ˆf (Xi,Qi)− f0(Xi,Qi) �2�1/2 . (3.91) Deﬁne with α0 = � β ′ 0,δ ′ 0 �′. Then, we can rewrite the model as below Yi = Xi(τ0)′α0 +Ui, for i = 1,...,n. Let y ≡ (Y1,...,Yn)′. Then, for any ﬁxed τ ∈ T we consider the residual sum of squares given by Sn(α,τ) = 1 n n ∑ i=1 � Yi −X′ i β −X′ i 1{Qi < τ} �2 ≡ ∥y−X(τ)α∥2 n . (3.92) Remark 13. The examples discussed in the previous sections are concerned with the use of the shrinkage approach under the assumption of sparsity in high dimensional time series regression model for estima- tion purposes (see, also Basu and Michailidis (2015) and García-Donato and Paulo (2022)). Consider that we have p−variate time series then we can quantify the stability of the process based on the largest eigenvalue of the transition matrix. Since the largest eigenvalue is bounded, then a condition to roll out the cross-sectional and serial correlation on the data is ensured. The literature on high-dimensional time series modelling allows for dominant cross-sectional autocorrelations to be accounted for, using com- mon factors. In particular, Cho et al. (2023) propose the factor-adjusted VAR model, in which case the time series is decomposed into two latent components. A relevant framework for robust estimation of high-dimensional VAR Models is given by Wang and Tsay (2023). The frameworks mentioned in Sec- tion 3.8. are suitable for statistical estimation in high dimensional regression models under the presence of structural breaks4. One may be interested to develop an econometric framework for structural break detection within these data-dependent model speciﬁcations such as as the piecewise stationary factor- adjusted VAR models (see, Breitung and Eickmeier (2011)). An alternative algorithmic procedure5 for structural change detection in high dimensional time series models is proposed by Li et al. (2021). 4Dr. Haeran Cho (University of Bristol) gave a seminar titled: "High-dimensional time series segmentation via factor- adjusted VAR modelling", at the S3RI Departmental Seminar Series, University of Southampton on the 24th of March 2022. 5Professor Degui Li (University of York) gave a seminar titled: "Detection of Multiple Structural Breaks in Large Covari- ance Matrices", at the S3RI Departmental Seminar Series, University of Southampton on the 2nd of December 2021. 523.9. Classical Shrinkage Type Estimation Approach In this section, we present the Score function approach proposed in the framework of Sen and Saleh (1987), although the particular methodology doesn’t correspond to a high-dimensional environment, it has many possible applications such as a "Shrinkage type Structural Break Estimation in High-Dimensional Predictive Regressions" or "Testing for Predictability in High Dimensional Environments via a Shrinkage type Structural Break Estimator". Consider the score function given by ψ = {ψ(x),x ∈ R} required for the deﬁnition of M-estimators. Moreover, suppose that ψ(x) = � ψ1(x) + ψ2(x) � , where ψ1(.) and ψ2(.) are both non-decreasing and skew-symmetric such that, ψj(x) +ψj(−x) = 0,∀ x ∈ R, j = 1,2. Denote with AT = (a1,..,aT) and for every b ∈ Rp we deﬁne the following p−dimensional functional with T elements in each dimension MT (b) = � MT1(b),...,MT p(b) �′ = T ∑ t=1 atψ � yt −a′ tb � , b ∈ Rp. (3.93) Then, under the assumption of stationarity then, the RME for ˆβ 1T of the model parameter β 1 is a solution to the following equation MT(1) � b1,0 � = 0. (3.94) We introduce a suitable M−test statistic for testing the null hypothesis H0 : β 2 = 0, ˆMT(2) = MT(2) � ˆβ 1,0 � , (3.95) where ˆβ 1T is the restrained M-estimator (RME) of β 1. Notice that under stationarity, it can be proved that the restrained M-estimator (RME) generally performs better than the unrestrained M-estimator (URE) especially when β 2 is close to 0 (see, Sen and Saleh (1987)). Furthermore, denote with S 2 T = T −1 T ∑ t=1 ψ2 � yt − ˆβ ′ 1Tx(j)t−1 � , xt−1 = � x(1)t−1,x(2)t−1 � , (3.96) Cii. j = Cii −Ci jC−1 j j C ji, for i ̸= j = {1,2}. (3.97) Therefore, an appropriate aligned M−test is given by LT = S−2 T � ˆMT(2)C22.1 ˆMT(2) � . (3.98) Under the null hypothesis, H0, LT has asymptotically the chi-square distribution with p2 degrees of freedom. Therefore, based on the signiﬁcance level α such that 0 < α < 1, the preliminary test under the null, H0, is rejected based on the critical values. 53Considering for example one is interested to the conditional mean speciﬁcation of a predictive regression model, focusing on the OLS against the IVX estimation approaches based on the full set of regressors in the model. The, the corresponding expressions are: S 2 T = T −1 T ∑ t=1 ψ2 � yt − ˆβ ′ 1T,olsx(j)t−1 � , xt−1 = � x(1)t−1,x(2)t−1 � , j ∈ {1,2}, (3.99) and � S 2 T = T −1 T ∑ t=1 ψ2 � yt − ˆβ ′ 1T,ivxx(j)t−1 � , xt−1 = � x(1)t−1,x(2)t−1 � , j ∈ {1,2}. (3.100) In other words, the authors of the particular framework focus on preliminary test M-estimation (PTME) formulation, thus the ˆβ PT 1n is chosen as the RME or UME, according as this preliminary test leads to the acceptance or rejection of the null hypothesis H0. Moreover, the shrinkage M−estimator (SME), based on the usual James-Stein rule incorporates the same test statistic in a smoother manner. When β 2 is very close to 0, generally both the PTME and SME perform better than the UME, but the RME may still be better than either of them. On the other hand, for β 2 away from 0, the RME may perform rather poorly, while both the PTME and SME are robust. This relative picture on the performance characteristics of all four versions of M-estimators can be best studies within an asymptotic framework. One can consider the notion of asymptotic distributional risk (ADR) as well as the asymptotic risk efﬁciency (ARE) for the various versions of the M-estimators (see, Sen and Saleh (1987)). 3.9.1. Limiting Distributional Risk In the multivariate location model Sen and Saleh (1985) have pointed out that shrinkage estimation works out well only in a shrinking neighbourhood of the pivot. Since for β 2 the pivot is taken as 0, we consider a shrinking neighbourhood of 0 and we consider a sequence {KT} of alternatives, KT : β 2 = β 2(T) = T −1/2ξ, ξ = � ξp1+1,...,ξp �′ ∈ Rp2, (3.101) which implies that the null hypothesis H0 reduces to H0 : ξ = 0. Using a suitable estimator β ⋆ 1T of β 1, we denote by G⋆(u) = lim T→∞ P � T −1/2� β ⋆ 1T −β 1 � ≤ u ��KT � , u ∈ Rp1, where we assume that u is nondegenerate. Then, with a quadratic error loss such that T � β ⋆ 1T − β 1 �′W � β ⋆ 1T − β 1 � , for a suitable probability distribution function W, the ADR of β ⋆ 1T is deﬁned as below R � β ⋆ 1T;W � = trace � W � Rp ... � Rp uu′dG⋆(u) � ≡ trace{WV ⋆}, (3.102) where V ⋆ is the dispersion matrix for the asymptotic distribution of G⋆(.). 543.9.2. Asymptotic representation of estimators We follow the framework presented in Mukherjee (1998). Notation The i−th row and the j−th column of any matrix S are denoted by si• and s•j, respectively. Then if S has p columns, Ds denotes the maximal diagonal � ∥s•1∥,..., ��s•p ��� , where ��s•j �� is the usual Euclidean norm of the j−th column of S. Moreover, the usual partitions, Sii. j stands for the matrix Sii −Si jSj jS ji. Then, we deﬁne the following matrices which appear as the scaling factors in the asymp- totic distributions of the different estimators. We denote with Ax = n−dDx, Bx = ndDx, Ax1 = n−dDx1. Moreover, we have that lim n→∞n−1X′X exists and equals C, with C being positive deﬁnite. Consequently, Rn := D−1 x X′XD−1 x p→ R := D−1 c CD−1 c (3.103) Denote with Σn to denote the dispersion matrix of ε. Then, the dispersion of B−1 x X′ΣnXB−1 x and it follows that the limit of En exists and equals E, say. As in Sen and Saleh (1987), the shrinkage least-squares estimator (SLSE) ˆβs can be motivated as a smoothed version of a preliminary test estimator. Thus, for each c > 0, this is deﬁned as below ˆβs = ˆβ1 + � 1− κ LT �� ˜β1 − ˆβ1 � ,c > 0. (3.104) For large values of LT, ˆβs yields to ˜β1, whereas it performs quite differently from ˆβpt form small values of LT. Deﬁne the dispersion matrix H := � 0 ... R22.1 � R−1ER−1� 0 ... R22.1 � (3.105) Then, it can be proved that the second term converges to R22.1Dc2η. Therefore, we deﬁne with Hn := � 0 ... Rn22.1 � R−1 n EnR−1 n � 0 ... Rn22.1 � (3.106) implying that Hn converging to H and ξ := Dc2η ∈ Rp2. Then, Ln is deﬁned as Ln := ���H−1/2 n B−1 x2 X′ 2 � Y −X1 ˆβ 1 ���� 2 (3.107) which converges to a noncentral chi-square random variable with p2 degrees of freedom and noncentrality parameter δ := ξ ′L−1 22 ξ. Therefore, under the null hypothesis η = 0 or equivalently, ξ = 0, the test statistic Ln converges to a chi-square random variable with p2 degrees of freedom. Deﬁne the projection matrix Px1 = X1(X′ 1X1)X′ 1. Let Dn := R−1 n and D := R−1. Then, the covariance matrices of the random vectors DnUn and DU are denoted by Ln and L respectively. 553.10. Lasso Inference for High Dimensional Time Series under NED Following the framework proposed by Adamek et al. (2023), consider the linear time series regression yt = x′ tβ 0 +ut, t = 1,...,T, (3.108) where xt = (x1,t,...,xN,t)′ is an (N × 1) vector of explanatory variables and we assume that we have a high-dimensional time series model where N can be larger than T. Assumption 4. Let zt = (x′ t,ut)′ and let there exists some constants ¯m > m > 2 and d ≥ max � 1, ¯m m−1 ¯m−2 � , E � zt � = 0, E � xtut � = 0 and max 1≤j≤N+1,1≤t≤T E|z j,t|2 ¯m ≤ C. (3.109) Assumption 5. Let sT,t denote a k(T)−dimensional triangular array that is α−mixing of size d � 1 m − 1 ¯m �, with σ −ﬁeld F s t := σ � sT,t,sT,t−1,... � (3.110) such that zt is F s t −measurable. The process � z j,t � is L2m−near-epoch dependent (NED) of size −d on sT,t with positive bounded NED constants, uniformly over j ∈ {1,...,N +1}. Remark 14. The ﬁrst assumption above ensures that the error terms are contemporaneously uncorrelated with each of the regressors, and that the process has ﬁnite and constant unconditional moments. The second assumption above implies that sT,t is an underlying shock process driving the regressors and errors in zt, where we assume zt to depend almost nearly entirely on the "near epoch" of sT,t. In other words, the near epoch dependence of the vector zt can be interpreted as zt being "approximately" mixing, in the sense that it can be well-approximated by a mixing process. The NED framework allows for general forms of dependence and that are often encountered in econometrics applications including, but not limited to, strong mixing processes, linear processes including ARMA models, various types of stochastic volatility and GARCH models as well as nonlinear processes (see, Davidson (2002)). The framework proposed by Adamek et al. (2023) is the ﬁrst to utilize the NED framework for establish- ing uniformly valid high-dimensional inference. For instance, Wong et al. (2020) consider time series models with β−mixing errors, which has the advantage of allowing for general forms of dynamic mis- speciﬁcation resulting in serially correlated error terms, but, rules out several relevant data generating processes, and is in addition typically difﬁcult to verify. On the other hand, Masini et al. (2022) use an m.d.s assumption on the innovations in combination with sub-Weibull tails and a mixing assumption on the conditional covariance matrix. However, the particular m.d.s assumption does not allow for dynamic misspeciﬁcation of the full model. Importantly, the NED assumption on ut does allow for misspeciﬁed models as well, in which case we view β 0 as the coefﬁcients of the pseudo-true model when restricting the class of models to those linear in xt. 56Therefore, this econometric intuition allows to view the high-dimensional time series model of Adamek et al. (2023) as simply the linear projection of yt on all the variables in xt, with β 0 in that case representing the best linear projection coefﬁcients, which implies that E[ut] = 0 and E[utxj,t] = 0. On the other hand, ut is not likely to be an m.d.s. Thus, allowing for misspeciﬁed dynamics is crucial for developing the theory for the nodewise regressions underlying the desparsiﬁed lasso. The NED-order m and sequence of size −d play a key role in the development of the limit results in which these parameters enter as the corresponding asymptotic rates of related quantities. Obviously, there is a trade-off between the thickness of the tails allowed for and the moment of dependence - which is measures via the mixing rate. Example 13 (ARCL Model with GARCH errors, see Adamek et al. (2023)). Consider the autoregressive distributed lag (ARDL) model with GARCH errors such that yt = p ∑ i=1 ρiyt−i + q ∑ j=0 θ ′ jwt−j +ut = x′ tβ 0 +ut (3.111) ut = √ htεt, εt ∼ N (0,1), (3.112) ht = β0 +β1ht−1 +β2u2 t−1, (3.113) where the roots of the lag polynomial ρ(z) = 1 −∑ j = 1pρ jz j are outside the unit circle. Then, ut is a strictly stationary geometrically β−mixing process. Moreover, we assume that the vector of exogenous variables wt is stationary and geometrically β−mixing as well with 2 ¯m ﬁnite moments. Given the invertability of the lag polynomial, we may then write yt = ρ−1(L)vt, vt = q ∑ j=0 θ′ jwt−j +ut, (3.114) and the inverse lag polynomial ρ−1(z) has geometrically decaying coefﬁcients. Then, it follows directly that yt is NED on vt, where vt is strong mixing of size −∞ as its components are geometrically β−mixing, and the sum inherits the mixing properties. Example 14 (Equation-by-Equation VAR, see Adamek et al. (2023)). Consider the VAR model below yt = p ∑ j=1 Φjyt−j +ut, (3.115) where yt is a K × 1 vector of dependent variables and E|ut|2 ¯m ≤ C and the K × K matrices Φi satisfy appropriate stationarity and summability conditions. The equivalent equation-by-equation representation yk,t = p ∑ j=1 � Φk,1, j,...,Φk,K, j � yt−j +uk,t = � y′ t−1,...,y′ t−p � β k +uk,t, k ∈ {1,...,K}. (3.116) Therefore, assuming a well-speciﬁed model with E � ut|yt−1,...,yt−p � = 0, then the NED conditions are then satisﬁed trivially. 57Notice that the above examples, provide cases in which although the classical martingale difference sequence framework fails to apply, the more general NED framework extends the applicability of certain time series processes. Example 15 (Misspeciﬁed AR Model, see Adamek et al. (2023)). Consider an autoregressive AR model of order 2 as yt = ρ1yt−1 +ρ2yt−2 +vt, vt ∼ N (0,1). (3.117) where the roots of the characteristic polynomial (1 −ρ1L−ρ2L2) are outside the unit circle. Moreover, deﬁne the misspeciﬁed model yt = ˜ρyt−1 +ut, where ˜ρ = argmin ρ E � (yt −ρyt−1)2� = E � ytyt−1 � E � y2 t−1 � = ρ1 1−ρ2 (3.118) and ut is autocorrelated. Therefore, an m.d.s assumption would be inappropriate in this case, since E � ut|σ {yt−1,yt−2,....} � = E � yt = ˜ρyt−1 +ut|σ {yt−1,yt−2,....} � = − ρ1ρ2 1−ρ2 yt−1 +ρ2yt−2 ̸= 0. (3.119) On the other hand, it can be shown that � yt−1,ut �′ satisﬁes the NED-condition by considering the moving average representation of yt and thus by extension, of ut = yt − ˜ρyt−1. In addition, since the coefﬁcients are geometrically decaying, ut is thus clearly NED on vt and therefore the NED-condition is satisﬁed. Next, we focus on the related assumptions/conditions on the sparsity properties of the parameter vector. Speciﬁcally, the key condition to apply the lasso successfully is that the parameter vector β 0 is (at least approximately) sparse. This is given by the following assumption. Assumption 6. For some 0 ≤ r ≤ 1 and sparsity level sr, deﬁne the N−dimensional sparse compact parameter space such that BN(r,sr) := � β ∈ RN : ∥β∥r r ≤ sr,∥β∥∞ ≤ C,∃ C < ∞ � , (3.120) and assume that β 0 ∈ BN(r,sr) Example 16 (Inﬁnite Order AR,see Adamek et al. (2023)). Consider an inﬁnite order autoregressive model as below yt = ∞ ∑ j=1 ρ jyt−j +εt, (3.121) where εt is a stationary m.d.s with sufﬁcient moments existing, and the lag polynomial � 1−∑∞ j=1 ρ jLj� is invertible and satisﬁes the summability condition ∑∞ j=1 jα|ρ j| < ∞ for some α ≥ 0. 583.10.1. Inference on low-dimensional parameters In this section we establish the uniform asymptotic normality of the desparsiﬁed lasso focusing on low- dimensional parameters of interest (see, Adamek et al. (2023)). Speciﬁcally, we consider testing P joint hypotheses of the form RNβ 0 = q, via a Wald statistic, where RN is an appropriate P ×N matrix whose non-zero columns are indexed by the set H := � j : P ∑ p=1 |rN,p, j| > 0 � (3.122) with cardinality h := |H|. Notice that we can allow for h to increase in N (and therefore T). Focusing on inference on ﬁnite set of parameters such that we can apply a standard central limit theorem. Therefore, given our time series setting, the long-run covariance matrix is ΩN,T = E � 1 T � T ∑ t=1 wt �� T ∑ t=1 w′ t �� , (3.123) where wt = � v1,tut,...,vN,tut �′, enters the asymptotic distribution. Then, ΩN,T can equivalently be written ΩN,T ≡ Ξ(0)+ T−1 ∑ ℓ=1 � Ξ(ℓ)+Ξ′(ℓ) � , (3.124) Ξ(ℓ) ≡ 1 T T ∑ t=ℓ+1 E � wtw′ t−ℓ � . (3.125) In order to estimate the asymptotic variance Ψ, we suggest to estimate ΩN,T with the long-run variance kernel estimator such that Ω ≡ �Ξ(0)+ QT−1 ∑ ℓ=1 K � ℓ QT �� Ξ(ℓ)+Ξ′(ℓ) � , (3.126) Ξ(ℓ) = 1 T −ℓ T ∑ t=ℓ+1 ˆwt ˆw′ t (3.127) with ˆwj,t = ˆvj,t ˆut, the kernel K(.) can be taken as the Bartlett Kernel K( ℓ QT ) = � 1− ℓ QT � . 3.10.2. Error Bound and the Consistency for the Lasso In this section, we consider a new error bound for the lasso in a high-dimensional time series model. Thus, the Lasso estimator of the parameter vector β 0 in the model (see, Adamek et al. (2023)) ˆβ := argmin β∈RN � 1 T ∥y−Xβ∥2 2 +2λ ∥β∥1 � . (3.128) 59where y = (y1,...,yT)′ is the T × 1 response vector, X = � x1,....,xT �′ is the (T × N) design matrix and λ > 0 a tuning parameter. In other words, the above optimization problem adds a penalty term to the least squares objective to penalize parameters that are different from zero. Thus, when deriving this error bound, we typically require that λ is chosen sufﬁciently large to exceed the empirical process such that max j �� 1 T ∑T t=1xj,tut ��, with high probability. Moreover, we deﬁne the following set such that ET(z) := � max j≤N,ℓ≤T ����� ℓ ∑ t=1 utxj,t ≤ z ����� � (3.129) and establish the conditions under which P � ET � Tλ 4 �� → 1. 3.10.3. Uniformly Valid Inference via the Disparsiﬁed Lasso Following the framework of Adamek et al. (2023), we consider an application of the disparsiﬁed Lasso. Example 17 (Sparse Factor Models). Consider the factor model yt = β 0′xt +ut, ut ∼ N (0,1) (3.130) xt = Λf t +vt, vt ∼ N (0,Σv), f t ∼ (0,Σf ). (3.131) Moreover, we assume that Σ = ΛΣf Λ′ +Σv. Example 18 (Sparse VAR(1)). Consider a stationary VAR(1) model for zt = (yt,x′ t)′ zt = Φzt−1 +ut, E � utu′ t � := Ω, E � utu′ t � = 0, ∀ ℓ ̸= 0. (3.132) Then, the regression yt = φ1zt−1 +u1,t, where φ j is the j−th row of Φ, i.e., the ﬁrst line of the VAR. Example 19. Consider the population nodewise regressions deﬁned by the linear projections below xj,t = x′ −j,tγ0 j +vj,t, γ0 j := argmin γ E � 1 T T ∑ t=1 � xj,t −x′ −j,tγ �2 � , j = 1,....,N. (3.133) with τ2 j := 1 T ∑T t=1E � v2 j,t � . Speciﬁcally, if we let ΦφI, with |φ| < 1, and let Ω have a Toeplitz structure, which implies that the elements of the matrix are deﬁned such that ωi, j = ρ|i−j|,|ρ| < 1. Therefore, the nodewise regression parameter γ0 j is only weakly sparse, in that it contains no zeroes, but its entries follow a geometrically decaying pattern6, meaning that max j ���γ0 j ��� r r ≤ C. 6Notice that in high dimensional settings relevant conditions that capture features such as sub-Gaussianity, heavy tailed observations with slowy decaying temporal dependence are essential (see, Baek et al. (2021) and Guillaumin et al. (2022). Dr. Adam Sykulski gave a seminar with title: "The Debiased Spatial Whittle Likelihood", at the S3RI Departmental Seminar Series at the University of Southampton on the 5th of May 2022. 604. High Dimensional Feature Selection Methods High dimensional statistical problems address the challenge of conducting robust estimation and in- ference when the number of unknown parameters, p, is much larger than the number of observations, n. The literature on high-dimensional feature selection methods include the conditional sure inde- pendence screening7 approach as discussed by Fan and Lv (2008), Wang (2009), Ke et al. (2014) and Barut et al. (2016) as well as He et al. (2013) and Kong et al. (2019) in the case of a conditional quan- tile functional form is employed. The covariate screening approach as a tool for variable selection works well in ultra high-dimensional linear regression models with stationary covariates, without re- quiring distributional restrictions. Statistical estimation methodologies in high-dimensional settings in- clude Belloni and Chernozhukov (2011), Belloni et al. (2014), Wang et al. (2012), Chernozhukov et al. (2015), Fan et al. (2017) as well as Sun et al. (2015) and Wang and Leng (2016) for the cases of projec- tion screening. A fundamental research question remains in most cases when considering modelling a high-dimensional vector of regressors is determining the statistical signiﬁcance of either individual covariates or the model selection mechanism. Lockhart et al. (2014) propose a test for detemining statistical signiﬁcance with the Lasso, while Chudik et al. (2018) propose a methodology for selecting covariates in a high-dimensional environment using the multiple testing approach. Furthermore, Battey et al. (2018) considers hypothesis testing and parameter estimation in the context of the divide-and-conquer algorithm. In particular, in a uniﬁed likelihood-based framework, we propose new test statistics and point estimators obtained by aggregating various statistics from k subsamples of size n/k, where n is the sample size. Speciﬁcally, in low dimensional and sparse high dimensional settings, Battey et al. (2018) examine how large k can be, as n grows large, such as the loss of efﬁciency due to the divide-and-conquer algorithm is negligible. In other words, the resulting estimators have the same inferential efﬁciencies and estimation rates as an oracle with access to the full sample. While hypothesis testing in a low dimensional context is straightforward, in the sparse high dimensional setting, nuisance parameters introduce a nonnegligible bias, causing classical low dimensional theory to break down. Therefore, in their high dimensional Wald construction, the phenomenon is remedied through debiasing of the estimator, which gives rise to a test statistic with tractable limiting distribution. Thus, they ﬁnd that the theoretical upper bound on the number of subsamples guaranteeing the same inferential or estimation efﬁciency as the whole-sample procedure is k = o((slogd)−1√n) in the linear model, where s is the sparsity of the parameter vector. Lemma 10 (Battey et al. (2018)). Assume that Σ = E � XiX⊤ i � satisﬁes Cmin < λmin(Σ) ≤ λmin(Σ) ≤Cmax as well as ���Σ−1/2X1 ��� ψ2 = κ, then it holds that P � max j=1,...,k ���M(j)�Σ(j) −I ��� max ≤ α � logd n � ≥ 1−2kd−c2, c2 = α2Cmin 24e2κ4Cmax −2. (4.1) 7Note that the covariate screening approach is widely used in the applied statistics literature, see Aschard et al. (2017). 61On the other hand, in time series regression models incorporating high dimensionality features requires to develop further tools such as Gaussian approximations (e.g., see Chernozhukov et al. (2014) and Zhang and Wu (2017)) and consistent model selection techniques robust to the presence of heteroscedas- ticity (see, Halunga et al. (2017)) and nonstationarity. In particular in the time series econometrics lit- erature many open problems remain such as robust model selection methodologies for ultra-high di- mensional environments with heterogenous data. We return back to Vector Autoregression Processes, although not necessarily a high-dimensional VAR(p) process. In practise, we consider the case where the dimension of the time-series vector is less than the sample size n. Formally, a p−dimensional vector- valued stationary time series Xt = (X1t,...,Xpt), with t = 1,...,n, can be modelled using a VAR represen- tation of lag d with serially uncorrelated Gaussian errors, which takes the following form Xt = A1Xt−1 +...+AdXt−d +et, et ∼ N (0,Σe) (4.2) where A1,...,Ad are (p x p) matrices and et is a p-dimensional vector of possibly correlated innovation shocks. Therefore, the main objective in VAR models is to estimate the transition matrices A1,...,Ad, together with the order of the model d, based on time series realizations (X0,X1,...,Xn). Then, the structure of the transition matrices provides insights into the complex temporal relationships amongst the p time series and the particular representation provides a way to apply forecasting techniques (see, Michailidis and d’Alché Buc (2013), Basu and Michailidis (2015) and Basu and Rao (2021)). 4.1. Ultra-high dimensionality under dependence According to Yousuf (2018), it is possible to achieve the sure screening property in the ultrahigh di- mensional setting with dependent errors and covariates. However, in order to do that we need to make stronger assumptions on the moments of both the error and covariate processes. Speciﬁcally, if the error and covariate processes are assumed to follow a stronger moment condition, such as, ∆0,q(ε) < ∞ and Φ0,q(x) < ∞ for arbitrary q > 0, we can then achieve a much larger range of pn which will cover the ultra-high dimensional case. Speciﬁcally, we need a condition that implies that the tails of the covariate and error processes are exponentially tight. A wide range of processes satisfy the above condition. Suppose that εi is a linear process such that εi = ∞ ∑ j=0 f jei−j, ei ∼ i.i.d, with ∑∞ ℓ=0| fℓ| < ∞ then it holds that ∆0,q(ε)(εℓ) = ∥e0 −e∗ 0∥q ∞ ∑ ℓ=0 | fℓ| (4.3) Thus, if we assume that e0 is sub-Gaussian, then ˜αε = 1 2 since ∥e0∥q = Op(√q) then, when ei is sub- exponential then it holds that ˜αε = 1. 62More generally, for ei := ∑∞ j=0 f jep i−j and thus if ei is sub-exponential, we have that ˜αε = p, where p is the number of components of the high-dimensional vector. The high dimensionality as well as the non- parametric aspect can slow down the rate of convergence and thus especially the variance of the estimator can have slower convergence to the corresponding variance of the process. Thus, for any ﬁxed q, we are not placing additional assumptions on the temporal decay rate of the co- variate error processes besides requiring ∆0,q(ε),Φ0,q(ε) < ∞. On the other hand, the ultrahigh dimen- sional setting requires geometrically decaying strong mixing coefﬁcients, in addition to requiring sub- exponential tails for the response. Consider for example the case that εi = ∑∞ j=0 f jei−j, geometrically decaying strong mixing coefﬁcients would require the coefﬁcients, f j, to decay geometrically. Theorem 12 (Theorem 2 in Yousuf (2018)). Deﬁne with α = 2 1+4 ˜αx , then it holds that for any c2 > 0 P � max j≤pn �� ˆρ j −ρ j �� > c2n−k � ≤ Op  snpnexp � −n1/2−κ ν2x sn � ˜α +Op  pnexp � −n1/2−κ νxνε � ˜α′  (4.4) 4.1.1. SIS with dependent observations Sure Independence Screening, is a method of variable screening based on ranking the magnitudes of the pn marginal regression estimates. Under appropriate conditions, this simple procedure is shown to possesses the sure screening property. Deﬁne with ˆρ = ( ˆρ1,..., ˆρpn), where ˆρ j = � n ∑ t=1 X2 t j �−1� n ∑ t=1 Xt jYt � . (4.5) Therefore, ˆρ j is the OLS estimate of the linear projection of Yt onto Xt j. Let M∗ = � 1 ≤ i ≤ pn : βi ̸= 0 � (4.6) and let |M∗| = sn << n be the size of the true sparse model. Moreover, we sort the elements of ˆρ by their magnitudes. Thus, for any given γn, deﬁne a sub-model such that ˆ Mγn = � 1 ≤ i ≤ pn : | ˆρi| ≥ γn � (4.7) and let �� ˆ Mγn �� = dn be the size of the selected model. Furthermore, notice that the screening property states that for an appropriate choice of γn, we have that P � M∗ ⊂ ˆ Mγn � → 1. 634.1.2. Experimental design examples Experimental Design I: Uncorrelated Features Consider the model for the covariate process we have xt = A1xt−1 +ηt (4.8) where A1 = diag{γ} and γ ∼ Unif[0.4,0.6]. Moreover, we set with ηt ∼ N (0,Ση). In particular, when we are dealing with uncorrelated predictors we set Ση = Ipn which represents the no correlation case. Experimental Design II: Correlated Features One can compare the performance of SIS and GLSS in the case of correlated predictors. The covariate process is generated with A1 = diag � 0.4|i−j|+1� i, j≤pn such that ηt ∼ N (0,Ση) and ηt ∼ t5(0,V) , with Ση = � 0.3|i−j|� (4.9) Proof. Recall that it holds that ∑pn k=1 1{|βk| > 0} = sn. Hence, we obtain the following expression P � |S1 −E(S1)| > c2n−κ 2 � ≤ ∑ k∈M∗ P ����� Xt j (Xtkβk) n −βkE � Xt jXtk ����� > c2n−κ 2sn � (4.10) Moreover, it holds that ��Xi j �� r ≤ ∆0,r(X j) ≤ Φ0,r(x). Using this we compute the cumulative functional dependence measure of XtkXt j such that +∞ ∑ t=m ��Xt jXtk −X∗ t jX∗ tk �� r/2 ≤ ∞ ∑ t=m ���Xt j �� r ∥Xtk −X∗ tk∥r ��Xt j −X∗ t j �� r � ≤ 2Φ0,r(x)Φm,r(x) = Op � m−αx� . Therefore, we obtain that sup m (m+1)αx ∞ ∑ t=m ��Xt j �� r ∥Xtk −X∗ tk∥r/2 ≤ 2K2 x,r. (4.11) Using the above results we get the following probability bound P � |S1 −E[S1]| > c2n−κ 2 � ≤ Csn × � nωKr x,r (n/sn)r/2−rk/2 +exp � −n1−2κ s2nK4x,r �� (4.12) Hence, by our choice of γn, we obtain that P � M∗ ⊂ ˆ Mγn � > P(An). In other words, this allow us to obtain a probability bound with respect to the spectral radius of the corresponding covariance matrix pn ∑ k=1 ρ2 k = Op(λmax(Σ)). (4.13) 64Then, we extend the above result to the corresponding set deﬁned by Bn such that Bn := � max k≤pn | ˆρk −ρk| ≤ c4n−κ � (4.14) Therefore, we obtain a measurability condition for the number of � k : | ˆρk| > 2c4n−4� cannot exceed the number of � k : |ρk| > c4n−k� which is bounded by Op � n2kλmax(Σ) � . Thus, by choosing c4 = c3/2 we can obtain the correct probability bound such that P ��� ˆ Mγn �� < Op � n2kλmax(Σ) �� > P(Bn) (4.15) We are interested to bound the following probability event A1 := P ��� ˆγi,k −γi,k �� > cn−k ℓn � (4.16) where a probability bound for the event A1, can be obtained as below: A1 ≤ P ������ 1 n n−|i| ∑ t=1 εt,kεt+|i|,k −E � 1 n n−|i| ∑ t=1 εt,kεt+|i|,k ������ � +P ������E � 1 n n−|i| ∑ t=1 εt,kεt+|i|,k � −γi,k ����� > cn−k/4ℓn � +P ������ 1 n n−|i| ∑ t=1 εt+|i|,kXt+|i|,k � ∑n j=1 Xjkε j,k/n ∑n j=1X2 jk/n ������ > cn−k/4ℓn � +P   ������ 1 n n−|i| ∑ t=1 XtkXt+|i|,k � ∑n j=1 Xjkε j,k/n ∑n j=1X2 jk/n �2������ > cn−k/4ℓn   Furthermore, we can determine the bias of the term as below �����E �n−|i| ∑ t=1 εt,kεt+|i|,k n � −γi,k ����� ≤ iγi,k n . (4.17) Moreover, it holds that P ������ 1 n n−|i| ∑ t=1 εt,kXt+|i|,k > M ����� � ≤ P ������ 1 n n−|i| ∑ t=1 εt,kXt+|i|,k −E � 1 n n−|i| ∑ t=1 εt,kXt+|i|,k ������ > M − �����E � 1 n n−|i| ∑ t=1 εt,kXt+|i|,k ������ � M > max k≤pn max i≤ℓn 2 ��E � εt,kXt+|i|,k ���+ε, for some ε > 0. (4.18) 654.2. Uniform Asymptotic Inference and Model Selection This is another important topic which we cannot cover here extensively (see, Tibshirani et al. (2018)). 4.2.1. Uniform-in-Submodel Bounds Following the framework proposed by Kuchibhotla et al. (2021), suppose that M = {M1,...,ML} denotes a collection of submodels, where Mj represents a subset of covariates for 1 ≤ j ≤ L. Also, denote with �βMj to represent the least-squares estimator for the linear regression of the response on the covariates of the model Mj. Then, by simultaneous consistency, we mean the existence of target vectors � βMj : 1 ≤ j ≤ L � such that the following result holds sup M∈M ����βM − �β ��� = op(1), as n → ∞, for some ∥.∥. (4.19) More speciﬁcally, if � M is a selected model, then one can perform inference of β � M by estimating the distribution of �β � M. According to Kuchibhotla et al. (2021), in practise, even though various model- selection criteria like Cp (AIC), (BIC) and lasso have been recommended for covariate selection in linear regression, developing more general asymptotic results to the asymptotic uniform linear representation in the special case of the least-squares linear regression estimator are still an ongoing research ﬁeld. The framework of variable selection and linear regression is often considered in the context of high- dimensional linear regressions. Although, the procedure uses only a reduced set of variables in the ﬁnal regression, it uses all variables in the proceding covariate selection stage of estimation. Suppose that � M ∈ M is the ﬁnal selected submodel, where M is some ﬁnite and countable collection of models, with �β � M the corresponding least-squares estimator. The estimator �β � M is known as the post- regularization estimator in the high-dimensional statistics literature when � M is obtained from a regular- ized least-squares procedure. A relevant question regarding the asymptotic behaviour of the particular estimator is "What does �β � M estimate consistently?". These considerations are relevant to the literature of variable and model selection, which includes: methodologies for determining the statistical signiﬁcance of predictors in settings with many covariates (see, Chudik et al. (2018)) as well as the uniform inference post-model selection which includes among others the studies of Belloni et al. (2016) and Farrell (2015)). Here we focus on the framework proposed by Kuchibhotla et al. (2021) and present key results. Suppose that for any M ⊂ {1,2,..., p} the ordinary least-squares (OLS) empirical risk (or objective) function is ˆRn (θ;M) := 1 n n ∑ i=1 � Yi −X⊤ i (M)θ �2 , for θ ∈ R|M|. (4.20) Then, by expanding the square function, it is clear that ˆRn (θ;M) = 1 n n ∑ i=1 Y 2 i − 2 n n ∑ i=1 YiX⊤ i (M)θ +θ⊤ � 1 n n ∑ i=1 Xi(M)X⊤ i (M) � θ. (4.21) 66Furthermore, deﬁne the following quantities ˆΣn := 1 n n ∑ i=1 XiX⊤ i ∈ Rp×p, and ˆΓn := 1 n n ∑ i=1 XiYi ∈ Rp×p. (4.22) Then, the least-squares linear regression estimator ˆβn,M is deﬁned as below ˆβn,M = arg min θ∈R|M| ˆRn (θ;M) = arg min θ∈R|M| � θ⊤ ˆΣn(M)θ −2θ⊤ ˆΓn(M) � . (4.23) The notation arg minθ f(θ) denotes the minimized of f(θ). Based on the quadratic expansion of the empirical objective function ˆRn(θ;M), the estimator �βn,M is given by the closed form expression below �βn,M = �ˆΣn(M) �−1 ˆΓn(M), (4.24) assuming nonsingularity of ˆΣn(M). Therefore, it is clear that �βn,M is a smooth (nonlinear) function of two averages ˆΣn(M) and ˆΓn(M). Then, assuming that the random vectors (Xi,Yi) are i.i.d with ﬁnite fourth moments, it follows that ˆΣn(M) and ˆΓn(M) converge in probability to their expectations. Thus, we can deﬁne the expected value in terms of matrices and vectors as below Σn := 1 n n ∑ i=1 E � XiX⊤ i � ∈ Rp×p, and Γn := 1 n n ∑ i=1 E[XiYi] ∈ Rp×p. (4.25) Following the notation above �βn,M = �ˆΣn(M) �−1 ˆΓn(M), and if �ˆΣn − ˆΣn, ˆΓn −Γn � p→ 0 as n → ∞, using a Slutsky-type argument, it follows that ��βn,M −βn,M � p→ 0, as n → ∞, where βn,M is: βn,M := [Σn(M)]−1Γn(M) ≡ arg min θ∈R|M| � θ⊤Σn(M)θ −2θ⊤Γn(M) � . (4.26) Thus, the convergence statement above only concerns a single submodel M and is not uniform over M. By uniform-in-submodel ∥.∥2 −norm consistency of ˆβn,M to βn,M, for M ∈ M (k), we mean that sup M∈M (k) ����βn,M −βn,M ��� = op(1) as n → ∞. (4.27) Thus, converges of �βn,M to βn,M only requires convergence of �Σn(M) to Σn(M) and �Γn(M) to Γn(M). Remark 15. A relevant stream of literature to uniform inference for submodel selection, is the test- ing methodologies which consider the nested property between a comparison of model as a mechanism for speciﬁcation testing. In particular, Hagemann (2012) considers a non-nested speciﬁcation test (see, also MacKinnon (1983)) while Rinaldo et al. (2019) develops a framework for model selection based on sample-splitting for high dimensional assumption lean inference purposes. 674.3. Non-nested Regressions and Variable Selection Speciﬁcation Testing Following the framework proposed by Hagemann (2012), suppose that we observe covariates �� x⊤ i,1,...,x⊤ i,M �⊤ ∈ Rd1+...+dM : i = 1,...,n � that give rise to M ≥ 2 different possible linear regression models for y := (y1,...,yn)⊤ ∈ Rn, such that y = Xmβm +um, m ∈ M := {1,...,M}, (4.28) where Xm := � x1,m,...,xn,m �⊤ ∈ Rn×dm is the design matrix of model m. Then, the matrices X1,....,XM are assumed to be non-nested, which implies that for any two matrices with index m ̸= ℓ ∈ M , no matrix can be obtained by another by a linear transformation. Furthermore, the particular assumption does not rule out the possibility that some of the columns of Xm and Xℓ are identical or that they may be nonlinear transformations of another. In particular, suppose that there is an unobserved design matrix Xm∗ := � x1,m∗,...,xn,m∗�∗ associated with the correct model m∗. Suppose that F := � Xm : m ∈ M ∪{m∗} � has the following properties: Assumption 7. Consider that � yi,(xi,m)⊤ m∈M ∪{m∗} : i ≥ 1 � is a sequence of i.i.d random vectors. We have that E|xi,m|4 < ∞ for all m ∈ M ∪{m∗}, where the number of elements of M (number of covariates) does not depend on n (sample size). For all m ∈ M , the matrices E � xi,mx⊤ i,m � are positive deﬁnite. Next, we impose a condition for the existence of a correct model speciﬁcation. Assumption 8. Model m∗ satisﬁes E � y|F � = Xm∗βm∗. Deﬁne with ui,m∗ := yi −x⊤ i,mβm∗ for all i ≥ 1 and Eu4 i,m∗ < ∞. The proposed J test presumes that for some predetermined m ∈ M , the null hypothesis is formulated such as m = m∗ against the alternative hypothesis m ̸= m∗ in the presence of non-nested alternatives ℓ ∈ M \{m}. To do this, we artiﬁcially nesting the models via an additional parameter vector αm := � αℓ,m � ℓ∈M \{m} ∈ RM−1 such that y = Xmbm + ∑ ℓ∈M \{m} αℓ,mXℓβℓ +u, (4.29) bm := � 1− ∑ ℓ∈M \{m} αℓ,m � βm. (4.30) Therefore, since the vectors � αℓ,m,βℓ � ℓ∈M \{m} of the nesting model may not be identiﬁed, we can replace the βℓ by the OLS estimates such that ˆβℓ = � X⊤ ℓ Xℓ �−1X⊤ ℓ y. (see, Hagemann (2012)). 68Remark 16. In particular, the minimum J-test proposed by Hagemann (2012) does not require the correct model to be among the considered speciﬁcations and avoids ambiguous test outcomes. More speciﬁcally, the MJ test determines with asymptotically correct size if the correct model is among the speciﬁcations under consideration. Furthermore, if the correct model is present, it is chosen with probability approach- ing one as the sample size becomes large. After redeﬁning the error term appropriately, this yields that y = Xmbm + ∑ ℓ∈M \{m} αℓ,mXℓ ˆβℓ +u, (4.31) Therefore, a Wald test for testing the null hypothesis that αm = 0 is a Jtest for the validity of model m in the presence of the alternatives M \{m}. Thus, to construct the test statistic, let λn,m := n−1/2 � y⊤PℓMmy � ℓ∈M \{m} (4.32) ˆΣn,m := n−1 � y⊤PℓMm ˆΩMmPℓ′y � ℓ,ℓ′∈M ∈M \{m} (4.33) Pm := Xm � X⊤ m Xm �−1 X⊤ m and Mm := In −(Pm)n×n (4.34) are the usual projection matrices and ˆΩn,m is an estimate of the conditional expectation of E � u∗ mu∗⊤ m |F � . The J test statistic for model m is then given by Jn,m := λ ⊤ n,m ˆΣ−1 n,mλn,m (4.35) In other words, the hypothesis that the model m is the true model is rejected for large values of Jn,m. The following asymptotic results holds for obtaining the empirical size and power of the test statistic. Lemma 11 (Hagemann (2012)). Suppose that Assumptions are satisﬁed. Then, it holds that (i). If m∗ ∈ M , then Jn,m∗ ⇒ χ2 M−1. (ii). For every m ∈ M \{m} and every B ∈ R, we have that limn→∞P � Jn,m > B � = 1. Lemma 12 (Hagemann (2012)). Suppose that the conditions of the Theorem hold. If m∗ ∈ M , then plim n→∞ sup x∈R ��P∗� MJ∗ n ≤ x � −P∗� MJ∗ n ≤ x ��� = 0. (4.36) Proof. Let ˆm := argminJ ∗ n . We ﬁrst show that if m∗ ∈ M , then ˆm∗ approximates m∗. Without loss of generality, ﬁx any 0 < ε < 1, then P � 1{∃ m ∈ M \{ ˆm} : Jn,m ≤ B} > ε � ≤ P � ∃ m ∈ M \{ ˆm} : Jn,m ≤ B � +P � ˆm ̸= m∗� , (4.37) which converges to zero as n → ∞. 69In summary, it can be proved that plim n→∞ P∗� J∗ n, ˆm∗ ≤ x � ≡ K(x), (4.38) where K(x) is the distribution function of a χ2 M−1 variable. Remark 17. Notice that although the methodology proposed by Hagemann (2012) considers testing for correct model speciﬁcation it differs from the literature of conditional subvector testing. Speciﬁcally, the subvector testing method considers the roots of the following characteristic polynomial ��� ˆκIp −n−1 �G−1/2 n � ¯Y0,W �′Z �HnZ′�¯Y0,W � �G−1/2 n ��� = 0. (4.39) Remark 18. A different stream of literature considers the classiﬁcation of non-overlapping and over- lapping models and discuss how the relationship between candidate models affects the asymptotic dis- tributions of the test statistics. The particular stream of literature was initiated by Vuong (1989). This approach considers measuring the distance from the model under investigation to the true distribution, which requires to solve the minimization problem infP∈Pd(P,µ). 4.4. Divide and Conquer Variable Selection Algorithm Further algorithmic procedures for model selection purposes in high dimensional linear regression models includes the framework of Meinshausen and Bühlmann (2010) propose a variable selection methodology where the algorithm repeatedly employs a subset selection and apply the backward algorithm. Then, the ﬁnal stage provides all the signiﬁcantly selected covariates with a pre-speciﬁed control rate. Therefore, the particular stability selection procedure is designed to address the issue of choosing the amount of regularization such that a certain familywise type I error rate in multiple testing can be conservatively controlled for ﬁnite sample size. The issue of correlated covariates can affect the behaviour of stability selection for high correlated designs. Speciﬁcally, the stability selection algorithm puts a large emphasis on avoiding false positive selections and, as a consequence, might miss important variables if they are highly correlated with irrelevant variables. Moreover, Battey et al. (2018) (see, also Fan et al. (2015)) consider the so-called "divide and conquer" variable selection algorithm as we brieﬂy explain below. On each subset Dj, we compute the debiased estimator of β ∗ such that �β d = �β d Lasso(Dj)+ 1 nk M(j) � X(j)�⊤� Y (j) −X(j)�β d Lasso(Dj) � , (4.40) where d is used to indicate the debiased version of the estimator, such that M(j) = � m(j) 1 ,...,m(j) d �⊤ and mv is the solution of m(j) v = argmin m m⊤�Σ (j)m s.t ����Σ (j)m−ev ��� ∞ ≤ ϑ1, ����Σ (j)m ��� ≤ ϑ2. (4.41) 70Theorem 13. Suppose that E � ε4 1 � < ∞ and choose ϑ1,ϑ2 and k such that θ1 ≈ � klogd/n,θ2n−1/2 = o(1) and k = o �√n(slogd)−1� . For any v ∈ {1,...,d}, √n k k ∑ j=1 �β d v (Dj)−β ∗ v �Q(j) v ⇒ N (0,σ2), where �Qv = � m(j)⊤ v �Σ (j)m(j) v �1/2 (4.42) The above procedure implies a divide-and-conquer Wald statistic of the following form ¯Sn = √n k k ∑ j=1 �β dv(Dj)−β H v ¯σ � m(j⊤) v �Σ (j)m(j) v �1/2. (4.43) for β ∗ v , where ¯σ is an estimator for σ based on the k subsamples. Consider the desparsiﬁed estimator for sub-sample Dj is given by �β d(Dj) = �β λ(Dj)− �Θ(j)∇ℓ(j) nk � β λ(Dj) � , (4.44) where �Θ(j) is a regularized inverse of the Hessian matrix of second-order derivatives of ℓ(j) nk (β) evaluated at β λ(Dj), denote by �J(j) = ∇2ℓ(j) nk � �β λ(Dj) � (4.45) The proposed approach for estimating �J(j), reduces to the empirical covariance of the design matrix in the case of the linear model. The, the aggregated debiased estimator over the k subsamples is ¯β d := 1 k k ∑ j=1 �β d � Dj � . (4.46) Thus, to approximate the nodewise Lasso requires to approximately invert �J(j) via L1−regularization. The basic idea is to ﬁnd the regularized invert row via a penalized L1−regression, which is the same as regressing the variable Xν on X−ν but expressed in the sample covariance form. In particular for each row ν ∈ {1,...,d}, consider the optimization as below: �κν � Dj � = argmin κ∈Rd−1 � �J(j) νν −2 �J(j) ν,−νκ +κ⊤ �J(j) −ν,−νκ +2λν ∥κ∥1 � , (4.47) where �J(j) ν,−ν denotes the ν−th row of �J(j) without the (ν,ν)−th diagonal element and �J(j) −ν,−ν is the principal submatrix without the ν−th row and ν−th column. Remark 19. Notice that Meinshausen and Bühlmann (2006) established a link between the nodewise regression and the optimal linear prediction of excess asset returns under the assumption that the returns are normally distributed. Relevant studies include Callot et al. (2021) and Caner et al. (2023). 71Introduce the following matrix �C :=       1 −�κ1,2(Dj) ... −�κ1,d(Dj) −�κ2,1(Dj) 1 ... −�κ1,d(Dj) ... ... ... ... −�κd,1(Dj) −�κd,2(Dj) ... 1       . (4.48) Denote with �Ξ(j) = diag � τ1(Dj),...,τd(Dj) � , where �τν � Dj �2 ≡ �J(j) νν − �J(j) ν,−ν�κν(Dj). (4.49) Moreover, the estimation of the matrix �Θ(j) is given by �Θ(j) := � �Ξ(j)�−2 �C(j), (4.50) Therefore, we consider establishing the limit distribution of the following term ¯Sn := √n k k ∑ j=1 β d ν (Dj)−β H ν √Θ∗νν (4.51) for any ν ∈ {1,...,d} under the null hypothesis that H0 : βν = β H ν . Remark 20. Therefore, the above statistical procedure provides the basis for the statistical testing based on divide and conquer. On the other hand, the particular methodology is only implemented for estimation and inference purposes in high dimensional sparse models. In the sparse high dimensional setting, nui- sance parameters introduce a nonnegligible bias, causing classical low dimensional theory to break down. On the other hand, in their high dimensional Wald construction, the phenomenon is remedied through a debiasing of the estimator, which gives rise to a test statistic with tractable limiting distribution. Open Problems An open problem in econometrics and statistics when considering model comparison and selection methodologies remains the aspect of robust statistical inference for nonnested environ- ments. More speciﬁcally, various studies in the literature have considered methodologies for establishing the consistency of the Bayes factor in these settings. A particular interesting solution is the approach of converting a nonnested problem to a nested statistical problem which can provide a pseudo-distance be- tween the base model and the full model. Another approach is to have on overlapping window where the comparisons between the nested models is made (see, Berger and Pericchi (1996), Berger and Mortera (1999)). Moreover, the framework proposed by Vuong (1989) relates the probabilistic model selection approach to the classical nested-hypothesis testing situation. Lastly, in the next section we discuss an application from the statistics literature on sample-splitting to distinguish it from subsampling. Although the particular implementation corresponds to i.i.d data, an extension to dependent data (e.g., time series data with a weak form of dependence) could be an interesting fruitful avenue for future research. 724.5. Sample-Splitting and Variable Importance Algorithm In this section, we follow and discuss the framework proposed by Rinaldo et al. (2019). Speciﬁcally, consider a distribution-free regression framework, where the pair Z = (X,Y) ∈ Rd ×R of d−dimensional covariates and response variables has an unknown distribution belonging to a large non-parametric class of Q. Then, using minimal assumptions (assumption-lean inference) on the regression function x ∈ Rd �→ µ(x) := E � Y|X = x � where µ(x) describes the relationship between the vector of covariates and the expected value of the response variable. We observe the set of data observations Dn = (Z1,...,Zn) where Zi = (Xi,Yi) ∈ Rd+1, where i = 1,...,n and the class Q = Qn, which may depend on the sample size. Then, we apply to the data a procedure wn, which returns both a subset of the covariates and an estimator of the regression function over the selected covariates. Formally, we have that Dn �→ wn (Dn) = � �S, �µ�S � , (4.52) where �S is the selected model, is a random, nonempty subset of {1,...,d} and �µ�S is an estimator of the regression function x ∈ Rd �→ E � Y|X�S = x�S � restricted to �S, where (X,Y) ∼ P independent of Dn and, for a vector x = (x(1),...,x(d)) ∈ Rd, we set x�S = � x( j), j ∈ �S � . The only assumption that is imposed on wn is that the maximum size of the selected model is controlled by the experimenter, that is, 1 ≤ |�S| ≤ k, for a predeﬁned positive integer k ≤ d, where k and d can both increase with the sample size. Therefore, Rinaldo et al. (2019) propose a methodology for deﬁning �S such that it contains any optimal model. In particular, their framework allows for arbitrary procedures including sparse variable selection, and stepwise-forward regression. Thus, the goal of the framework proposed by Rinaldo et al. (2019), is to provide statistical guarantees for various measures of variable importance applied to the covariates in �S, uniformly over the choice of wn and over all the distributions P ∈ Qn. Then conﬁdence sets are considered for four random parameters taking values in R�S, each providing a different assessment of the level of statistical signiﬁcance of the variables in �S from a purely predictive standpoint. All the random parameters under consideration are function of the data generating distribution P, of the sample Dn and its size n, as well as of the mechanism for model selection and the estimation procedure of wn. 4.5.1. The projection parameter β�S Consider a linear estimator of the form x �→ �µ�S(x) = �β ⊤ �S x�S, where �β�S is any estimator of the linear regression coefﬁcients for the selected predictors based on OLS. Then, the linear projection parameter �β�S is deﬁned to be the vector of coefﬁcients of the best linear predictor of Y using X�S, given by �β�S = arg min β∈R�S EX,Y � Y −β ⊤X�S �2 , (4.53) 73where EX,Y denote the expectation with respect to the joint distribution of the random variables (X,Y). In other words, based on the framework proposed by Rinaldo et al. (2019), these projection parameters correspond to the fact that X⊤β�S is the L2 projection of Y into the linear space of all random variables that can be obtained as linear functions of X�S. The projection parameter is well-deﬁned even though the true regression function µ is not linear. Indeed, it is immediate that β�S = Σ−1 �S Q�S, with Q�S := � Q�S, j ∈ �S � (4.54) where Q�S = EX,Y � YX�S( j)|Dn � and Σ�S = E � X�SX⊤ �S |Dn � 4.5.2. The LOCO parameters γ�S and φ�S A commonly used measure of the importance of the selected covariates is β�S, but there are of course other ways to quantify variable signiﬁcance. We consider two parameters of variable importance, which we refer to as Leave Out COvariate Inference-or LOCO-parameters (see, Rinaldo et al. (2019)). The ﬁrst LOCO parameters is γ�S = � γ�S( j) : j ∈ �S � , where γ�S( j) = E ���Y − �β ⊤ �S(j)X�S(j) ��− ��Y − �β ⊤ �S X�S �� ����Dn � . (4.55) In the last expression, �β�S is any estimator of the projection parameter β�S and �S( j) and �β�S(j) are obtained by rerunning the model selection and estimation procedure after removing the j−th predictor. In other words, for each j ∈ �S, �S( j) is a subset of size at most k of {1,...,d} { j}. Notice that the selected model can be different when the j−th covariate is held out from the data, so that the intersection between �S( j) and �S can be smaller than k−1. The interpretation of γ�S( j) is simple: it is the increase in prediction error by not including the j−th predictor in the model. For instance, it is easy to extend the deﬁnition of this parameter by leaving out several variables from �S at once without additional conceptual difﬁculties. Moreover, the parameter γ�S has advantages over the projection parameter β�S and this is because it refers directly to prediction error and as also demonstrate the accuracy of the Normal approximation and the bootstrap is much higher. The second type of LOCO parameters that we consider are the median LOCO parameters, φ�S = � φ�S( j), j ∈ �S � with φ�S( j) = median ���Y − �β ⊤ �S(j)X�S(j) ��− ��Y − �β ⊤ �S X�S �� ����Dn � . (4.56) As with γ�S, we may leave out multiple predictors at the same time. 744.5.3. The prediction parameter ρ�S Another interesting measure of variable importance is an omnibus parameter that measures how well the selected model will predict future observations. To this end, we deﬁne the future prediction error as ρ�S = E ���Y − �β ⊤ �S X�S �� ����Dn � , (4.57) where �β�S is computed based on Dn. In particular, the main idea of the approach proposed by Rinaldo et al. (2019) relies on sample splitting: assuming for notational convenience that the sample size is 2n, we randomly split the data D2n into two halves, D1,n and D2,n. Next, we run the model selection and estimation procedure wn on D1,n, obtaining both �S and �µ�S. We then use the second half of the sample D2,n to construct an estimator �θ�S and a conﬁdence set �C�S for θ�S satisfying the following properties: • Concentration: lim sup n→∞ sup wn∈Wn sup P∈Qn P ����θ�S −θ�S �� > rn � → 0, (4.58) • Coverage Validity: lim inf n→∞ inf wn∈Wn inf P∈QnP � θ�S ∈ �C�S � ≥ 1−α, (4.59) • Accuracy: lim sup n→∞ sup wn∈Wn sup P∈Qn P � ν � �C�S � > εn � → 0, (4.60) where α ∈ (0,1) is a prespeciﬁed level of signiﬁcance, Wn is the set of all the model selection and estimation procedures on samples of size n, rn and εn both vanish as n → ∞ and ν is the volume (Lebsegue measure) of the set. The probability statements above take into account both the randomness in the sample Dn and the randomness associated to splitting it into halves. Remark 21. The property that the coverage of �C�S is guaranteed uniformly over the entire class Qn is known as (asymptotic) honesty. Moreover, the conﬁdence sets are for the random parameters (based on half the data) but the uniform coverage , accuracy and concentration guarantee to hold with respect to the distribution of the entire sample and the randomness associated to the splitting of the full sample. Remark 22. Since we are particularly interested to study suitable variable selection methodologies for time series regression models, notice that usually the subsampling approach in such modelling settings requires to consider the dependence structure of the data. For example, the greater the number of sub- samples, the larger the bias due to the fact that consistent estimation of long-memory parameters requires larger samples. Determining the length of subsamples under the presence of long memory in time series data is another aspect of concern. 754.5.4. Conﬁdence sets for the projection parameters: The bootstrap Following Rinaldo et al. (2019), the conﬁdence set based on the Normal approximation require the eval- uation of both the matrix �Γ�S and the quantile�tα which may be computationally inconvenient. The authors show that the paired bootstrap can be deployed to construct analogous conﬁdence sets, centered at �β�S, without knowledge of �Γ�S. Thus, the bootstrap distribution corresponds to the empirical probability mea- sure associated to the subsample D2,n and conditionally on D1,n and the outcome of the sample splitting procedure. Denote �β ∗ �S the estimator of the projection parameters β�S arising from i.i.d sample of size n drawn from the bootstrap distribution. For a given α ∈ (0,1), let �t∗ α be the smallest positive number P �√n ����β ∗ �S − �β�S ��� ≤�t∗ α ��D2,n � ≥ 1−α. (4.61) Next, let � �t∗ j , j ∈ �S � be such that P �√n ����β ∗ �S ( j)− �β�S( j) ��� ≤�t∗ j , ∀ j ��D2,n � ≥ 1−α. (4.62) By the union bound, each �t∗ j can be chosen to be the largest positive number such that P �√n ����β ∗ �S ( j)− �β�S( j) ��� >�t∗ j , ∀ j ��D2,n � ≤ α k . (4.63) Consider the following two bootstrap conﬁdence sets: �C∗ �S = � β ∈ R�S : ���β − �β�S ��� ≤ �t∗ α √n � , (4.64) �C∗ �S = � β ∈ R�S : ���β( j)− �β�S( j) ��� ≤ �t∗ j √n, ∀ j ∈ �S � . (4.65) It is immediate that �C∗ �S and �C∗ �S are just the bootstrap equivalent of the conﬁdence sets above. Therefore, the case of sparse ﬁtting where k = O(1) implies that the size of the selected model is not allowed to increase with n. The standard central limit theorem shows that √n ��β −β � → N (0,Γ), Γ = Σ−1E �� Y −β ⊤X �2 � Σ−1. (4.66) Note that Γ can be consistently estimated by the sandwich estimator �Γ = �Σ−1A�Σ−1, where A = n−1X⊤RX, Xi j = Xi( j), R is the k ×k diagonal matrix with Rii = � Yi −X⊤ i �β �2 . By Slutsky’s theorem, valid asymp- totic conﬁdence sets can be based on the Normal distribution with �Γ in place of Γ. However, as Rinaldo et al. (2019) points out there is a clear prediction/accuracy trade-off when employing the sample splitting approach for variable selection. In other words, the selected model may be less accurate because only part of the data are used to select the model. Thus, although splitting creates gains in accuracy and robustness for inference it with some loss of prediction accuracy. 76Open Problems An interesting application would be to consider the feasibility of using sample split- ting as an estimation and inference approach in time series regression models. For instance as in the case, when quantile regression models are employed for modelling risk measures such as the VaR and the CoVaR. In that case, sample splitting can be seen as a methodology for out-of-sample forecasting of the unknown risk quantities. Thus, developing robust methodologies for accommodating not only the uncertainty induced by the selection of the optimal size of the out-of-sample period but also to be able to capture correctly the persistence properties of regressors included in the model. Another example, consider the block bootstrap which is commonly which as a resampling method to preserve the depen- dence structure in predictive regression models. Thus, in the case of time-varying predictive regression models, essentially one models the effect of time-varying persistence using a rolling window. Combining these sets of information in a meaningful way is crucial in understanding how the persistence properties of predictors affect the asymptotic efﬁciency of statistics such as the bootstrap conﬁdence set or other methods of variable selection in high-dimensional settings with dependent data. 4.6. Multiple Testing Procedure and Variable Selection In this section, we consider an alternative methodology proposed in the statistical literature especially, since the pioneered work of Abraham Wald on Sequential tests of statistical hypotheses. Towards the direction of the multiple testing approach applied to high dimensional regression models for variable selection and statistical inference purposes, a relevant framework is proposed by Chudik et al. (2018). Therefore, in this section we discuss their proposed statistical methodology, so-called One Covariate at a Time Multiple Testing (OCMT) (see, also Zhang (1993)). Speciﬁcally, the particular procedure is computationally simple and fast even for extremely large data sets (see, also Romano and Wolf (2005)). Thus, when a Lasso penalization is allowed then the estimator of β is obtained via ˆβ = arg min β∈Rp T ∑ t=1 �� yt −x′ ntβ �2 +Pλ(β) � . (4.67) A certain degree of sparsity is required in order to apply standard penalized linear models in high- dimensional settings. However, the methodology does not require that the regressor vector xnt to have a sparse covariance matrix, and therefore it is still applicable even if the covariance matrix of the noise variables, is not sparse. The particular algorithm can be thought as a model selection device since the main idea of this procedure is to test the statistical signiﬁcance of the net contribution of all n available potential covariates in explaining yt individually, while accounting for the multiple testing nature of the problem under consideration. However, the OCMT procedure is not sequential and selects in a single step all covariates whose t−ratios exceed a given threshold. Moreover, using the OCMT procedure post- selection is only applied when there are still covariates whose net contribution to yt is zero, despite the fact that they belong to the true model for yt. From the statistics perspective sequential model selection is discussed by Fithian et al. (2015). These approaches construct regression models by selecting variables from active sets, based on a sequence of sets. 774.6.1. Rejection Principle of Familywise Error Control From the statistical theory perspective the partitioning principle is a powerful tool in multiple decision theory. The following theorem explains the concept of familywise error rejection, (FWER). Theorem 14 (Finner and Strassburger (2002)). Let α,γ ∈ (0,1). If the null hypothesis H0, j : β j = 0 gets rejected whenever Qj(γ) ≤ α, then the FWER is asymptotically controlled at level α, that is, lim sup n→∞ P � min j∈N Qj(γ) ≤ α � ≤ α. (4.68) Closed testing and partitioning are recognized as fundamental principles of familywise error control, (FWE). In practice, various multiple testing procedures that control the FWE are often sequential, in the sence that rejection of some of the hypotheses may make rejection of the remaining hypothesis eas- ier. Sequential rejective procedures in the literature include that of Romano and Wolf (2005) (see, also Politis et al. (1999)). Moreover, the paper of Goeman and Solari (2010) presents a uniﬁed approach to the class of sequentially rejective multiple testing procedures, emphasizing the sequential aspect. Thus, the authors consider a general sequentially rejective procedure as a sequence of single-step methods, deter- mined by a rule for setting the rejection regions for each null hypothesis based on the current collection of unrejected regions for each null hypothesis based on the current collection of unrejected null hypotheses. Thus, the sequential rejection principle, implies that a single-step familywise error controlling procedure is turned into a sequential one which is a general principle of familywise error control. Theorem 15 (Sequential rejection principle, Goeman and Solari (2010)). Suppose that for every R ⊆ S ⊂ H , almost surely, N (R) ⊆ (S )∪S and that for every M ∈ M, P � N (F(M)) ⊆ F(M) � ≥ 1−α. (4.69) Then, for every M ∈ M, P � R∞ ⊆ F(M) � ≥ 1−α. Remark 23. The ﬁrst condition - monotonicity condition, guarantees that no false rejection in the critical case (during the single-step), which implies no false rejection in situations with fewer rejections than in the critical case so that type I error control in the critical case is sufﬁcient for overall FWE control of the sequential procedure. The second condition above - single-step condition, guarantees FWE control when we have rejected all false null hypotheses and none of the true ones. A general admissibility criterion is the case of restricted combinations8 can be also constructed. Restricted combinations occur if, for some R ⊆ H , there is no model M ∈ M such that R = F(M) (see, Goeman and Solari (2010)). 8A standard example concerns testing pairwise equality of means in a one-way ANOVA model: if any single null hypoth- esis is false, it is not possible that all other null hypotheses are simultaneously true (see, also Vesely et al. (2021)). 78• Resampling-based multiple testing procedures use resampling techniques to let the multiple test- ing procedure estimate or accommodate the actual dependence structure between the test statistics. For example, resampling of a test statistic under the complete null hypothesis, using permutations or the bootstrap, can give consistent estimates of the desired quantiles. However, regardless of the underlying assumptions, consistent estimation of the quantiles of maxH∈T (M)SH only guarantees control of the familywise error in an asymptotic sense. Resampling-based methods with exact FWE control, imply that one can obtain control of the FWE by generalizing the treatment of permutation testing to a multiple testing procedure. In order to deﬁne a resampling-based sequentially rejective multiple testing procedure with exact familywise error control, we choose a set π = {π1,...,πr} of r functions that we shall refer to as null-invariant transformations. • Graph-based procedures. Another application of the sequential rejection principle, which is of interest in the statistics, econometrics and ﬁnance literature, is the development of multiple testing9 procedures for graph-structured hypotheses. Speciﬁc procedures for controlling the familywise error for graph-structured hypotheses have been proposed by several authors. Example 20 (Neighborhood Selection with the Lasso, Meinshausen and Bühlmann (2006)). Consider the p−dimensional multivariate normal distributed random variable X = (X1,...,Xp) ∼ N (µ,Σ). This includes the Gaussian linear models where X1 is the response variable and {Xk,2 ≤ k ≤ p} are the predictor variables. The conditional independence structure of the distribution can be represented by a graphical model G = (V ,E ), where V = {1,..., p} is the set of nodes and E is the set of edges in V ×V . A pair (a,b) is contained in the edge set E if and only if Xa is conditionally dependent on Xb, given all remaining variables XV \{a,b}. Every pair of variables not contained in the edge set is conditionally independent, given all remaining variables, and corresponds to a zero entry in the inverse covariance matrix. When predicting a variable Xa with all remaining variables {Xk;k ∈ Γ(n)\{a}}, the vanishing Lasso coefﬁcient estimates identify asymptotically the neighborhood of a node a in the graph. Let the n × p(n)−dimensional matrix X contain n independent observations of X, so that the columns Xa cor- respond for all a ∈ Γ(n) to the vector of n independent observations of Xa. Let ⟨.,.⟩ be the usual inner product on Rn and ∥ . ∥2 the corresponding norm. The Lasso estimate ˆθa,λ of θa is given by ˆθa,λ = arg min θ:θa=0 � n−1∥Xa −Xθ∥2 2 +λ ∥θ∥1 � , ∥θ∥1 = ∑ b∈Γ(n) |θb|, (4.70) where ℓ1−norm of the coefﬁcient vector. Furthermore, Salgueiro et al. (2005) and Salgueiro et al. (2006) propose a formal statistical framework for edge exclusion in graphical Gaussian models. Moreover, sta- tistical theory for constructing relevant matrix moments functions is presented by Roverato and Whittaker (1998). Recently, Fan et al. (2020) propose a conditional dependence measure in high-dimensional undi- rected graphical models (UGM). In particular, the UGM approach examines the internal conditional dependency structure of a multivariate random vector. 9Notice that for multiple testing procedures researchers are also interested in reporting multiplicity-adjusted p-values. Such multiplicity-adjusted p-values are deﬁned for each null hypothesis as the smallest α−level that allows rejection of that hypothesis (see also the method proposed by McKeague and Qian (2015) and Huang et al. (2019)). 794.6.2. Subgroup Selection Methodology Lastly, we brieﬂy discuss the subgroup selection methodology proposed by Reeve et al. (2021)10. In subgroup selection, the main objective of the statistician is to leverage the underline structure dynamics in data to identify a subset ˆ A of the population to treat. Suppose that we have a distribution P on a covariate-response pairs (X,Y) in Rd ×R. Let µ := µP denote the marginal distribution of the covariate X ∈ Rd. Let η : Rd → R be the regression function deﬁned by η(x) := E � Y|X = x � for some x ∈ Rd. Then, one would like to select a subgroup A ⊂ Rd such that η is above a user-speciﬁed threshold τ ∈ R on A. Hence, we are interested in subsets of the τ−super level set such that the following set holds Xτ(η) := � x ∈ Rd : η(x) ≥ τ � . (4.71) In other words, the statistical mechanism of Reeve et al. (2021) implies that the user chooses a data- dependent subgroup ˆA ≡ ˆA(A ), which is a random subset of Rd, taking values in A . Speciﬁcally, the practitioner has access to a sample D := � (X1,Y1),...,(Xn,Yn) � i.i.d ∼ P. Therefore, from the objective function perspective the aim is to select a data-dependent subgroup ˆA(D) ⊂ Xτ(η), with high-probability where Xτ(η) := � x ∈ Rd : η(x) ≥ x � . Proposition 3 (Type 1 error guarantee,Reeve et al. (2021)). Let P be a family of distributions P on pairs (X,Y) and choose a signiﬁcance level α ∈ (0,1). We say that the data-dependent subgroup ˆA controls Type 1 error at the level α over the class P if inf P∈P PP � ˆA(D) ⊆ Xτ(η) � ≥ 1−α. (4.72) Therefore, our objective is to choose ˆA ≡ ˆA(D) which minimizes regret Rτ( ˆA), subject to inf P∈P PP � ˆA(D) ⊆ Xτ(η) � ≥ 1−α. Deﬁnition 7 (Holder Class, Reeve et al. (2021)). Given that (β,CS) ∈ (0,1]×[1,∞), we let PHol,τ(β,CS) denote the class of all distributions P on Rd × [0,1] with marginal µ on Rd such that the regression function η is (β,CS)−Holder on Xτ(η)∩supp(µ) in the sence that ��η(x′)−η(x) �� ≤ CX. ��x′ −x ��β ∞ , (4.73) for all x,x′Xτ(η)∩supp(µ). Further details on the implementation of this methodology as well as related statistical theory and nu- merical illustrations are presented in Reeve et al. (2021). More recently the isotonic subgroup selection approach is proposed by Müller et al. (2023). 10Dr. Henry Reeve gave a seminar with title: "Subgroup Selection in nonparametric regimes", at the S3RI Departmental Seminar Series at the University of Southampton on the 10th of November 2022. 80Remark 24. Notice that according to Reeve et al. (2021), the subgroup selection approach requires from the statistician to use both FWE control as well as the selection step. In other words, the subgroup selec- tion approach corresponds to a data-dependent selection methodology, such that ˆ A = ˆ A (D). Therefore, a special class of functions needs to be considered which allows to test for local null hypothesis. Thus, the statistical problem corresponds to minimizing the regret that controls the type I error. Although one in practice needs to check the exact set that corresponds to the minimized regret. Therefore, the following expression holds: inf p∈P PP � ˆ A (D) ⊂ Xτ(n) � ≥ 1−α. (4.74) Then, the Type I error guarantee implies that the statistical subgroup selection procedure is constructed by sequential partial ordering of multiple covariates. Then, the statistician chooses only the one null hypothesis with the highest partial ordering. In other words, using a graph with acyclic structure allows to expand the multiple testing procedures (the null hypothesis of all ancestors is true). Thus, if the null at that node is false, then the null hypothesis associated with all ancestors is also false. 814.7. Model Selection in Cointegrating Regressions According to Mendes (2011), usually under the presence of cointegration in the VAR system, which implies that there are (k − m) linear combinations of Xt which are I(0), a bias analysis is necessary to obtain a robust econometric estimation methodology. To do this, we consider the m common stochastic trends and at most (k − m) cointegrating relations amongst the I(1) components are represented by the same generating process as ∆Xt−1 = (A − Ik)Xt−1 + εt, where εt ∼ N (0,Ω). In particular, these m components are represented by m unit roots, and the (k − m) stable roots of the stochastic difference equation represent the I(0) components of Xt as well as cointegrating relations between the I(1) variates of Xt (see, Abadir et al. (1999)). Example 21. Consider the following cointegration regression model given by the expression yt = α0 +β ′ 0xt +γ′ 0zt +ut (4.75) where β0 is an n1 × 1 vector of parameters and γ0 is an n2 × 1 vector of parameters. Furthermore, the process {xt}∞ t=1 satisﬁes the following integrated stochastic process xt = xt−1 +vt, (4.76) Therefore, the main aim shall be to control the number of I(0) variables in the model and we assume that n ≡ n1 +n2 to possibly be grater than T, but only a fraction of those coefﬁcients are in fact nonzero. Furthermore, we assume without loss of generality that each coefﬁcient vectors can be partitioned into zero and non-zero coefﬁcients, such that β0 = � β0(1)′,β0(2)′�′ and γ0 = � γ0(1)′,γ0(2)′�′, with all non- zero coefﬁcients stacked ﬁrst, where β0(1) is q1 ×1 and γ0(1) is q2 ×1. In particular, we assume that the number of non-zero coefﬁcients, measured by q1, is ﬁxed (does not depend on T), while the number of zero coefﬁcients, measured by q2, may depend on T, also set q = (q1 +q2). Then, the Adaptive Lasso estimate is given by (see, also Medeiros and Mendes (2017)) � ˆβ, ˆγ � = argmin β,γ ∥Y −Xβ −Zγ∥2 2 +λ1 n ∑ j=1 λ1j|β j|+λ2 n ∑ j=1 λ2j|γj|, (4.77) Lemma 13 (KKT Conditions). The solutions ˆβ = � ˆβ(1)′, ˆβ(2)′� and ˆγ = � ˆγ(1)′, ˆγ(2)′� to the minimiza- tion problem above exists if: ∂ ∥Y −Xβ −Zγ∥2 2 ∂β j ���� β j(1)= ˆβ j(1) = sgn � ˆβ j(1) � λ1λ1j (4.78) ∂ ∥Y −Xβ −Zγ∥2 2 ∂β j ���� γ j(1)= ˆγ j(1) = sgn � ˆγj(1) � λ2λ2j (4.79) 824.7.1. Model Selection Consistency and Oracle Property We focus on showing that under certain conditions on n, p and λ ′s the Adaptive Lasso selects the correct subset of variables - sign consistency and it has the oracle property, meaning that our estimate has the same asymptotic distribution of the OLS as if we knew a priori the selected model and at optimal rate. Lemma 14. Let Ω∞ = � ΩX,∞ 0 0′ ΩX,∞ � (4.80) ΩX,∞ = � 1 0 BX(1)BX(1)′(r)dr and ΩZ,∞ = ΣZ(1)2, (4.81) where for any 0 ≤ r ≤ 1, BX(1)(r) = lim T→∞∑ ⌊rT⌋ t=1 vt(1). Similarly, split the matrix Ω11 into the partition Ω11 = � ΩX(1)2 ΩZ(1)X(1) Ω′ Z(1)X(1) ΩZ(1)2 � = � 1 T 2X(1)′X(1) 1 T 3/2Z(1)′X(1) 1 T 3/2X(1)′Z(1) 1 T Z(1)′Z(1) � . (4.82) Hence, P � A c T (X) � = P �����T −1Ω−1 X,∞X(1)′U �� � j > T|β0j| � , j = 1,...,q1 � +op(1), ≤ q1 ∑ j=1 P ����T −1Ω−1 X,∞X(1)′U �� � j > T|β0j| � +op(1), ≤ q1 T 2β 2∗ max 1≤j≤q1 E �� T −1��Ω−1 X,∞X(1)′U �� �2 j � → 0. Further aspects of consideration in the more recent literature include the development of robust frame- works for time series regressions within a high-dimensional environment for the purpose of estimation, inference and forecasting (see, Gupta and Seo (2019), Baillie et al. (2022)). Moreover, the development of a framework that allows for the use of an ultra-high dimensional environment in nonstationary time series models can be useful when considering statistical properties such as model selection consistency and hypothesis testing accuracy. An econometric framework for high-dimensional quantile predictive regressions is proposed by Fan et al. (2023). Various studies consider methods for variable screening in high dimensional linear models, however less attention is paid on how these methodologies perform in the case of nonstationary time series models, especially when for the conditional quantile functional form. Relevant research aspects include the development of a statistical mechanism for dimension re- duction or screening methodology in the presence of a high-dimensional vector of possibly nonstationary predictors, which can improve forecasting performance (see, Pitarakis (2023) and Gonzalo and Pitarakis (2023)). Moreover, choosing variables from irrelevant cointegrating relations is another important issue (see, Khalaf and Richard (2020) and Richard (2023)). 834.7.2. Model Selection and Rank of a Matrix In many econometric and statistic applications knowing the rank of a matrix is crucial to ensure robust inference and testing. For example, from the ﬁnancial economics literature one can test the implications of Arbitrage Pricing Theory by testing the corresponding rank restrictions. Moreover, the identiﬁcation of parameters in econometric models depends on conditions and restrictions regarding the rank of related moment matrices such as the Jacobian matrix. Speciﬁcally, various tests for the rank of a matrix can be found in the literature. In particular, these testing methodologies are constructed under the assumption that the unrestricted matrix estimator has a kronecker covariance matrix, however this approach can be sensitive to the ordering of the variables (see, Kleibergen and Paap (2006)). Furthermore, Cragg and Donald (1997) propose suitable testing pro- cedures for determining the rank of a matrix. In particular, the authors examine the use of model selection criterion and sequential hypothesis testing methods to estimate the rank consistently. On the other hand, Kleibergen and Paap (2006) propose a novel rank statistic which uses a √n−consistent estimator of the unrestricted matrix which does not have to have a kronecker covariance matrix. To do this, the authors decompose the estimator of the unrestricted matrix using the spectral vector decomposition. Testing for the rank of the matrix can be also used in cointegration testing for non-stationary time series models. Speciﬁcally, in this case the limiting distribution is found to be functional of Brownian motions and is equal to the asymptotic distribution of the Johansen (1991) trace test. Example 22. Consider the following ECM model ∆xt = Πxt−1 + k−1 ∑ i=1 Ψi∆xt−i +Φdt +εt (4.83) where Π = αβ ′ and β is the p × r matrix with the cointegration vectors and α is the p × r matrix of adjustment coefﬁcients. The number of long-rank relations is equal to the rank of Π, which is called the cointegration rank. Both α and β are full rank matrices. Moreover, the symmetric p × p, Ψi matrix governs the short-term dynamics of the system. 845. Statistical Learning Methods in Time Series Analysis 5.1. Motivation In this section we discuss some key applications of statistical learning methodologies in time series analysis. In particular the use of Neural Networks is now widely spread in the ﬁnance, economics and actuarial statistics ﬁelds. Some applications worth mentioning include: (i) asset pricing modelling (see, Feng et al. (2018), Guijarro-Ordonez et al. (2021), Fan et al. (2022), Chen et al. (2023) and Caner and Daniele (2023)). Relevant applications of the Lasso shrinkage in asset pricing and ﬁnance theory include the studies of Feng et al. (2020) and Chinco et al. (2019). (ii) heterogeneity in mortality modelling(see, Pitacco (2019)). A relevant question for the latter is: "What is the link between heterogeneity of unobservable factors and mortality deceleration?". (iii) credit risk and correlated defaults modelling in ﬁnancial markets (see, Angelini et al. (2008) and Bhatore et al. (2020)). Furthermore, on the aspect of modelling heterogeneity in mortality, more speciﬁcally the statistician can employ a functional form represented by f ("biometric" function) to represent the age pattern of mortality. Then, the following representation follows: f = w1 · f (1) +w2 · f (2) +...+wN · f (N) and µx = aeβx δeβx +1, (5.1) where � f (1), f (2),..., f (N)� represent risk factors (such as health status, occupation etc.) in order to capture the non-linear effects of mortality over time. In particular, these risk factors are considered as contributing factors that worsens the mortality level or probability of an individual that can be incorpo- rated into a rating system. Therefore, splitting individuals with similar risk into groups allows to gain more information about uncertainty in heterogeneity11 (which is a well-known principle in the statistical literature of credibility theory and risk premium models). Moreover, according to Pitacco (2019) a rating system can be constructed based on the form qspec t+x = qt+h · � 1+∑r j=1 ρ(j)� , in which case peak mortality is achieved after entering disable stage, such that qspec t+x = qt+x ·∆(x+h;α,γ). Then, the unobservable risk factors can be captured via the form µspec x = Φ(µx+t;zx,t); known as random heterogeneity. The aforementioned examples provide some brief illustrations of the various applications from the econo- metrics and statistics literature that can motivate the investigation of relevant research questions both from the theoretical as well as the empirical perspective when we consider statistical learning methods for time series analysis purposes. Further resources related to deep learning theory and applications can be found in Goodfellow et al. (2016) (see, also LeCun et al. (2015)). 11Professor Ermanno Pitacco gave a seminar with title: "Heterogeneity in mortality: A survey with an actuarial focus" at the S3RI Departmental Seminar Series at the University of Southampton on 24 of September 2018. 855.2. Non-Asymptotic Probability Theory One of the main goals of learning theory is the development of stability bounds of algorithmic procedures. Following the framework of Mohri and Rostamizadeh (2010) we employ the following deﬁnitions. Deﬁnition 8. A learning algorithm is said to be (uniformly) �β−stable if the hypotheses it returns for any two training samples S and S′ that differ by removing a single point satisfy ∀ z ∈ X ×Y, ��c(hS,z)−c(h′ S,z) �� ≤ �β. (5.2) Remark 25. Notice that a �β−stable algorithm is also stable with respect to replacing a single point. Let S and Si be two sequences differing in the i−th coordinate, and S|i be equivalent to S and Si but with the i−th point removed. then, for a �β−stable algorithm we have that ��c(hS,z)−c(h′ S,z) �� ≤ 2�β. (5.3) Speciﬁcally, the use of stability allow us to derive generalization bounds and an exponential concentration bounds of the following form P(|Φ−E[Φ]| ≥ ε) ≤ exp � −mε2 τ2 � , (5.4) where the probability is over a sample of size m and where τ m is the Lipschitz parameter of Φ, with τ a function of m. In the ﬁrst section below, we consider the problem of shallow-vs-deep expressiveness from the perspective of approximation theory and general spaces of functions having derivatives up to certain order (Sobolev-type spaces). Speciﬁcally, in this framework the problem of expressiveness is very well studied in the case of shallow networks with a single hidden layer, where it is known, in particular, that to approximate a Cn−function on a d−dimensional set with inﬁnitesimal error ε one needs a network of size about ε−d/n, assuming a smooth activation function. Deﬁnition 9 (Lipschitz functions). Consider the class of Lipschitz functions such that FL := � g : [0,1] → R|g(0) = 0 and ��g(x)−g′�� ≤ L|x−x′| ∀ x,x′ ∈ [0,1] � . where L > 0 is a ﬁxed constant, and all of the functions in the class obey the Lipschitz bound uniformly condition over all of [0,1]. Remark 26. Learning theory and non-asymptotic probability theory is useful for understanding the local behaviour of statistical learners for a class of functions based on regularity conditions. Further applica- tions include the aspect of robustness and generalization for metric learning12 (see, Yang et al. (2022)). 12Dr. Xiaochen Yang gave a seminar with title: "Towards better robustness and generalisation of metric learning methods", at the S3RI Departmental Seminar Series at the University of Southampton on the 3rd of November 2022. 865.3. Shallow Neural Network Estimate learned by Gradient Descent Deﬁnition 10. A shallow neural network with one output is a function f : Rd → R of the form f(x) = m ∑ j=1 cjσ � w⊤ j x+vj � , wj ∈ Rd, vj,cj ∈ R, (5.5) where σ : R → R is the activation function (see, Braun et al. (2019) and Braun et al. (2021)). Consider functions of the form f(x) = σ � d ∑ j=1 wj.x(j) +w0 � , where x = � x(1),...,x(d)�⊤ ∈ Rd. (5.6) where w0,...,wd ∈ R the weights of the neuron and σ : R → R the activation function. Shallow Neural Networks have only one hidden layer where a simple linear combination of neurons is used to deﬁne a function f : Rd → R by f(x) = K ∑ k=1 αK.σ � d ∑ j=1 βk, j.x(j) +βk,0 � +α0. (5.7) Denote the weight between neuron j in layer (s − 1) and neuron i in layer s by w(s) i, j . This leads to the following recursive deﬁnition of a neural network with L layers and ks neurons in layer s ∈ {1,...,L } such that the following representation applies: f(x) = kL ∑ i=1 w(L ) 1,i f (L ) i (x)+w(L ) 1,0 (5.8) for some w(L ) 1,0 ,...,w(L ) 1,kL and for f (L ) i ’s recursively deﬁned by f (s) i (x) = σ � ks−1 ∑ j=1 w(s−1) i, j f (s−1) j (x)+w(s−1) i,0 � . (5.9) Main results in the literature show that NNs can achieve dimension reducion provided the regression function is a composition of sums of functions, where the input dimension of each of the functions is at most d∗ < d. Denote by fnet,w the neural network with weight vector w = � w(s) j,k � s=0,..,L , j=1,...,ks+1,k=0,...,ks and set the following function F(w) = 1 n n ∑ i=1 |Yi − fnet,w|2 , w(0) = v, (5.10) for some randomly chosen initial vector v. 87Then the optimization problem can be written as below: w(t +1) = w(t)−λn.∇wF(w(t)), for t ∈ {0,...tn −1}. (5.11) Example 23. We approximate m by networks with one hidden layer and K.r neurons in this hidden layer: fnet,(a,b)(x) = K.r ∑ k=1 αk.σ � d ∑ j=1 bk, j.x(j) +bk,0 � +α0. (5.12) where K.r ∈ N is the number of neurons and σ : R → R is the activation function. Furthermore, the unknown optimal vector of weights is obtained by employing the gradient descent algorithm. More precisely, we minimize the penalized empirical L2 risk as F(a,b) = 1 n n ∑ i=1 ��fnet,(a,b)(Xi)−Yi ��2 + c1 n . K.r ∑ k=0 a2 k. (5.13) by choosing the appropriate starting value � a(0),b(0)� and by setting � a(t+1) b(t+1) � = � a(t) b(t) � −λn. � ∇(a,b)F �� a(t),b(t)� (5.14) for some λn > 0 chosen below and t ∈ {0,1,...,tn−1}. Theorem 16. Let n ≥ 1, let A ≥ 1 and let {(X1,Y1),...,(Xn,Yn)} be i.i.d random variables with values in [−A,A]d ×R. Set m(x) = E[Y|X = x] and assume that (X,Y) satisﬁes E � ec2.|Y|2� < +∞ (5.15) for some constants c2 > 0, and that m satisﬁes m(x) = r ∑ s=1 gs � c⊤ s x � , x ∈ Rd. (5.16) for some r ∈ N,cs ∈ [−1,1]d, where ∥cs∥ = 1 and gs : R → R for s ∈ {1,...,r}. Remark 27. From the statistical perspective, relevant properties and convergence rates for gradient descent algorithms are given in the studies of Braun et al. (2019), Shao and Zhang (2022) (see, also Toulis and Airoldi (2017)) as well as by Schmidt-Hieber (2020). Furthermore, in recent years the use of ANN and DNN for econometric applications has seen growing attention (see, White (1990) Kuan and White (1994), Farrell et al. (2021)). 885.3.1. Learning of linear penalized least squares estimates by gradient descent Let (x1,y1),...,(xn,yn) ∈ Rd × R, let K ∈ N and let B1,...,BK : Rd → R, with c1 > 0. We consider the problem to minimize as below F(a) = 1 n n ∑ i=1 ����� K ∑ k=1 ak.Bk(xi)−yi ����� 2 + c1 n .∥a∥2 (5.17) where a = (a1,...,aK)⊤ and ∥a∥2 = ∑K j=1a2 j, by gradient descent. To obtain the solution of the optimization problem, we choose a(0) ∈ RK and set with a(t+1) = a(t) −λn.(∇aF) � a(t)� (5.18) for some chosen λn > 0. Consider that a Lipschitz continuity condition holds ∥(∇aF)(α1)−(∇aF)(α2)∥ ≤ Ln.∥a1 −a2∥ with (a1,a2) ∈ RK. (5.19) Then, we have that F � a(t+1) −F � a(t)�� ≤ − 1 2.Ln . ���(∇aF) � a(t)���� 2 . (5.20) Lemma 15. Let σ be the logistic squasher. Let ¯c ∈ [−1,1]d with ∥¯c∥ = 1 and let g : R → R be (p,C)−smooth for some p ∈ (0,1] and C > 0. Let ρn > 0, K ∈ N and choose b1,b2,...,bK ∈ R such that b1 < b2 < ... < bK and b1 ≤ −A. √ d and bK ≥ A. √ d − 4.A. √ d K −1 (5.21) such that A. √ d (n+1).(K −1) ≤ |bk+1 −bk| ≤ 4.A. √ d K −1 , k ∈ {1,...,K −1}. (5.22) Let a0 = g(b1) and ak = g(bk)−g(bk−1) with k ∈ {1,...,K} where K is the number of layers. Then, we have that sup x∈[−A,A]d �����a0 + K ∑ k=1 ak.σ � ρn. � ¯c⊤x−bk �� −g � ¯c⊤x ������ ≤ 3.(4.A. √ d)p.C (K −1)p +C.(4.A. √ d)p.(K −1)1−p.e− ρn.(A. √ d) (n+1).(K−1). 89Proof. Notice that we assume that the logistic activation function σ(x) = 1 1+e−x is approximated by the step function 1{[0,∞)}(x). In other words, for any x ∈ R we have that |σ(x)−1{[0,∞)}(x)| ≤ e−|x|. (5.23) We have that �����a0 + K ∑ k=1 ak.σ � ρn. � ¯c⊤x−bk �� −g � ¯c⊤x ������ ≤ �����a0 + K ∑ k=1 ak.σ � ρn. � ¯c⊤x−bk �� − K ∑ k=1 ak.1[bk,∞) � ¯c⊤x ������ + �����a0 + K ∑ k=1 ak.1[bk,∞) � ¯c⊤x � −g � ¯c⊤x ������ Moreover, for each bj ≤ ¯c⊤x < bj+1, where j{1,...,K −1}, we can conclude that the deﬁnition of ak, from the (p,C)−smoothness of g and from our choice of the bk we obtain �����a0 + K ∑ k=1 ak.1[bk,∞) � ¯c⊤x � −g � ¯c⊤x ������ = �����a0 + j ∑ k=1 ak −g � ¯c⊤x ������ = ���g(bj)−g � ¯c⊤x ���� ≤ C. ���bj − ¯c⊤x ��� p ≤ C. ��bj+1 −bj ��p ≤ C. � 4.A. √ d �p (K −1)p . Therefore, we have shown that sup x∈[−A,A]d �����a0 + K ∑ k=1 ak.1[bk,∞) � ¯c⊤x � −g � ¯c⊤x ������ ≤ C. � 4.A. √ d �p (K −1)p . (5.24) We complete the proof by showing that sup x∈[−A,A]d �����a0 + K ∑ k=1 ak.σ � ρn. � ¯c⊤x−bk �� − K ∑ k=1 ak.1[bk,∞) � ¯c⊤x ������ ≤ 2. � 4.A. √ d �p .C (K −1)p +C. � 4.A. √ d �p .(K −1)1−p.e− ρn.(A. √ d) (n+1).(K−1). 90Lemma 16 (Braun et al. (2019)). Let σ be the logistic activation function. Deﬁne F to be the ridge regression and set with ¯b = b−λn.(∇bF)(a,b) (5.25) for some λn > 0, where a = � a1,...,aK �⊤ ∈ RK and b = � b1,0,b1,1,...,b1,d,...,bK,0,bK,1,...,bK,d �⊤ ∈ RK.(d+1). Then, we have that for any k ∈ {1,...,K} and any j ∈ {0,...,d} such that: ��¯bk, j −bk, j �� ≤ λn.2. � F(a,b).max � 1,max ����x(ℓ) i ��� �� .exp � − min i=1,...,n � d ∑ j=1 bk, j.x(j) i +bk,0 �� . Consider the Cauchy-Schawrz inequality and since the activation function σ is Lipschitz continuous 1 n n ∑ i=1 � fnet,(¯a,b(t))(xi)− fnet,(¯a,b(0))(xi) �2 = 1 n n ∑ i=1 � K ∑ k=1 ¯ak. � σ � d ∑ j=1 b(t) k, j.x(j) i +b(t) k,0 � −σ � d ∑ j=1 b(t) k, j.x(j) i +b(0) k,0 ���2 ≤ K ∑ k=1 ¯a2 k.max � 1,max i, j |x(j) i |2 � .(d +1). K ∑ k=1 d ∑ j=0 ���b(t) k, j −b(0) k, j ��� Remark 28. Notice that dimensionality reduction is crucial concept that commonly discussed in the neural network literature from the perspective of layers and related algorithms via the implementation of feedforward neural networks techniques. Alternative approaches include the implementation of taylor expansions which is more commonly used as a methodology for estimating econometric models (e.g. see Olmo (2022)) as well as the method of sieves and sieve estimators (see, Chen (2007)). Speciﬁcally, focusing on comparing these two estimation methodologies can provide some intuition on the main im- plications of the choice of the contraction mapping (see, Keeler and Meir (1969), Reich (1971)) to the underline asymptotic theory. In particular, the framework of Olmo (2022) corresponds to a nonparamet- ric linear regression model with a lagged regressor. Due to the fact that the functional form is estimated using a taylor expansion with partioning13, implies that the corresponding stochastic approximation has discontinuous increments. Thus, the asymptotic behaviour of estimators and test statistics consists of asymptotic functionals that correspond to the supremum of Gaussian processes (e.g., see Beder (1987)) which might not even satisfy regularity conditions such as tightness. However, this is problematic for several reasons and especially when the interest of the econometrician is the modeling of nonstation- arity in regressors but the partitioning and estimation methodology does not correspond to the relevant stochastic approximations. A good understanding of the principles of contraction mappings is crucial. 13On the large sample properties of partitioning-based series estimators see, Cattaneo et al. (2020). 915.4. Deep Neural Network Estimate learned by Gradient Descent Based on the framework proposed by Shen et al. (2021), we present the following results which are useful to investigate the properties of Deep Neural Networks learned by gradient descent with respect to their dimensionality and complexity. In particular, the concept of pseudo-dimension is considered as a measure of complexity (see, Mohri and Rostamizadeh (2008)). Thus, the framework of Shen et al. (2021) considers the implementation of deep neural network for a high-dimensional quantile regression. An application to forecasting in time series is presented by Chronopoulos et al. (2023). A key ingredient to derive excess risk bounds for the deep quantile regression framework is to consider the properties of the functional classes within which identiﬁcation and estimation holds. Assumption 9 (Shen et al. (2021)). To derive excess risk bounds the following conditions hold: (i) The conditional τ−th quantile of η given X = x is 0 and E � |η||X = x � < ∞ for almost every x ∈ X. (ii) The support of covariates X is a bounded compact set in Rd, and without loss of generality X = [0,1]d. (iii) The response variable Y has a ﬁnite p−th moment for some p > 1, that is, there exists a ﬁnite constant M > 0 such that E|Y|p ≤ M. Lemma 17 (Lemma 2 in Shen et al. (2021)). Consider the d−variate nonparametric regression model with an unknown regression function f0. Let F = FD,W ,U ,S ,B be a Holder class of feed-forward neural networks with a continuous piecewise-linear activation function of ﬁnite elements ("pieces") and ˆfφ ∈ argmin f ∈Fφ Rnτ( f) (5.26) be the empirical risk minimizer over Fφ. Assume that Assumption 9 above holds and that ∥ f0∥∞ ≤ B for B ≥ 1. Then, for 2n ≥ Pdim(Fφ) and any τ ∈ (0,1), it holds that sup f ∈Fφ ��Rτ( f)−Rnτ( f) �� ≤ c0 max{τ,1−τ}B n1−1/p log � N2n � n−1,∥.∥∞ ,Fφ � , (5.27) Notice that this quantity is considered as an upper bound of the empirical risk minimizer, where c0 > 0 is a constant independent (i.e., not depending upon) of n,d,τ and the remaining components. Moreover, its expected value (here an aspect of interest is to determine the underline distribution theory which relates these quantities w.r.t to marginal and joint distributions) has the following upper bound E � Rτ( ˆfφ)−Rnτ( f0) � ≤ C0 max{τ,1−τ}BBBlog(S)log(n) n1−1/p +2 inf f ∈Fφ ��Rτ( f)−Rτ( f0) �� (5.28) Notice that the denominator can be improved to n if the response Y is assumed to be sub-exponentially distributed, that is, there exists a constant σY > 0 such that E � exp � σY|Y| �� . 92Moreover, further interesting aspects here include the technical results with respect to the DNN archi- tecture (e.g., composition), the relation of continuous functionals with the underline quantile regression model and the number of variables as well as linearity of the model and how it relates with the dimen- sionality of the problem. In particular, one can use numerical experiments to evaluate the performance of the standard kernel-based method for quantile regressions against the DQR estimation approach. Remark 29. A statistical procedure should be providing such evidence (such as a formal speciﬁcation test approach). On the other hand, starting with some comparisons of relative efﬁciency and a bias anal- ysis can provide some insights. Various open problems remain in the literature of DNN especially when considering the correct model speciﬁcation. However, such a research endeavour will require to deﬁne what DNN estimable means. Then this could allow us to construct a speciﬁcation test for a testing that a function under the null hypothesis is correctly DNN estimable against the alternative of a nonparametric kernel-estimation approach. To do this we need to deﬁne what a distance function is between a DNN estimable functional form against a nonparametric kernel-based estimation approach. Relevant litera- ture where these aspects are discussed are the studies that present statistical frameworks for consistent speciﬁcation testing (see, Stinchcombe and White (1998) and White (1996)). Nevertheless, the key point is consider a suitable functional class within which a statistical test can be constructed for evaluating differences between a DNN estimable functional form and a nonparametrically ﬁtted functional form. Deﬁnition 11 (Shen et al. (2021)). For a class F of functions: X → R, its pseudo-dimension, denoted by Pdim(F), is deﬁned to be the largest integer m for which there exists � x1,...,xm,y1,...,ym � ∈ X mRm such that for any (b1,...,bm) ∈ {0,1}m there exists f ∈ F such that ∀i : f(xi) > yi ⇐⇒ bi = 1. Remark 30. For a class of real-valued functions generated by neural networks, pseudo dimension is a natural measure of its complexity. In particular, if F is the class of functions generated by a neural network with a ﬁxed architecture and ﬁxed activation functions, we have that Pdim(F) = VCdim(F). Thus, a usual assumption under these settings is to require the sample size n to be greater than the pseudo dimension of the class of neural networks considered. Furthermore, the smoothness index works as a covering number to measure the complexity or dependence structure of the DNN. For instance, a function of the form f d 0 = � hq ◦ ... ◦ h0 � is a recursive function which implies that for each composition of functions, its argument is converted into one-dimensional functional form. In other words, the main intuition here is that by assuming a compositional structure of high-dimensional quantile regressions, this provides an efﬁcient solution to the "curse of dimensionality" problem, since in each composition applied, there is a reduction in the dimensions of the statistical problem. Lemma 18 (Lemma 1 in Shen et al. (2021)). For any random sample S = � (Xi,Yi)n i=1 � , the excess risk of the DQR estimator ˆfn satisﬁes Rτ( ˆfn)−Rτ( f0) ≤ 2 sup f ∈Fn ��Rτ( f)−Rnτ( f) ��+ inf f ∈Fn � Rτ( f)−Rτ( f0) � , (5.29) where Rnτ is deﬁned above. 93Remark 31. The excess risk of the DQR estimator is bounded above by the sum of two terms: the stochastic error 2 sup f ∈Fn ��Rτ( f)−Rnτ( f) �� and the approximation error inf f ∈Fn � Rτ( f)−Rτ( f0) � . Further- more, an interesting conjecture from the above result is that the upper bound no longer depends on the DQR estimators itself, but the function class Fn, the loss function ρτ and the random sample S. Then, the stochastic error 2 sup f ∈Fn ��Rτ( f) −Rnτ( f) �� can be analyzed using the empirical process theory. A key ingredient is to calculate the complexity measure Fn in terms of its covering (bracketing) number (see, Vaart and Wellner (2023), Van der Vaart (2000), Massart (2007) and Wainwright (2019)). Furthermore, for the excess risk of the DQR estimator and the error bounds for the models, based on appropriately speciﬁed network parameters (depth, width and size of the network), we have the following upper bound for the excess risk, E � Rτ( ˆfφ)−Rτ( f0) � ≤ C0Cd,d∗� logn �2n − � 1− 1 p � 2α∗ 2α∗+t∗ (5.30) • C0 is a constant only depending on the model parameters such as the smoothness index (smoothness parameter) of the underlying conditional quantile function. • Cd,d∗ is the prefactor depending on d, the dimension of the predictor variable in the model. Al- though a question remains whether this parameter has a ﬁxed functional form when considering each lower dimension term. • d∗ is determined by the dimensions of the component functions in the composite function. Then, the convergence rate which is part of the error bound, n− � 1− 1 p � 2α∗ 2α∗+t∗ , is determined by the number of moments p of the response Y, the smoothness index of the composite function α∗ and the intrinsic dimension of the model t∗. Other relevant aspects include whether imposing further distributional as- sumptions (such as heavy-tailed errors versus Gaussian errors) can affect the limit theory of estimators as well as its min-max optimality properties. Next, we employ the risk function (i.e., the expectation of the loss function) such that Rτ( f) := E � ρτ(Y − f(x)) � as well as the empirical risk, which is the minimizer of the empirical risk function (i.e., it allows to restrict the functional class). For the proof of the above lemma (Lemma 1 in Shen et al. (2021)) the following two steps are necessary and sufﬁcient: Step 1. Prediction error decomposition (see, Shen et al. (2021)). The "best in class" estimator f ∗ φ is deﬁned as the estimation in the function class F with minimal L risk such that: f ∗ φ = argmax f ∈Fφ Rτ( f). (5.31) Then, the approximation error of f ∗ φ , Rτ( f ∗ φ)−Rτ( f0) only depends on the function class and the distribution of data. Moreover, by the deﬁnition of the empirical risk minimizer, the following inequality result holds 94ES � 1 n n ∑ i=1 g � ˆfφ,Zi � � ≤ ES � 1 n n ∑ i=1 g � f ∗ φ ,Zi � � . (5.32) Therefore, it can be proved that the predictor error is upper bounded by the sum of an expectation of a stochastic term and an approximation error. Step 2. Bounding the stochastic term (see, Shen et al. (2021)). To bound the stochastic term, we obtain an upper bound of the expression which includes the stochastic term, using a truncation argument and the classical chaining technique for empirical processes.Denote with G( f,Zi) := ES′� g( f,Z′ i) � − 2g( f,Zi) for any function such that f ∈ Fφ. Given a δ−uniform covering of Fφ, we denote the centers of the balls by f j, j = 1,2,...N2n, where N2n = N2n � δ,∥.∥∞ ,Fφ � is the uniform covering number with radius δ(δ < B) under the norm ∥.∥∞. By the deﬁnition of covering, there exists a (random) j∗ element such that �� ˆfφ(x)− f j∗(x) �� ∞ ≤ δ on x = � X1,...,Xn,X′ 1,...,X′ n � ∈ X 2n, (5.33) Recall that we have g � f,Zi � = � ρτ � f(Xi)−Yi � −ρτ � f0(Xi)−Yi �� . Denote with λτ = max{τ,1−τ}, then by the Lipschitz property of ρτ, for a,b ∈ R it holds that ��ρτ(a)−ρτ(b) �� ≤ max{τ,1−τ}|a−b| = λτ|a−b|, (5.34) Therefore, for i = 1,...,n we have that ��g � ˆfφ(x),Zi � −g � f j∗(x),Zi ��� ≤ λτδ, ��ES′ � g � ˆfφ(x),Z′ i �� −ES′ � g � f j∗(x),Z′ i ���� ≤ λτδ (5.35) Then, it holds that ES � 1 n n ∑ i=1 g � ˆfφ(x),Z′ i � � ≤ 1 n n ∑ i=1 ES � g � f j∗(x),Z′ i �� +λτδ (5.36) Moreover, denote with βn ≥ B ≥ 1 to be a positive number who may depend on the sample size n. Denote with Tβn as the truncation operator at level βn, such that it holds that for any Y ∈ R, TβnY = Y, if |Y| ≤ βn and TβnY = βn.sign(Y) otherwise. Deﬁne the function f ∗ βn : X → R pointwisely by f ∗ βn = arg min f (x):∥f ∥∞≤βn E � ρτ � f(X)−TβnY � |X = x � , (5.37) for each x ∈ X . Moreover, recall that ∥ f ∗∥∞ ≤ B ≤ βn and f0(x) := arg min f (x):∥ f ∥∞≤βn E � ρτ � f(X)−Y � |X = x � . (5.38) 955.5. Deep Neural Network Architecture Approximations Related studies on the theoretical aspects of network achitecture include Farrell et al. (2021), Zeng (2021) and D˜ung et al. (2021). The approximation by deep ReLU neural networks of functions having a mixed smoothness is related to the high-dimensional sparse-grid approach which was introduced by Zenger for numerical solving partial differential equations (D˜ung et al. (2021)). Next, we consider the mathematical analysis for deriving error bounds of DNNs (see also Yarotsky (2017) and Yarotsky (2018)). 5.5.1. Error Bounds of DNNs Consider the following gs function with 2s−1 uniformly distributed "triangles". The key observation here is that the function f(x) = x2 can be approximated by linear combinations of the functions gs. Speciﬁcally, fm is a piece-wise linear interpolation of f with 2m + 1 uniformly distributed knots (breakpoints) k 2m , k ∈ {0,...,2m}, such that fn � k 2m � = � k 2m �2 , k ∈ {0,...,2m} (5.39) Thus, the function fm approximates f with the error εm = 2−2(m+1). Applying the linear interpolation method from fm−1 to fm amounts to adjusting it by a function proportional to a sawtooth function given by fm−1(x)− fm(x) = gm(x) 22m . (5.40) Hence, it holds that fm(x) = x− m ∑ s=1 gs(x) 22s . (5.41) Proposition 4. Given M > 0 and ε ∈ (0,1), there is a ReLU network η with two input units that imple- ments a function �× : R2 → R such that (a) for any inputs x,y, if |x| ≤ M and |y| ≤ M, then ���×(x,y)−xy �� ≤ ε (5.42) (b) if x = 0 or y = 0, then �×(x,y) = 0. Remark 32. Notice that the derivations of these error bounds relies on piecewise linear (or quadratic) approximations of the function fm. Some relevant discussion is given by Pottmann et al. (2000). 96Proof. Let �fsq,δ be the approximate squaring function such that �fsq,cδ(0) = 0 and ��� �fsq,δ(0)−x2��� < δ, for x ∈ [0,1]. (5.43) Assume without loss of generality that M ≥ 1 and set �×(x,y) = M2 8 � �fsq,δ �|x+y| 2M � − �fsq,δ � |x| 2M � − �fsq,δ � |y| 2M �� (5.44) where δ = 8ε 3M2. Theorem 17. For any d,n and ε ∈ (0,1), there is a ReLU network architecture that (i) is capable of expressing any function from Fd,n with error ε; (ii) has the depth at most c(ln(1/ε) + 1) and at most cε−d/n(ln(1/ε) + 1) wights and computation units, with some constant c = c(d,n). Remark 33. Notice that the main idea of the proof of the above theorem is the use of piecewise linear approximation of quadratic functions (see, Pottmann et al. (2000)). For example, one can consider the squared distance of the two vertices vi,vj in the Caley-Klein metric induced by Q which is deﬁned by d2� vi,vj � = � vi −v j �⊤Q � vi −v j � (5.45) In other words, according to their Theorem 12, an L∞−optimal piece-wise linear approximant over a triangulation of R2 to a quadratic bivariate function f whose quadratic form is indeﬁnite is deﬁned over a triangulation which is regular in the pseudo-Euclidean metric induced by f. Thus, the linear approximant interpolates the function values at the vertices of the triangulation. Discussion about the use of triangu- lation can be found in the studies of Montanelli and Du (2019). In particular, the curse of dimensionality can be lessened by establishing a connection with sparse grids. Proof. The ﬁrst key part of the proof is to approximate f by a sum-product combination f1 of local Taylor polynomials and one-dimensional piecewise-linear functions. Moreover, we can also approximate f1 using a neural network. Let N be a positive integer. Consider a partition of unity formed by a grid of (N +1)d functions φm on the domain [0,1]d such that ∑ m φm(x) ≡ 1, x ∈ [0,1]d. (5.46) 97Here, we have that m = (m1,...,md) ∈ {0,1,...,N}d, and the function φm is deﬁned as the product below φm(x) = d ∏ k=1 ψ � 3N � xk − mk N �� (5.47) where ψ(x) =        1, |x| < 1, 0, 2 < |x|, 2−|x|, 1 ≤ |x| ≤ 2. (5.48) Moreover, it holds that supp φm ⊂ � x : ���xk − mk N ��� < 1 N ∀ k � . (5.49) Then, for any m ∈ {0,...,N}d, consider the degree−(n−1) Taylor polynomial for the function f at x = m N Pm = ∑ n:|n|<n Dn f n! ���� x= m N � x− m N �n , (5.50) with the usual notation n! = d ∏ k=1 nk! and � x− m N �n = d ∏ k=1 � xk − mk N �nk . (5.51) Therefore, an approximation to f by f1 is given by f1 = ∑ m∈{0,...,N}d φmPm. (5.52) Therefore, for the proof we bound the approximation error using the Taylor expansion of f as | f(x)− f1(x)| = ����∑ m φm(x) � f(x)−Pm(x) ����� ≤ ∑ {m:|xk− mk N |< 1 N ∀ k} ��f(x)−Pm(x) �� ≤ 2d max {m:|xk− mk N |< 1 N ∀ k} ��f(x)−Pm(x) �� ≤ 2ddn n! � 1 N �n max n:|n|=n ess sup x∈[0,1]d |Dn f(x)| ≤ 2ddn n! � 1 N �n . 98We have that the coefﬁcients of the polynomials Pm are uniformly bounded for all f ∈ Fd,n : Pm(x) = ∑ n:|n|<n am,n � x− m N �n , |am,n| ≤ 1. (5.53) The main intuition of the above result is that we can construct a network architecture capable of approx- imating with uniform error ε 2 any function with a similar form as the function f1, assuming that N, has the form N = ⌊ � n! 2ddn ε 2 �− 1 n⌋ and the polynomials can be written as expression (5.53). Therefore, expanding f1 based on the above conditions we obtain f1(x) = ∑ m:{0,...,N}d ∑ n:|n|<n am,nφm(x) � x− m N �n . (5.54) Thus, we consider the approximation of the product φm(x) � x− m N �n. 5.5.2. Topological Space for DNN Architecture Assumption 10 (Farrell et al. (2021)). Assume that f∗ lies in the Holder ball W β,∞ � [−1,1]d� , with smoothness β ∈ N such that f∗(x) ∈ W β,∞� [−1,1]d� := � f : max α,|α|≤β ess sup x∈[−1,1]d |Dα f(x)| ≤ 1 � , (5.55) where α = (α1,....,αd), |α| = α1 +...+αd and Dα f, is the weak derivative. Then, we focus based on Assumptions 1 and 2 above, on deriving high-probability bounds. Assumption 11. Let f∗ lie in a class F. For the feedforward network class FDNN, let the approximation error εDNN be εDNN := sup f∗∈F inf f ∈FDNN ∥ f − f∗∥∞. (5.56) Remark 34. In other words, many recent studies focus on the aspects of how DNN solve the problem of the curse of dimensionality. In other words, various studies consider the approximation for compo- sitional functions by deep neural networks. Speciﬁcally, by assuming that the sparse structure in each composition layer, we can show that the total compositional function in high dimensional space owns a low dimensional property, in the sense that it can be approximated by DNN with a convergence rate only dependent on the intrinsic low dimension. 995.6. Statistical Inference with Stochastic Gradient Descent 5.6.1. Literature Review Notice that both for the estimation of Shallow as well as Deep Neural Networks, useful optimization al- gorithms under examination are the Gradient Descent and Stochastic Gradient Descent. Related studies that discuss the implementation and applications of these algorithms are presented by Chen et al. (2020). Furthermore, Toulis and Airoldi (2017) study the asymptotic theory analysis and ﬁnite-sample proper- ties of these estimators (see also Tran et al. (2015)). Moreover, a selective overview of deep learning estimation methodologies are discussed in the study of Fan et al. (2021a) and Braun et al. (2021). • The framework of Zhang and Simon (2022) consider a sieve SGD algorithm which is a nonpara- metric estimation approach in the Sobolev ellipsoid space. Moreover, Chen and Liao (2014) pro- pose a framework for Sieve M estimation on irregular parameters. • SGD procedures involve parameter updates that are implicitly deﬁned. Implicit updates shrink standard SGD updates. The amount of shrinkage depends on the observed Fisher information matrix, which does not need to be explicitly computed. • Theoretical analysis gives a full characterization of the asymptotic behaviour of both standard and implicit SGD-based estimators, including ﬁnite-sample error bounds. Moreover, Shao and Zhang (2022) present estimation error bounds of M-estimators and SGD algorithms. Following Chen et al. (2020), assume that we want to estimate the true parameter θ0 ∈ Rp of a distribution f from i.i.d data points (Xi,Yi) such that conditional on covariate Xi ∈ Rp outcome Yi ∈ Rd is distributed according to f(Yi;Xi,θ0). Such statistical problems reduce to optimization. More speciﬁcally, suppose that our aim is to estimate the true parameter θ0 ∈ Rp of a distribution f from i.i.d data points (Xi,Yi) such that conditional on covariate Xi ∈ Rp outcome Yi ∈ Rd is distributed according to f(Yi;Xi,θ0). Such statistical problems reduce to optimization. θsgd n = θsgd n−1 +γnCn∇log � f � Yn;Xn,θsgd n−1 �� (5.57) where γn is the learning rate sequence. Consider the equivalent moment condition below E � ∇log f(Y;X,θ0)|X � = 0, (5.58) where the expectation corresponds to the true conditional distribution of outcome Y given covariate X. 100Remark 35. From computational perspective, SGD is appealing because it avoids expensive matrix inversions and single data point (Xn,Yn) evaluations. Implicit SGD does not condition on the observed ordering of data points, but conditions on a random ordering instead. θim n = θim n−1 +γnCn∇log � f � Yn;Xn,θ im n �� , (5.59) Therefore, it holds that θim n = argmax θ � − 1 2γn ��θ −θim n−1 ��2 +log[f (Yn;Xn,θ)] � , (5.60) Recall the Fisher Information matrix is denoted ˆ In(θ) = −∇2ℓ(X′ nθ;Yn). Using a Taylor approximation of the gradient ∇log � f � Yn;Xn,θim n �� yields ∆θim n ≈ � I +γn ˆ In(θ0) �−1 ∆θsgd n , (5.61) where ∆θim n = θim n −θ0 and ∆θsgd n = θsgd n −θ0. Remark 36. Notice that these Gradient Descent Algorithms have a required ﬁrst step the initialization of the vector. Although the choice of this initial vector is just an approximation it provides a good starting point so that the algorithmic procedure converges to the optimal choice vector. 5.6.2. Preliminary Theory Throughout, we use ∥x∥p to denote the ℓp−norm of x, ∥X∥ the matrix operator norm of X and ∥X∥∞ = maxi, j|Xi, j| the elementwise ℓ∞−norm of X. For any sequences {an} and {bn} of positive numbers, we write with an ≥ bn and an ≤ bn. Consider the SGD method where the iteration is given by (see, Chen et al. (2021a)) xn = xn−1ηn∇F(xn−1)+ηnξn, (5.62) where ξn := ∇F(xn−1)−∇ f(xn−1,ζn). In particular, the above formulation decomposes the descent into two parts: ∇F(xn−1) which represents the direction of population gradient which is the major driving force behind the convergence of SGD and ξn is a martingale difference sequence under the above as- sumption. Furthermore, it holds that En−1 [ξn] := ∇F(xn−1)−En−1 � ∇ f(xn−1,ζn) � = 0. (5.63) 101Notice that En(.) denotes the conditional expectation En(.|Fn), where Fn is the σ−algebra generated by {ζ1,...,ζn}. Let ∆n := xn −x∗ be the error of the n−th iterate. An equivalent expression gives us ∆n = ∆n−1 −ηn∇F(xn−1)+ηnξn, (5.64) Given the SGD recursion and under suitable assumptions it can be shown that when the step size sequence ni = ηi−α, for i ∈ {1,...,n} with α ∈ (1/2,1), we have that √n.¯∆n ⇒ N � 0,A−1SA−1� (5.65) where ¯∆n = 1 n ∑n i=1 ∆i = ¯xn −x∗. 5.6.3. Assumptions and error bounds Assumption 12 (Strong convexity and Lipschitz continuity of the gradiet). Assume that the objective function F(x) is continuously differentiable and strongly convex with parameter µ > 0, that is, for any x1 and x2, F(x2) ≥ F(x1)+⟨∇F(x1),x2 −x1⟩+ µ 2 ∥x1 −x2∥2 2. (5.66) Further assume that ∇2F(x∗) exists, and ∇F(x) is Lipschitz continuous with a constant LF, that is for any x1 and x2 ∥∇F(x1)−∇F(x2)∥2 2 ≤ LF ∥x1 −x2∥2. (5.67) Remark 37. Notice that strong convexity of F(x) can be assumed to hold in order to derive the limiting distribution of averaged SGD. In fact, the strong convexity of F(x) implies that λmin(A) = λmin � ∇2F(x∗) � ≥ µ is an important condition for parameter estimation and inference. Assumption 13. The following conditions hold for the sequence ξn = ∇F(xn−1)−∇ f(xn−1,ζn): 1. Assume that f(x,ζ) is continuously differentiable in x for any ζ and ∥∇ f(x,ζ)∥2 is uniformly integrable for any x so that En−1ξn = 0. 2. The conditional covariance of ξn has an expansion around x = x∗ such that En−1 � ξnξ ⊤ n � = S +Σ(∆n−1) (5.68) 102Lemma 19. If there is a function H(ζ) with bounded fourth moment, such that the Hessian of f(x,ζ) is bounded by ��∇2 f(x,ζ) �� ≤ H(ζ) (5.69) for all x and ∇ f(x∗,ζ) have a bounded fourth moment. 5.6.4. Estimators for asymptotic covariance Consider two consistent estimators, the plug-in estimator and the batch-means estimator (see, Chen et al. (2021a)). Consider the sample estimate as below An := 1 n n ∑ i=1 ∇2 f(xi−1,ζi), Sn := 1 n n ∑ i=1 ∇ f(xi−1,ζi)∇ f(xi−1,ζi)⊤ (5.70) Consider the following nodewise Lasso approach �γ j = argmin γ j∈Rd−1 1 2n ��D., j −D.,−jγ j��2 2 +λ j ��γ j�� 1 , (5.71) where D., j is the j−th column of the design matrix D and D.,−j is the design submatrix without the j−th column (see also Guo et al. (2022)). Further, one can estimate Ω j, j by �τ j = 1 n � D., j −D.,−j�γ j�⊤D., j. (5.72) Remark 38. Notice that �γ j is the output of a stochastic gradient-based algorithm. 5.6.5. Averaged SGD Algorithms Consider the statistical problem of searching for the minimum point θ0 of a smooth function f(θ) where θ ∈ Θ ⊂ Rd. The stochastic GD method provides a direct way to solve the minimization problem. The algorithm is given as below. Let θ0 ∈ Rd be the initial value and for n ≥ 1, we update θn using θn = θn−1 −γn � ∇ f(θn−1)+ζn � (5.73) where θn = 1 n ∑n−1 i=0 θi. Remark 39. Notice that the convergence rate of E∥θn −θ∗∥2 and E �� ¯θn −θ∗��2 is examined in the literature as well as the normality of √n � ¯θn −θ∗� (see, Shao and Zhang (2022)). Assumption 14. There exists positive constants c2 and β such that ∀ θ with ∥θ −θ∗∥ ≤ β, ��∇2 f(θ)−∇2 f(θ∗) �� ≤ c2∥θ −θ∗∥. (5.74) 1035.7. Sieve Estimation Applications Consider the knowledge transmission through neural networks as a recursive process that focuses on the diffusion of processes. Similar to neurobiological processes in the brain neural networks can activate certain neuros in the brain for transmission of understanding. Stochastic approximations recursively approximate the zeros of an unknown function Ψ(θ), say θ∗, ˆθt+1 = ˆθt +atψ(zt, ˆθt), t = 1,2,... (5.75) where at is a "learning rate" tending to zero and ψ(Zt,θ) is a measurement of Ψ(θ) at time t, inﬂu- enced by random variables Zt. Furthermore, when Ψ(θ) ≡ E � ψ(Zt,θ) � this method yields a recursive implementation of the method of m-estimation of Huber (see, Kuan and White (1994)). The particular methodology can be used to estimate recursively the parameters of nonlinear regression models, such as those arising in neural network applications. Speciﬁcally, ofﬂine nonparametric estimation methods can usually be applied to the case of mixing processes using results for the methods of sieves. On the other hand, online nonparametric estimation methods require convergence to a global optimum of the underlying least squares problem, not just the local optimum. 5.7.1. Asymptotic Properties of Sieve Estimation Another application in which constraint optimization is used is when estimating and simulating neural networks with sieves. In particular, based on a construction of the neural network sieve estimators, in each sieve space Frn, there is a constraint on the ℓ1 norm for which ∑rn i=0|αi| ≤ Vn. Therefore, ﬁnding the nearly optimal function in Frn for Qn( f) is in fact a constrained optimization problem. A classical way to conduct this optimization is through introducing a Lagrange multiplier for each constraint. On the other hand, due to the difﬁculty in ﬁnding an explicit connection between the Lagrange multiplier and the upper bound in the inequality constraint we use instead the subgradient method (see, Shen et al. (2023)). The main idea is to update the parameter {α0,....,αrn} through α(k+1) i = α(k) i −δkg(k), i = 0,...,rn, (5.76) This allows us to investigate the asymptotic properties, including consistency, rate of convergence and asymptotic normality for neural network sieve estimators with one hidden layer. Using the Generalized Dominated Convergence Theorem, we have that E∗ � sup f ∈Fn ����� 1 n n ∑ i=1 εi � f(xi)− f0(xi) � ����� � ≤ 2EεEξ � sup f ∈Fn ����� 1 n n ∑ i=1 ξεi � f(xi)− f0(xi) � ����� � → 0. (5.77) 104Example 24. An application of sieve estimation in econometrics is presented in the study of Su and Hoshino (2016), where the authors propose a framework for sieve instrumental variable quantile regression esti- mation of functional cofﬁcient models. More precisely, one can demonstrate that √n � β 2τ −β2τ � d→ N � 0,τ(1−τ) lim K→∞S2ΩBτΨKΩ⊤ BτS⊤ 2 � . (5.78) Consequently, one can conduct statistical inference on β2τ as usual by estimating the AVC matrix given above. Alternatively, one can apply the boostrap method to obtain standard errors and make inference. Furthermore, we can construct a speciﬁcation test where we are interested in testing the null hypothesis H0 : δ1τ(Ui) ≡ Sδτ(Ui) = δ1τ. (5.79) almost surely for some parameter δ1τ ∈ Rr. Under H0, r of the (k1 +k2) functional coefﬁcients are con- stant, whereas under the alternative hypothesis H1, we have that at least one of the functional coefﬁcients in δ1τ(.) is not constant. Note that within the setting of nonstationary time series a relevant framework is presented by Dong et al. (2021). 5.7.2. Sieve Estimation for Panel Data A growing interest in the estimation of panel data models with cross-section dependence but most of the literature focuses on the linear speciﬁcation of the regression relationship. In particular, let yit, t = 1,...,n and t = 1,...,T be the i−th cross section unit at time t. We suppose that yit is generated according to the following semiparametric panel data generating process (see, Su and Jin (2012)) yit = gi(xit)+γ′ 1i f1t +eit, (5.80) where xit ∈ Xi ∈ Rd is a vector of observed individual-speciﬁc regressors on the i−th cross-section unit at time t. In this section, we focus on the sieve estimation of semiparametric panel data models with multi-factor error structure. We develop the asymptotic theory under fairly general conditions when both the cross-section and time-dimensions are large. For instance, if only homogeneous regression relationships are of interest, the time dimension need not pass to inﬁnity. Moreover one can consider testing the constancy of the nonparametric relationship over individuals in the presence of multi-factor error structure. In addition the individual speciﬁc regressors have the following structure xit = Γ′ 1i f1t +Γ′ 2i f2t +vit, (5.81) In practice, one may also be interested in estimating a restricted submodel of the following form yit = g(xit)+γ′ 1i f1t +eit. (5.82) 1055.7.3. Sieve M inference on irregular parameters We follow the framework proposed by Chen and Liao (2014) who consider plug-in sieve M estimators. More precisely, we assume that the data {Zi}n i=1 is a random sample from the distribution of Z deﬁned on an underlying complete probability space. Let L (.,.) : Z × A �→ R be a measurable function and E[L (Z,α)] be a population criterion. For simplicity we assume that there is a unique α0 ∈ (A ,dA) such that E[L (Z,α0)] > E[L (Z,α)] for all α ∈ (A ,dA) with dA(α,α0) > 0. Different models in economics correspond to different choices of the criterion function E[L (Z,α)] and the parameter space (A ,dA). A model does not need to be correctly speciﬁed and α0 could be pseudo true parameter. In this paper, we are interested in the estimation of and inference of a functional f (α0) via the method of sieves. Let An be a sieve space for the whole parameter space A . Then an approximate sieve M estimator �an ∈ An of α0 solves 1 n n ∑ i=1 L (Zi, �an) ≥ sup �an∈An 1 n n ∑ i=1 L (Zi,an)−op �1 n � (5.83) Example 25 (A partially additive quantile regression model, see Chen and Liao (2014)). Suppose that the i.i.d data � Yi,X′ i = � X′ 0i,X1i...,Xqi ��n i=1 is generated according to the process below: Y −i = X′ 0iθ0 + q ∑ j=1 hj,0 � Xj,i � +Ui, (5.84) with E � 1{Ui ≤ 0}|Xi � = τ ∈ (0,1), where dim(X0) = dθ, dim(Xj) = 1 for j = 1,...,q, dim(X) = dθ + q and dim(Y) = 1. Let α0 = (θ0,h0), where θ0 ∈ Θ and h0 = � h1,0,...,hq,0 � ∈ H . A functional of interest could be for instance, f (α0) = λ ′θ0 for any λ ∈ Rdθ with λ ̸= 0. This is an extension of the parametric quantile regression model of Koenker and Bassett Jr (1978) to allow for unknown additive functions ∑q j=1hj,0 � Xj,i � . We can estimate α0 = (θ0,h0) by the sieve QR estimator �αn = � �θn,�hn � that solves max θ∈Θ,h∈Hn n ∑ i=1 � 1 � Yi ≤ X′ 0,iθ + q ∑ j=1 hj,0 � Xj,i � � −τ � × � Yi −X′ 0,iθ − q ∑ j=1 hj,0 � Xj,i � � (5.85) Speciﬁcally, if A is a Holder, Sobolev or Besov space of functions with bounded supports and An is a linear sieve space consisting of spline, wavelet, or cosine bases, then one typically has dA (πn (α0),α0) = ∥πn(α0)−α0∥sup (5.86) Given the existing results on the convergence rates for sieve M estimators of semi-nonparametric models, we can restrict our attention to a shrinkage neighborhoud of α0. Let δA,n = δ ∗ A,nγn and δs,n = δ ∗ s,nγn, where γn is a positive sequence that diverges to inﬁnity very slowly (say log log n) such that δA,n = 0(1). 106In the example above, we have that L (Z,α) = � 1{Y ≤ α(Y)−τ} �� Y −α(Y) � (5.87) with α(Y) = X′ 0,iθ +∑q j=1hj,0 � Xj,i � . For any α ∈ A , we deﬁne a strong metric ∥.∥s as below ∥α −α0∥s = E ������X′ 0,i(θ −θ0)+ q ∑ j=1 � hj � Xj � −hj,0 � Xj,i �� ����� � (5.88) By the deﬁnitions of the metrics, we have that for any α ∈ A , (see, Chen and Liao (2014)) ∥α −α0∥2 = E � f(0|X)|α(X)−α0(X)|2 � (5.89) Let v∗ n = � v∗ θ,n,v∗ h,n � be the Riesz representer of the functional ∂ f (α0) ∂α [v] on Vn. Denote with v∗ n(X) = X′ 0,iv∗ θ,n + q ∑ j=1 v∗ hj,n � Xj � (5.90) Then, the variance of the plug-in sieve M estimator f (�αn) of f (α0) is expressed as below ∥v∗ n∥2 sd = τ(1−τ)E ���v∗ n(X) ��2 � = τ (1−τ)∥v∗ n∥2 s . (5.91) Proposition 5 (Chen and Liao (2014)). Under Assumptions and Conditions we have that √n f (�αn)− f (α0)   τ (1−τ)E   �����X′ 0v∗ θ,n + q ∑ j=1 v∗ hj,n � Xj � ����� 2     1/2 → N (0,1). (5.92) An application to speciﬁcation testing in econometrics using invariance principles on Sobolev spaces is proposed by Kuersteiner (2019). Furthermore, an application of general sieves for inference purposes in nonparametric time series regression models identiﬁed with conditional moment restrictions is studied by Chen et al. (2022). In particular, nonlinear sieve learning employs a more general functional class of sieves that can approximate nonlinear functions of high dimensional variables (such as in the case of an inﬁnite-dimensional parameter space). 1075.8. Further Statistical Algorithms in Economic Applications This section is motivated from the Seminar of Dennis Kristensen, Professor of Economics at the Depart- ment of Economics of University College London (UCL), who presented the paper with title: "Iterative estimation of structural models with an application to perturbed utility models", in May 2023 at the Department of Economics, University of Exeter Business School. Speciﬁcally, the particular steam of literature is based on the framework proposed by Hotz et al. (1994) and is relevant to estimators for dynamic models of discrete choice, that is, a general class of iterative estimators. Take for example, the classical likelihood estimation, which is a statistical methodology that iteratively solves the statistical problem until convergence to an estimate that is close to the true unknown parameter of interest. In the same spirit, without sacriﬁcing efﬁcient statistical estimation we assume that such a sequence of estimators converge to the full solution of the optimization problem. These optimization steps are summarized as below: 1. Obtain a nonparametric estimate of the solution as the global optimizer from a local estimator ˆθ. 2. Use ﬁrst order conditions to update the initial conditions estimate. 3. Use the updated solution path based on the estimated parameters of the previous steps. 4. Update solution using parameter estimates. 5. Repeat Steps 2-4 until convergence. Remark 40. Notice that the existence of some noise due to the parametric estimation is unavoidable. Therefore, the larger the presence of noise during the ﬁrst stage of estimation the more iterations will be needed for algorithmic convergence (uncertainty quantiﬁcation). Furthermore, the error bound of the true parameter vector based on the estimated parameters from the above algorithmic procedure is heavily based on the assumed estimator of the ﬁrst stage (large sample theory). Thus, the resulting sequence of iterative estimators is assumed to have asymptotic behaviour that is within the vicinity of the domain of attraction; although might not necessarily result in solving the structural model under examination. A different stream of literature considers doubly robust estimation methods which have been developed in various semiparametric problems, including partially linear models, instrumental variable analysis, and dimension reduction among others. Therefore, as a somewhat under-appreciated limit result, we point out that the familiar least-squares estimator for each individual coefﬁcient in linear regression is doubly robust in the context of a partially linear model. This result is also closely related to debiased Lasso estimation in high-dimensional linear regression (e.g., see Guo et al. (2022)). 108A Elements of Weak Convergence of Empirical Processes Example 26. (M-dependent sequences) Let Xn,n ∈ Z be a stationary sequence with EXn = 0, EX2 n < ∞. Assume that σ �� Xj, j ≤ 0 �� and σ �� Xj, j ≥ M �� are independent. A1. Sub-Gaussian processes Notice that Sub-Gaussian processes satisfy the increment bound ∥Xs −Xt∥ψ2 ≤ √ 6d(s,t). Therefore, the general maximal inequality leads for sub-Gaussian processes to a bound in terms of an entropy integral. Lemma 20. Let {Xt : t ∈ T} be a separable sub-Gaussian process. Then, for every δ > 0, E sup d(s,t)≤δ |Xs −Xt| ≤ K � δ 0 � logD(ε,d)dε, (A.1) for a constant K. In particular, for any t0, E sup t |Xt| ≤ E|Xt0|+K � ∞ 0 � logD(ε,d)dε. (A.2) Proof. Apply the general maximal inequality with ψ2(x) = ex2 −1 and η = δ. Since ψ−1 2 (m) = � log(1+m), we have that ψ−1 2 � D2(δ,d) � ≤ √ 2ψ−1 2 � D(δ,d) � . Thus, the second term in the maximal inequality can ﬁrst be replaced by √ 2δψ−1� D(δ,d) � , (at the cost of increasing the constant) gives ����� sup d(s,t)≤δ |Xs −Xt| ����� ψ2 ≤ K � δ 0 � log(1+D(ε,d))dε. (A.3) Symmetrization Let ε1,...,εn be i.i.d Rademacher random variables. Replace the empirical process f �→ (Pn −P) f = 1 n n ∑ i=1 � f(Xi)−P f � , (A.4) with the corresponding symmetrized empirical process deﬁned by f �→ Po n f = 1 n n ∑ i=1 εi f(Xi),where ε1,...,εn are independent of (X1,...,Xn). Both processes have mean function zero. Lemma 21. For every nondecreasing, convex Φ : R → R and class of measurable function F E∗Φ � ∥Pn −P∥F � ≤ E∗Φ � 2 ��P0 n �� F � (A.5) 109A2. Clivenko-Cantelli theorems In this section, we prove two types of Clivenko-Cantelli theorems. The ﬁrst theorem is the simplest and is based on entropy with bracketing. The second theorem, uses random L1−entropy numbers and is proved through symmetrization followed by a maximal inequality. Deﬁnition 12. (Covering numbers) The covering numbers N (ε,F,∥.∥) is the minimal number of balls {g : ∥g− f∥ < ε} of radius ε needed to cover the set F. The entropy (without bracketing) is the loga- rithm of the covering numbers. Deﬁnition 13. (bracketing numbers) Given two functions l and u, the bracket [l,u] is the set of all func- tions f with l ≤ f ≤ u. An ε−bracket is a bracket [l,u] with ∥u−l∥ < ε. Then, the bracketing number N[ ] (ε,F,∥.∥) is the minimum number of ε−brackets needed to cover F. The entropy with bracketing is the logarithm of the bracketing number. Theorem 18. Let F be a class of measurable functions such that N[ ] (ε,F,L1(P)) < ∞ for every ε > 0. Then, F is Glivenko-Cantelli. Proof. Fix ε > 0. Choose ﬁnitely many ε−brackets [ℓi,ui] whose union contains F and such that P(ui − ℓi) < ε for every i. Then, for every f ∈ F, there is a bracket such that (Pn −P) f ≤ (Pn −P)ui +P(ui − f) ≤ (Pn −P)ui +ε (A.6) Consequently, sup f ∈F (Pn −P) ≤ max i (Pn −P)ui +ε. (A.7) The right side converges almost surely to ε by the strong law of large numbers for real variables. Theorem 19. Let F be a P−measurable class of measurable functions with envelope F such that P∗F < ∞. Let FM be the class of functions f1{F ≤ M} when f ranges over F. If logN[ ] (ε,F,L1(Pn)) = o∗ p(n) for every ε and M > 0, then ∥Pn −P∥∗ F → 0 both almost surely and in mean. In particular, F is GC. Proof. By the symmetrization lemma, measurability of the class F, and Fubini’s theorem E∗ ∥Pn −P∥F ≤ 2EXEε ����� 1 n n ∑ i=1 εi f(Xi) ����� F ≤ 2EXEε ����� 1 n n ∑ i=1 εi f(Xi) ����� FM +2P∗F {F > M} by the triangle inequality, for every M > 0. 110Thus, for sufﬁciently large M, the last term is arbitrarily small. To prove convergence in mean, it sufﬁces to show that the ﬁrst term converges to zero for ﬁxed M. Fix X1,...,Xn. If G is an ε−net in L1(Pn) over FM, then the following inequality holds Eε ����� 1 n n ∑ i=1 εi f(Xi) ����� FM ≤ Eε ����� 1 n n ∑ i=1 εi f(Xi) ����� G +ε. (A.8) The cardinality of G can be chosen equal to N(ε,FM,L1(Pn)). Bound the L1−norm on the right using the Orlicz-norm for ψ2(x) = exp(x2)−1, and using the maximal inequality to ﬁnd that the last expression does not exceed a multiple of � 1+logN(ε,FM,L1(Pn)) sup f ∈G ����� 1 n n ∑ i=1 εi f(Xi) ����� ψ2|X +ε, (A.9) where the Orlicz-norm ∥.∥ψ2|X are taken over ε1,...,εn with X1,...,Xn ﬁxed. By Hoeddding’s inequality, then can be bounded by � 6/n � Pn f 2�1/2, which is less than � 6/nM. A3. Donsker Theorems Uniform Entropy: We establish the weak convergence of the empirical process under the condition that the envelope function F be square integrable, combined with the uniform entropy bound � ∞ 0 � logN � ε,FQ,2,L2(Q) � dε < ∞. (A.10) Theorem 20. Let F be a class of measurable functions that satisﬁes the uniform entropy bound. Let the class Fδ = � f −g : f,g,∈ F,∥ f −g∥P,2 < δ � and F 2 ∞ be P−measurable for every δ > 0. If P∗F2 < ∞, then F is P−Donsker. Proof. Let δn → 0 be a ﬁxed constant. Using Markov’s inequality and the symmetrization lemma: P∗ � ∥Gn∥Fδn > x � ≤ 2 xE∗ ����� 1 √n n ∑ i=1 εi f(Xi) ����� Fδn . (A.11) Therefore, we can see that the inner expectation is bounded as below Eε ����� 1 √n n ∑ i=1 εi f(Xi) ����� Fδn ≤ � ∞ 0 � logN � ε,Fδn,L2(Pn) � dε. (A.12) Notice that for large values of ε, the set Fδn ﬁts in a single ball of radius ε around the origin, in which case the integrand is zero.Furthermore, we have that the covering numbers of the class Fδn are bounded by covering numbers of F∞ = {f −g : f,g ∈ F} (see, Vaart and Wellner (2013)). 111B Elements of Stochastic Processes B1. Asymptotic Equicontinuity A class of measurable functions is called pre-Gaussian if the (tight) limit process G in the uniform central limit theorem exists. We focus on Brownian bridge processes. Firstly, it is desirable to have a more concrete description of the tightness property of a Brownian bridge and hence of the motion of pre-Gaussianity. More speciﬁcally, tightness of a random map into ℓ∞(F) is closely connected to continuity of its sample paths. A Donsker class F satisﬁes a stronger condition that the sequence Gn is asymptotically tight. Therefore, this entails replacing the condition that the sample paths of the limit process are continuous by the condition that the empirical process is asymptotically continuous. Thus, for every ε > 0 it holds that lim δ→0 lim sup n→∞ P∗ � sup ρp(f −g)<δ ��Gn( f −g) �� > ε � = 0. (B.1) Related discussion can be found in Newey (1991) and Hagemann (2014) among others. Maximal Inequalities Therefore, it follows that the law of large numbers and the central limit theorem are concerned with showing that the supremum of real-valued variables are converges to zero. Thus, to show these results we need to make use of maximal inequalities that bound probabilities involving suprema of random variables. Notice that bounds on ﬁnite suprema can be extended to general maximal inequalities with the help of the chaining method. CLT in Banach Spaces In this particular section it is shown that any CLT in a Banach space can be stated in terms of empirical processes. Furthermore, a class of maximal inequalities can be used to establish the asymptotic equicontinuity of the empirical process. Theorem 21. Let ψ be convex, nondecreasing, nonzero function with ψ(0) = 0 and lim sup x,y→∞ ψ(x)ψ(y)/ψ(cxy) < ∞, for some constant c. Let � Xt : t ∈ T � be a separable stochastic process with ∥Xs −Xt∥ψ ≤ Cd(s,t), for every s,t, (B.2) for some semi-metric d on T and a constant C. Then for any η,δ > 0, ����� sup d(s,t)≤δ ��Xs −Xt �� ����� ψ ≤ K �� η 0 ψ−1� D(ε,d) � dε +δψ−1� D2(ε,d) �� , (B.3) for a constant K depending on ψ and C only. 112Corollary 9. The constant K can be chosen such that ∥Xs −Xt∥ψ ≤ K � diamT 0 ψ−1� D(ε,d) � dε, (B.4) where diamT is the diameter of T. In practise, maximal inequality means that no point can be added without destroying the validity of the inequality. Furthermore, a stochastic process is called sub-Gaussian with respect to the semimetric d on its index set if P � |Xs −Xt| � ≤ 2e− 1 2x2/d2(s,t), for every s,t ∈ T,x > 0. (B.5) Another example, is the Rademacher process given by Xα = n ∑ i=1 αiεi, α ∈ Rn, (B.6) for Rademacher variables ε1,...,εn. Therefore, by Hoeffding’s inequality, this is sub-Gaussian for the Euclidean distance d(a,b) = ∥a−b∥. Tightness under an Increment Bound We focus on deriving a general CLT for empirical processes through the application of maximal inequalities. Example 27. Let � Xn(t) :t ∈ [0,1] � be a sequence of separable stochastic processes with bounded sample paths and increments satisfying the following condition E ��Xn(s)−Xn(t) ��p ≤ K |s−t|1+r , (B.7) for constants p,K,r > 0 independent of n. Assume that the sequences of martingales � Xn(t1),...,Xn(tk) � converge weakly to the corresponding marginals of a stochastic process � X(t) : t ∈ [0,1] � . Then, there exists a version of X with continuous sample paths and Xn ⇒ X in ℓ∞[0,1]. Hence, also in D[0,1] or C[0,1], provided every Xn has all its sample paths in these spaces. 113B2. LLNs for Hilbert Space-Valued Mixingales Example 28. (Regression via Orthonormal Bases) Let {Xt} and {Yt} be real-valued random sequences (see, also Hu (2004)): E[Yt|Xt = x] := θ0(x) (B.8) Deﬁne εt ≡ Yt −θ0(Xt), and suppose E � ε2 t |Xt � = ρ2 t ≤ σ2 < ∞. Suppose that, for all t, Xt has the same marginal density f with bounded support and 0 < inf f(x) ≤ sup f(x) < ∞ where M0(x) ≡ θ0(x) and f(x) belong to an inﬁnite-dimensional separable Hilbert space H with inner product induced norm | . |. Let � gj � be an orthonormal basis for H and {Jn} a nondecreasing integer sequence. Estimate M0 as ˆMn(x) = ∑ 1≤j≤Jn � n−1 n ∑ t=1 Ytgj(Xt) � gj(x). (B.9) For known f, estimate θ0 by ˆθn = ˆMn/ f. For unknown f, estimate θ0 by ˆθn = ˆMn/ ˆfn, ˆfn ≡ ∑ 1≤j≤Jn � n−1 n ∑ t=1 gj(Xt) � gj(x). (B.10) Furthermore, if {Xt} and {Yt} are R−valued near-epoch dependent (NED) functions of some mixing ran- dom sequences, � ˆMn −E ˆMn � and � ˆfn −E ˆfn � , become weighted sums of Hilbert space-valued mixingale arrays. Our results, in the next section can establish the convergence of E � | ˆθn −θ0| � . For example, when H = L2(R), we obtain that � R � ˆθn(x)−θ0(x) �2 dx → 0, in probability. Theorem 22. Let � Wn,i,F n,i� be an Lp(H) mixingale with p ≥ 1. (i) If p ≥ 2, then there exists a doubly inﬁnite summable sequence of positive constants deﬁned with a ≡ {am : −∞ < m < ∞} where am = a−m such that E  max j≤k ����� j ∑ i=1 Wn,i ����� 2  ≤ K(ψ,a) k ∑ i=1 c2 n,i, (B.11) K(ψ,a) = 4 � ∑ −∞<m<∞ am �� a−1 0 � ψ2 0 +ψ2 1 � +2 ∞ ∑ m=1 ψ2 m|a−1 m −a−1 m−1| � (B.12) (ii) If 1 < p ≤ 2, then there exists a constant Cp > 0 depending only on p such that �����max j≤k ����� j ∑ i=1 Wn,i ����� ����� p ≤ Cp ∑ −∞<m<∞ � k ∑ i=1 E ���W m n,i ��p� �1/p ≤ 2Cp ∞ ∑ m=1 ψm k ∑ i=1 � cp n,i �1/p , where W m n,i ≡ E � Wn,i|F n,i−m� −E � Wn,i|F n,i−m−1� . 114C Elements of Bayesian Statistics "...the simple idea of splitting a sample in two and then developing the hypothesis on the basis of one part and testing it on the remainder may perhaps be said to be one of the most seriously neglected ideas in statistics...". The applications of the Bayesian framework in economics, econometrics and statistics are widely spread and it is worth mentioning some key elements here, especially due to the fact that model selection is commonly presented using the Bayesian approach. Generally, statistical problems are concerned for the model f(x|θ), such that θ ∈ Θ with the associated hypothesis testing of interest is formulated as below: H0 : θ = θ0 vs θ ̸= θ0, (C.1) where the data X = � X1,...,Xn � i.i.d ∼ f(X|θ). Denote the prior probability function with π(θ), then one can construct the observed posterior π(θ|X), assumed to be a proper density function even if the prior is im- proper (see, ). Next, one needs to consider what we compare π(θ|X) with. Assume that we can generated data Y = � Y1,...,Yn � i.i.d ∼ f(Y|θ0) under the null hypothesis. Using the same prior we can construct π(θ|Y) and hence we can deﬁne the expected posterior under the null; π0(θ) = � π(θ|y) f(y|θ0)dy, where y is an n−vector. Then, the test statistic of interest is the KL divergence between the expected posterior under the null and the observed posterior such that T(X) = � π0(θ)log � π0(θ) π(θ|X) � dθ. (C.2) Therefore, it can be shown that T(X) is related to the Bayes factor. Example 29. Consider the probability distributions P(x|H0) and P(x|H1). Moreover, assume some prior knowledge on prior probabilities P(H0) and P(H1). Then, Bayes theorem combines the prior probabilities and the data to produce posterior probabilities P(H0|x) and P(H1|x). Therefore, the transformation of prior to posterior itself represents evidence provided by the data, which takes the following form P(Hk|x) = P(x|Hk)P(Hk) P(x|H0)P(H0)+P(x|H1)P(H1) (C.3) In other words, the Bayes factor represents the the likelihood ratio. When there is an unknown param- eter θk, corresponding to hypothesis Hk, the Bayes factor is the marginal likelihood ratio, where the marginal likelihood densities mk(x) are obtained by integrating over the parameter space with respect to the speciﬁed prior π(θk|Hk), such that mk(x) = � fk(x|θk)πk(θk)dθk, for k ∈ {0,1}. (C.4) However, there are difﬁculties with the Bayes factor when prior information about the unknown parame- ters of the models is weak, in particular, with the use of improper priors Chen and Walker (2021). 115Below, we present some popular econometric models. Some relevant literature that studies Bayesian asymptotics within the time series econometric context include among others Kim (1994). Example 30. Consider the autoregressive model yt = ρyt−1 +εt (C.5) Without imposing the normality assumption on the innovation sequences, we can consider the asymptotic posterior distribution. A posterior density is deﬁned as below (see, e.g., Kim (1994)) PT(g,YT(ω)) = � G π(θ|YT(ω))dθ (C.6) The Bayesian posterior is determined by the prior and the likelihood function. Therefore, in order to verify the asymptotic normality of the posterior distribution, then we need to determine the limiting behaviour of each of the components comprising the prior and the posterior distributions. Additional useful reading within the context of model selection and empirical bayes include among others Varin et al. (2011)14, Efron (2014) as well as Jewson and Rossell (2022)15. Moreover, a framework for Bayesian Inference in Econometric Models using Monte Carlo Integration is proposed by Geweke (1989). Example 31. Consider the structural regression model given by Yt = α +βXt−1 +Ut (C.7) Xt = µ +ρXt−1 +Vt (C.8) where (Ut,Vt)⊤ is a sequence of independent and identically distributed random vectors with means zeros and ﬁnite variances. As it is well documented in the literature the least squares estimator for β based on the predictive regression is biased in ﬁnite sample behaviour due to the correlation structure between the innovation sequences of the equations of the system Ut and Vt. Hence, several bias-corrected estimators and tests for both stationary (|ρ| < 1) and nonstationary or nearly nonstationary, ρ = � 1− c n � have been proposed in the literature. In other words, inference on the parameter β of the model is challenging due to the fact that the asymptotic distribution of an estimators or a test statistic depends heavily on whether Xt is stationary or nearly integrated or unit root, and whether the model intercept φ, of the nonstationary autoregressive equation is zero or not. Therefore, is of paramount importance to have a uniﬁed inference approach to avoid making a mistake in characterizing the predicting variable. The persistence endogeneity of covariates in predictive regressions has been studied by several authors. 14Professor Cristiano Varin gave a seminar with title "An approximate empirical Bayes approach to paired comparisons", at the S3RI Departmental Seminar Series at the University of Southampton on the 28th of April 2022. 15Professor David Rossell gave a seminar with title "Improper models for data analysis", at the S3RI Departmental Seminar Series at the University of Southampton on the 10th of February 2022. 116Therefore an estimation methodology is to employ the empirical likelihood approach which is deﬁned: Ln(β) = sup � n ∏ t=1 (npt) : p1 ≥ 0,..., pn ≥ 0, n ∑ t=1 pt = 1, n ∑ t=1 ptZt(β) � (C.9) where Zt(β) = � Yt − βXt−1 � Xt−1 �� 1+X2 t−1. Then, based on the Lagrange multiplier optimization method it follows that ℓn(β) := −2logLn(β) = 2 n ∑ t=1 log � 1+λZt(β) � (C.10) Then, taking the ﬁrst-order-condition for some λ = λ(β) satisﬁes n ∑ t=1 Zt(β) 1+λZt(β) ≡ 0. (C.11) Remark 41. The ﬁrst rigorous work to deﬁne and construct tests which are asymptotically optimal was Wald (1943). He argued that maximum likelihood estimators may be asymptotically sufﬁcient for de- tecting local deviations from the null hypothesis and showed that the Wald test - is asymptotically most stringent: the asymptotic power function is closest to the asymptotic envelope power function in the minimax sense in local neighborhoods of the null hypothesis. In particular, Wald tests can be used for more general problems if we can ﬁnd asymptotically efﬁcient estimates for the parameters of interest - an estimator ˆϑn with √n � ˆϑn −ϑn � asymptotically N � 0,B∗−1� under every � ϑn(hϑ),ηn(hn) � . Moreover, the notion of asymptotically uniformly most powerful (AUMP), AUMPU (unbiased) and AUMPI (invariant) tests are useful in semiparametric econometrics. In particular, characterization is done by stating the asymptotic local power function. Furthermore, sufﬁcient for optimality is that a test be equivalent to a canonical effective score test - an optimal test requiring knowledge of nuisance pa- rameters. In addition, Stein’s notion of adaptation implies replacing, in tests which are optimal when certain nuisance parameters are known, these parameters by estimates without affecting the asymptotic performance of the test. The particular aspect is the equivalent of large-sample studentization, a variance parameter can be replaced by an estimate without large-sample penalty. Note that a statistical problem is invariant under locally linear transformation of the parameter of interest, iff standardized effective score tests are rotation invariant (asymptotically). Furthermore, another large stream of literature in econometrics focuses on estimation and inference meth- ods for conditional moment models which can be extended within a high-dimensional environment using techniques based on large sample approximations (see, Ai and Chen (2003) and Domínguez and Lobato (2004) among others). A Bayesian perspective is presented by Chib et al. (2022). 117References Abadir, K. M., Hadri, K., and Tzavalis, E. (1999). The inﬂuence of var dimensions on estimator biases. Econometrica, 67(1):163–181. Adamek, R., Smeekes, S., and Wilms, I. (2023). Lasso inference for high-dimensional time series. Journal of Econometrics, 235(2):1114–1143. Ai, C. and Chen, X. (2003). Efﬁcient estimation of models with conditional moment restrictions contain- ing unknown functions. Econometrica, 71(6):1795–1843. Amann, N. and Schneider, U. (2018). Uniform asymptotics and conﬁdence regions based on the adaptive lasso with partially consistent tuning. Econometric Theory, pages 1–26. Angelini, E., Di Tollo, G., and Roli, A. (2008). A neural network approach for credit risk evaluation. The quarterly review of economics and ﬁnance, 48(4):733–755. Aschard, H., Guillemot, V., Vilhjalmsson, B., Patel, C. J., Skurnik, D., Ye, C. J., Wolpin, B., Kraft, P., and Zaitlen, N. (2017). Covariate selection for association screening in multiphenotype genetic studies. Nature genetics, 49(12):1789–1795. Ashby, W. R. (1957). An introduction to cybernetics. Babii, A., Ball, R. T., Ghysels, E., and Striaukas, J. (2022). Machine learning panel data regressions with heavy-tailed dependent data: Theory and application. Journal of Econometrics. Baek, C., Düker, M.-C., and Pipiras, V. (2021). Local whittle estimation of high-dimensional long-run variance and precision matrices. arXiv preprint arXiv:2105.13342. Baillie, R. T., Diebold, F. X., Kapetanios, G., and Kim, K. H. (2022). On robust inference in time series regression. arXiv preprint arXiv:2203.04080. Barut, E., Fan, J., and Verhasselt, A. (2016). Conditional sure independence screening. Journal of the American Statistical Association, 111(515):1266–1277. Basu, S. and Michailidis, G. (2015). Regularized estimation in sparse high-dimensional time series models. Annals of statistics, 43(4):1535–1567. Basu, S. and Rao, S. S. (2021). Graphical models for nonstationary time series. arXiv preprint arXiv:2109.08709. Battey, H., Fan, J., Liu, H., Lu, J., and Zhu, Z. (2018). Distributed testing and estimation under sparse high dimensional models. Annals of statistics, 46(3):1352. Beder, J. H. (1987). A sieve estimator for the mean of a gaussian process. The Annals of Statistics, 15(1):59–78. Belloni, A. and Chernozhukov, V. (2011). Penalized quantile regression in high-dimensional sparse models. Annals of Statistics, 39(1):82–130. Belloni, A., Chernozhukov, V., and Hansen, C. (2014). Inference on treatment effects after selection among high-dimensional controls. The Review of Economic Studies, 81(2):608–650. Belloni, A., Chernozhukov, V., and Wei, Y. (2016). Post-selection inference for generalized linear models with many controls. Journal of Business & Economic Statistics, 34(4):606–619. Benko, M., Hardle, W., and Kneip, A. (2009). Common functional principal components. The Annals of Statistics, 37(1):1–34. 118Berger, J. O. and Mortera, J. (1999). Default bayes factors for nonnested hypothesis testing. Journal of the american statistical association, 94(446):542–554. Berger, J. O. and Pericchi, L. R. (1996). The intrinsic bayes factor for model selection and prediction. Journal of the American Statistical Association, 91(433):109–122. Bhatore, S., Mohan, L., and Reddy, Y. R. (2020). Machine learning techniques for credit risk evaluation: a systematic literature review. Journal of Banking and Financial Technology, 4:111–138. Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, 37(4):1705–1732. Bing, X., Bunea, F., and Wegkamp, M. (2022). Inference in latent factor regression with clusterable features. Bernoulli, 28(2):997–1020. Boot, T. and Nibbering, D. (2019). Forecasting using random subspace methods. Journal of Economet- rics, 209(2):391–406. Braun, A., Kohler, M., Langer, S., and Walk, H. (2021). The smoking gun: Statistical theory improves neural network estimates. arXiv preprint arXiv:2107.09550. Braun, A., Kohler, M., and Walk, H. (2019). On the rate of convergence of a neural network regression estimate learned by gradient descent. arXiv preprint arXiv:1912.03921. Breheny, P. and Huang, J. (2015). Group descent algorithms for nonconvex penalized linear and logistic regression models with grouped predictors. Statistics and computing, 25:173–187. Breitung, J. and Eickmeier, S. (2011). Testing for structural breaks in dynamic factor models. Journal of Econometrics, 163(1):71–84. Cai, T. T. and Jiang, T. (2011). Limiting laws of coherence of random matrices with applications to testing covariance structure and construction of compressed sensing matrices. The Annals of Statistics, 39(3):1496–1525. Callot, L., Caner, M., Önder, A. Ö., and Ula¸san, E. (2021). A nodewise regression approach to estimating large portfolios. Journal of Business & Economic Statistics, 39(2):520–531. Campbell, F. and Allen, G. I. (2017). Within group variable selection through the exclusive lasso. Elec- tronic Journal of Statistics, 11(2):4220–4257. Candes, E. and Tao, T. (2007). The dantzig selector: Statistical estimation when p is much larger than n. The Annals of Statistics, 35(6):2313–2351. Caner, M. and Daniele, M. (2023). Deep learning based residuals in non-linear factor models: Precision matrix estimation of returns with low signal-to-noise ratio. arXiv preprint arXiv:2209.04512. Caner, M. and Knight, K. (2013). An alternative to unit root tests: Bridge estimators differentiate between nonstationary versus stationary models and select optimal lag. Journal of Statistical Planning and Inference, 143(4):691–715. Caner, M., Medeiros, M., and Vasconcelos, G. F. (2023). Sharpe ratio analysis in high dimensions: Residual-based nodewise regression in factor models. Journal of Econometrics, 235(2):393–417. Cattaneo, M. D., Farrell, M. H., and Feng, Y. (2020). Large sample properties of partitioning-based series estimators. Annals of Statistics, 48(3):1718–1741. Cattaneo, M. D., Jansson, M., and Newey, W. K. (2018). Inference in linear regression models with many covariates and heteroscedasticity. Journal of the American Statistical Association, 113(523):1350– 1191361. Chan, N. H., Yau, C. Y., and Zhang, R.-M. (2014). Group lasso for structural break time series. Journal of the American Statistical Association, 109(506):590–599. Chang, J., Chen, X., and Wu, M. (2021). Central limit theorems for high dimensional dependent data. arXiv preprint arXiv:2104.12929. Chatterjee, A., Gupta, S., and Lahiri, S. (2015). On the residual empirical process based on the alasso in high dimensions and its functional oracle property. Journal of Econometrics, 186(2):317–324. Chen, H., Lu, W., and Song, R. (2021a). Statistical inference for online decision making via stochastic gradient descent. Journal of the American Statistical Association, 116(534):708–719. Chen, L., Pelger, M., and Zhu, J. (2023). Deep learning in asset pricing. Management Science. Chen, S. and Walker, S. G. (2021). A new statistic for bayesian hypothesis testing. Econometrics and Statistics. Chen, X. (2007). Large sample sieve estimation of semi-nonparametric models. Handbook of economet- rics, 6:5549–5632. Chen, X., Lee, J. D., Tong, X. T., and Zhang, Y. (2020). Statistical inference for model parameters in stochastic gradient descent. The Annals of Statistics, 48(1):251–273. Chen, X., Liao, Y., and Wang, W. (2022). Inference on time series nonparametric conditional moment restrictions using general sieves. arXiv preprint arXiv:2301.00092. Chen, X. and Liao, Z. (2014). Sieve m inference on irregular parameters. Journal of Econometrics, 182(1):70–86. Chen, X., Xu, M., and Wu, W. B. (2013). Covariance and precision matrix estimation for high- dimensional time series. The Annals of Statistics, 41(6):2994–3021. Chen, Y., Cheng, C., and Fan, J. (2021b). Asymmetry helps: Eigenvalue and eigenvector analyses of asymmetrically perturbed low-rank matrices. Annals of statistics, 49(1):435. Cheng, X. and Hansen, B. E. (2015). Forecasting with factor-augmented regression: A frequentist model averaging approach. Journal of Econometrics, 186(2):280–293. Cheng, X., Liao, Z., and Schorfheide, F. (2016). Shrinkage estimation of high-dimensional factor models with structural instabilities. The Review of Economic Studies, 83(4):1511–1543. Chernozhukov, V., Chetverikov, D., and Kato, K. (2014). Gaussian approximation of suprema of empiri- cal processes. The Annals of Statistics, 42(4):1564–1597. Chernozhukov, V., Hansen, C., and Spindler, M. (2015). Post-selection and post-regularization inference in linear models with many controls and instruments. American Economic Review, 105(5):486–490. Chetverikov, D., Liao, Z., and Chernozhukov, V. (2021). On cross-validated lasso in high dimensions. The Annals of Statistics, 49(3):1300–1317. Chib, S., Shin, M., and Simoni, A. (2022). Bayesian estimation and comparison of conditional moment models. Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(3):740–764. Chinco, A., Clark-Joseph, A. D., and Ye, M. (2019). Sparse signals in the cross-section of returns. The Journal of Finance, 74(1):449–492. Cho, H., Maeng, H., Eckley, I. A., and Fearnhead, P. (2023). High-dimensional time series segmentation via factor-adjusted vector autoregressive modelling. Journal of the American Statistical Association, 120(just-accepted):1–28. Chronopoulos, I. C., Raftapostolos, A., and Kapetanios, G. (2023). Forecasting value-at-risk using deep neural network quantile regression. Journal of Financial Econometrics. Chudik, A., Kapetanios, G., and Pesaran, M. H. (2018). A one covariate at a time, multiple testing ap- proach to variable selection in high-dimensional linear regression models. Econometrica, 86(4):1479– 1512. Coroneo, L. and Iacone, F. (2020). Comparing predictive accuracy in small samples using ﬁxed- smoothing asymptotics. Journal of Applied Econometrics, 35(4):391–409. Cragg, J. G. and Donald, S. G. (1997). Inferring the rank of a matrix. Journal of econometrics, 76(1- 2):223–250. Davidson, J. (2002). Establishing conditions for the functional central limit theorem in nonlinear and semiparametric time series processes. Journal of Econometrics, 106(2):243–269. Dhrymes, P. J. (2013). Mathematics for econometrics. Springer. Domínguez, M. A. and Lobato, I. N. (2004). Consistent estimation of models deﬁned by conditional moment restrictions. Econometrica, 72(5):1601–1615. Dong, C., Linton, O., and Peng, B. (2021). A weighted sieve estimator for nonparametric time series models with nonstationary variables. Journal of Econometrics, 222(2):909–932. Doukhan, P., Massart, P., and Rio, E. (1995). Invariance principles for absolutely regular empirical processes. In Annales de le IHP Probabilites et statistiques, volume 31, pages 393–427. D˜ung, D. et al. (2021). Deep relu neural networks in high-dimensional approximation. Neural Networks, 142:619–635. Efron, B. (2014). Two modeling strategies for empirical bayes estimation. Statistical science: a review journal of the Institute of Mathematical Statistics, 29(2):285. Engelke, S. and Hitz, A. S. (2020). Graphical models for extremes. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 82(4):871–932. Fan, J., Feng, Y., and Xia, L. (2020). A projection-based conditional dependence measure with applica- tions to high-dimensional undirected graphical models. Journal of Econometrics, 218(1):119–139. Fan, J., Ke, Z. T., Liao, Y., and Neuhierl, A. (2022). Structural deep learning in conditional asset pricing. Available at SSRN 4117882. Fan, J., Li, Q., and Wang, Y. (2017). Estimation of high dimensional mean regression in the absence of symmetry and light tail assumptions. Journal of the Royal Statistical Society Series B: Statistical Methodology, 79(1):247–265. Fan, J. and Li, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American statistical Association, 96(456):1348–1360. Fan, J., Liao, Y., and Yao, J. (2015). Power enhancement in high-dimensional cross-sectional tests. Econometrica, 83(4):1497–1541. Fan, J. and Lv, J. (2008). Sure independence screening for ultrahigh dimensional feature space. Journal of the Royal Statistical Society Series B: Statistical Methodology, 70(5):849–911. Fan, J., Ma, C., and Zhong, Y. (2021a). A selective overview of deep learning. Statistical science: a review journal of the Institute of Mathematical Statistics, 36(2):264. 121Fan, J., Masini, R., and Medeiros, M. C. (2021b). Bridging factor and sparse models. arXiv preprint arXiv:2102.11341. Fan, R., Lee, J. H., and Shin, Y. (2023). Predictive quantile regression with mixed roots and increasing dimensions: The alqr approach. Journal of Econometrics. Farrell, M. H. (2015). Robust inference on average treatment effects with possibly more covariates than observations. Journal of Econometrics, 189(1):1–23. Farrell, M. H., Liang, T., and Misra, S. (2021). Deep neural networks for estimation and inference. Econometrica, 89(1):181–213. Feng, G., Giglio, S., and Xiu, D. (2020). Taming the factor zoo: A test of new factors. The Journal of Finance, 75(3):1327–1370. Feng, G., He, J., Polson, N. G., and Xu, J. (2018). Deep learning in characteristics-sorted factor models. arXiv preprint arXiv:1805.01104. Finner, H. (1992). A generalization of holder’s inequality and some probability inequalities. The Annals of probability, pages 1893–1901. Finner, H. and Strassburger, K. (2002). The partitioning principle: a powerful tool in multiple decision theory. Annals of statistics, pages 1194–1213. Fithian, W., Taylor, J., Tibshirani, R., and Tibshirani, R. (2015). Selective sequential model selection. arXiv preprint arXiv:1512.02565. García-Donato, G. and Paulo, R. (2022). Variable selection in the presence of factors: a model selection perspective. Journal of the American Statistical Association, 117(540):1847–1857. Geweke, J. (1989). Bayesian inference in econometric models using monte carlo integration. Economet- rica: Journal of the Econometric Society, pages 1317–1339. Giurcanu, M. (2016). Thresholding least-squares inference in high-dimensional regression models. Elec- tronic Journal of Statistics, 10(2):2124–2156. Goeman, J. J. and Solari, A. (2010). The sequential rejection principle of familywise error control. The Annals of Statistics, pages 3782–3810. Gonzalo, J. and Pitarakis, J.-Y. (2023). Out of sample predictability in predictive regressions with many predictor candidates. arXiv preprint arXiv:2302.02866. Goodfellow, I., Bengio, Y., and Courville, A. (2016). Deep learning. MIT press. Guijarro-Ordonez, J., Pelger, M., and Zanotti, G. (2021). Deep learning statistical arbitrage. arXiv preprint arXiv:2106.04028. Guillaumin, A. P., Sykulski, A. M., Olhede, S. C., and Simons, F. J. (2022). The debiased spatial whittle likelihood. Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(4):1526– 1557. Guo, Z., Cevid, D., and Buhlmann, P. (2022). Doubly debiased lasso: High-dimensional inference under hidden confounding. Annals of statistics, 50(3):1320–1347. Gupta, A. and Seo, M. H. (2019). Robust inference on inﬁnite and growing dimensional time series regression. arXiv preprint arXiv:1911.08637. Gupta, S. (2012). A note on the asymptotic distribution of lasso estimator for correlated data. Sankhya A, 74(1):10–28. 122Hagemann, A. (2012). A simple test for regression speciﬁcation with non-nested alternatives. Journal of econometrics, 166(2):247–254. Hagemann, A. (2014). Stochastic equicontinuity in nonlinear time series models. The Econometrics Journal, 17(1):188–196. Halunga, A. G., Orme, C. D., and Yamagata, T. (2017). A heteroskedasticity robust breusch–pagan test for contemporaneous correlation in dynamic panel data models. Journal of econometrics, 198(2):209– 230. Hansen, B. E. (2007). Least squares model averaging. Econometrica, 75(4):1175–1189. Hansen, B. E. (2008). Least-squares forecast averaging. Journal of Econometrics, 146(2):342–350. Hansen, B. E. (2010). Averaging estimators for autoregressions with a near unit root. Journal of Econo- metrics, 158(1):142–155. He, X., Wang, L., and Hong, H. G. (2013). Quantile-adaptive model-free variable screening for high- dimensional heterogeneous data. Annals of Statistics, 41(1):342–369. Hebb, D. O. (1949). The organization of behavior: A neuropsychological theory. Wiley. Hotz, V. J., Miller, R. A., Sanders, S., and Smith, J. (1994). A simulation estimator for dynamic models of discrete choice. The Review of Economic Studies, 61(2):265–289. Hu, Y. (2004). Complete convergence theorems for lp-mixingales. Journal of mathematical analysis and applications, 290(1):271–290. Huang, J., Breheny, P., and Ma, S. (2012). A selective review of group selection in high-dimensional models. Statistical science: a review journal of the Institute of Mathematical Statistics, 27(4). Huang, J., Ma, S., and Zhang, C.-H. (2008). Adaptive lasso for sparse high-dimensional regression models. Statistica Sinica, pages 1603–1618. Huang, T.-J., McKeague, I. W., and Qian, M. (2019). Marginal screening for high-dimensional predictors of survival outcomes. Statistica Sinica, 29(4):2105. James, G. M., Radchenko, P., and Lv, J. (2009). Dasso: connections between the dantzig selector and lasso. Journal of the Royal Statistical Society Series B: Statistical Methodology, 71(1):127–142. Jewson, J. and Rossell, D. (2022). General bayesian loss function selection and the use of improper models. Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(5):1640–1665. Johansen, S. (1991). Estimation and hypothesis testing of cointegration vectors in gaussian vector au- toregressive models. Econometrica: journal of the Econometric Society, pages 1551–1580. Kapetanios, G. and Zikes, F. (2018). Time-varying lasso. Economics Letters, 169:1–6. Karmakar, S., Chud`y, M., and Biao Wu, W. (2022). Long-term prediction intervals with many covariates. Journal of Time Series Analysis, 43(4):587–609. Katsouris, C. (2021). Forecast evaluation in large cross-sections of realized volatility. arXiv preprint arXiv:2112.04887. Katsouris, C. (2023). Statistical estimation for covariance structures with tail estimates using nodewise quantile predictive regression models. arXiv preprint arXiv:2305.11282. Kaul, A. (2014). Lasso with long memory regression errors. Journal of Statistical Planning and Infer- ence, 153:11–26. Ke, T., Jin, J., and Fan, J. (2014). Covariance assisted screening and estimation. Annals of statistics, 12342(6):2202. Keeler, E. and Meir, A. (1969). A theorem on contraction mappings. J. Math. Anal. Appl, 28:326–329. Kejriwal, M. and Yu, X. (2021). Generalized forecast averaging in autoregressions with a near unit root. The Econometrics Journal, 24(1):83–102. Khalaf, L. and Richard, F. (2020). Simulation-based multiple testing for many non-nested multivariate models. Kim, J.-Y. (1994). Bayesian asymptotic theory in a time series model with a possible nonstationary process. Econometric Theory, pages 764–773. Klau, S., Jurinovic, V., Hornung, R., Herold, T., and Boulesteix, A.-L. (2018). Priority-lasso: a simple hi- erarchical approach to the prediction of clinical outcome using multi-omics data. BMC bioinformatics, 19(1):322. Kleibergen, F. and Paap, R. (2006). Generalized reduced rank tests using the singular value decomposi- tion. Journal of econometrics, 133(1):97–126. Kock, A. B. (2016). Consistent and conservative model selection with the adaptive lasso in stationary and nonstationary autoregressions. Econometric Theory, 32(1):243–259. Koenker, R. and Bassett Jr, G. (1978). Regression quantiles. Econometrica: journal of the Econometric Society, pages 33–50. Koltchinskii, V. (2009). The dantzig selector and sparsity oracle inequalities. Bernoulli, pages 799–828. Kong, Y., Li, Y., and Zerom, D. (2019). Screening and selection for quantile regression using an alterna- tive measure of variable importance. Journal of Multivariate Analysis, 173:435–455. Kontorovich, L. and Ramanan, K. (2008). Concentration inequalities for dependent random variables via the martingale method. The Annals of Statistics, 36(6):2126–2158. Koo, B., Anderson, H. M., Seo, M. H., and Yao, W. (2020). High-dimensional predictive regression in the presence of cointegration. Journal of Econometrics, 219(2):456–477. Kuan, C.-M. and White, H. (1994). Artiﬁcial neural networks: An econometric perspective. Econometric reviews, 13(1):1–91. Kuchibhotla, A. K., Brown, L. D., Buja, A., George, E. I., and Zhao, L. (2021). Uniform-in-submodel bounds for linear regression in a model-free framework. Econometric Theory, pages 1–47. Kuersteiner, G. M. (2019). Invariance principles for dependent processes indexed by besov classes with an application to a hausman test for linearity. Journal of econometrics, 211(1):243–261. Kwiatkowski, D., Phillips, P. C., Schmidt, P., and Shin, Y. (1992). Testing the null hypothesis of station- arity against the alternative of a unit root: How sure are we that economic time series have a unit root? Journal of econometrics, 54(1-3):159–178. LeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. nature, 521(7553):436–444. Ledoit, O. and Wolf, M. (2004). A well-conditioned estimator for large-dimensional covariance matrices. Journal of multivariate analysis, 88(2):365–411. Lee, J. H., Shi, Z., and Gao, Z. (2022). On lasso for predictive regression. Journal of Econometrics, 229(2):322–349. Li, Y., Chan, N. H., Yau, C. Y., and Zhang, R. (2021). Group orthogonal greedy algorithm for change- point estimation of multivariate time series. Journal of Statistical Planning and Inference, 212:14–33. 124Lin, Y. and Tu, Y. (2020). Robust inference for spurious regressions and cointegrations involving pro- cesses moderately deviated from a unit root. Journal of Econometrics, 219(1):52–65. Lockhart, R., Taylor, J., Tibshirani, R. J., and Tibshirani, R. (2014). A signiﬁcance test for the lasso. Annals of statistics, 42(2):413. MacKinnon, J. G. (1983). Model speciﬁcation tests against non-nested alternatives. Econometric Re- views, 2(1):85–110. Masini, R. P., Medeiros, M. C., and Mendes, E. F. (2022). Regularized estimation of high-dimensional vector autoregressions with weakly dependent innovations. Journal of Time Series Analysis, 43(4):532–557. Massart, P. (2007). Concentration inequalities and model selection: Ecole de Ete de Probabilites de Saint-Flour XXXIII-2003. Springer. McKeague, I. W. and Qian, M. (2015). An adaptive resampling test for detecting the presence of signiﬁ- cant predictors. Journal of the American Statistical Association, 110(512):1422–1433. Medeiros, M. C. and Mendes, E. F. (2016). Norm 1-regularization of high-dimensional time-series mod- els with non-gaussian and heteroskedastic errors. Journal of Econometrics, 191(1):255–271. Medeiros, M. C. and Mendes, E. F. (2017). Adaptive lasso estimation for ardl models with garch inno- vations. Econometric Reviews, 36(6-9):622–637. Meinshausen, N. and Bühlmann, P. (2006). High-dimensional graphs and variable selection with the lasso. Annals of statistics, 34(3):1436–1462. Meinshausen, N. and Bühlmann, P. (2010). Stability selection. Journal of the Royal Statistical Society Series B: Statistical Methodology, 72(4):417–473. Mendes, E. F. (2011). Model selection consistency for cointegrating regressions. arXiv preprint arXiv:1104.5667. Michailidis, G. and d’Alché Buc, F. (2013). Autoregressive models for gene regulatory network infer- ence: Sparsity, stability and causality issues. Mathematical biosciences, 246(2):326–334. Mohri, M. and Rostamizadeh, A. (2008). Rademacher complexity bounds for non-iid processes. Ad- vances in Neural Information Processing Systems, 21. Mohri, M. and Rostamizadeh, A. (2010). Stability bounds for stationary ϕ-mixing and β-mixing pro- cesses. Journal of Machine Learning Research, 11(2). Montanelli, H. and Du, Q. (2019). New error bounds for deep relu networks using sparse grids. SIAM Journal on Mathematics of Data Science, 1(1):78–92. Mukherjee, K. (1998). On preliminary test and shrinkage estimation in linear models with long-memory errors. Journal of statistical planning and inference, 69(2):319–328. Muller, H.-G. and Stadtmuller, U. (2005). Generalized functional linear models. The Annals of Statistics, 33(2):774–805. Müller, M. M., Reeve, H. W., Cannings, T. I., and Samworth, R. J. (2023). Isotonic subgroup selection. arXiv preprint arXiv:2305.04852. Muller, U. K. (2008). The impossibility of consistent discrimination between i (0) and i (1) processes. Econometric Theory, 24(3):616–630. Newey, W. K. (1991). Uniform convergence in probability and stochastic equicontinuity. Econometrica: 125Journal of the Econometric Society, pages 1161–1167. Nielsen, M. Ø. (2009). A powerful test of the autoregressive unit root hypothesis based on a tuning parameter free statistic. Econometric Theory, 25(6):1515–1544. Olmo, J. (2022). A nonparametric predictive regression model using partitioning estimators based on taylor expansions. Journal of Time Series Analysis. Osborne, M. R., Presnell, B., and Turlach, B. A. (2000). On the lasso and its dual. Journal of Computa- tional and Graphical statistics, 9(2):319–337. Park, T. and Casella, G. (2008). The bayesian lasso. Journal of the American Statistical Association, 103(482):681–686. Pitacco, E. (2019). Heterogeneity in mortality: a survey with an actuarial focus. European Actuarial Journal, 9:3–30. Pitarakis, J.-Y. (2023). A novel approach to predictive accuracy testing in nested environments. Econo- metric Theory, pages 1–44. Poignard, B. and Asai, M. (2023). Estimation of high-dimensional vector autoregression via sparse precision matrix. The Econometrics Journal, 26(2):307–326. Politis, D. N., Romano, J. P., and Wolf, M. (1999). Subsampling. Springer Science & Business Media. Potscher, B. M. (1991). Effects of model selection on inference. Econometric Theory, 7(2):163–185. Pottmann, H., Krasauskas, R., Hamann, B., Joy, K., and Seibold, W. (2000). On piecewise linear approx- imation of quadratic functions. Journal for Geometry and Graphics, 4(1):31–53. Qian, J. and Su, L. (2016). Shrinkage estimation of regression models with multiple structural changes. Econometric Theory, 32(6):1376–1433. Reeve, H. W., Cannings, T. I., and Samworth, R. J. (2021). Optimal subgroup selection. arXiv preprint arXiv:2109.01077. Reich, S. (1971). Some remarks concerning contraction mappings. Canadian Mathematical Bulletin, 14(1):121–124. Richard, F. (2023). Model conﬁdence sets in multivariate systems. Rinaldo, A., Wasserman, L., and G’Sell, M. (2019). Bootstrapping and sample splitting for high- dimensional, assumption-lean inference. Annals of statistics, 47(6):3438–3469. Romano, J. P. and Wolf, M. (2005). Exact and approximate stepdown methods for multiple hypothesis testing. Journal of the American Statistical Association, 100(469):94–108. Rosenblatt, F. (1958). The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review, 65(6):386. Roverato, A. and Whittaker, J. (1998). The isserlis matrix and its application to non-decomposable graphical gaussian models. Biometrika, 85(3):711–725. Salgueiro, M. F., Smith, P. W., and McDonald, J. W. (2005). Power of edge exclusion tests in graphical gaussian models. Biometrika, 92(1):173–182. Salgueiro, M. F., Smith, P. W., and McDonald, J. W. (2006). Power of edge exclusion tests for graphical log-linear models. Journal of multivariate analysis, 97(8):1691–1701. Schmidt-Hieber, J. (2020). Nonparametric regression using deep neural networks with relu activation function. Annals of Statistics, 48(4):1875–1897. 126Sen, P. K. and Saleh, A. E. (1987). On preliminary test and shrinkage m-estimation in linear models. The Annals of Statistics, pages 1580–1592. Sen, P. K. and Saleh, A. M. E. (1985). On some shrinkage estimators of multivariate location. The Annals of Statistics, 13(1):272–281. Shao, Q.-M. and Zhang, Z.-S. (2022). Berry–esseen bounds for multivariate nonlinear statistics with applications to m-estimators and stochastic gradient descent algorithms. Bernoulli, 28(3):1548–1576. Shen, G., Jiao, Y., Lin, Y., Horowitz, J. L., and Huang, J. (2021). Deep quantile regression: Mitigating the curse of dimensionality through composition. arXiv preprint arXiv:2107.04907. Shen, X., Jiang, C., Sakhanenko, L., and Lu, Q. (2023). Asymptotic properties of neural network sieve estimators. Journal of Nonparametric Statistics, pages 1–30. Shibata, R. (1986). Consistency of model selection and parameter estimation. Journal of applied proba- bility, 23(A):127–141. Smith, S. C., Timmermann, A., and Zhu, Y. (2019). Variable selection in panel models with breaks. Journal of econometrics, 212(1):323–344. Stinchcombe, M. B. and White, H. (1998). Consistent speciﬁcation testing with nuisance parameters present only under the alternative. Econometric theory, 14(3):295–325. Su, L. and Hoshino, T. (2016). Sieve instrumental variable quantile regression estimation of functional coefﬁcient models. Journal of Econometrics, 191(1):231–254. Su, L. and Jin, S. (2012). Sieve estimation of panel data models with cross section dependence. Journal of Econometrics, 169(1):34–47. Sun, Q., Zhu, H., Liu, Y., and Ibrahim, J. G. (2015). Sprem: sparse projection regression model for high- dimensional linear regression. Journal of the American Statistical Association, 110(509):289–302. Swanson, N. R., Xiong, W., and Yang, X. (2020). Predicting interest rates using shrinkage methods, real- time diffusion indexes, and model combinations. Journal of Applied Econometrics, 35(5):587–613. Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Methodological), 58(1):267–288. Tibshirani, R. J., Rinaldo, A., Tibshirani, R., and Wasserman, L. (2018). Uniform asymptotic inference and the bootstrap after model selection. The Annals of Statistics, 46(3):1255–1287. Toulis, P. and Airoldi, E. M. (2017). Asymptotic and ﬁnite-sample properties of estimators based on stochastic gradients. The Annals of Statistics, 45(4):1694–1727. Tran, D., Toulis, P., and Airoldi, E. M. (2015). Stochastic gradient descent methods for estimation with large data sets. arXiv preprint arXiv:1509.06459. Vaart, A. v. d. and Wellner, J. A. (2013). Weak convergence and empirical processes: with applications to statistics. Springer Science & Business Media. Vaart, A. v. d. and Wellner, J. A. (2023). Empirical processes. In Weak Convergence and Empirical Processes: With Applications to Statistics, pages 1–403. Springer. van de Geer, S. A. (2002). On hoeffding’s inequality for dependent random variables. In Empirical process techniques for dependent data, pages 161–169. Springer. Van der Vaart, A. W. (2000). Asymptotic statistics, volume 3. Cambridge university press. Varin, C., Reid, N., and Firth, D. (2011). An overview of composite likelihood methods. Statistica Sinica, 127pages 5–42. Vershynin, R. (2018). High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press. Vesely, A., Finos, L., and Goeman, J. J. (2021). Permutation-based true discovery guarantee by sum tests. arXiv preprint arXiv:2102.11759. Vuong, Q. H. (1989). Likelihood ratio tests for model selection and non-nested hypotheses. Economet- rica: journal of the Econometric Society, pages 307–333. Wainwright, M. J. (2019). High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam- bridge university press. Wald, A. (1943). On the efﬁcient design of statistical investigations. The annals of mathematical statis- tics, 14(2):134–140. Wang, D. and Tsay, R. S. (2023). Rate-optimal robust estimation of high-dimensional vector autoregres- sive models. The Annals of Statistics, 51(2):846–877. Wang, H. (2009). Forward regression for ultra-high dimensional variable screening. Journal of the American Statistical Association, 104(488):1512–1524. Wang, L., Wu, Y., and Li, R. (2012). Quantile regression for analyzing heterogeneity in ultra-high dimension. Journal of the American Statistical Association, 107(497):214–222. Wang, X. and Leng, C. (2016). High dimensional ordinary least squares projection for screening vari- ables. Journal of the Royal Statistical Society Series B: Statistical Methodology, 78(3):589–611. Wei, W., Zhou, Y., Zheng, Z., and Wang, J. (2023). Inference on the best policies with many covariates. Journal of Econometrics, page 105460. White, H. (1990). Connectionist nonparametric regression: Multilayer feedforward networks can learn arbitrary mappings. Neural networks, 3(5):535–549. White, H. (1996). Estimation, inference and speciﬁcation analysis. Number 22. Cambridge university press. Wong, K. C., Li, Z., and Tewari, A. (2020). Lasso guarantees for β-mixing heavy-tailed time series. The Annals of Statistics, 48(2):1124–1142. Wüthrich, K. and Zhu, Y. (2023). Omitted variable bias of lasso-based inference methods: A ﬁnite sample analysis. The review of economics and statistics, 105(4):982–997. Xiao, H. and Wu, W. B. (2012). Covariance matrix estimation for stationary time series. The Annals of Statistics, 40(1):466–493. Xiao, H. and Wu, W. B. (2013). Asymptotic theory for maximum deviations of sample covariance matrix estimates. Stochastic Processes and their Applications, 123(7):2899–2920. Yang, X., Guo, Y., Dong, M., and Xue, J.-H. (2022). Toward certiﬁed robustness of distance metric learning. IEEE Transactions on Neural Networks and Learning Systems. Yang, Y. (2007). Consistency of cross validation for comparing regression procedures. Annals of statis- tics, 35(6):2450–2473. Yarotsky, D. (2017). Error bounds for approximations with deep relu networks. Neural Networks, 94:103–114. Yarotsky, D. (2018). Optimal approximation of continuous functions by very deep relu networks. In 128Conference on learning theory, pages 639–649. PMLR. Yousuf, K. (2018). Variable screening for high dimensional time series. Electonic Journal of Statistics, 12(1):667–702. Yousuf, K. and Ng, S. (2021). Boosting high dimensional predictive regressions with time varying parameters. Journal of Econometrics, 224(1):60–87. Yu, B. (1994). Rates of convergence for empirical processes of stationary mixing sequences. The Annals of Probability, pages 94–116. Yuan, M. and Cai, T. T. (2010). A reproducing kernel hilbert space approach to functional linear regres- sion. The Annals of Statistics, 38(6):3412–3444. Zeng, G. L. (2021). A deep-network piecewise linear approximation formula. IEEE Access, 9:120665– 120674. Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty. Annals of Statistics, 38(2):894–942. Zhang, D. and Wu, W. B. (2017). Gaussian approximation for high dimensional time series. Annals of statistics, 45(5):1895–1919. Zhang, P. (1993). Model selection via multifold cross validation. The annals of statistics, pages 299–313. Zhang, T. and Simon, N. (2022). A sieve stochastic gradient descent estimator for online nonparametric regression in sobolev ellipsoids. The Annals of Statistics, 50(5):2848–2871. Zou, H. (2006). The adaptive lasso and its oracle properties. Journal of the American statistical associ- ation, 101(476):1418–1429. Zou, H. and Hastie, T. (2005). Regularization and variable selection via the elastic net. Journal of the royal statistical society: series B (statistical methodology), 67(2):301–320. 129
arXiv:2308.16059v1 [stat.ML] 30 Aug 2023 A Parameter-Free Two-Bit Covariance Estimator with Improved Operator Norm Error Rate Junren Chen∗, Michael K. Ng† August 31, 2023 Abstract A covariance matrix estimator using two bits per entry was recently developed by Dirksen, Maly and Rauhut [Annals of Statistics, 50(6), pp. 3538-3562]. The estimator achieves near minimax rate for general sub-Gaussian distributions, but also suﬀers from two downsides: the- oretically, there is an essential gap on operator norm error between their estimator and sample covariance when the diagonal of the covariance matrix is dominated by only a few entries; prac- tically, its performance heavily relies on the dithering scale, which needs to be tuned according to some unknown parameters. In this work, we propose a new 2-bit covariance matrix estimator that simultaneously addresses both issues. Unlike the sign quantizer associated with uniform dither in Dirksen et al., we adopt a triangular dither prior to a 2-bit quantizer inspired by the multi-bit uniform quantizer. By employing dithering scales varying across entries, our estimator enjoys an improved operator norm error rate that depends on the eﬀective rank of the under- lying covariance matrix rather than the ambient dimension, thus closing the theoretical gap. Moreover, our proposed method eliminates the need of any tuning parameter, as the dithering scales are entirely determined by the data. Experimental results under Gaussian samples are provided to showcase the impressive numerical performance of our estimator. Remarkably, by halving the dithering scales, our estimator oftentimes achieves operator norm errors less than twice of the errors of sample covariance. 1 Introduction Given i.i.d. samples X1, ..., Xn from a zero-mean random vector X, a fundamental problem in multivariate analysis is to accurately estimate the covariance matrix Σ := E(XX⊤), which fre- quently arises in principle component analysis [22], regression analysis [16], ﬁnance [25], massive MIMO system [28], and so on. Despite the vast literature on covariance estimation, we study in this paper a less well-understood setting where we can only access samples coarsely quantized ∗J. Chen is with Department of Mathematics, The University of Hong Kong. The work was done when he was a visiting Ph.D. student at School of Computing, National University of Singapore. (Corresponding author. e-mail: chenjr58@connect.hku.hk) †M. K. Ng is with Department of Mathematics, Hong Kong Baptist University. (e-mail: michael-ng@hkbu.edu.hk) 1to a small number of bits. This setting is of particular interest in signal processing or dis- tributed learning where it is expensive or even impossible to acquire or transmit high-precision data (e.g., [3,6,18,28,38,39]). Deﬁning the ψ2 norm and L2 norm of a random variable X respec- tively as ∥X∥ψ2 = inf{t > 0 : E(exp(X2 t2 )) ≤ 2} and ∥X∥L2 = (E[X2])1/2, we follow prior works (e.g., [24,27,35,36]) and formulate sub-Gaussian random vector X in Deﬁnition 1. Deﬁnition 1. A d-dimensional random vector X is K-sub-Gaussian if ∥⟨X, v⟩∥ψ2 ≤ K∥⟨X, v⟩∥L2, ∀ v ∈ Rd. (1.1) For given K, we use the shorthand ˆK := K√log K throughout this work. In this paper, we propose a 2-bit covariance estimator1 for general sub-Gaussian X and establish the non-asymptotic guarantee, which indicates the required sample size for achieving operator norm error lower than some desired accuracy. 1.1 Related Works Under a direct 1-bit sign quantizer, many works have investigated covariance estimation from {sgn(Xi)}n i=1 under the name of "arcsin-law" (e.g., [5, 19, 34, 36]), but it is impossible to estimate the full covariance in this case as the magnitudes of the samples are completely lost.2 For estimating the full covariance Σ, as we pursue in this paper, one must introduce non-zero thresholds τi ∈ Rn called "dither" before the 1-bit quantization and collect sgn(Xi + τi) instead. Such a technique known as "dithering", with its aid on signal reconstruction observed in engineering works [20,26,29] and theory established in [17,30], recently again received much attention from researchers working on various estimation problems like compressed sensing, covariance estimation, matrix completion and reduced-rank regression (e.g., [2–4,7,10,11,23,32,37]). Speciﬁcally, for covariance estimation, some works proposed to use random Gaussian dithering [12–14] or deterministic ﬁxed dithering (e.g., [1,15]), but to our knowledge, these two lines of works are restricted to Gaussian samples and does not establish non-asymptotic result, thus provide little guarantee for the practical ﬁnite-sample setting. We will not further explain these works since our interest is on estimators for sub-Gaussian distributions with non-asymptotic guarantee. The work most relevant to ours is Dirksen et al. [10] who developed the ﬁrst 2-bit covariance estimator that applies to sub-Gaussian samples and enjoys non-asymptotic near minimax error rate. Let {τi1, τi2}n i=1 ∼ U [−λ, λ]d be i.i.d. uniform dithers independent of the K-sub-Gaussian {Xi}n i=1, they proposed to quantize the i-th sample Xi to ˙Xi1 = λ·sgn(Xi+τi1) and ˙Xi2 = λ·sgn(Xi +τi2), thus only retaining two bits per entry. Based on the observation E( ˙Xi1 ˙X⊤ i2) = Σ that holds if 1Throughout this work, 2-bit estimator refers to the one that only relies on two bits from each entry of {Xi}n i=1. 2Indeed, we can only hope to estimate the correlation matrix with all-ones diagonal, see [10, Sec. 1.2]). 2λ ≥ ∥Xi∥∞ [10, Lem. 15], the 2-bit estimator in [10] is deﬁned as �Σna = 1 2n n � i=1 ( ˙Xi1 ˙X⊤ i2 + ˙Xi2 ˙X⊤ i1), with    ˙Xi1 = λ · sgn(Xi + τi1) ˙Xi2 = λ · sgn(Xi + τi2) . (1.2) The non-masked case of [10, Thm. 4] implies the error rate that is near minimax optimal up to logarithmic factors [8]. Theorem 1. Suppose that X1, ..., Xn are i.i.d. copies of the zero-mean K-sub-Gaussian ran- dom vector X ∈ Rd, we consider the 2-bit estimator �Σna in (1.2) with uniform dithers τi1, τi2 ∼ U [−λ, λ]d. If λ2 = C(K) log n∥Σ∥∞ for some large enough constant C(K) depending on K, then it holds with probability at least 1 − 2d−10 that ∥ �Σna − Σ∥op ≲K � d∥Σ∥op∥Σ∥∞ log d(log n)2 n + d∥Σ∥∞ log n log d n . (1.3) The novel estimator in (1.2) represents a signiﬁcant progress in covariance estimation, which has already sparked interest and led to subsequent research endeavors: Chen et al. extended it to high- dimensional sparse case and heavy-tailed samples in [3, Sec. II], then also developed a multi-bit covariance estimator for heavy-tailed distribution with near minimax non-asymptotic guarantees in [2, Sec. 3.1]; Yang et al. [38] extended it to the complex domain and applied to massive MIMO system; more recently, Dirksen and Maly proposed its tuning-free version by using data-driven dithering [9]. 1.2 Two Downsides of the Estimator Due to Dirksen et al. Nonetheless, the estimator �Σna also suﬀers from two frustrating downsides. Theoretical Gap. Although the rate in (1.3) is near minimax over certain set of covariance matrices, it is essentially sub-optimal for covariance matrices whose diagonals are dominated by only a few entries, as described by Tr(Σ) ≪ d∥Σ∥∞. Indeed, for the behaviour of the sample covariance �Σ = 1 n �n i=1 XiX⊤ i on sub-Gaussian Xi, the operator norm error rate O �∥Σ∥op �� r(Σ) n + r(Σ) n �� depending on the eﬀective rank r(Σ) := Tr(Σ) ∥Σ∥op rather than the ambient dimension d has been established in the literature, see for instance [27, Prop. 3], [24, Coro. 2], [36, Thm. 9.2.4]; this rate is tight up to multiplicative constant for Gaussian samples [24, Thm. 4] and near optimal in a minimax sense [27, Thm. 2]. To be more precise, we present Theorem 2 which provides an incremental extension of [36, Thm. 9.2.4] towards independent samples that may not be identically distributed (this will be useful in the proof of our main theorem); it also slightly reﬁnes the dependence on K by the recent results from [21]. Theorem 2. Suppose that X1, ..., Xn are independent, zero-mean, K-sub-Gaussian d-dimensional random vectors sharing the same covariance matrix Σ ∈ R. Let r(Σ) := Tr(Σ) ∥Σ∥op , then for any u ≥ 0, 3with probability at least 1 − 3e−u we have ����� 1 n n � i=1 XiX⊤ i − Σ ����� op ≤ C∥Σ∥op   � ˆK2(r(Σ) + u) n + ˆK2(r(Σ) + u) n   . (1.4) Proof. The proof can be founded in Section 5. Therefore, the sample covariance satisﬁes an operator norm rate O �� Tr(Σ)∥Σ∥op n + Tr(Σ) n � that is essentially tighter than (1.3) when Tr(Σ) ≪ d∥Σ∥∞. To our knowledge, there has not been any attempt to close this gap between 2-bit covariance and the sample covariance based on full data. Tuning Parameter. More practically, while the behaviour of �Σna heavily relies on a suitable dithering scale λ, this parameter is in general hard to tune, e.g., its theoretical choice depends on K, ∥Σ∥∞. To alleviate this issue, a recent pre-print [9] proposed a tuning-free variant of �Σna that uses data-driven dithering to get rid of the dependence of dithering scales on ∥Σ∥∞, as contrasted to �Σna that is non-adaptive to the data. Particularly, they add an additional sample X0 and calculate λi0 = 1 i �i−1 k=1 ∥Xk∥∞, and then use a uniform dither with scale λi = C(K)λi0 √log i for the i-th sample Xi before the 1-bit quantization, where C(K) is a ﬁxed constant depending on K (but not on ∥Σ∥∞). More precisely, let {τi1, τi2}n i=1 iid ∼ U [−1, 1]d be independent of {Xi}n i=0, they collect ˙Xi1 = λi · sgn(Xi + λiτi1) and ˙Xi2 = λi · sgn(Xi + λiτi2) from each Xi, and then construct the estimator �Σa = 1 2n n � i=1 ( ˙Xi1 ˙X⊤ i2 + ˙Xi2 ˙X⊤ i1) analogously to (1.2).3 The main theorem of [9] implies that ∥ �Σa − Σ∥op ≲K � d∥Σ∥∞∥Σ∥op(log d)7(log n)2 n + d∥Σ∥∞(log d)4 log n n holds with probability at least 1 − 2d−10, which coincides with (1.3) up to logarithmic factors. Nonetheless, �Σa does not fully address the tuning issue since the unknown multiplicative constant still depends on K that could vary among diﬀerent distributions. Besides, as will be shown by the numerical examples below, �Σa does not perform well empirically. 1.2.1 Numerical Examples We pause to provide numerical results to demonstrate the aforementioned weaknesses of existing 2-bit estimators. We ﬁx (n, d) = (500, 10) and test Gaussian samples Xi ∼ N(0, Σ), for which the dithering scale is λ = C√log n in �Σna and λ = C � 1 i �i−1 k=1 ∥Xk∥∞ �√log i in �Σa. Based on 50 repetitions, we plot the curves of "operator norm error v.s. C" in Figure 1, where we also include the results of sample covariance �Σ and a new (non-adaptive) 2-bit estimator that we propose in 3Compared to �Σna, this estimator requires storing the additional {λi}n i=1 as 32-bit data, while such memory is relatively minor compared to the savings of quantization [9, Rem. 2]; this remark also applies to the our main estimator �Σ in Section 3 that additionally requires d full-precision scalars. 4Section 2. Let Σ(a, b, c) = (a − b)Id + b11⊤ + (c − a)e1e⊤ 1 be the covariance matrix with diagonal being [c, a, ..., a] and non-diagonal entries being b, we test the low-correlation case Σ = Σ(1, 0.2, 1) in Figure 1(a) and the high-correlation case Σ = Σ(1, 0.9, 1) in Figure 1(b). Further, we test Σ(1, 0.2, 10) (that changes the (1, 1)-th entry of Σ(1, 0.2, 1) to 10) in Figure 1(c) to simulate the setting of Tr(Σ) ≪ d∥Σ∥∞. Comparing (a) and (c) in Figure 1 clearly corroborates the performance gap between �Σna under Tr(Σ) ≪ d∥Σ∥∞ and �Σ and the dependence of "optimal C" (for �Σna) on ∥Σ∥∞. Consistent with the theoretical progress made by [9], the optimal C for �Σa roughly remain in [0.3, 0.4], but its numerical performance is in general much worse than �Σna, especially when the correlations are high or Tr(Σ) ≪ d∥Σ∥∞ (Figure 1(b)-(c)). We conjecture that this numerical degradation stems from the scaling √log i in the λi, which may facilitate theoretical analysis but could make the dithering scale of Xi1 considerably larger than Xi2 if i1 ≫ i2; this may not be sensible since there is no reason to believe that the entries of Xi1 are of magnitudes greater than those of Xi2. 0 0.5 1 1.5 0 0.5 1 1.5 2 2.5 3 3.5 4 (a) 0 0.5 1 1.5 2 0 1 2 3 4 5 6 0 0.5 1 1.5 2 2.5 0 2 4 6 8 10 (a) (b) (c) Figure 1: The curves of "operator norm error v.s. C", with the optimal C and the corresponding minimum error reported in the labels. 1.3 Our Contributions In this work, we develop a new 2-bit covariance estimator to address the above two issues. Our quantization procedure is an essential departure from [9,10] in the sense that we use triangular dither and a 2-bit quantizer reduced from the multi-bit uniform quantizer. Indeed, our main estimator is built upon a new non-adaptive 2-bit estimator �Σ2b inspired by the multi-bit estimator developed in our prior work [2]; compared to �Σna, this new 2-bit estimator has comparable theoretical guarantee and numerically performs better over Gaussian samples. Then, we modify the proposed �Σ2b by using dithering scales varying across diﬀerent entries of Xi that are entirely speciﬁed by the given data. This not only remove the tuning parameter but also allows for a tight operator norm error bound — as shown by our main theorem that presents a high-probability operator norm error rate O �∥Σ∥op �� r(Σ)(log d)2 n + r(Σ)(log d)2 n ��, which matches Theorem 2 for sample covariance up to (a 5small number of) logarithmic factors and hence closes the theoretical gap. We conclude this section by ﬁxing notations and providing an overview. Notations. We use regular letters to denote scalars, and boldface symbols are for vectors and matrices. For vector a = (ai) ∈ Rd we let ∥a∥∞ = max1≤i≤d |ai| and ∥a∥2 = (�d i=1 a2 i )1/2; for matrix A = (aij) ∈ Rd×d we let ∥A∥op be the operator norm and ∥A∥∞ = maxi,j∈[d] |aij|. Given symmetric A, B ∈ Rd×d, we write A ⪯ B if B − A is positive semi-deﬁnite. Recall that ∥X∥ψ2 has been deﬁned for a random variable X (before Deﬁnition 1), now we further deﬁne ∥X∥ψ2 := supv∈Sd−1 ∥⟨v, X⟩∥ψ2 for a random vector X ∈ Rd, where Sd−1 = {v ∈ Rd : ∥v∥2 = 1} is the standard Euclidean sphere, ⟨a, b⟩ = a⊤b is the inner product. We use C, c, C1, C2, ... to denote absolute constants whose value may vary from line to line. We denote multivariate Gaussian distribution with covariance matrix Σ by N(0, Σ), and the uniform distribution over W ⊂ Rd by U (W). Given two terms T1, T2, we write T1 ≲ T2 or T1 = O(T2) if T1 ≤ CT2 for some C, and conversely write T1 ≳ T2 or T1 = Ω(T2) if T1 ≥ cT2 for some c. We may also use notations like ≲K, ≳K to indicate that the implied constant may depend on K. We use T1 ≍ T2 to state that T1 = O(T2), T1 = Ω(T2) simultanesouly hold. In this work, the quantizers operate on vectors in an element-wise manner. In our problem setting, we let Xij be the j-th entry of the sample Xi, Σij be the (i, j)-th entry of the underlying covariance Σ. Overview. We propose a new 2-bit estimator �Σ2b in Section 2 that is comparable to �Σna. Our primary contribution, a parameter-free estimator with improved error rate, is presented in Section 3, where we also provide numerical examples to validate our theory. The proof of our main theorem (Theorem 5) is given in Section 4. We collect the deferred proofs in Section 5. The paper is concluded with some discussions in Section 6. 2 A New 2-bit Covariance Matrix Estimator 2.1 A Multi-Bit Covariance Estimator Using the uniform quantizer associated with triangular dithers, a multi-bit covariance estimator was developed in [2, Sec. 3.1]. We shall introduce this estimator (denoted by �Σmb) ﬁrst in order to inspire the new 2-bit estimator �Σ2b. We need some necessary preliminaries of the dithered uniform quantizer to get started. The (multi-bit) uniform quantizer with resolution λ is deﬁned for a ∈ R as Qλ(a) = λ(⌊ a λ⌋ + 1 2). It is now well-understood that dithering prior to uniform quantization beneﬁts signal recovery or parameter from diﬀerent aspects (e.g., [2, 4, 23, 31, 32, 37]); throughout this work, the dithering vectors (i.e., dithers) τi's are independent of everything else and have independent entries.4 Under uniform quantizer Qλ(·), while a uniform dither τi ∼ U [− λ 2 , λ 2]d is adopted in most prior works, the triangular dither given by the sum of two independent uniform dither τi ∼ U [− λ 2, λ 2]d +U [− λ 2 , λ 2]d was adopted in [2,4] to allow for covariance estimation. Our subsequent developments hinge on the 4For data-driven dithers with dithering scale depending on Xi's, we will describe the procedure as drawing τi's with constant scale ﬁrst and then rescaling the dithers (e.g., our description for �Σa above). 6following properties of the dithered quantizer Qλ(· + τ) with triangular dither. Lemma 1. For random or deterministic X ∈ Rd and some λ > 0, we let τ ∼ U [− λ 2, λ 2]d + U [− λ 2 , λ 2]d be independent of X and quantize X to ˙X = Qλ(X + τ). Then the quantization noise ξ = ˙X − X satisﬁes ∥ξ∥∞ ≤ 3λ 2 , E[ξ] = 0, ∥ξ∥ψ2 = O(λ), E(ξξ⊤) = λ2 4 Id. Proof. The proof can be found in Section 5. Lemma 1 allows for a covariance estimator based on Qλ(·+τ) with triangular dither τ. Specif- ically, we let {τi}n i=1 iid ∼ U [− λ 2, λ 2]d + U [− λ 2 , λ 2]d be independent of {Xi}n i=1, and then quantize Xi to ˙Xi = Qλ(Xi + τi). Deﬁne ξi := ˙Xi − Xi, then we have E( ˙Xi ˙X⊤ i ) = E(XiX⊤ i ) + E(ξiξ⊤ i ) + E(Xiξ⊤ i ) + E(ξiX⊤ i ) (i) = Σ + λ2 4 Id, (2.1) where we use E(ξiξ⊤ i ) = λ2 4 Id and Eξi|Xi(ξi) = 0 from Lemma 1 in (i). Equation (2.1) motivates the estimator [2] �Σmb = 1 n n � i=1 ˙Xi ˙X⊤ i − λ2 4 Id, with ˙Xi = Qλ(Xi + τi). (2.2) Note that [2, Sec. 3.1] focused on heavy-tailed Xi (assumed to have bounded fourth moments) and overcame the heavy-tailedness by incorporating an additional truncation step before the dithered quantization. As a result, it was mentioned in Remark 1 therein that "the results for sub-Gaussian distributions can be analogously established and are also new to the literature." To proceed, we present here the sub-Gaussian counterpart of [2, Thm. 3]. Theorem 3. Suppose that X1, ..., Xn are i.i.d. copies of the zero-mean K-sub-Gaussian random vector X ∈ Rd. Given λ > 0, using the triangular dithers τi ∼ U [− λ 2, λ 2]d + U [− λ 2, λ 2]d, the estimator �Σmb in (2.2) satisﬁes �� �Σmb − Σ �� op ≤ C � ∥Σ∥op + λ2 4 �   � ˆK2�r(Σ + λ2 4 Id) + u � n + ˆK2�r(Σ + λ2 4 Id) + u � n   with probability at least 1 − 3e−u, where r(Σ + λ2 4 Id) = Tr(Σ)+λ2d/4 ∥Σ∥op+λ2/4 is the eﬀective rank. Proof. In order to invoke Theorem 2 to bound ∥ �Σmb − Σ∥op, we ﬁrst write �Σmb − Σ = 1 n n � i=1 ˙Xi ˙X⊤ i − � Σ + λ2 4 Id � . We let ξi = Qλ(Xi +τi)−Xi be the quantization noise, then Lemma 1 gives ∥ξi∥ψ2 = O(λ). Thus, for any v ∈ Rd we have ∥⟨ ˙Xi, v⟩∥ψ2 ≤ ∥⟨Xi, v⟩∥ψ2 + ∥⟨ξi, v⟩∥ψ2 ≤ K∥⟨Xi, v⟩∥ψ2 + O(λ∥v∥2). (2.3) 7Note that (2.1) yields ∥⟨ ˙Xi, v⟩∥L2 = � v⊤ E[ ˙Xi ˙X⊤ i ]v = Ω(∥⟨Xi, v⟩∥L2) + Ω(λ∥v∥2), which together with (2.3) implies ∥⟨ ˙Xi, v⟩∥ψ2 = O(K + 1)∥⟨ ˙Xi, v⟩∥L2. Since we must have K = Ω(1), ˙Xi is O(K)-sub-Gaussian. The result follows by invoking Theorem 2. 2.2 A New 2-Bit Covariance Estimator Having introduced the multi-bit estimator �Σmb, we propose a new 2-bit estimator by properly restricting the number of bits needed for the uniform quantizer. For the quantization of a scalar a, we ﬁrst note that the uniform quantizer Qλ(·) reduces to the 1-bit quantizer sgn(·) up to a scaling if the resolution λ dominates the signal magnitude: Qλ(a) = λ 2 sgn(a), when |a| < λ. This remains true when the quantizer is associated with a uniform dither τ ∼ U [− λ 2, λ 2]: Qλ(a + τ) = λ 2 sgn(a + τ), when |a| < λ 2. Nonetheless, such relation between Qλ(·) and sgn(·) is no longer valid if a triangular dither τ ∼ U [− λ 2 , λ 2] + U [− λ 2 , λ 2] is adopted. Actually, as a triangular dither takes value on [−λ, λ], for a non-constant signal a, the dithered signal a + τ falls on more than two bins of Qλ(·) with positive probability, making it impossible to reduce Qλ(·) to 1-bit quantization. In fact, when τ ∼ U [− λ 2 , λ 2] + U [− λ 2, λ 2], Qλ(·) reduces to a 2-bit quantizer when signal magnitude is bounded by λ: Qλ(a + τ) = Qλ,2b(a + τ), when |a| < λ, (2.4) where Qλ,2b(·) is deﬁned as Qλ,2b(a) := −3λ 2 1(a < −λ) − λ 2 1(−λ ≤ a < 0) + λ 2 1(0 ≤ a < λ) + 3λ 2 1(a ≥ λ). (2.5) In essence, (2.4) states that under triangular dither τi, Qλ(· + τi) and Qλ,2b(· + τi) are close (or the same) if the signal magnitude is dominated by (or exactly bounded by) λ. This inspires us to modify �Σmb in (2.2) and propose the following new non-adaptive 2-bit covariance matrix estimator �Σ2b: �Σ2b = 1 n n � i=1 ˙Xi ˙X⊤ i − λ2 4 Id, with ˙Xi = Qλ,2b(Xi + τi). (2.6) Theorem 4. In the setting of Theorem 3, we let λ2 = CK2∥Σ∥∞ log(nd) with some large enough 8C. Then with probability at least 1 − 10(nd)−10, the 2-bit estimator �Σ2b satisﬁes ∥ �Σ2b − Σ∥op ≲ � ˆK2d∥Σ∥∞∥Σ∥op(log(nd))2 n + ˆK2d∥Σ∥∞ log(nd) n . (2.7) Proof. Our strategy is to show the two events "∥ �Σmb − Σ∥op is bounded as in (2.7)" (denoted by E1) and " �Σmb = �Σ2b" (denoted by E2) simultaneously hold with the promised probability. First, by invoking Theorem 3 with u = Tr(Σ) + λ2d/4 ∥Σ∥op + λ2/4 (i) = Ω(log(nd)) (here, (i) follows from ∥Σ∥op ≤ d∥Σ∥∞) and performing some algebra, we obtain that with proba- bility at least 1 − 3(nd)−10, ∥ �Σmb − Σ∥op ≲ � ˆK2d∥Σ∥∞∥Σ∥op log(nd) �1 + ∥Σ∥∞∥Σ∥−1 op log(nd) � n + ˆK2d∥Σ∥∞ log(nd) n . (2.8) By ∥Σ∥∞ ≤ ∥Σ∥op, this implies E1. Second, we note that E2 holds if Qλ(Xi + τi) = Qλ,2b(Xi + τi) for any i ∈ [n], and from (2.4) this can be implied by λ ≥ max1≤i≤n ∥Xi∥∞ = maxi,j |Xij|. As Xi is K-sub-Gaussian, for any (i, j) we have ∥Xij∥ψ2 ≤ K �Σjj ≤ K � ∥Σ∥∞. Thus, for any t ≥ 0 we have P � |Xij| ≥ t � ≤ 2 exp � − C1t2 K2∥Σ∥∞ � . Taking a union bound yields P � max i,j |Xij| ≥ t � ≤ 2nd exp � − C1t2 K2∥Σ∥∞ � . Letting t = C2K � ∥Σ∥∞ log(nd) with large enough C2, we obtain maxi,j |Xij| = O(K � ∥Σ∥∞ log(nd)) with probability at least 1 − 3(nd)−10. Under our choice λ2 = CK2∥Σ∥∞ log(nd) with suﬃciently large C, E2 thus holds with the promised probability. The proof is complete. Despite the distinct quantization processes and constructions, our new 2-bit estimator �Σ2b is comparable to �Σna in [10] in the following senses: 1) they both rely on two bits per entry; 2) they enjoy similar non-asymptotic operator norm error rate (comparing (1.3) and (2.7)); 3) they both involve a tuning parameter of the dithering scale λ that should be tuned according to (∥Σ∥∞, K). Thus, like �Σna, our �Σ2b still suﬀers from the theoretical gap when Tr(Σ) ≪ d∥Σ∥∞ and the tuning issue; this can be seen by comparing (a) and (c) of Figure 1, where we also include �Σ2b with λ = C√log n for comparison. On the other hand, in all three numerical examples �Σ2b notably outperforms �Σna, which suggests that our new 2-bit estimator maybe preferable for Gaussian 9samples. 3 A Parameter-Free Estimator with Improved Error Rate As an intuitive explanation on the gap between �Σna, �Σ2b and the full-data-based sample covariance, it was written on [10, P. 3544] that "in order to accurately estimate all diagonal entries of Σ, the (maximal) dithering level λ needs to be on the scale ∥Σ∥∞; if Tr(Σ) ≪ d∥Σ∥∞, then most of the diagonal entries are much smaller than ∥Σ∥∞, and hence λ is on a sub-optimal scale for these entries." In a nutshell, the common dithering scale λ should be large enough to accommodate the large entries and hence could be highly sub-optimal for other small entries. While [9] deployed data-driving dithering, the dithering scale remains the same across entries of a speciﬁc sample, thus their estimator suﬀers from the same gap. Indeed, as also reﬂected by the discussion above, a possible strategy to overcome the limitation is to adopt dithering scales adaptive to diﬀerent entries of Xi. Along this idea, we propose in this section our main estimator that simultaneously resolves the issues of sub-optimal rate and tuning parameter. Speciﬁcally, let λj be the dithering scale for the j-th entry {Xij : i ∈ [n]}, we will quantize Xij to ˙Xij = Qλj,2b(Xij + λjτij) with (normalised) triangular dither τij ∼ U [− 1 2, 1 2] + U [− 1 2, 1 2]. We comment that a good λj should represent a reasonable trade-oﬀ between bias and variance. In particular, (2.4) indicates that using λj ≪ |Xij| induces large bias between Qλj,2b(·) and Qλj(·) (and hence between our 2-bit estimator and its unbiased multi-bit counterpart), while λj ≫ |Xij| also results in slow concentration (e.g., reﬂected by ∥ξ∥ψ2 = O(λ) in Lemma 1). Fortunately, as we will see, the tightest unbiased choices λj,min = max 1≤i≤n |Xij|, j = 1, ..., d (3.1) are suﬃcient for closing the theoretical gap. Additionally, such λj,min is speciﬁed by the given data, so (3.1) also completely addresses the tuning issue. Now we are ready to precisely propose our estimator. Let the triangular dithers {τi}n i=1 iid ∼ U [− 1 2, 1 2]d + U [− 1 2, 1 2]d be independent of {Xi}n i=1, Λ = diag(λ1,min, ..., λd,min) where λj,min is given in (3.1), then we deﬁne QΛ,2b(·) := (Qλ1,min,2b(·), ..., Qλd,min,2b(·))⊤, which element-wisely quantizes Xi to ˙Xi = QΛ,2b(Xi + Λτi) : = � Qλ1,min,2b(Xi1 + λ1,minτi1), ..., Qλd,min,2b(Xid + λd,minτid) �⊤ (i) = � Qλ1,min(Xi1 + λ1,minτi1), ..., Qλd,min(Xid + λd,minτid) �⊤ ; (3.2) note that (i) follows from (2.4) and (3.1). Then, as will be shown in the proof of Theorem 5 by 10Lemma 1, we have E( ˙Xi ˙X⊤ i ) = Σ + 1 4Λ2, hence we propose our main estimator as �Σ = 1 n n � i=1 ˙Xi ˙X⊤ i − 1 4Λ2, with ˙Xi = QΛ,2b(Xi + Λτi). (3.3) Our parimary theoretical result is given as below. When omitting dependence on K and some factors of log d, there is no diﬀerence between our (3.4) for �Σ and (1.4) for sample covariance. Theorem 5. Suppose that X1, ..., Xn are i.i.d. copies of the zero-mean K-sub-Gaussian random vector X ∈ Rd. For estimating Σ = E(XX⊤) with eﬀective rank r(Σ) = Tr(Σ) ∥Σ∥op , under the scaling of n = Ω( ˆK2 log d), max{n, K} = O(d10), the proposed estimator �Σ satisﬁes ∥ �Σ − Σ∥op ≲ K2∥Σ∥op   � r(Σ)(log d)2 n + r(Σ)(log d)2 n   (3.4) with probability at least 1 − 10d−10 − 10e−10r(Σ). Speciﬁcally, if assuming further the scaling n ≳ r(Σ)(log d)2, then ∥ �Σ − Σ∥op ≲ K2 � Tr(Σ)∥Σ∥op(log d)2 n . (3.5) Proof. The key challenge in the proof is that the actual dithering vectors Λτi's depend on {Xi}n i=1 through the quantities {λj,min}d j=1; thus, when simultaneously handling the randomness of {Xi, τi}n i=1, { ˙Xi ˙X⊤ i }n i=1 are no longer i.i.d. matrices but correlated in a rather delicate man- ner, making the simple arguments in Theorem 3 invalid. To get started, we must decompose the quantized sample into ˙Xi = Xi + ξi, and recall that the properties of the quantization noise ξi are developed in Lemma 1. This allows us to divide the estimation error into three pieces as in (4.2). To bound each piece separately, we often need to ﬁrst deal with the randomness of {τi}n i=1 by conditioning on {Xi}n i=1 to render "independence", and then deal with {Xi}n i=1 that are again independent. In addition, we need to carefully take the technical tools of Theorem 2 or matrix Bernstein's inequality (Lemma 2) to allow for a sharp rate depending on the eﬀective rank r(Σ) instead of d (note that some other approaches like covering argument would be insuﬃcient). It is also worth mentioning that we put a bit eﬀorts on reﬁning the dependence on sub-Gaussian parameter K via the recent results of [21]. The complete proof is provided in Section 4. 3.1 Numerical Simulations In our simulations, we compare the operator norm errors of �Σna in (1.2), our new 2-bit estimator �Σ2b in (2.6), the sample covariance �Σ based on full data, and our parameter-free estimator �Σ in (3.3). Recall that in �Σ, our choice of dithering scales in (3.1) is unbiased and do not balance the bias and concentration. Though this is suﬃcient for closing the theoretical error rate gap, numerically it may be preferable to slightly lower the dithering scales to allow for a better trade-oﬀ between bias and variance. To validate this over Gaussian samples, let we shrink the dithering scale by a 11commonly factor s and consider the estimator �Σ = 1 n n � i=1 ˙Xi ˙X⊤ i − 1 4(sΛ)2, with ˙Xi = QsΛ,2b(Xi + sΛτi), (3.6) and we include �Σ(0.9), �Σ(0.7), �Σ(0.5) for comparison. Note that �Σ(1) = �Σ. The non-adaptive estimators �Σna, �Σ2b require a tuning parameter λ = C√log n for some C depending on (K, ∥Σ∥∞), and we provide the (near) optimal C numerically found in the case of (n, d) = (500, 10) (e.g., from Figure 1(a), under Σ(1, 0.2, 1) we take C = 0.7 for �Σna, C = 0.65 for �Σ2b), but note that it is hard to tune C to such extent in practice. In contrast, our key estimator �Σ does not require any parameter, and its more "balanced" version �Σ(s) is also user-friendly in that one can manually set s as a constant moderately less than 1 (e.g., s = 0.9, 0.7, 0.5 tested here). We will use Xi ∼ N(0, Σ) and obtain each data point from 50 repetitions. 200 400 600 800 1000 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 (a) 200 400 600 800 1000 0 1 2 3 4 5 6 (b) 200 400 600 800 1000 0 2 4 6 8 10 12 14 (c) 10 20 30 40 50 0 0.5 1 1.5 2 (d) 10 20 30 40 50 0 1 2 3 4 5 6 (e) 10 20 30 40 50 2 4 6 8 10 12 14 (f) Figure 2: The curves of "operator norm error v.s. n" and "operator norm error v.s. d". We ﬁrst ﬁx d = 10 and test n = 100 : 100 : 1000, with the results over Σ = Σ(1, 0.2, 1), Σ(1, 0.2, 10), Σ(1, 0.2, 25) reported in Figure 2(a)-(c), respectively. For Σ = Σ(1, 0.2, 1) that represents the case of Tr(Σ) = d∥Σ∥∞, in which (1.3), (2.7) and (3.4) almost coincide, �Σna, �Σ2b with optimal C achieves smaller errors than �Σ; while as explained, the dithering scales (3.1) may be (slightly) numerically sub-optimal and proper shrinkage could further lower the errors of �Σ; particularly, �Σ(0.5) performs comparably to �Σ2b with the best C. 12To corroborate our theoretical achievement, of more interest are the cases of Σ = Σ(1, 0.2, 10), Σ(1, 0.2, 25) where Tr(Σ) ≪ d∥Σ∥∞ and hence (3.4) essentially improves on (1.3), (2.7). From Figure 2(b)-(c), �Σ already outperforms �Σna and performs only slightly worse than �Σ2b, although the latter two involve unrealistic tuning. By shrinking the dithering scales, �Σ(0.7) and �Σ(0.5) provide operator norm errors notably lower than �Σ2b, thus validating the beneﬁt of data-driven dithering whose scales diﬀer across entries. To compare with sample covariance based on full data, we use the unlabeled black dashed curve to double the sample covariance's errors. Remarkably, the curve of �Σ(0.5) are almost always below the black dashed one, indicating that by incorporating a proper shrinkage the proposed estimator oftentimes achieves operator norm errors no greater than twice of the errors of sample covariance, as highlighted in the abstract. Furthermore, we also test a ﬁxed n = 500 and the increasing ambient dimension d = 5 : 5 : 50 under the above three covariance matrices. The results with similar implications are provided in Figure 2(d)-(f). 4 The Proof of Main Theorem The most important ingredient in our analysis is the following matrix Bernstein's inequality. Lemma 2. (Matrix Bernstein's Inequality, e.g., [33, Thm. 6.1.1]). Let S1, S2, ..., Sn be independent, zero-mean, d1 × d2 random matrices such that ∥Si∥op ≤ L almost surely for all i. Let σ2 := max ���� n � i=1 E(SiS⊤ i ) ��� op, ��� n � i=1 E(S⊤ i Si) ��� op � , then for some absolute constant c, for any t ≥ 0 we have P ���� n � i=1 Si ��� op ≥ t � ≤ (d1 + d2) exp � −c min � t2 σ2 , t L �� . We will also use the concentration of the norm of a sub-Gaussian vector. Lemma 3. (Adapted from [21, Thm. 4.1]) Let a = (a1, ..., an) ∈ Rn be a random vector with independent sub-Gaussian coordinates ai that satisfy E(a2 i ) = 1. Let A := max1≤i≤n ∥ai∥ψ2 and ˆA = A√log A, then for some absolute constant C we have ��∥a∥2 − √n �� ψ2 ≤ C ˆA, which further implies the following for some C1: P ���∥a∥2 − √n �� ≥ t � ≤ 2 exp � −C1t2 ˆA2 � , ∀ t ≥ 0. The proof of Theorem 5. We only need to prove (3.4) because (3.5) is a direct outcome of (3.4). 13To get started, we prove a useful observation: ˙Xi = QΛ,2b(Xi + Λτi) = ΛQ1(Λ−1Xi + τi). (4.1) Note that the ﬁrst equality is from (3.2), and now let us prove the second equality. To this end, we start from the last line of (3.2) and calculate the j-th entry of ˙Xi as ˙Xij = Qλj,min(Xij + λj,minτij) = λj,min �� Xij + λj,minτij λj,min � + 1 2 � = λj,min �� Xij λj,min + τij � + 1 2 � = λj,min · Q1 � xij λj,min + τij � , then (4.1) follows by noting that the last quantity in the above equation is just the j-th entry of ΛQ1(Λ−1Xi +τi). To proceed, we deﬁne ξi = Q1(Λ−1Xi +τi)−Λ−1Xi as the quantization noise, which provides ˙Xi = ΛQ1(Λ−1Xi + τi) = Xi + Λξi and further leads to �Σ − Σ = 1 n n � i=1 �Xi + Λξi ��Xi + Λξi �⊤ − 1 4Λ2 − Σ = � 1 n n � i=1 XiX⊤ i − Σ � + � 1 n n � i=1 Λξiξ⊤ i Λ⊤ − 1 4Λ2� + 1 n n � i=1 � Xiξ⊤ i Λ + ΛξiX⊤ i � := I1 + I2 + I3. (4.2) Thus, by triangular inequality we have ∥ �Σ − Σ∥op ≤ ∥I1∥op + ∥I2∥op + ∥I3∥op. Step 1. Bounding ∥I1∥op Theorem 2 gives that for any u1 ≥ 0, with probability at least 1 − 3e−u1 it holds that ∥I1∥op ≤ C∥Σ∥op   � ˆK2[r(Σ) + u1] n + ˆK2[r(Σ) + u1] n   . (4.3) Setting u1 = 10r(Σ), we obtain that ∥I1∥op ≲ ∥Σ∥op   � ˆK2r(Σ) n + ˆK2r(Σ) n   (4.4) holds with probability at least 1 − 3e−10r(Σ). Step 2. Bounding ∥I2∥op 2.1) Conditioning on {Xi}n i=1 and using the randomness of {τi}n i=1 Since ξi = Q1(Λ−1Xi + τi) − Λ−1Xi, conditioning on {Xi}n i=1, ξi's are independent of each 14other, and Lemma 1 gives E(ξiξ⊤ i ) = 1 4Id. Hence, by letting ηi = Λξi, I2 can be written as I2 = 1 n n � i=1 (Λξi)(Λξi)⊤ − E[(Λξi)(Λξi)⊤] = 1 n n � i=1 ηiη⊤ i − E[ηiη⊤ i ]. By Lemma 1 we have ∥ξi∥ψ2 = O(λ), and thus ∥⟨ηi, v⟩∥ψ2 = ∥⟨ξi, Λv⟩∥ψ2 = O(∥Λv∥2). Combin- ing with ∥⟨ηi, v⟩∥2 L2 = ∥⟨ξi, Λv⟩∥2 L2 = 1 4∥Λv∥2 2, we know that ηi's are O(1)-sub-Gaussian under Deﬁnition 1. Thus, Theorem 2 implies that with probability at least 1 − 3e−u2, it holds that ∥I2∥op ≲ � Tr(Λ2)∥Λ∥2op + ∥Λ∥4opu2 n + Tr(Λ2) + ∥Λ2∥opu2 n . Setting u2 = 10r(Σ), we obtain that ∥I2∥op ≲ � Tr(Λ2)∥Λ∥2op + ∥Λ∥4opr(Σ) n + Tr(Λ2) + ∥Λ2∥opr(Σ) n (4.5) holds with probability at least 1 − 3e−10r(Σ). 2.2) Dealing with the randomness of {Xi}n i=1 Recall that Λ = diag(λ1,min, ..., λd,min) where λj,min = max1≤i≤n |Xij|. Since ∥Xij∥ψ2 ≤ K � E|Xij|2 = K �Σjj, for any u3 ≥ 0 we have P(|Xij| ≥ u3) ≤ 2 exp � − C4u2 3 K2Σjj � , then a union bound over i ∈ [n] gives P �λj,min ≥ u3 � ≤ 2n exp � − C4u2 3 K2Σjj � . Note that n = O(d10) implies log n = O(log d), we set u3 = C5K �Σjj log d with suﬃciently large C5 to obtain that, with probability at least 1 − 2d−11 we have λj,min ≤ C6K �Σjj log d. Further, a union bound over j ∈ [d] yields that P � λj,min ≤ C6K � Σjj log d, ∀j ∈ [d] � ≥ 1 − 2d−10. (4.6) On this high-probability event, it immediately follows that ∥Λ∥op = max 1≤j≤d λj,min ≤ C6K max 1≤j≤n � Σjj log d = C6K � ∥Σ∥∞ log d, (4.7) Tr(Λ2) = d � j=1 λ2 j,min ≤ C2 6K2 d � j=1 Σjj log d = C2 6K2 Tr(Σ) log d. (4.8) 15Substituting (4.7), (4.8) into (4.5) yields that, with the promised probability, ∥I2∥op ≤ C7K2 log d   � Tr(Σ)∥Σ∥∞ n + Tr(Σ) n   . (4.9) Step 3. Bounding ∥I3∥op It suﬃces to bound �� 1 n �n i=1 Xiξ⊤ i Λ �� op since ∥I3∥op ≤ 2 �� 1 n �n i=1 Xiξ⊤ i Λ �� op. We utilize matrix Bernstein's inequality for this purpose. 3.1) Conditioning on {Xi}n i=1 and using the randomness of {τi}n i=1 Given {Xi : i ∈ [n]}, ξi's are independent, and E(ξi) = 0 implies E(Xiξ⊤ i Λ) = 0. Our goal is to bound the operator norm of the following independent sum of zero-mean matrices: ��� n � i=1 Wi ��� op, where Wi := 1 nXiξ⊤ i Λ. (4.10) Since ∥ξi∥∞ ≤ 3 2, we have ∥Λξi∥2 2 ≤ 9 4 Tr(Λ2), which yields ∥Wi∥op ≤ 1 n∥Xi∥2∥Λξi∥2 ≤ 3 � Tr(Λ2) 2n max 1≤i≤n ∥Xi∥2 := L. (4.11) Moreover, we estimate the matrix variance statistic. First, note that n � i=1 E(WiW ⊤ i ) = n � i=1 E �∥Λξi∥2 2 � n2 XiX⊤ i ⪯ 9 Tr(Λ2) 4n2 n � i=1 XiX⊤ i , which gives �� �n i=1 E(WiW ⊤ i ) �� op ≤ 9 Tr(Λ2) 4n2 �� �n i=1 XiX⊤ i �� op. Second, we have n � i=1 E(W ⊤ i Wi) = n � i=1 ∥Xi∥2 2 n2 E �Λξiξ⊤ i Λ � = �n i=1 ∥Xi∥2 2 4n2 Λ2, which yields �� �n i=1 E(W ⊤ i Wi) �� op = �n i=1 ∥Xi∥2 2 4n2 ∥Λ∥2 op. Thus, recalling (4.11) and deﬁne σ2 := 9 4n2 � Tr(Λ2) · ��� n � i=1 XiX⊤ i ��� op + ∥Λ∥2 op · n � i=1 ∥Xi∥2 2 � . (4.12) we can invoke matrix Bernstein's inequality (Lemma 2) to obtain P ���� n � i=1 Wi ��� op ≥ u4 � ≤ 2d exp � −C8 min �u2 4 σ2 , u4 L �� for any u4 ≥ 0. Setting u4 = C9[σ√log d + L log d] with large enough C9 yields P ���� n � i=1 Wi ��� op ≤ C9 �σ � log d + L log d � � ≥ 1 − 2d−10. (4.13) 163.2) Dealing with the randomness of {Xi}n i=1 Note that σ and L depend on {Xi}, so we still need to bound them using the randomness of {Xi}. Recall that, on the high-probability event in (4.6), ∥Λ∥op and Tr(Λ2) can be bounded as in (4.7)-(4.8). Combining with max 1≤i≤n ∥Xi∥2 = max 1≤i≤n � � � � n � j=1 X2 ij ≤ � � � � n � j=1 max 1≤i≤n X2 ij = � Tr(Λ2), we have L ≤ 3 Tr(Λ2) 2n ≲ K2 Tr(Σ) log d n . (4.14) For bounding σ2 in (4.12) we still need to control ∥ �n i=1 XiX⊤ i ∥op and �n i=1 ∥Xi∥2 2. Recalling from (4.4) that we have �� 1 n �n i=1 XiX⊤ i − Σ �� op ≲ ∥Σ∥op �� ˆ K2r(Σ) n + ˆ K2r(Σ) n � with the promised probability, and note that this implies ��� n � i=1 XiX⊤ i ��� op ≤ ��� n � i=1 XiX⊤ i − nΣ ��� op + n∥Σ∥op ≲ �n + ˆK2r(Σ) �∥Σ∥op. (4.15) To bound �n i=1 ∥Xi∥2 2, recall that Σjj = E(X2 ij) = ∥Xij∥2 L2, so we can write it as n � i=1 ∥Xi∥2 2 = n � i=1 d � j=1 X2 ij = d � j=1 Σjj � n � i=1 � Xij ∥Xij∥L2 �2� = d � j=1 Σjj∥Yj∥2 2, (4.16) where in the last equality we let Yj = ∥Xij∥−1 L2 �X1j, ..., Xnj �⊤. Observe that {Yj}d j=1 are random vectors with independent sub-Gaussian coordinates (since the coordinates are from independent samples) satisfying �� Xij ∥Xij∥L2 �� ψ2 ≤ K, hence we can invoke Lemma 3 and a union bound over 1 ≤ j ≤ d to obtain that, for any u5 ≥ 0, P � max 1≤j≤d ���∥Yj∥2 − √n ��� ≥ u5 � ≤ 2d exp � −C10u2 5 ˆK2 � . We let u5 = C11 ˆK√log d with suﬃciently large C11, then we obtain max1≤j≤d ��∥Yj∥2 − √n �� = O( ˆK√log d) with probability at least 1 − 2d−10, which implies max1≤j≤d ∥Yj∥2 ≲ √n under the scaling of n ≳ ˆK2 log d. Substituting this into (4.16), we arrive at n � i=1 ∥Xi∥2 2 ≤ d � j=1 ΣjjC12n = C12n · Tr(Σ). (4.17) Then, we combine (4.7), (4.8), (4.15), (4.17) to bound σ2 in (4.12) as σ2 ≲ K2∥Σ∥op Tr(Σ) log d n + K2 ˆK2[Tr(Σ)]2 log d n2 . (4.18) 17Further, substituting (4.14), (4.18) into (4.13), we obtain that ∥I3∥op ≤ 2 ��� n � i=1 Wi ��� op ≲ � K2∥Σ∥op Tr(Σ)(log d)2 n + K2 Tr(Σ)(log d)2 n (4.19) holds with probability at least 1 − 2d−10. Step 4. Combining Everything. Recall that ∥ �Σ−Σ∥op ≤ ∥I1∥op +∥I2∥op +∥I3∥op, by combining the bounds for I1, I2, I3 derived in (4.4), (4.9), (4.19), we obtain that ∥ �Σ − Σ∥op ≲ K2   � Tr(Σ)∥Σ∥op(log d)2 n + Tr(Σ)(log d)2 n   holds with the promised probability. Recalling Tr(Σ) = r(Σ)∥Σ∥op, the result follows. ■ 5 Deferred Proofs 5.1 The Proof of Theorem 2 The proof is adjusted from [36, Thm. 9.2.4]. The key technical tool is the matrix deviation inequality with optimal dependence on K developed in [21]. Lemma 4. (Corollary 1.2 in [21]) Consider A = [a1, ..., an]⊤ ∈ Rn×d with independent isotropic rows ai's satisfying max1≤i≤n ∥ai∥ψ2 ≤ K and a bounded subset T ⊂ R with radius rad(T ) := supx∈T ∥x∥2, Gaussian width ω(T ) := E supx∈T ⟨g, x⟩ for g ∼ N(0, Id). Let ˆK = K√log K, then for any t ≥ 0, with probability at least 1 − 3e−t2 we have sup x∈T ��∥Ax∥2 − √n∥x∥2 �� ≤ C ˆK �ω(T ) + t · rad(T ) �. The proof of Theorem 2. We ﬁrst consider the case of rank(Σ) = d and let Yi = Σ− 1 2Xi. Note that Yi is isotropic since E(YiY ⊤ i ) = E(Σ− 1 2 XiX⊤ i Σ− 1 2 ) = Id, and further, its ψ2 norm is bounded by K because ∥Yi∥ψ2 = sup a∈Sd−1 ∥⟨a, Σ− 1 2 Xi⟩∥ψ2 = sup a∈Sd−1 ∥⟨Σ− 1 2 a, Xi⟩∥ψ2 ≤ sup a∈Sd−1 K∥⟨Σ− 1 2 a, Xi⟩∥L2 = sup a∈Sd−1 K∥⟨a, Yi⟩∥L2 = K. 18Then, we let Y = [Y1, ..., Yn]⊤ ∈ Rn×d and can write the operator norm error as ����� 1 n n � i=1 XiX⊤ i − Σ ����� op = sup a∈Sd−1 �����a⊤ � 1 n n � i=1 XiX⊤ i − Σ � a ����� = sup a∈Sd−1 1 n ����� n � i=1 ���Yi, Σ 1 2a ���2 − n ��Σ 1 2 a ��2 2 ����� = sup b∈Σ 1 2 Sd−1 1 n ���∥Y b∥2 2 − n∥b∥2 2 ��� ≤ sup b∈Σ 1 2 Sd−1 1 n ���∥Y b∥2 − √n∥b∥2 ��� ���∥Y b∥2 + √n∥b∥2 ���. (5.1) Now we invoke Lemma 4 to obtain that, for any t ≥ 0, with probability at least 1 − 3e−t2 sup b∈Σ 1 2 Sd−1 ���∥Y b∥2 − √n∥b∥2 ��� ≤ C ˆK � ω(Σ 1 2 Sd−1) + t · rad(Σ 1 2 Sd−1) � . (5.2) Note that rad(Σ 1 2 Sd−1) ≤ ∥Σ∥ 1 2op, and let g ∼ N(0, Id) we have ω �Σ 1 2 Sd−1� = E sup a∈Sd−1⟨g, Σ 1 2a⟩ = E ��Σ 1 2 g �� 2 ≤ � E∥Σ 1 2g∥2 2 = � Tr(Σ). Thus, the right-hand side of (5.2) is bounded by C ˆK �� Tr(Σ) + t � ∥Σ∥op �, and it further implies sup b∈Σ 1 2 Sd−1 ���∥Y b∥2 + √n∥b∥2 ��� ≤ sup b∈Σ 1 2 Sd−1 ���∥Y b∥2 − √n∥b∥2 ��� + 2√n rad �Σ 1 2 Sd−1� ≤ C ˆK �� Tr(Σ) + t � ∥Σ∥op � + 2 � n∥Σ∥op. Substituting these into (5.1) and let t = √u, some algebra immediately yields the desired bound. All that remains is to deal with the case when rank(Σ) := r < d. Let the singular value decomposition be Σ = UΣ0U ⊤ for some U ∈ Rd×r with orthonormal columns and positive deﬁnite diagonal Σ0 ∈ Rr×r. Then we construct the isotropic and K-sub-Gaussian Yi = Σ − 1 0 2 U ⊤Xi. Note that E∥(In − UU ⊤)Xi∥2 2 = (In − UU ⊤)UΣ0U ⊤(In − UU ⊤) = 0, hence almost surely we have Xi = UU ⊤Xi = UΣ 1 2 0 Yi. Thus, we let Xi = U0Yi with U0 = UΣ 1 2 0 satisfying rad(U ⊤ 0 Sd−1) ≤ ∥Σ∥1/2 op and ω(U ⊤ 0 Sd−1) ≤ � Tr(Σ), hence we can plug this into the ﬁrst line of (5.1) and run the mechanism again. ■ 5.2 The Proof of Lemma 1 To be self-contained, we give a proof for Lemma 1. Our proof is based on the following result from [17], and similar materials can be found in [2, Sec. 2.2]. Theorem 6. (Adapted from [17, Thms. 1-2]) We let a ∈ R, τ be some random dither and quantize a to Qλ(a + τ), then we deﬁne quantization error as w = Qλ(a + τ) − (a + τ), and quantization 19noise as ξ = Qλ(a + τ) − a. We assume that τ ′ ∼ U [− λ 2, λ 2] is independent of τ. Denote by ı the complex unit, if f(u) = E(exp(ıuτ)) satisﬁes f(2πl λ ) = 0 for any non-zero integer l, then for any a we have w ∼ U [− λ 2 , λ 2]; if g(u) = E(exp(ıuτ))E(exp(ıuτ ′)) satisﬁes g′′(2πl λ ) = 0 for any non-zero integer l, then for any a ∈ R we have E(ξ2) = E(τ + τ ′)2. Now we are ready to prove Lemma 1. Proof of Lemma 1. Observe that supx∈R |Qλ(x) − x| = λ 2, so the boundedness follows from ∥ξ∥∞ ≤ ∥Qλ(X + τ) − (X + τ)∥∞ + ∥τ∥∞ ≤ λ 2 + λ = 3λ 2 . For E[ξ] = 0 and E[ξξ⊤] = λ2 4 Id, it suﬃces to show that they hold when conditioning on X since we have E(·) = EX Eτ|X(·), thus we assume X ∈ Rd is ﬁxed. We let w = Qλ(X + τ) − (X + τ), which gives ξ = τ + w. We now verify that a triangular dither τ = τ1 + τ2 with independent τ1, τ2 ∼ U [− λ 2 , λ 2] satisﬁes both conditions in Theorem 6. First, we calculate that f(u) = E(exp(ıuτ)) = E(exp(ıuτ1) exp(ıuτ2)) = [E(exp(ıuτ1))]2, and further E(exp(ıuτ1)) = � λ/2 −λ/2 λ−1[cos(uτ1) + ı sin(uτ1)]dτ1 = 2 λu sin λu 2 , which vanishes at u = 2πl λ for non-zero integer l. Thus, when given X, Theorem 6 implies that entries of w follow U [− λ 2 , λ 2], which leads to E(ξ|X) = E(τ|X) + E(w|X) = 0. Second, we let τ ′ ∼ U [− λ 2, λ 2] be independent of {τ1, τ2}, then g(u) = f(u)E(exp(ıuτ ′)) = ( 2 λu sin λu 2 )3 satisﬁes g′′(2πl λ ) = 0 for any non-zero integer l. Thus, let ξi be the i-th entry of ξ, when given X Theorem 6 implies that E(ξ2 i |X) = E(τ + τ ′)2 = E(τ)2 + E(τ ′)2 = λ2 4 . For non-diagonal entries, given X, ξi and ξj are independent if i ̸= j, and so E(ξiξj|X) = E(ξi|X)E(ξj|X) = 0. Overall, we have shown that E(ξξ⊤|X) = λ2 4 Id. Finally, we prove ∥ξ∥ψ2 = O(λ) by using ∥X∥ψ2 ≍ supp≥1 [ E|X|p]1/p √p (e.g., [36, Prop. 2.5.2]). Conditioning on X, we have shown that ∥ξ∥∞ ≤ 3λ 2 , E(ξ|X) = 0. Combined with the fact that ξ have independent entries, we have ∥ξ|X∥ψ2 = O(λ) [36, Lem. 3.4.2]. Thus, we have Eξ|X|⟨v, ξ⟩|p ≤ (Cλ√p)p for some absolute constant C and for any v ∈ Sd−1; further averaging over X, we obtain supv∈Sd−1 E|⟨v, ξ⟩|p ≤ (Cλ√p)p, which leads to ∥ξ∥ψ2 = supv∈Sd−1 ∥⟨v, ξ⟩∥ψ2 = O(λ). The proof is complete. ■ 206 Conclusions and Discussions For sub-Gaussian distributions, we proposed in this paper a 2-bit covariance estimator �Σ (3.3) that possesses near optimal operator norm error rate (up to logarithmic factors), as can be clearly seen by comparing our main result (Theorem 5) with Theorem 2. This improves on the estimators in [9, 10], which are essentially sub-optimal over Σ satisfying Tr(Σ) ≪ d∥Σ∥∞. More practically, our second signiﬁcant advantage is that our estimator is free of any tuning parameter, unlike �Σna (1.2) in [10] that requires tuning according to some unknown parameters. As signiﬁcant departures from [9,10] who used uniformly dithered 1-bit quantizer, our quanti- zation procedure is motivated by [2] and based on the triangular dither and a 2-bit quantizer (2.5). This leads to a new 2-bit estimator �Σ2b (2.6) comparable to �Σna in several respects (Section 2), and further, our key estimator �Σ is obtained from �Σ2b by using dithering scales varying across entries and determined by the data (Section 3). Our simulations demonstrate for Gaussian samples the following: (i) �Σ2b numerically outper- forms �Σna quite notably, (ii) our parameter-free �Σ outperforms �Σna, �Σ2b with optimally tuned parameter in the case of Tr(Σ) ≪ d∥Σ∥∞, (iii) the performance of �Σ can be further improved by properly shrinking the dithering scales in (3.1). For (iii), it is remarkable that �Σ(0.5) (deﬁned in (3.6)) oftentimes achieves operator norm errors less than twice of the errors of sample covariance. We close this paper by some discussions. While the numerical improvement of our �Σ2b over �Σna is quite notable over Gaussian samples (Figure 2), this is in general not theoretically provable since it may not be true for certain distributions. As an extreme example,5 suppose that entries of Xi are ±1, then �Σna in (1.2) with τi1, τi2 ∼ U [−1, 1] (i.e., λ = 1) simply reduces to sample covariance, meaning that there is no information loss in the quantization. Thus, our �Σ2b is expected to behave (at least slightly) worse than �Σna. Therefore, �Σna and �Σ2b correspond to two parallel dithered quantization methods that allow for covariance estimation, with the former applying a uniformly dithered sign quantizer twice, while the latter being the 2-bit quantizer in (2.5) associated with a triangular dither; it may be a rushed judgment to declare one superior to the other. Built upon �Σ2b, our major contribution is to devise �Σ that enjoys near optimal operator norm rate and requires no tuning, which is done by using dithering scales in (3.1). It is interesting to note that this development straightforwardly carries over to �Σna. In particular, let τi1, τi2 ∼ U [−1, 1]d and Λ = diag(λ1,min, ..., λd,min) where λj,min is given in (3.1), we consider the following parameter-free estimator �Σ′ = 1 2n n � i=1 ( ˙Xi1 ˙X⊤ i2 + ˙Xi2 ˙X⊤ i1), with    ˙Xi1 = Λ · sgn(Xi + Λτi1) ˙Xi2 = Λ · sgn(Xi + Λτi2) . Then, using techniques provided in the proof of Theorem 5, one can show that �Σ′ possesses a near optimal operator norm error rate comparable to �Σ. Additionally, we expect that properly shrinking 5It is extreme in the sense that there is no quantization need. 21the dithering scales like (3.6) also improves the numerical performance of �Σ′ over Gaussian samples. References [1] F. Chapeau-Blondeau, S. Blanchard, and D. Rousseau, Fisher information and noise- aided power estimation from one-bit quantizers, Digital Signal Processing, 18 (2008), pp. 434– 443. [2] J. Chen, M. K. Ng, and D. Wang, Quantizing heavy-tailed data in statistical estima- tion: (near) minimax rates, covariate quantization, and uniform recovery, arXiv preprint arXiv:2212.14562, (2022). [3] J. Chen, C.-L. Wang, M. K. Ng, and D. Wang, High dimensional statistical estimation under uniformly dithered one-bit quantization, IEEE Transactions on Information Theory, 69 (2023), pp. 5151–5187. [4] J. Chen, Y. Wang, and M. K. Ng, Quantized low-rank multivariate regression with random dithering, arXiv preprint arXiv:2302.11197, (2023). [5] J. Choi, J. Mo, and R. W. Heath, Near maximum-likelihood detector and channel esti- mator for uplink multiuser massive mimo systems with one-bit adcs, IEEE Transactions on Communications, 64 (2016), pp. 2005–2018. [6] A. Danaee, R. C. de Lamare, and V. H. Nascimento, Distributed quantization-aware rls learning with bias compensation and coarsely quantized signals, IEEE Transactions on Signal Processing, 70 (2022), pp. 3441–3455. [7] S. Dirksen, Quantized compressed sensing: a survey, in Compressed Sensing and Its Appli- cations: Third International MATHEON Conference 2017, Springer, 2019, pp. 67–95. [8] S. Dirksen and J. Maly, Supplement to "covariance estimation under one-bit quantization", (2022), https://doi.org/10.1214/22-AOS2239SUPP. [9] S. Dirksen and J. Maly, Tuning-free one-bit covariance estimation using data-driven dither- ing, arXiv preprint arXiv:2307.12613, (2023). [10] S. Dirksen, J. Maly, and H. Rauhut, Covariance estimation under one-bit quantization, The Annals of Statistics, 50 (2022), pp. 3538–3562. [11] S. Dirksen and S. Mendelson, Non-gaussian hyperplane tessellations and robust one-bit compressed sensing, Journal of the European Mathematical Society, 23 (2021), pp. 2913–2947. [12] A. Eamaz, F. Yeganegi, and M. Soltanalian, Modiﬁed arcsine law for one-bit sam- pled stationary signals with time-varying thresholds, in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2021, pp. 5459–5463. 22[13] A. Eamaz, F. Yeganegi, and M. Soltanalian, Covariance recovery for one-bit sampled non-stationary signals with time-varying sampling thresholds, IEEE Transactions on Signal Processing, 70 (2022), pp. 5222–5236. [14] A. Eamaz, F. Yeganegi, and M. Soltanalian, Covariance recovery for one-bit sam- pled stationary signals with time-varying sampling thresholds, Signal Processing, 206 (2023), p. 108899. [15] J. Fang and H. Li, Adaptive distributed estimation of signal power from one-bit quantized data, IEEE Transactions on Aerospace and Electronic Systems, 46 (2010), pp. 1893–1905. [16] D. A. Freedman, Statistical models: theory and practice, cambridge university press, 2009. [17] R. M. Gray and T. G. Stockham, Dithered quantizers, IEEE Transactions on Information Theory, 39 (1993), pp. 805–812. [18] O. A. Hanna, Y. H. Ezzeldin, C. Fragouli, and S. Diggavi, Quantization of distributed data for learning, IEEE Journal on Selected Areas in Information Theory, 2 (2021), pp. 987– 1001. [19] G. Jacovitti and A. Neri, Estimation of the autocorrelation function of complex gaussian stationary processes by amplitude clipped signals, IEEE transactions on information theory, 40 (1994), pp. 239–245. [20] N. Jayant and L. Rabiner, The application of dither to the quantization of speech signals, Bell System Technical Journal, 51 (1972), pp. 1293–1304. [21] H. Jeong, X. Li, Y. Plan, and O. Yilmaz, Sub-gaussian matrices on sets: Optimal tail dependence and applications, Communications on Pure and Applied Mathematics, 75 (2022), pp. 1713–1754. [22] I. T. Jolliffe, Principal component analysis for special types of data, Springer, 2002. [23] H. C. Jung, J. Maly, L. Palzer, and A. Stollenwerk, Quantized compressed sensing by rectiﬁed linear units, IEEE transactions on information theory, 67 (2021), pp. 4125–4149. [24] V. Koltchinskii and K. Lounici, Concentration inequalities and moment bounds for sample covariance operators, Bernoulli, (2017), pp. 110–133. [25] O. Ledoit and M. Wolf, Improved estimation of the covariance matrix of stock returns with an application to portfolio selection, Journal of empirical ﬁnance, 10 (2003), pp. 603–621. [26] J. Limb, Design of dither waveforms for quantized visual signals, The Bell System Technical Journal, 48 (1969), pp. 2555–2582. [27] K. Lounici, High-dimensional covariance matrix estimation with missing observations, Bernoulli, 20 (2014), pp. 1029 – 1058. 23[28] J. Maly, T. Yang, S. Dirksen, H. Rauhut, and G. Caire, New challenges in covariance estimation: multiple structures and coarse quantization, in Compressed Sensing in Information Processing, Springer, 2022, pp. 77–104. [29] L. Roberts, Picture coding using pseudo-random noise, IRE Transactions on Information Theory, 8 (1962), pp. 145–154. [30] L. Schuchman, Dither signals and their eﬀect on quantization noise, IEEE Transactions on Communication Technology, 12 (1964), pp. 162–165. [31] Z. Sun, W. Cui, and Y. Liu, Quantized corrupted sensing with random dithering, IEEE Transactions on Signal Processing, 70 (2022), pp. 600–615. [32] C. Thrampoulidis and A. S. Rawat, The generalized lasso for sub-gaussian measurements with dithered quantization, IEEE Transactions on Information Theory, 66 (2020), pp. 2487– 2500. [33] J. A. Tropp et al., An introduction to matrix concentration inequalities, Foundations and Trends® in Machine Learning, 8 (2015), pp. 1–230. [34] J. H. Van Vleck and D. Middleton, The spectrum of clipped noise, Proceedings of the IEEE, 54 (1966), pp. 2–19. [35] R. Vershynin, Introduction to the non-asymptotic analysis of random matrices, Cambridge University Press, 2012, p. 210–268. [36] R. Vershynin, High-dimensional probability: An introduction with applications in data sci- ence, vol. 47, Cambridge university press, 2018. [37] C. Xu and L. Jacques, Quantized compressive sensing with rip matrices: The beneﬁt of dithering, Information and Inference: A Journal of the IMA, 9 (2020), pp. 543–586. [38] T. Yang, J. Maly, S. Dirksen, and G. Caire, Plug-in channel estimation with dithered quantized signals in spatially non-stationary massive mimo systems, arXiv preprint arXiv:2301.04641, (2023). [39] H. Zhang, J. Li, K. Kara, D. Alistarh, J. Liu, and C. Zhang, Zipml: Training lin- ear models with end-to-end low precision, and a little bit of deep learning, in International Conference on Machine Learning, PMLR, 2017, pp. 4035–4043. 24
PAVI: Plate-Amortized Variational Inference Louis Rouillard louis.rouillard-odera@inria.fr Université Paris-Saclay, Inria, CEA Palaiseau, 91120, France Alexandre Le Bris alexandre.le-bris@inria.fr Université Paris-Saclay, Inria, CEA Palaiseau, 91120, France Thomas Moreau thomas.moreau@inria.fr Université Paris-Saclay, Inria, CEA Palaiseau, 91120, France Demian Wassermann demian.wassermann@inria.fr Université Paris-Saclay, Inria, CEA Palaiseau, 91120, France Abstract Given observed data and a probabilistic generative model, Bayesian inference searches for the distribution of the model’s parameters that could have yielded the data. Inference is challenging for large population studies where millions of measurements are performed over a cohort of hundreds of subjects, resulting in a massive parameter space. This large cardi- nality renders off-the-shelf Variational Inference (VI) computationally impractical. In this work, we design structured VI families that efficiently tackle large population studies. Our main idea is to share the parameterization and learning across the different i.i.d. variables in a generative model, symbolized by the model’s plates. We name this concept plate amortiza- tion. Contrary to off-the-shelf stochastic VI, which slows down inference, plate amortization results in orders of magnitude faster to train variational distributions. Applied to large-scale hierarchical problems, PAVI yields expressive, parsimoniously parameterized VI with an af- fordable training time. This faster convergence effectively unlocks inference in those large regimes. We illustrate the practical utility of PAVI through a challenging Neuroimaging example featuring 400 million latent parameters, demonstrating a significant step towards scalable and expressive Variational Inference. 1 Introduction Inference in graphical models allows to explain data in an interpretable and uncertainty-aware manner (Koller & Friedman, 2009; Gelman et al., 2004; Zhang et al., 2019). Such graphical models are leveraged in Neu- roimaging to capture complex phenomena, for instance, the localization of cognitive function across the human brain cortex (Thomas Yeo et al., 2011; Kong et al., 2019). Yet, current inference methods struggle with the high dimensionality of Neuroimaging applications, which can feature hundreds of millions of latent parameters. This work unlocks inference for those large-scale applications via novel scalable VI methodolo- gies. Our target applications are population studies. Population studies analyze measurements over large cohorts of subjects. They allow to model population parameters —for instance, the mean weight in a population and different groups of subjects— along with individual variability —the variation of each subject’s weight compared to the group weight. These studies are ubiquitous in health care (Fayaz et al., 2016; Towsley 1 arXiv:2308.16022v1 [stat.ML] 30 Aug 2023et al., 2011), and can typically involve hundreds of subjects and individual measurements per subject. For instance, in Neuroimaging (e.g. Kong et al., 2019), measurements X can correspond to signals in thousands of locations in the brain for a thousand subjects. Given this observed data X, and a generative model that can produce X given model parameters Θ, we want to recover the parameters Θ that could have yielded the observed X. In our Neuroimaging example, Θ can be local labels for each brain location and subject, together with global parameters common to all subjects, such as the connectivity fingerprint corresponding to each label. In many real-world applications, the link between latent parameters and observed data is not deterministic. For instance, the observed signal can be noisy, or the modeled process in itself may be stochastic (Koller & Friedman, 2009). As such, several parameters Θ instances could yield the same data X. We want to model that parameter uncertainty by recovering the distribution of the Θ that could have produced X. Following the Bayesian formalism (Gelman et al., 2004), we cast both Θ and X as random variables (RVs), and our goal is to recover the posterior distribution p(Θ|X). Due to the nested structure of our applications, we focus on the case where p corresponds to a hierarchical Bayesian model (HBM) (Gelman et al., 2004). In population studies, the multitude of subjects and measurements per subject implies a large dimensionality for both Θ and X. This large dimensionality in turn creates computational hurdles that we tackle through our method. Several inference methods for HBMs exist in the literature. Earliest works resorted to Markov chain Monte Carlo (Koller & Friedman, 2009), which become slower as dimensionality increases (Blei et al., 2017). Recent approaches coined Variational Inference (VI) cast the inference as an optimization problem (Blei et al., 2017; Zhang et al., 2019; Ruiz & Titsias, 2019). In VI, inference reduces to choosing a variational family Q and finding inside that family the distribution q(Θ; ϕ) ∈ Q closest to the unknown posterior p(Θ|X). Historically, VI required to manually derive and optimize over Q, which remains an effective method where applicable (Dao et al., 2021). This manual VI requires technical mastery. The experimenter has to choose an appropriate family Q to approximate the unknown posterior closely. At the same time, the experimenter must rely on properties such as conjugacy to derive an optimizing routine (Dao et al., 2021; Kucukelbir et al., 2016). This means that time and effort must be spent not only on the modeling part —laying out hypothesis in a generative model— but also on the inference part —inferring parameters given the generative model. For many experimenters, manual VI thus features strong barriers to entry, both in time and technical skill (Kucukelbir et al., 2016). In contrast, we follow the idea of automatic VI: deriving an efficient family Q directly from the HBM p. Automatic VI reduces inference to the modeling part only, simplifying the experimenter’s research cycle (Kucukelbir et al., 2016; Ambrogioni et al., 2021a;b). We propose an automatic VI method computationally applicable to large population studies. Our method relies on the pervasive idea of amortization in VI. Amortization can be broadly defined as finding posteriors usable across multiple data points (Zhang et al., 2019). As such, amortization can be interpreted as the meta-learning of the inference problem (Ravi & Beatson, 2019; Iakovleva et al., 2020; Yao et al., 2019). A particular example of meta-learning is Neural Processes, which share with our method the conditioning of a density estimator by the output of a permutation-invariant encoder (Garnelo et al., 2018; Dubois et al., 2020; Zaheer et al., 2018). Though close in spirit to our work, meta-learning studies problems with a single hierarchy. One-hierarchy cases correspond to models distinguishing local parameters, representing the subjects, from global parameters, representing the population (Ravi & Beatson, 2019; Tran et al., 2017). As a notable example, Agrawal & Domke (2021) provide theoretical guarantees in the single- hierarchy case. In contrast, our focus is rather computational. We furthermore study generic HBMs with an arbitrary number of hierarchies —population, group and subject— to tackle large population studies efficiently. Modern VI is effective in low-dimensional settings but does not scale up to large population studies, which can involve millions of random variables (e.g. Kong et al., 2019). In this work, we identify and tackle two challenges to enable this scale-up. A first challenge with scalability is a detrimental trade-off between expressivity and high-dimensionality (Rouillard & Wassermann, 2022). To reduce the inference gap, VI requires the variational family Q to contain distributions closely approximating p(Θ|X) (Blei et al., 2017). Yet the form of p(Θ|X) is usually unknown to the experimenter. Instead of a lengthy search for a valid family, one can resort to universal density approximators: normalizing flows (Papamakarios et al., 2019). But the cost for this generality is a heavy parameterization, and normalizing flows scale poorly with the 2dimensionality of Θ. As a result, in large population studies, the parameterization of normalizing flows becomes prohibitive. To tackle this challenge, Rouillard & Wassermann (2022) recently proposed, via the ADAVI architecture, to partially share the parameterization of normalizing flows across the hierarchies of a generative model. We build upon this shared parameterization idea while improving the ADAVI architecture on several aspects: removing the mean-field approximation; extending from pyramidal to arbitrary graphs; generalizing to non-sample-amortized and stochastically trained schemes. Critically, while ADAVI tackled the over-parameterization of VI in population studies, it still could not perform inference in large data regimes. This is due to a second challenge with scalability: as the size of Θ increases, evaluating a single gradient over the entirety of an architecture’s weights quickly requires too much memory and computation. This second challenge can be overcome using stochastic VI (SVI, Hoffman et al., 2013), which subsamples the parameters Θ inferred for at each optimization step. However, using SVI, the weights for the posterior for a local parameter θ ∈ Θ are only updated when the algorithm visits θ. In the presence of hundreds of thousands of such local parameters, stochastic VI can become prohibitively slow. This work introduces the concept of plate amortization (PAVI) for fast inference in large-scale HBMs. Instead of considering the inference over local parameters θ as separate problems, our main idea is to share both the parameterization and learning across those local parameters, or equivalently across a model’s plates. We first propose an algorithm to automatically derive an expressive yet parsimoniously-parameterized variational family from a plate-enriched HBM. We then propose a hierarchical stochastic optimization scheme to train this architecture efficiently. PAVI leverages the repeated structure of plate-enriched HBMs via a novel combination of amortization and stochastic training. Through this combination, our main claim is to enable inference over arbitrarily large population studies with reduced parameterization and training time as the cardinality of the problem augments. Critically, while traditional Stochastic VI unlocked large-scale inference at the cost of slower convergence, PAVI does not feature such a detrimental trade-off. On the contrary, PAVI converges orders of magnitude faster than non-stochastic VI in large regimes. In practice, this quicker training unlocks inference in applications previously unattainable, bringing inference time down from weeks to hours. We illustrate this by applying PAVI to a challenging human brain cortex parcellation, featuring inference over a cohort of 1000 subjects with tens of thousands of measurements per subject, for a total of half a billion RVs. This demonstrates a significant step towards scalable, expressive and fast VI. 2 Problem statement: inference in large population studies Here we introduce a motivating example for our work that we’ll use as an illustration in the rest of this section. The concepts introduced in this section will be more formally defined below in Section 3. Our objective is to perform inference in large population studies. As an example of how inference becomes impractical in this context, consider M in Figure 1 (top) as a model for the weight distribution in a pop- ulation. θ2,0 denotes the mean weight across the population. θ1,0, θ1,1, θ1,2 denote the mean weights for 3 groups of subjects, distributed around the population mean. X0, X1 represent the observed weights of 2 subjects from group 0, distributed around the group mean. Given the observed subject weights X, the goal is to determine the posterior distributions of the group and population means p(θ1,0, θ1,1, θ1,2, θ2,0|X). To infer using the VI framework, we choose a variational family Q and search inside this family for the distribution closest to our unknown distribution of interest: q(Θ) ≃ p(Θ|X). Applying automatic VI to the example in Figure 1, the variational family Q will oftentimes factorize to (Ambrogioni et al., 2021a;b): q(Θ; Φ) = q(θ2,0; ϕ2,0) 2 � n=0 q(θ1,n|θ2,0; ϕ1,n) (1) where Φ = {ϕ2,0, ϕ1,0, ..., ϕ1,3} represent the weights associated to the variational family Q and each factor in q will approximate the corresponding posterior distribution: as an example q(θ2,0; ϕ2,0) ≃ p(θ2,0|X). Compared to the mean-field factorization (Blei et al., 2017), in Equation (1) the posterior for the RVs θ1,n is conditional to the RV θ2,0 for greater expressivity. We will follow this dependency scheme in Section 4.1. During training, we will search for the optimal weights Φ to best approximate the posterior distribution p(Θ|X). 3Yet, as the number of groups and subjects per group augments, this inference problem becomes computa- tionally intractable. On the parameterization side, each additional group in the population study requires additional weights ϕ1,n. This constitutes a first computational hurdle: the total number of weights Φ in the variational family becomes prohibitively large. On the computing side, optimizing over an increasing num- ber of weights ϕ1,n also requires a growing amount of calculation. This constitutes a second computational hurdle. To circumvent this issue, one must resort to Stochastic VI and only optimize over a subset of the weights Φ at a given training step. In practice, this amounts to inferring the group weight of only a subset of the subject groups at a time. Yet, stochastic training means that the weights ϕ1,n corresponding to a given subject group will only be optimized for a fraction of the training time. Consequently, the larger the population study becomes, the slower the inference. Our goal is to keep inference computationally tractable as the cardinality of the problem augments. To keep the inference tractable, we will build upon the natural factorization of the problem into subjects and groups of subjects. This factorization is symbolized by a model’s plates as detailed in Section 3. First, we will harness this natural factorization in the parameterization of our variational family –to reduce its number of weights. This strategy, coined plate amortization, will be detailed in Section 4.1. Second, we will reflect this factorization in the design of a stochastic training scheme –to control the computing required during the training. This will be detailed in Section 4.2. Critically, our contribution does not lie only in adding those two items. Combining shared parametrization and stochastic training, PAVI yields orders- of-magnitude speedups in inference. Consequently, contrary to Stochastic VI, inference does not become prohibitively slow as the problem’s cardinality augments. In practice, this unlocks inference in very large population studies. We illustrate this speed-up in our experiments in Section 5 and the practical utility of our method in a challenging Neuroimaging setup in Section 5.4. 3 Generative modeling 3.1 hierarchical Bayesian models (HBMs), templates and plates Here we define more formally the inference problem and the associated notations. Population studies can be compactly represented via plate-enriched directed acyclic graph (DAG) templates T (plates are defined in chapter 8.1 in Bishop, 2006; Gilks et al., 1994; Koller & Friedman, 2009, describes plates and template variables in chapter 6.3). T feature RV templates that symbolise multiple similar ground RVs. As an example, the RV template θ1 in Figure 1 (left) represents a generic group of subjects, of which there would be as many instances as the cardinality of the plate P1. The plate structure in T also denotes that the multiple ground RVs corresponding to the same RV template are conditionally i.i.d. As an example, two different group weights could be two Gaussian perturbations of the same population weight. The two groups weight RVs thus have the same conditional distribution, and are independent given the population weight. The plate structure in T thus symbolizes a strong symmetry in the problem that we exploit in our method. We denote the template T ’s vertices, corresponding to RV templates, as X and Θ = {θi}i=1..I. X denotes the RVs observed during inference, and Θ the parameters we infer: our goal is to approximate the posterior distribution p(Θ|X). We denote T ’s plates as {Pp}p=0..P , and the plates θi belongs to as Plates(θi). I and P respectively denote the number of latent RV templates and plates in T , which are in general not equal. In the toy example from Figure 1, there are two latent RV templates: θ1 and θ2, respectively the group and population mean weights. T also features two plates P1, P0, which respectively denote groups in the population and the subjects in each group. Graphically, we can see that Plates(θ2) = ∅, whereas Plates(θ1) = {P1} and Plates(X) = {P0, P1}. To understand how we could have P ̸= I, one could keep the plates P1, P0, but add two more RV templates to symbolize the population and group heights, in addition to their weights. In this case, we would have P = 2 and I = 4. By instantiating the repeated structures symbolized by the plates P in T , we obtain a heavier graph repre- sentation: the hierarchical Bayesian model (HBM) M. This instantiation is visible in Figure 1, where we go from the template T (left) to the model M (top). In the context of Neuroimaging (Kong et al., 2019), M could feature millions of RVs, corresponding to thousands of subjects and measurements per subject, making 4A B C Reduced distribution stochastic training grounding grounding Full model Reduced model A B C D E A C B E D t=0 t=1 At optimization step t we optimize over: Using as observed RVs: Template automatic VI Figure 1: PAVI working principle: The template T (left) can be grounded into the full model M (top). We aim to perform inference over M. Yet, M can feature large cardinalities in population studies. As an example, instead of θ1,0, ..., θ1,2, M can feature θ1,0, ..., θ1,1000 –corresponding to a thousand different subject groups. This can make inference over M computationally intractable. To circumvent this issue, we will train over M stochastically. To this end, we instantiate M’s template into a smaller replica: the reduced model Mr (bottom left). Following the automatic VI framework, we derive the reduced distribution qr (bottom right) directly from Mr. The reduced distribution qr features 2 conditional normalizing flows F1 and F2 respectively associated to the RV templates θ1 and θ2. During the stochastic training (right), qr is instantiated over different branchings of the full model M –highlighted in blue. The branchings have Mr’s cardinalities and change at each stochastic training step t. The branching determine the encodings E conditioning the flows F –as symbolised by the letters A, B, C– and the observed data slice –as symbolised by the letters D, E. 5it a much less compact representation. We build upon the interplay between the template T and the HBM M. To go from one representation to the other, T can be grounded into M given some plate cardinalities {Card(Pp)}p=0..P (Koller & Friedman, 2009). Card(P) represents the number of elements in the plate P, for instance the number of groups in the study. Going from T to M, a RV template θi is instantiated into multiple ground RVs {θi,n}n=0..Ni with the same parametric form, where Ni = � P∈Plates(θi) Card(P). Note that when RV templates belong to multiple plates, they symbolize as many ground RVs as the product of the cardinalities of those plates. In Figure 1, the RV template θ1 is grounded into the ground RVs θ1,0, θ1,1, θ1,2. There are as many ground RVs Xi as the product of the number of groups on the study Card(P1) times the number of subjects per group Card(P0). We denote as π(θi,n) the (potentially empty) set of parents of the RV θi,n in the ground graph corresponding to the model M. π(θi,n) are the RVs whose value condition the distribution of θi,n. For instance, in Figure 1, a subject’s weight —the child RV— is a perturbation of the group’s weight —the parent RV, conditioning the child RV’s distribution. This is denoted as π(X0) = {θ1,0}. The benefit of the template T and the plates P is thus to symbolize the natural factorization of the graphical model M into i.i.d. RVs. This factorization de-clutters the representation of hierarchical models. PAVI exploits this simplifying factorization: we consider the inference over the ground RVs θi,n as symmetrical problems, over which we share parameterization and learning, as described in Section 4. 3.2 Full model The full model M is associated with the density p. In p, the plate structure indicates that a RV template θi is associated to a conditional distribution pi shared across all ground RVs θi,n: log p(Θ, X) = logp(X|Θ) + log p(Θ) = NX � n=0 log pX(xn|π(xn)) + I � i=1 Ni � n=0 log pi(θi,n|π(θi,n)) (2) where π(θi,n) is the (potentially empty) set of parents of the RV θi,n, which condition its distribution. We denote with a •X index all variables related to the observed RVs X. Exploiting the factorization visible in Equation (2), our goal is to obtain a variational distribution q(Θ) usable to approximate the unknown posterior p(Θ|X) for the target model M. 4 Methods 4.1 PAVI architecture Full variational family design Here we define the variational distribution q corresponding to the full model M. To derive q, we push forward the prior p(Θ) using trainable normalizing flows, denoted as F (Papamakarios et al., 2019). A normalizing flow is an invertible and differentiable neural network, also known as a diffeomorphism, that transforms samples from a base distribution into samples from a target distribution. To every ground RV θi,n, we associate the learnable flow Fi,n to approximate its posterior distribution: log q(Θ) = I � i=1 Ni � n=0 log qi,n(θi,n|π(θi,n)) θi,n = Fi,n(ui,n) ui,n ∼ pi(ui,n|π(θi,n)) (3) where qi,n is the push-forward of the prior pi through the flow Fi,n. This push-forward is illustrated in Figure 1, where flows F push RVs u into θ. This cascading scheme was first introduced by Ambrogioni et al. (2021b), and makes q inherit the conditional dependencies of the prior p. As a result, q(Θ) in Equation (3) mimics the right-hand term p(Θ) in Equation (2). More details about the dependencies modeled in the variational distribution can be found in Appendix C.5. 6Plate amortization Here we introduce plate amortization: sharing the parameterization of density esti- mators across a model’s plates. Plate amortization reduces the number of weights in a variational family as the cardinality of the inference problem augments. In Section 4.2, we show that plate amortization also results in faster inference. VI searches for a distribution q(Θ; ϕ) that best approximates the posterior of Θ given a value X0 for X: q(Θ; ϕ0) ≃ p(Θ|X = X0). When presented with a new data point X1, optimization has to be performed again to search for the weights ϕ1, such that q(Θ; ϕ1) ≃ p(Θ|X = X1). Instead, sample amortized (sa) inference (Zhang et al., 2019; Cremer et al., 2018) infers in the general case, regressing the weights ϕ using an encoder f of the observed data X: q(Θ; ϕ = f(Xi)) ≃ p(Θ|X = Xi). The cost of learning the encoder weights is amortized since inference over any new sample X requires no additional optimization. In Section 5.3, we compare PAVI to several sample amortized baselines, which will be denoted by the suffix (sa), and should not be confused with plate amortized methods. A more in-depth discussion about amortization can be found in Appendix C.1. We propose to exploit the concept of amortization but to apply it at a different granularity, leading to our notion of plate amortization. Similar to amortizing across the different data samples X, we amortize across the different ground RVs {θi,n}n=0..Ni corresponding to the same RV template θi. Instead of casting every flow Fi,n, defined in Equation (3), as a separate, fully-parameterized flow, we will share some parameters across the Fi,n. To the template θi, we associate a conditional flow Fi( · ; ϕi, •) with weights ϕi shared across all the {θi,n}n=0..Ni. The flow Fi,n associated with a given ground RV θi,n will be an instance of this conditional flow, conditioned by an encoding Ei,n: Fi,n = Fi( · ; ϕi, Ei,n) yielding qi,n = qi,n(θi,n|π(θi,n); ϕi, Ei,n) (4) The distributions qi,n thus have 2 sets of weights, ϕi and Ei,n, creating a parameterization trade-off. Con- centrating all of qi,n’s parameterization into ϕi results in all the ground RVs θi,n having the same posterior distribution. On the contrary, concentrating all of qi,n’s parameterization into Ei,n allows the θi,n to have completely different posterior distributions. But in a large cardinality setting, this freedom can result in a massive number of weights, proportional to the number of ground RVs times the encoding size. This double parameterization is therefore efficient when the majority of the weights of qi,n is concentrated into ϕi. Using normalizing flows Fi, the burden of approximating the correct parametric form for the posterior is placed onto ϕi, while the Ei,n encode lightweight summary statistics specific to each θi,n. For instance, Fi could learn to model a Gaussian mixture distribution, while the Ei,n would encode the location and variance of each mode for each ground RV. Encodings Ei,n allow to individualize for θi,n only the strictly necessary information necessary to approximate θi,n’s posterior. A more in-depth analysis of the impact of plate amortization on the expressivity of the variational family Q can be found in Appendix A.4. Intermediate summary This section defined q, the variational distribution to approximate full model M’s posterior. q features plate amortization, which helps maintain a tractable number of weights as the cardinality of M augments. The next section introduces a stochastic scheme to train q. Critically, combining shared parameterization and stochastic training does not simply result in combining the advantages of both, but features synergies resulting in faster training, as further explained in Section 4.2. 4.2 PAVI stochastic training Reduced model Our goal is to train the variational distribution q(Θ), defined in Equation (3), that approximates the posterior p(Θ|X). q corresponds to the full model M. M typically features large plate cardinalities Card(P) —with many subject groups and subjects per group— and thus many ground RVs, making it computationally intractable. We will therefore train q stochastically, over smaller subsets of RVs at a time. In this section, we interpret stochastic training as the training over a reduced model Mr. Instead of inferring directly over M, we will train over a smaller replica of M. To this end, we instantiate the template T into a second HBM Mr, the reduced model, of tractable plate cardinalities Cardr(P) ≪ Card(P). Mr has the same template as M, meaning the same dependency structure and the same parametric form 7for its distributions. The only difference lies in Mr’s smaller cardinalities, resulting in fewer ground RVs, as visible in Figure 1. Reduced distribution and loss Here we define the distribution qr used in stochastic training. qr features the smaller cardinalities of the reduced model Mr, making it computationally tractable. At each optimization step t, we randomly choose inside M paths of reduced cardinality, as visible in Figure 1. Selecting paths is equivalent to selecting from X a subset Xr[t] of size N r X, and from Θ a subset Θr[t]. For a given θi, we denote as Bi[t] the batch of selected ground RVs, of size N r i . BX[t] equivalently denotes the batch of selected observed RVs. Inferring over Θr[t], we will simulate training over the full distribution q: log qr(Θr[t]) = I � i=1 Ni N r i � n∈Bi[t] log qi,n(θi,n|π(θi,n)) (5) where the factor Ni/N r i emulates the observation of as many ground RVs as in M by repeating the RVs from Mr (Hoffman et al., 2013). Similarly, the loss used at step t is the reduced ELBO constructed using Xr[t] as observed RVs: ELBOr[t] = EΘr∼qr [log pr(Xr[t], Θr[t]) − log qr(Θr[t])] log pr(Xr[t], Θr[t]) = NX N r X � n∈BX[t] log pX(xn|π(xn)) + I � i=1 Ni N r i � n∈Bi[t] log pi(θi,n|π(θi,n)) (6) This scheme can be viewed as the instantiation of Mr over batches of M’s ground RVs. In Figure 1, we see that qr has the cardinalities of Mr and replicates its conditional dependencies. This training is analogous to stochastic VI (Hoffman et al., 2013), generalized with multiple hierarchies, dependencies in the posterior, and mini-batches of RVs. Sharing learning across plates Here we detail how our shared parameterization, detailed in Section 4.1, combined with our stochastic training scheme, results in faster inference. In stochastic VI (SVI, Hoffman et al., 2013), every θi,n corresponding to the same template θi is associated with individual weights. Those weights are trained only when the algorithm visits θi,n, that is to say, at step t when n ∈ Bi[t]. As plates become larger, this event becomes rare. If θi,n is furthermore associated with a highly-parameterized density estimator —such as a normalizing flow— many optimization steps are required for qi,n to converge. The combination of those two items leads to slow training and makes inference impractical in contexts such as Neuroimaging, which can feature millions of RVs. With plate amortization, we aim to unlock inference in those large regimes by reducing the training time. Instead of treating the ground RVs θi,n independently, we share the learning across plates. Due to the problem’s plate structure, we consider the inference over the θi,n as different instances of a common density estimation task. In PAVI, a large part of the parameterization of the estimators qi,n(θi,n|π(θi,n); ϕi, Ei,n) is mutualized via the plate-wide-shared weights ϕi. This means that most of the weights of the flows Fi,n, concentrated in ϕi, are trained at every optimization step across all the selected batches Bi[t]. This results in drastically faster convergence than SVI, as seen in experiment 5.1. 4.3 Encoding schemes PAVI-F and PAVI-E schemes PAVI shares the parameterization and learning of density estimators across an HBM’s plates. In practice the distributions qi,n(θi,n|π(θi,n); ϕi, Ei,n) from Equation (4) with different n only differ through the value of the encodings Ei,n. We detail two schemes to derive those encodings: Free plate encodings (PAVI-F) In our core implementation, Ei,n are free weights. We define encoding arrays with the cardinality of the full model M, one array Ei = [Ei,n]n=0..Ni per template θi. This means that an additional ground RV —for instance, adding a subject in a population study— requires an additional encoding vector. The associated increment in the total number of weights is much lighter than adding a 8fully parameterized normalizing flow, as would be the case in the non-plate-amortized regime. The PAVI-F scheme cannot be sample amortized: when presented with an unseen X, though ϕi can be kept as an efficient warm start, the optimal values for the encodings Ei,n have to be searched again. During training, the encodings Ei,n corresponding to n ∈ Bi[t] are sliced from the arrays Ei and are optimized for along with ϕi. In the toy example from Figure 1, at t = 0, B1[0] = {1, 2} and the trained encodings are {E1,1, E1,2}, and at t = 1 B1[1] = {0, 1} and we train {E1,0, E1,1}. Deep set encoder (PAVI-E) The parameterization of PAVI-F scales lightly but linearly with Card(P). Though lighter than the non-plate-amortized case, this scaling could still become unaffordable in large population studies. We thus propose an alternate scheme, PAVI-E, with a parameterization independent of cardinalities. In this more experimental scheme, free encodings are replaced by an encoder f with weights η applied to the observed data: E = f(X; η). As encoder f we use a deep-set architecture, detailed in Appendix A.1.3 (Zaheer et al., 2018; Lee et al., 2019; Agrawal & Domke, 2021). Due to the plate structure, the observed X features multiple permutation invariances —across data points corresponding to i.i.d. RVs. Deep sets are attention architectures that can model generic permutation invariant functions. As such, they constitute a natural design choice to incorporate the problem’s invariances. The PAVI-E scheme allows for sample amortization across different data samples X0, X1, ..., as described in Section 4.1. Note that an encoder will be used to generate the encodings whether the inference is sample amortized or not. During training, shared learning is further amplified as all the architecture’s weights —ϕi and η— are trained at every step t. To collect the encodings to plug into qr, we build up on a property of f: set size generalization (Zaheer et al., 2018). Instead of encoding the full-sized data X, f is applied to the slice Xr[t]. This amounts to aggregating summary statistics across a subset of the observed data instead of the full data (Lee et al., 2019; Agrawal & Domke, 2021). The PAVI-E scheme has an even greater potential in the sample amortized context: we train a sample amortized family over the lightweight model Mr and use it "for free" to infer over the heavyweight model M. Stochastic training and bias A key consideration is a potential bias introduced by our stochastic scheme. Namely, training stochastically over a variational family Q, we want to converge to the same solution q∗ as if we trained over the entirety of Q. In this section, we show that the PAVI-F scheme is unbiased. In contrast, the PAVI-E scheme is theoretically biased —though we seldom noticed any negative impact of that bias in practice. A more thorough analysis can be found in Appendix A.2. Note that in this manuscript the term bias systematically refers to the stochastic training scheme. A different form of bias consists in the limited expressivity of the variational family Q which may not contain the true posterior p(Θ|X). We refer to this other bias as the variational family’s gap, as further detailed in Appendix A.4. To show that our training scheme is unbiased, we need to prove that the expectation of our stochastic loss is equal to the non-stochastic loss. This amounts to showing that: Epaths [ELBOr[t]] = ELBO = EΘ∼q [log p(X, Θ) − log q(Θ)] (7) where Epaths loosely refers to the paths that are sampled stochastically into the full model M’s graph, as defined in Section 4.2. Equation (7) means that the expectation of the reduced ELBO over all the stochastic paths that can be sampled inside the full model’s graph is equal to the full ELBO. To prove Equation (7), it is sufficient to prove that: Epaths [log qr(Θr[t])] = log q(Θ) (8) meaning that the expectation of the reduced distribution qr over the stochastic paths equals the full distri- bution q. In Appendix A.2 we show that: Epaths [log qr(Θr[t])] = I � i=1 Ni � n=0 Epaths [log qi,n(θi,n|π(θi,n); Ei,n)] (9) an expression that mimics the definition of q in Equation (3) —albeit the expectations over paths. The rest of the bias analysis depends on the encoding scheme. 9Free plate encodings (PAVI-F) In the PAVI-F scheme, the encodings Ei,n are free weights. As a consequence, their value does not depend on the paths that are selected by the stochastic algorithm. This means that Epaths [log qi,n(θi,n; Ei,n)] = log qi,n(θi,n; Ei,n), proving Equation (8) and the unbiasedness of the PAVI-F scheme. With the PAVI-F scheme, training over Mr, we converge to the same distribution as if training directly over M. Deep set encoder (PAVI-E) In the PAVI-E scheme, encodings are obtained by applying the encoder f to a slice of the observed data. With the implementation presented in this work, the value of Ei,n will depend on which children of θi,n the stochastic algorithm selects. For instance, the encoding for a group will depend on which subjects are selected inside the group to compute the group’s summary statistics. Though computationally efficient, our implementation is thus theoretically biased. The negative impact of this bias on PAVI-E’s performance was seldom noticeable and always marginal throughout our experiments. Technical summary In Section 4.1, we derived an architecture sharing its parameterization across a model’s plates. This allows performing inference in large cardinality regimes without incurring an exploding number of weights. In Section 4.2 we derived a stochastic scheme to train this architecture over batches of data. Stochastic training allows large-scale inference without incurring an exploding memory and computing. Off-the-shelf stochastic VI would however result in significantly slower inference. Our novelty lies in our original combination of amortization and stochastic training, which avoids this slow-down, as demonstrated in the following experiments. 5 Results and discussion In this section, we show how PAVI unlocks hierarchical Bayesian model inference for large-scale problems by matching the inference quality of SOTA methods while providing faster convergence and lighter param- eterization. Our experiments also highlight the differences between our two encoding schemes PAVI-E and PAVI-F. In summary: 1. Section 5.1 shows how plate amortization results in faster convergence compared to non-plate- amortized SVI; 2. Section 5.2 illustrates the role of the encodings Ei,n as summary statistics in inference; 3. Section 5.3 exemplifies –against baselines– PAVI’s favorable scaling as a problem’s cardinality aug- ments; 4. Section 5.4 showcases the practical utility of PAVI by applying the method to a challenging popu- lation study. Supplemental experiments In Appendix B.3, we evaluate the impact of the reduced model cardinalities on performance. In Appendix B.4, we compare our method against baselines over a mixture model; over a model featuring the aggregation of higher-order summary statistics; and over a smaller version of our Neuroimaging model used in Section 5.4. Those models complement the analysis of our main text, which is mostly concentrated on Gaussian models. Gaussian models constitute a standard model featuring an approximate closed-form posterior which we use for validation. ELBO metric Throughout this section, we use the ELBO as a proxy for the KL divergence between the variational posterior and the unknown true posterior (Blei et al., 2017). ELBO is measured across 20 samples X, with 5 repetitions per sample. The ELBO allows us to compare the relative performance of different architectures on a given inference problem. In Appendix B.2, we provide sanity checks to assess the quality of the results. Gaussian random effects (GRE) model In our experiments Section 5.1, 5.2, 5.3, we focus on a standard hierarchical model: a Gaussian random effects model (GRE)(Diggle et al., 2013; Gelman et al., 2004). The GRE model constitutes an intuitive inference problem —inferring group and population means given 10Figure 2: Left panel: Plate amortization increases convergence speed Plot of the ELBO (higher is better) as a function of the optimization steps (log-scale) for our methods PAVI-F (in green) and PAVI-E (in blue) versus a non-plate-amortized baseline (in purple). Standard deviation across repetitions is displayed as a shaded area. A dashed line denotes the asymptotic closed-form performance. Due to plate amortization, our method converges ten to a hundred times faster to the same asymptotic ELBO as its non-plate-amortized counterpart.; Right panel: Encodings as ground RVs summary statistics Plot of the ELBO (higher is better) as a function of the optimization steps for the PAVI-F architecture with increasing encoding sizes. Standard deviation across repetitions is displayed as a shaded area. A dashed line denotes the asymptotic closed-form performance. As the encoding size augments, so does the asymptotic performance until reaching the dimensionality of the posterior’s sufficient statistics (D = 8), after which performance plateaus. Encoding size allows for a clear trade-off between memory and inference quality. observations— and features an approximate closed-form solution, as described in the next paragraph. This closed-form solution helps us display theoretical bounds for the ELBO, for instance in Figure 2. The GRE model can be described using the following set of equations: Xn1,n0|θ1,n1 ∼ N(θ1,n1, σ2 x) ∀n1=1.. Card(P1) ∀n0=1.. Card(P0) θ1,n1|θ2,0 ∼ N(θ2,0, σ2 1) ∀n1=1.. Card(P1) θ2,0 ∼ N(⃗0D, σ2 2) , (10) where D represents the data X’s feature size, with group means θ1 and population means θ2 as D-dimensional Gaussians. The GRE model features two nested plates: the group plate P1 and the sample plate P0 as in Figure 1. Taking our introductory example from Section 3, Xn1,n0 represents the weight for subject n0 in group n1. θ1,n1 represents the mean weight in the group n1. θ2,0 represents the mean weight in the population. Inferring over the GRE model, the objective is to retrieve the posterior distribution of the group and population means given the observed sample. Asymptotic closed-form ELBO As a baseline for comparison, we compare the ELBO of various methods to an approximate closed-form baseline’s ELBO. Though a closed-form posterior cannot be derived in the 3-level case, a good approximation can be constructed using Gaussian distributions centered on the empirical group and population means. This asymptotic ELBO is represented using dashed lines in Figure 2 and 3. 5.1 Plate amortization and convergence speed In this experiment, we illustrate how plate amortization results in faster training. We use the GRE model, described in Equation (10), setting D = 8, Card(P1) = 100 and Cardr(P1) = 2. In this experiment, we set Cardr(P1) ≪ Card(P1). This emulates a regime in which SVI is slow because only a small fraction —of size Cardr(P1)— of a large parameter space —of size Card(P1)— gets optimized at a given stochastic training step. We compare our PAVI architecture to a baseline with the same architecture, trained stochastically with SVI (Hoffman et al., 2013), but without plate amortization. The only difference is that ground RVs θi,n are associated in the baseline to individual fully-parameterized flows Fi,n instead of sharing the same conditional flow Fi, as further described in Section 4.1. 11Figure 2 (left) displays the evolution of the ELBO across training steps for the baseline and PAVI with free encoding (PAVI-F) and deep-set encoders (PAVI-E). We see that both plate-amortized methods reach asymptotic ELBO equal to the non-plate-amortized baseline’s but with orders of magnitudes faster conver- gence and more numerical stability. This stems from the individual flows Fi,n in the baseline only being trained when the stochastic algorithm visits the corresponding θi,n. In contrast, our shared flow Fi is updated at every optimization step in PAVI. Intuitively, the PAVI-E scheme should converge faster than PAVI-F by sharing the training not only of the conditional flows but also of the encoder across the different optimization steps. However, the computation required to derive the encodings from the observed data results in longer optimization steps and in slower inference, as illustrated in Section 5.3. The usage of an encoder nonetheless allows for sample amortization with the PAVI-E scheme, which is impossible with the PAVI-F scheme. 5.2 Impact of encoding size Here we illustrate the role of encodings as ground RV posterior’s summary statistics, as further described in Section 4.1. We use the GRE HBM detailed in Equation (10), using D = 8, Card(P1) = 20 and Cardr(P1) = 2. We use a single PAVI-F architecture, varying the size of the encodings Ei,n, which are defined in Section 4.3. Due to plate amortization, encodings determine how much individual information each RV θi,n’s posterior is associated with. The encoding size —varying from 2 to 16— is to be compared with the problem’s dimensionality, D = 8. In GRE, D = 8 corresponds to the size of the sufficient statistics needed to reconstruct the posterior of a group mean since all other statistics, such as the variance, are shared between the group means. Figure 2 (right) shows how the asymptotic performance steadily increases when the encoding size augments and plateaus once reaching the sufficient summary statistic size D = 8. Interestingly, increasing the encoding size also leads to faster convergence: redundancy can likely be exploited in the optimization. Increasing the encoding size also leads experimentally to diminishing returns in terms of performance. This property can be exploited in large settings to drastically reduce the memory footprint of inference while maintaining acceptable performance by choosing the encoding size approximately equal to the expected size of the sufficient statistics. Encoding size appears as an unequivocal hyperparameter allowing to trade inference quality for computational efficiency. 5.3 Scaling with plate cardinalities Here we put in perspective the gains from plate amortization when scaling up an inference problem’s car- dinality. We consider the GRE model in Equation (10) with D = 2 and augment the plate cardinalities (Card(P1), Cardr(P1)) : (2, 1) → (20, 5) → (200, 20). In doing so, we augment the number of parameters Θ : 6 → 42 → 402. Baselines We compare our PAVI architecture against three state-of-the-art automatic VI baselines. • Cascading Flows (CF) (Ambrogioni et al., 2021b) is a non-plate-amortized structured VI archi- tecture improving on the baseline presented in Section 5.1. CF pushes the prior p into the posterior q using Highway Flows. CF follows a cascading dependency structure complemented by a backward auxiliary coupling. CF thus consists in a structured baseline that does not pay particular attention to scalability to large cardinalities; • Automatic Dual Amortized VI (ADAVI) (Rouillard & Wassermann, 2022) is a structured VI architecture with constant parameterization with respect to a problem’s cardinality but large training times and memory. ADAVI has several limitations compared to PAVI. First, ADAVI implements a Mean Field approximation (Blei et al., 2017) while PAVI implements a cascading flow. This means that, contrary to PAVI, ADAVI can be biased in cases where the posterior features strong dependencies. Second, ADAVI is limited to pyramidal HBMs, while PAVI tackles generic plate- enriched HBMs, making it more general. Third, ADAVI is limited to a full-model sample-amortized variant, which ultimately caps the cardinality of problems it can be used on. ADAVI thus consists 12Figure 3: PAVI provides favorable parameterization and training time as the cardinality of the target model augments Baselines are compared in each panel, with the suffix (sa) indicating sample amortization. Our architecture PAVI is displayed on the right of each panel. We augment the cardinality Card(P1) of the GRE model, which is described in Equation (10). While doing so, we compare three different metrics. In the first panel: inference quality, as measured by the ELBO. An asymptotic closed-form ELBO is displayed using a dark blue dash. None of the presented SOTA architecture’s performance degrades as the cardinality of the problem augments. In the second panel: parameterization, comparing the number of trainable weights of each architecture. PAVI –similar to ADAVI– displays a constant number of weights as the cardinality of the problem increases —or almost constant for PAVI-F. Third panel: GPU training time. Benefiting from learning across plates, PAVI has a short and almost constant training time as the cardinality of the problem augments. At Card(P1) = 200, CF, UIVI, and ADAVI required large GPU memory, a constraint absent from PAVI due to its stochastic training. For convenience, the results used to plot this figure are reproduced in supplemental Table B.1. in a structured baseline that tackles the parameterization hurdle of large cardinalities, but not the computational efficiency hurdle; • Unbiased Implicit VI (UIVI) is an unstructured implicit VI architecture. UIVI infers over the full parameter space Θ, without any SVI-amenable factorization —contrary to CF, ADAVI, and PAVI. To do so, UIVI reparameterizes a base distribution with a stochastic transform. UIVI does not explicitly define a density q and relies on optimization steps intertwined with MCMC runs. UIVI thus consists in a non-structured VI baseline that does not pay particular attention to scalability to a large parameter space This means that UIVI could not be applied above a certain cardinality due to its impossibility to be stochastically trained. For all architectures, we indicate with the suffix (sa) sample amortization, as defined in Section 4.1. More implementation details can be found in Appendix B.5. As the cardinality of the problem augments, Figure 3 shows how PAVI maintains a state-of-the-art inference quality while being more computationally attractive. Parameterization In terms of parameterization, both ADAVI and PAVI-E provide a heavyweight but constant parameterization as the cardinality Card(P1) of the problem augments. This is due to both methods’ usage of an encoder of the observed data, which makes their parameterization independent of the problem’s cardinality. Comparatively, both CF and PAVI-F’s parameterization scale linearly with Card(P1), but with a drastically lighter augmentation for PAVI-F. The difference can be explained by the additional weights required by each architecture for an additional ground RV. CF requires an additional fully parameterized normalizing flow, whereas PAVI-F only requires an additional lightweight encoding vector. In detail, PAVI- F’s parameterization due to the plate-wide-shared ϕ1 represents a constant ≈ 2k weights, while the part due to the encodings E1,n grows linearly from 16 to 160 to 1.6k weights. UIVI’s parameterization scales quadratically with the size of the parameter space Θ. This is due to UIVI’s usage of a neural network to regress the weights of a transform applied to a base distribution with the size of Θ. As the cardinality 13augments, UIVI’s quadratic weight scaling would be limiting before CF and PAVI-F’s linear scaling, which would be limiting before ADAVI and PAVI-E’s constant scaling. Memory Regarding computational budget, PAVI’s stochastic training allows for controlled GPU memory during optimization. This removes the need for a larger memory as the cardinality of the problem aug- ments, a hardware constraint that can become unaffordable at very large cardinalities. CF could be trained stochastically to remove this memory constraint but, without plate amortization, would suffer from slower inference, as illustrated in Section 5.1. In contrast, UIVI could not be trained stochastically, as it infers over the full parameter space Θ at once instead of factorizing it. As a result, UIVI would be ultimately limited by memory to infer over larger problems. Speed Regarding convergence speed, PAVI benefits from plate amortization to have orders of magnitude faster convergence compared to structured VI baselines CF and ADAVI. This means that a stochastically- trained architecture (PAVI) trains faster than non-stochastic baselines (CF, ADAVI) This result is opposite to the result we would have obtained without plate amortization since SVI slows down inference. For UIVI, as the cardinality augments, training amounts to evaluating a neural network with increasingly larger layers. GPU training time is thus constant, but this property would not translate to larger problems as the GPU memory would become insufficient. Plate amortization is particularly significant for the PAVI-E(sa) scheme, in which a sample-amortized variational family is trained over a dataset of reduced cardinality yet performs "for free" inference over an HBM of large cardinality. Maintaining Cardr(P1) constant while Card(P1) augments allows for a constant parameterization and training time as the cardinality of the problem augments This property is not limited to any maximum cardinality, contrary to UIVI. This is a novel result with strong future potential. The effect of plate amortization is particularly noticeable at Card(P1) = 200 between the PAVI-E(sa) and CF(sa) architectures, where PAVI performs sample-amortized inference with 10× fewer weights and 100× lower training time. Scaling even higher the cardinality of the problem —Card(P1) = 2000 for instance— renders ADAVI, CF and UIVI computationally intractable In contrast, PAVI maintains a light memory footprint and a short training time, as exemplified in the next experiment. Using PAVI on small scale problems By design, PAVI is meant to tackle large cardinality problems effectively. The use of PAVI on smaller problems is thus an open question. As Figure 3 underlines, in the Card(P1) = 2 case, PAVI-F matches the ELBO and training speed of CF but features a heavier parameter- ization. In addition, plate amortizing a variational family theoretically reduces its expressivity, as further explained in Appendix A.4. In theory, a simpler, non-plate-amortized baseline such as CF could thus be preferred for small problems. However, in practice, VI amounts to an optimization problem, and the most expressive variational family will not necessarily yield the best performance on a complex problem (Bottou & Bousquet, 2007). Our comparative experiments in Appendix B.4 exemplify this. In Appendix B.4, PAVI’s repeated parameterization oftentimes eases the optimization and yields a better ELBO. We leave for future work the analysis of the impact of PAVI’s repeated parameterization —which could be considered as an inductive bias— on properties such as inference calibration (Talts et al., 2020). In conclusion, when its parameterization is affordable, PAVI remains a relevant choice for small plate-enriched problems. 5.4 Neuroimaging application: full cortex probabilistic parcellation for a cohort of 1,000 subjects To illustrate its usefulness, we apply PAVI to a challenging population study for full-cortex functional parcellation. A parcellation clusters brain vertices into different connectivity networks: fingerprints describing co-activation with the rest of the brain, as measured using functional Magnetic Resonance Imaging (fMRI). A parcellation is essentially a clustering of a subject’s cortex vertices based on their connectivity with the rest of the brain. Different subjects exhibit a strong variability, as visible in Figure 4. However, fMRI is costly to acquire: few noisy measurements are usually made for a given subject. It is thus essential to combine information across the different measurements for a given subject and across subjects and to display uncertainty in the results. Those 2 points motivate hierarchical Bayesian modelling and VI in Neuroimaging (Kong et al., 2019): we search the posteriors of connectivity networks and vertex labels, measuring fMRI over a large cohort of subjects. 14Accuracy for 13 cognitive measures Method (Higher is better) MS-HBM 0.1321 (±0.0053) from Kong et al. (2019) YeoBackProject 0.1057 (±0.0060) from Calhoun & Adali (2012) Gordon2017 0.0545 (±0.0062) from Gordon et al. (2017) Wang2015 0.1202 (±0.0054) from Wang et al. (2015) Ours (PAVI) 0.1645 (±0.0047) Figure 4: Probabilistic full cortex parcellation PAVI can be applied in the challenging context of Neuroimaging population studies. For a cohort of 1000 subjects, 2 of which are represented here (in the bottom 2 items) we cluster 60, 000 cortex vertices according to their connectivity with the rest of the brain. On the left, we show the obtained probabilistic parcellations. Each color in the parcellation corresponds to one of 7 functional network (Thomas Yeo et al., 2011). Networks represent groups of neurons that co- activate in the brain and can be associated with certain cognitive functions, such as vision or motor control. Through our method, we recover each subject’s parcellation (at the bottom), which are i.i.d. perturbations of the population’s parcellation (at the top). Our method also models uncertainty: coloring represents the dominant label for each vertex, and the level of white increases with the uncertainty in the labeling. We can use the subject parcellations as features for a cognitive score prediction task, as visible in the table on the right. The table shows the mean predictive accuracy across 13 cognitive measures, including memory, pronunciation, processing speed or spatial orientation. The baseline methods scores are reproduced from Kong et al. (2019)’s implementation. Our method produces individual maps that are predictive of the subject’s cognitive ability. We use the HCP dataset (Van Essen et al., 2012): 2 acquisitions from a 1000 subjects, with millions of measures per acquisition, and over 400 million parameters Θ to infer. We use a model with three plates: subjects, sessions and brain vertices. We cluster the brain vertices into 7 clusters, following Thomas Yeo et al. (2011). None of the baselines presented in Section 5.3 —CF, ADAVI, UIVI— can computationally tackle this high cardinality problem. We nevertheless show superior performance over those baselines over a tractable problem size with 2,000 parameters in Appendix B.4.3. Despite the massive dimensionality of the problem, thanks to plate amortization, PAVI converges in less than 5 hours of GPU time. Results are visible in Figure 4. Qualitatively, the recovered networks match existing expert knowledge on the brain’s functional organization. For instance, we recover networks responsible for motor control or vision (Heim et al., 2009; Zhang et al., 2020). Quantitatively, using the individual subject’s parcellation as feature for a cognitive score regression task, we obtain better scores compared to state-of- the-art methods (Thomas Yeo et al., 2011; Gordon et al., 2017; Wang et al., 2015; Kong et al., 2019). This means that a subject’s brain spatial organization —where each function is "located" in the brain— can be used to predict the subject’s cognitive ability, such as memory, reading, or spatial orientation. Using PAVI, we obtain state-of-the-art functional parcellations for a thousand subjects, capturing the uncer- tainty in this challenging task via VI while maintaining a manageable training time despite a 400-million-sized parameter space. 155.5 Conclusion In this work we present the novel PAVI architecture, combining a structured variational family and a stochas- tic training scheme. PAVI is based the on concept of plate amortization, allowing to share parameterization and learning across a model’s plates. We demonstrated the positive impact of plate amortization on training speed and scaling to large plate cardinality regimes, making a significant step towards scalable, expressive Variational Inference. Reproducibility statement All experiments were performed in Python using the Tensorflow Probability library (Dillon et al., 2017). All experiments were conducted on computational cluster nodes equipped with a Tesla V100-16Gb GPU and 4 AMD EPYC 7742 64-Core processors. VRAM intensive experiments in Figure 3 were performed on an Ampere 100 PCIE-40Gb GPU. Appendix B.5 lists implementation details for all our synthetic experiments. Appendix B.6 is related to our Neuroimaging experiment 5.4, and details both our data pre-processing steps and our implementation. As part of our submission we furthermore packaged and release the code associated to our experiments. Broader Impact Statement Our Neuroimaging data come from the Human Connectome Project dataset (Van Essen et al., 2012). All data in the HCP is strongly anonymized, as per the HCP protocols. We used in this paper only Open Access imaging data data, following the HCP data use terms. Acknowledgments This work was supported by the ERC-StG NeuroLang ID:757672. This work was performed using HPC resources from GENCI–IDRIS (Grant 2022-102043). References Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor- rado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available from tensorflow.org. Alexandre Abraham, Fabian Pedregosa, Michael Eickenberg, Philippe Gervais, Andreas Mueller, Jean Kos- saifi, Alexandre Gramfort, Bertrand Thirion, and Gael Varoquaux. Machine learning for neuroimaging with scikit-learn. Frontiers in Neuroinformatics, 8, 2014. ISSN 1662-5196. doi: 10.3389/fninf.2014.00014. URL https://www.frontiersin.org/article/10.3389/fninf.2014.00014. Abhinav Agrawal and Justin Domke. Amortized Variational Inference for Simple Hierarchical Models. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 21388–21399. Curran Associates, Inc., 2021. URL https: //proceedings.neurips.cc/paper/2021/file/b28d7c6b6aec04f5525b453411ff4336-Paper.pdf. Luca Ambrogioni, Kate Lin, Emily Fertig, Sharad Vikram, Max Hinne, Dave Moore, and Marcel van Gerven. Automatic structured variational inference. In Arindam Banerjee and Kenji Fukumizu (eds.), Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research, pp. 676–684. PMLR, 13–15 Apr 2021a. URL https://proceedings.mlr. press/v130/ambrogioni21a.html. 16Luca Ambrogioni, Gianluigi Silvestri, and Marcel van Gerven. Automatic variational inference with cascading flows. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 254–263. PMLR, 18–24 Jul 2021b. URL https://proceedings.mlr.press/v139/ambrogioni21a.html. Christopher M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag, Berlin, Heidelberg, 2006. ISBN 0387310738. David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational Inference: A Review for Statisticians. Journal of the American Statistical Association, 112(518):859–877, April 2017. ISSN 0162-1459, 1537-274X. doi: 10.1080/01621459.2017.1285773. URL http://arxiv.org/abs/1601.00670. arXiv: 1601.00670. Léon Bottou and Olivier Bousquet. The Tradeoffs of Large Scale Learning. In J. Platt, D. Koller, Y. Singer, and S. Roweis (eds.), Advances in Neural Information Processing Systems, vol- ume 20. Curran Associates, Inc., 2007. URL https://proceedings.neurips.cc/paper/2007/file/ 0d3180d672e08b4c5312dcdafdf6ef36-Paper.pdf. Vince D. Calhoun and Tülay Adali. Multisubject Independent Component Analysis of fMRI: A Decade of Intrinsic Networks, Default Mode, and Neurodiagnostic Discovery. IEEE Reviews in Biomedical Engineering, 5:60–73, 2012. ISSN 1937-3333, 1941-1189. doi: 10.1109/RBME.2012.2211076. URL http://ieeexplore.ieee.org/document/6268324/. Chris Cremer, Xuechen Li, and David Duvenaud. Inference Suboptimality in Variational Autoencoders. arXiv:1801.03558 [cs, stat], May 2018. URL http://arxiv.org/abs/1801.03558. arXiv: 1801.03558. Kamalaker Dadi, Gaël Varoquaux, Antonia Machlouzarides-Shalit, Krzysztof J. Gorgolewski, Demian Wassermann, Bertrand Thirion, and Arthur Mensch. Fine-grain atlases of functional modes for fMRI analysis. NeuroImage, 221:117126, 2020. ISSN 1053-8119. doi: https://doi.org/10.1016/j.neuroimage. 2020.117126. URL https://www.sciencedirect.com/science/article/pii/S1053811920306121. Viet-Hung Dao, David Gunawan, Minh-Ngoc Tran, Robert Kohn, Guy E. Hawkins, and Scott D. Brown. Efficient selection between hierarchical cognitive models: Cross-validation with variational bayes, 2021. URL https://arxiv.org/abs/2102.06814. Peter Diggle, Patrick Heagerty, Kung-Yee Liang, and Scott Zeger. Analysis of Longitudinal Data. OUP Oxford, March 2013. ISBN 978-0-19-967675-0. Google-Books-ID: ur0BlXPuOukC. Joshua V. Dillon, Ian Langmore, Dustin Tran, Eugene Brevdo, Srinivas Vasudevan, Dave Moore, Brian Patton, Alex Alemi, Matt Hoffman, and Rif A. Saurous. TensorFlow Distributions. arXiv:1711.10604 [cs, stat], November 2017. URL http://arxiv.org/abs/1711.10604. arXiv: 1711.10604. Yann Dubois, Jonathan Gordon, and Andrew YK Foong. Neural process family. http://yanndubs.github. io/Neural-Process-Family/, September 2020. A Fayaz, P Croft, RM Langford, LJ Donaldson, and GT Jones. Prevalence of chronic pain in the uk: a systematic review and meta-analysis of population studies. BMJ open, 6(6):e010364, 2016. Rémi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aurélie Boisbunon, Stanislas Cham- bon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, Léo Gautheron, Nathalie T.H. Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko, Antoine Rolet, Antony Schutz, Vivien Seguy, Danica J. Sutherland, Romain Tavenard, Alexander Tong, and Titouan Vayer. Pot: Python opti- mal transport. Journal of Machine Learning Research, 22(78):1–8, 2021. URL http://jmlr.org/papers/ v22/20-451.html. Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J. Rezende, S. M. Ali Eslami, and Yee Whye Teh. Neural processes, 2018. URL https://arxiv.org/abs/1807.01622. Andrew Gelman, John B. Carlin, Hal S. Stern, and Donald B. Rubin. Bayesian Data Analysis. Chapman and Hall/CRC, 2nd ed. edition, 2004. 17W. R. Gilks, A. Thomas, and D. J. Spiegelhalter. A Language and Program for Complex Bayesian Modelling. The Statistician, 43(1):169, 1994. ISSN 00390526. doi: 10.2307/2348941. URL https://www.jstor.org/ stable/10.2307/2348941?origin=crossref. Evan M. Gordon, Timothy O. Laumann, Adrian W. Gilmore, Dillan J. Newbold, Deanna J. Greene, Jef- frey J. Berg, Mario Ortega, Catherine Hoyt-Drazen, Caterina Gratton, Haoxin Sun, Jacqueline M. Hamp- ton, Rebecca S. Coalson, Annie L. Nguyen, Kathleen B. McDermott, Joshua S. Shimony, Abraham Z. Snyder, Bradley L. Schlaggar, Steven E. Petersen, Steven M. Nelson, and Nico U. F. Dosenbach. Precision Functional Mapping of Individual Human Brains. Neuron, 95(4):791–807.e7, 2017. ISSN 0896-6273. doi: https://doi.org/10.1016/j.neuron.2017.07.011. URL https://www.sciencedirect.com/science/ article/pii/S089662731730613X. Stefan Heim, Simon B. Eickhoff, Anja K. Ischebeck, Angela D. Friederici, Klaas E. Stephan, and Katrin Amunts. Effective connectivity of the left BA 44, BA 45, and inferior temporal gyrus during lexical and phonological decisions identified with DCM. Human Brain Mapping, 30(2):392–402, February 2009. ISSN 10659471. doi: 10.1002/hbm.20512. Matthew D. Hoffman and David M. Blei. Structured Stochastic Variational Inference. arXiv:1404.4114 [cs], November 2014. URL http://arxiv.org/abs/1404.4114. arXiv: 1404.4114. Matthew D. Hoffman, David M. Blei, Chong Wang, and John Paisley. Stochastic variational inference. Journal of Machine Learning Research, 14(4):1303–1347, 2013. URL http://jmlr.org/papers/v14/ hoffman13a.html. Ekaterina Iakovleva, Jakob Verbeek, and Karteek Alahari. Meta-learning with shared amortized variational inference. In Hal Daumé III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 4572–4582. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/v119/iakovleva20a.html. Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In In- ternational Conference on Learning Representations, 2017. URL https://openreview.net/forum?id= rkE3y85ee. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980. Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques. Adaptive computation and machine learning. MIT Press, Cambridge, MA, 2009. ISBN 978-0-262-01319-2. Ru Kong, Jingwei Li, Csaba Orban, Mert Rory Sabuncu, Hesheng Liu, Alexander Schaefer, Nanbo Sun, Xi-Nian Zuo, Avram J. Holmes, Simon B. Eickhoff, and B. T. Thomas Yeo. Spatial topography of individual-specific cortical networks predicts human cognition, personality, and emotion. Cerebral cortex, 29 6:2533–2551, 2019. Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and David M. Blei. Automatic Differ- entiation Variational Inference. arXiv:1603.00788 [cs, stat], March 2016. URL http://arxiv.org/abs/ 1603.00788. arXiv: 1603.00788. Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 3744–3753. PMLR, 09–15 Jun 2019. URL http:// proceedings.mlr.press/v97/lee19d.html. Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. CoRR, abs/1611.00712, 2016. URL http://arxiv.org/abs/1611.00712. 18George Papamakarios and Iain Murray. Fast $\epsilon$-free Inference of Simulation Models with Bayesian Conditional Density Estimation. arXiv:1605.06376 [cs, stat], April 2018. URL http://arxiv.org/abs/ 1605.06376. arXiv: 1605.06376. George Papamakarios, Theo Pavlakou, and Iain Murray. Masked Autoregressive Flow for Density Estimation. arXiv:1705.07057 [cs, stat], June 2018. URL http://arxiv.org/abs/1705.07057. arXiv: 1705.07057. George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshmi- narayanan. Normalizing Flows for Probabilistic Modeling and Inference. arXiv:1912.02762 [cs, stat], December 2019. URL http://arxiv.org/abs/1912.02762. arXiv: 1912.02762. Sachin Ravi and Alex Beatson. Amortized bayesian meta-learning. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=rkgpy3C5tX. Louis Rouillard and Demian Wassermann. ADAVI: Automatic Dual Amortized Variational Inference Applied To Pyramidal Bayesian Models. In ICLR 2022, Virtual, France, April 2022. URL https: //hal.archives-ouvertes.fr/hal-03267956. Francisco Ruiz and Michalis Titsias. A contrastive divergence for combining variational inference and MCMC. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 5537–5545. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.press/v97/ruiz19a.html. Hojjat Salehinejad, Julianne Baarbe, Sharan Sankar, Joseph Barfett, Errol Colak, and Shahrokh Valaee. Recent advances in recurrent neural networks. CoRR, abs/1801.01078, 2018. URL http://arxiv.org/ abs/1801.01078. Stephen M Smith, Christian F Beckmann, Jesper Andersson, Edward J Auerbach, Janine Bijsterbosch, Gwenaëlle Douaud, Eugene Duff, David A Feinberg, Ludovica Griffanti, Michael P Harms, et al. Resting- state fmri in the human connectome project. Neuroimage, 80:144–168, 2013. Sean Talts, Michael Betancourt, Daniel Simpson, Aki Vehtari, and Andrew Gelman. Validating Bayesian Inference Algorithms with Simulation-Based Calibration, October 2020. URL http://arxiv.org/abs/ 1804.06788. arXiv:1804.06788 [stat]. B. T. Thomas Yeo, Fenna M. Krienen, Jorge Sepulcre, Mert R. Sabuncu, Danial Lashkari, Marisa Hollinshead, Joshua L. Roffman, Jordan W. Smoller, Lilla Zöllei, Jonathan R. Polimeni, Bruce Fischl, Hesheng Liu, and Randy L. Buckner. The organization of the human cerebral cortex estimated by intrinsic functional connectivity. Journal of Neurophysiology, 106(3):1125–1165, 2011. doi: 10.1152/jn.00338.2011. URL https://doi.org/10.1152/jn.00338.2011. PMID: 21653723. Michalis K. Titsias and Francisco Ruiz. Unbiased implicit variational inference. In Kamalika Chaudhuri and Masashi Sugiyama (eds.), Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics, volume 89 of Proceedings of Machine Learning Research, pp. 167–176. PMLR, 16–18 Apr 2019. URL https://proceedings.mlr.press/v89/titsias19a.html. Kayle Towsley, Michael I Shevell, Lynn Dagenais, Repacq Consortium, et al. Population-based study of neuroimaging findings in children with cerebral palsy. European journal of paediatric neurology, 15(1): 29–35, 2011. Dustin Tran, Rajesh Ranganath, and David M. Blei. Hierarchical Implicit Models and Likelihood-Free Variational Inference. arXiv:1702.08896 [cs, stat], November 2017. URL http://arxiv.org/abs/1702. 08896. arXiv: 1702.08896. D. C. Van Essen, K. Ugurbil, E. Auerbach, D. Barch, T. E. Behrens, R. Bucholz, A. Chang, L. Chen, M. Corbetta, S. W. Curtiss, S. Della Penna, D. Feinberg, M. F. Glasser, N. Harel, A. C. Heath, L. Larson- Prior, D. Marcus, G. Michalareas, S. Moeller, R. Oostenveld, S. E. Petersen, F. Prior, B. L. Schlaggar, S. M. Smith, A. Z. Snyder, J. Xu, and E. Yacoub. The Human Connectome Project: a data acquisition perspective. Neuroimage, 62(4):2222–2231, Oct 2012. 19Danhong Wang, Randy L Buckner, Michael D Fox, Daphne J Holt, Avram J Holmes, Sophia Stoecklein, Georg Langs, Ruiqi Pan, Tianyi Qian, Kuncheng Li, Justin T Baker, Steven M Stufflebeam, Kai Wang, Xiaomin Wang, Bo Hong, and Hesheng Liu. Parcellating cortical functional networks in individuals. Nature Neuroscience, 18(12):1853–1860, December 2015. ISSN 1097-6256, 1546-1726. doi: 10.1038/nn.4164. URL http://www.nature.com/articles/nn.4164. Stefan Webb, Adam Golinski, Robert Zinkov, N. Siddharth, Tom Rainforth, Yee Whye Teh, and Frank Wood. Faithful Inversion of Generative Models for Effective Amortized Inference. arXiv:1712.00287 [cs, stat], November 2018. URL http://arxiv.org/abs/1712.00287. arXiv: 1712.00287. Karl Weiss, Taghi M. Khoshgoftaar, and DingDing Wang. A survey of transfer learning. Journal of Big Data, 3(1):9, May 2016. ISSN 2196-1115. doi: 10.1186/s40537-016-0043-6. URL https://doi.org/10. 1186/s40537-016-0043-6. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. A Comprehensive Survey on Graph Neural Networks. IEEE Transactions on Neural Networks and Learning Systems, pp. 1–21, 2020. ISSN 2162-237X, 2162-2388. doi: 10.1109/TNNLS.2020.2978386. URL http://arxiv.org/ abs/1901.00596. arXiv: 1901.00596. Huaxiu Yao, Ying Wei, Junzhou Huang, and Zhenhui Li. Hierarchically structured meta-learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 7045–7054. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.press/v97/yao19b.html. Mingzhang Yin and Mingyuan Zhou. Semi-implicit variational inference. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 5660–5669. PMLR, 10–15 Jul 2018. URL https://proceedings.mlr. press/v80/yin18b.html. Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, and Alexan- der Smola. Deep Sets. arXiv:1703.06114 [cs, stat], April 2018. URL http://arxiv.org/abs/1703.06114. arXiv: 1703.06114. Cheng Zhang, Judith Butepage, Hedvig Kjellstrom, and Stephan Mandt. Advances in Variational Inference. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(8):2008–2026, August 2019. ISSN 0162-8828, 2160-9292, 1939-3539. doi: 10.1109/TPAMI.2018.2889774. URL https://ieeexplore.ieee. org/document/8588399/. Yizhen Zhang, Kuan Han, Robert Worth, and Zhongming Liu. Connecting concepts in the brain by mapping cortical representations of semantic relations. Nature Communications, 11(1):1877, April 2020. ISSN 2041- 1723. doi: 10.1038/s41467-020-15804-w. 20Appendix A Supplemental methods A.1 PAVI implementation details A.1.1 Plate branchings and stochastic training As exposed in Section 4.2, at each optimization step t we randomly select branchings inside the full model M, branchings over which we instantiate the reduced model Mr. In doing so, we define batches Bi[t] for the RV templates θi. Those batches have to be coherent with one another: they have to respect the conditional dependencies of the original model M. As an example, if a ground RV is selected as part of Mr, then its parent RV needs to be selected as well. To ensure this, during the stochastic training we do not sample RVs directly but plates: 1. For every plate Pp, we sample without replacement Cardr(Pp) indices amongst the Card(Pp) possible indices. 2. Then, for every RV template θi, we select the ground RVs θi,n corresponding to the sampled indices for the plates Plates(θi). 3. The selected ground RVs θi,n constitute the set Θr[t] of parameters appearing in Equation (5). The same procedure yields the observed RV subset Xr[t] and the data slice Xr[t]. For instance, in the toy example from Figure 1, X2 will be chosen iff the index 1 is selected as part of sub-sampling P1 and the index 0 is selected as part of sub-sampling P0. Less formally, this is equivalent to going middle, then left in the full graph representing M. This stochastic choice is illustrated in Figure 1 at t = 1 where X2 corresponds to the node E. This stochastic strategy also applies to the selected encoding scheme —described in Section 4.3&4.2— as detailed in the next sections. A.1.2 PAVI-F details In Section 4.3 we refer to encodings Ei = [Ei,n]n=0..N corresponding to RV templates θi. In practice, we have some amount of sharing for those encodings: instead of defining separate encodings for every RV template, we define encodings for every plate level. A plate level is a combination of plates with at least one parameter RV template θi belonging to it: PlateLevels = {(Pk..Pl) = Plates(θi)}θi∈Θ (A.1) For every plate level, we construct a large encoding array with the cardinalities of the full model M: Encodings = {(Pk..Pl) �→ RCard(Pk)×..×Card(Pl)×D}(Pk..Pl)∈PlateLevels Ei = Encodings(Plates(θi)) (A.2) Where D is an encoding size that we kept constant to de-clutter the notation but can vary between plate levels. The encodings for a given ground RV θi,n then correspond to an element from the encoding array Ei. A.1.3 PAVI-E details In the PAVI-E scheme, encodings are not free weights but the output of en encoder f( · , η) applied to the observed data X. In this section we detail the design of this encoder. As in the previous section, the role of the encoder will be to produce one encoding per plate level. We start from a dependency structure for the plate levels: ∀(Pa..Pb) ∈ PlateLevels , ∀(Pc..Pd) ∈ PlateLevels , (Pa..Pb) ∈ π((Pc..Pd)) ⇔ ∃θi/ Plates(θi)=(Pa..Pb) ∃θj/ Plates(θj)=(Pc..Pd)/θj ∈ π(θi) (A.3) 21note that this dependency structure is in the backward direction: a plate level will be the parent of another plate level, if the former contains a RV who has a child in the latter. We therefore obtain a plate level dependency structure that reverts the conditional dependency structure of the graph template T . To avoid redundant paths in this dependency structure, we take the maximum branching of the obtained graph. Given the plate level dependency structure, we will recursively construct the encodings, starting from the observed data: ∀x ∈ X : Encodings(Plates(x)) = ρ(x) (A.4) where x is the observed data for the RV x, and ρ is a simple encoder that processes every observed ground RV’s value independently through an identical multi-layer perceptron. Then, until we have exhausted all plate levels, we process existing encodings to produce new encodings: ∀(Pk..Pl) ∈ PlateLevels / ̸ ∃x ∈ X, Plates(x) = (Pk..Pl) : Encodings((Pk..Pl)) = g(Encodings(π(Pk..Pl))) (A.5) where g is the composition of attention-based deep-set networks called Set Transformers (Lee et al., 2019; Zaheer et al., 2018). For every plate Pp present in the parent plate level but absent in the child plate level, g will compute summary statistics across that plate, effectively contracting the corresponding batch dimensionality in the parent encoding (Rouillard & Wassermann, 2022). In the case of multiple observed RVs, we run this "backward pass" independently for each observed data —with one encoder per observed RV. We then concatenate the resulting encodings corresponding to the same plate level. For more precise implementation details, we invite the reader to consult the codebase released with this supplemental material. A.2 Analysis of bias in the stochastic training A key concern in our stochastic training scheme is its unbiasedness: we want our stochastic optimization to converge to the same variational posterior as if we trained over the full model directly —without any stochasticity. In this section we first show that the PAVI-F is unbiased. Second, we identify strategies to obtain an unbiased PAVI-E scheme, while showing how the approximations we do in practice can theoretically result in biased training. As an important note, the negative impact of this bias on the performance PAVI-E remained limited throughout our experiments —as seen in experiments 5.3, B.3, B.2 and B.4. A.2.1 General derivation (applicable to both the PAVI-F and PAVI-E schemes) We first formalize the plate sampling strategy described in Appendix A.1.1. To every plate P we associate the RV IP corresponding to the Cardr(P)-sized set of indices sampled without replacement from the Card(P) possible index values. As an example, with a plate P0 with Card(P0) = 4 and Cardr(P0) = 2, {0, 2} or {2, 3} can be 2 different samples from IP0. At a given optimization step t, we sample independently from the RVs {IPp}p=0..P . This defines the batches Bi[t] and the distribution qr in Equation (5). To check the unbiasedness of our stochastic training, we need to show that: EIP0 . . . EIPP [ELBOr[t]] = ELBO (A.6) Where: ELBO = EΘ∼q [log p(X, Θ) − log q(Θ)] (A.7) And ELBOr[t] is defined in Equation (6). In that expression, q and p have symmetrical roles. As the ELBO amounts to the difference between the logarithms of distributions p and q, we can prove the equality in Equation (A.6) if we prove that the expectation of each reduced distribution is equal to the corresponding full distribution. To prove the equality in Equation (A.6), a sufficient condition is therefore to prove that: EIP0 . . . EIPP [log qr(Θr[t])] = EIP [log qr(Θr[t])] = log q(Θ) (A.8) 22where to de-clutter the notations we denote the expectation over the collection of RVs {IPp}p=0..P as EIP. Consider a given ground RV θi,n corresponding to the RV template θi and to the plates Plates(θi). At a given stochastic step t, θi,n will be chosen if and only if its corresponding branching is chosen. Recall that when sampling equiprobably without replacement a set of k elements from a population of n elements, a given element will be present in the set with probability k/n. We can apply this reasoning to the choice of branching corresponding to a given ground RV. For instance, in Figure 1, X2 will be chosen iff the index 1 is selected as part of sub-sampling P1 and the index 0 is selected as part of sub-sampling P0. As Cardr(P1) = 2 indices are chosen inside the plate P1 of full cardinality 3, and Cardr(P0) = 1 indices are chosen inside the plate P0 of full cardinality 2, X2 is therefore chosen with probability 2/3 × 1/2. More formally, for a ground RV θi,n we have: ∀n = 0..Ni : P(θi,n ∈ Bi[t]) = � P∈Plates(θi) Cardr(P) Card(P) = N r i Ni (A.9) Applying this reasoning to every RV template θi, we have that: EIP [log qr(Θr[t])] = I � i=1 Ni N r i EIP  � n∈Bi[t] log qi,n(θi,n|π(θi,n))   = I � i=1 Ni N r i EIP � Ni � n=0 1n∈Bi[t] log qi,n(θi,n|π(θi,n)) � = I � i=1 Ni N r i Ni � n=0 EIP � 1n∈Bi[t] log qi,n(θi,n|π(θi,n); ϕi, Ei,n) � (A.10) where we exploited the fact that the expectation of the sum of RVs is the sum of the expectations, even in the case of dependent RVs. The term 1n∈Bi[t] ×log qi,n(θi,n|π(θi,n); ϕi, Ei,n) is the product of 2 RVs —related to the stochastic choice of plate indices: • the RV 1n∈Bi[t] is an indicator that θi,n’s branching has been chosen via the stochastic sampling of plate indices. By construction, this RV depends only on the indices of the plates P ∈ Plates(θi). • the RV log qi,n(θi,n|π(θi,n); ϕi, Ei,n) depends on Ei,n, whose construction depends on the encoding scheme: – In the PAVI-F scheme, Ei,n is a constant. – In the PAVI-E scheme, Ei,n results of the application of an encoder to the observed data of a subset of θi,n’s descendants. By construction, this subset will only depend on the indices of plates containing θi’s descendants, but not containing θi. The value of Ei,n therefore only depends on the indices of plates P /∈ Plates(θi) As an example of this reasoning, consider the model M illustrated in Figure 1. We can evaluate both terms for the ground RV θ1,2 in the PAVI-E scheme: • 12∈B1[t] depends on whether the index 2 is chosen as part of sub-sampling the plate P1, and therefore only depends on the RV IP1. In this case, the associated probability is 2/3; • to evaluate log q1,2(θ1,2|θ2,0; ϕ1, E1,2), the value of E1,2 will result from the application of the encoder f over the value of either X4 or X5. This choice depends on whether the index 0 or 1 is chosen as part of sub-sampling the plate P0. Therefore, the value of the term log q1,2 only depends on the RV IP0. 23As a result, in both PAVI-F and PAVI-E, the terms 1n∈Bi[t] and log qi,n(θi,n|π(θi,n); ϕi, Ei,n) depend on the sampled indices of disjoint sets of plates, and are therefore independent. This means that the expectation of their product can be rewritten as the product of their expectations: EIP [log qr(Θr[t])] = I � i=1 Ni N r i Ni � n=0 EIP � 1n∈Bi[t] � EIP [log qi,n(θi,n|π(θi,n))] = I � i=1 Ni N r i Ni � n=0 N r i Ni EIP [log qi,n(θi,n|π(θi,n))] = I � i=1 Ni � n=0 EIP [log qi,n(θi,n|π(θi,n); Ei,n)] (A.11) This equality can be further simplified in the PAVI-F case —proving its unbiasedness— but not in the PAVI-E case, as detailed in the sections below. A.2.2 Unbiasedness of the PAVI-F scheme In the PAVI-F scheme, detailed in Section 4.3, the encodings Ei,n are constants with respect to the branching choice, therefore we have: EIP [log qr(Θr[t])] = I � i=1 Ni � n=0 EIP [log qi,n(θi,n|π(θi,n); Ei,n)] = I � i=1 Ni � n=0 log qi,n(θi,n|π(θi,n); Ei,n) = log q(Θ) (A.12) which proves Equation (A.8) and Equation (A.6). In the above example of θ1,2 in M, in the PAVI-F scheme the expression EIP [log qi,n(θi,n|π(θi,n); ϕi, Ei,n)] can be evaluated into log q1,2(θ1,2|θ2,0; ϕ1, E1,2). This demonstrates that the PAVI-F scheme is unbiased: training over stochastically chosen sub-graphs for qr is in expectation equal to training over the full graph of q. A.2.3 Approximations in the PAVI-E scheme In the PAVI-E scheme, detailed in Section 4.2, the encodings Ei,n are computed from the observed data X. Specifically, considering the ground RV θi,n, we have Ei,n = f(Xr i,n[t]) where Xr i,n[t] corresponds to the observed data of a subset of θi,n’s descendants. Depending on the chosen branching downstream of θi,n, the value of Ei,n can therefore vary. This means we cannot further simplify Equation (A.11): the terms log qi,n(θi,n|π(θi,n); Ei,n) are not constants with respect to the RVs IP. In the above example of θ1,2 in M, in the PAVI-E scheme the expression EIP [log qi,n(θi,n|π(θi,n); ϕi, Ei,n)] can be evaluated into: 1 2(log q1,2(θ1,2|θ2,0; ϕ1, f(X4)) + log q1,2(θ1,2|θ2,0; ϕ1, f(X5))) How could the PAVI-E scheme be made unbiased? Specifically, by making the value of Ei,n inde- pendent of the choice of downstream branching. A possibility would be to parameterize Ei,n as an average —an expectation— over all the possible sub-branchings downstream of θi,n. Yet, in practical cases, the cardinalities of the reduced model are much inferior to the ones of the full model: Cardr(P) ≪ Card(P). This means that numerous Cardr(P)-sized subsets can be chosen inside the Card(P) possible descendants. In order to average over all those subset choices to compute Ei,n, numerous encoding calculations would be required at each stochastic training step. For large-scale cases, we deemed this possibility impractical. Other possibilities could exist, all revolving around the problem of aggregating collections of stochastic estimators into one general estimator —in an unbiased and efficient manner. To our knowledge, this is a complex and still open research question, whose advancement could much benefit our applications. 24Practical approximation for the PAVI-E scheme In practice, we compute the encoding Ei,n based on the single downstream branching corresponding to the sampling of the RVs IP. Compared to the previous paragraph, this amounts to estimating the expectation of Ei,n —over all downstream branchings— using a single one of those branchings. Note that, even if this encoding estimate was unbiased, log qi,n would remain a highly non-linear function of Ei,n. As a consequence, we need to rely on the approximation: EIP � log qi,n(θi,n|π(θi,n); ϕi, f(Xr i,n[t])) � ≃ log qi,n(θi,n|π(θi,n); ϕi, f(Xi,n)) (A.13) which can theoretically introduce some bias in our gradients. The approximation Equation (A.13) can be interpreted as follow: "the expectation of the density of θi,n when collecting summary statistics over a stochastic subset of θi,n’s descendants is approximately equal to the density of θi,n when collecting summary statistics over the entirety of θi,n’s descendants". Another interpretation is that the distribution associated with the summary of the full data can be approximated by annealing the distributions associated with sum- maries of subsets of this data. In practice, this approximation did not yield significantly worse performance for the PAVI-E scheme over the generative models we tested. At the same time, computing the encodings over a single branching allows the computation of all the Ei,n encodings in a single lightweight pass over the data Xr[t]. This simple solution thus provided a substantial increase in training speed with seldom noticeable bias. Yet, we do not bar the existence of pathological generative HBMs where this approximation would become coarse. Experimenters should bear in mind this possibility when using the PAVI-E scheme. In practice, using the PAVI-F scheme as a sanity check over synthetic, toy-dimension implementations of the considered generative models is a good way to validate the PAVI-E scheme —before moving on to the real problem instantiating the same generative model with a larger dimensionality. A.3 PAVI algorithms More technical details can be found in the codebase provided with this supplemental material. A.3.1 Architecture build Algorithm 1: PAVI architecture build Input: Graph template T , plate cardinalities {(Card(Pp), Cardr(Pp))}p=0..P , encoding scheme Output: q distribution for i = 1..I do Construct conditional flow Fi; Define conditional posterior distributions qi,n as the push-forward of the prior via Fi, following Equation (3); Combine the qi,n distributions following the cascading flows scheme, as in Section 4.1 (Ambrogioni et al., 2021b) ; if PAVI-F encoding scheme then Construct encoding arrays {Ei = [Ei,n]n=0..Ni}i=1..I as in Appendix A.1.2 ; else if PAVI-E encoding scheme then Construct encoder f as in Appendix A.1.3 ; 25A.3.2 Stochastic training Algorithm 2: PAVI stochastic training Input: Untrained architecture q, observed data X, encoding scheme, number of steps T Output: trained architecture q for t = 0..T do Sample plate indices to define the batches Bi[t], the latent Θr[t] and the observed Xr[t] and Xr[t], following Appendix A.1.1 ; Define reduced distribution pr ; if PAVI-F encoding scheme then Collect encodings Ei,n by slicing from the arrays Ei the elements corresponding to the batches Bi[t] ; else if PAVI-E encoding scheme then Compute encodings as E = f(Xr[t]); Feed obtained encodings into qr ; Compute reduced ELBO as in Equation (6), back-propagate its gradient ; Update conditional flow weights {ϕi}i=1..I; if PAVI-F encoding scheme then Update encodings {Ei,n}i=1..I,n∈Bi,t; else if PAVI-E encoding scheme then Update encoder weights η; A.3.3 Inference Algorithm 3: PAVI inference Input: trained architecture q, observed data X, encoding scheme Output: approximate posterior distribution if PAVI-F encoding scheme then Collect full encoding arrays Ei ; else if PAVI-E encoding scheme then Compute encodings as E = f(X) using set size generalization ; Feed obtained encodings into q ; A.4 Inference gaps In terms of inference quality, the impact of our architecture can be formalized following the gaps terminology (Cremer et al., 2018). Consider a joint distribution p(Θ, X), and a value X for the RV template X. We pick a variational family Q, and in this family look for the parametric distribution q(Θ; ϕ) that best approximates p(Θ|X = X). Specifically, we want to minimize the Kulback-Leibler divergence (Blei et al., 2017) between our variational posterior and the true posterior, that Cremer et al. (2018) refer to as the gap G: G = KL(q(Θ; ϕ)||p(Θ|X)) = log p(X) − ELBO(q; ϕ) (A.14) We denote q∗(Θ; ϕ∗) the optimal distribution inside Q that minimizes the KL divergence with the true posterior: Gapprox(Q; ϕ∗) = log p(X) − ELBO(q∗; ϕ∗) ≥ 0 Gvanilla VI = Gapprox (A.15) The approximation gap Gapprox depends on the expressivity of the variational family Q, specifically whether Q contains distributions arbitrarily close to the posterior —in the KL sense. 26Note: Gapprox is a property of the variational family Q. Gapprox is an asymptotic bound for the KL divergence between any distribution q ∈ Q and the true posterior. This gap is therefore a form of bias but is not to be mistaken with the stochasticity-induced bias studied in Appendix A.2. The bias in Appendix A.2 relates to whether q∗ can be found by training stochastically over Q, whereas Gapprox relates to the the bias between q∗ and the true posterior. Cremer et al. (2018) demonstrate that, in the case of sample amortized inference, when the weights ϕ no longer are free but the output of an encoder f ∈ F, inference cannot be better than in the non-sample- amortized case, and a positive amortization gap is introduced: Gsa(Q, F; η∗) = Gapprox(Q; f(X, η∗)) − Gapprox(Q; ϕ∗) ≥ 0 Gsample amortized VI = Gapprox + Gsa (A.16) Where we denote as η∗ the optimal weights for the encoder f inside the function family F. The gap terminology can be interpreted as follow: "theoretically, sample amortization cannot be beneficial in terms of KL divergence for the inference over a given sample X." Using the same gap terminology, we can define gaps implied by our PAVI architecture. Instead of picking the distribution q inside the family Q, consider picking q from the plate-amortized family Qpa corresponding to Q. Distributions in Qpa are distributions from Q with the additional constraints that some weights have to be equal. Consequently, Qpa is a subset of Q: Qpa ⊂ Q (A.17) As such, looking for the optimal distribution inside Qpa instead of inside Q cannot result in better perfor- mance, leading to a plate amortization gap: Gpa(Q, Qpa; ψ∗, ϕ∗) = Gapprox(Qpa; ψ∗) − Gapprox(Q; ϕ∗) ≥ 0 GPAVI-F = Gapprox + Gpa (A.18) Where we denote as ψ∗ the optimal weights for a variational distribution q inside Qpa —in the KL sense. The equation A.18 is valid for the PAVI-F scheme —see Section 4.3. We can interpret it as follow: "theoretically, plate amortization cannot be beneficial in terms of KL divergence for the inference over a given sample X". Now consider that encodings are no longer free parameters but the output of an encoder f. Similar to the case presented in Equation (A.16), using an encoder cannot result in better performance, leading to an encoder gap: Gencoder(Qpa, F; ψ∗, η∗) = Gapprox(Qpa; f(X, η∗)) − Gapprox(Qpa; ψ∗) ≥ 0 GPAVI-E = Gapprox + Gpa + Gencoder (A.19) The equation Equation (A.19) is valid for the PAVI-E scheme —see Section 4.3. The most complex case is the PAVI-E(sa) scheme, where we combine both plate and sample amortization. Our argument cannot account for the resulting GPAVI-E(sa) gap: both the PAVI-E and PAVI-E(sa) schemes rely upon the same encoder f. In the PAVI-E scheme, f is overfitting over a dataset composed of the slices of a given data sample X. In the PAVI-E(sa) scheme, the encoder is trained over the whole distribution of the samples of the reduced model Mr. Intuitively, it is likely that the performance of PAVI-E(sa) will always be dominated by the performance of PAVI-E, but —as far as we understand it— the gap terminology cannot account for this discrepancy. Comparing previous equations, we therefore have: Gvanilla VI ≤ GPAVI-F ≤ GPAVI-E (A.20) Note that those are theoretical results, that do not necessarily pertain to optimization in practice. In particular, in Section 5.1&5.3, this theoretical performance loss is not observed empirically over the studied 27examples. On the contrary, in practice, our results can actually be better than non-amortized baselines, as is the case for the PAVI-F scheme in Figure 3 or experiments B.4. We interpret this as a result of a simplified optimization problem due to plate amortization —with fewer parameters to optimize for, and mini-batching effects across different ground RVs. A better framework to explain those discrepancies could be the one from Bottou & Bousquet (2007): performance in practice is not only the reflection of an approximation error but also of an optimization error. A less expressive architecture —using plate amortization— may in practice yield better performance. Furthermore, for the experimenter, the theoretical gaps Gpa, Gencoder are likely to be well "compensated for" by the lighter parameterization and faster convergence entitled by plate amortization. B Supplemental results In this section, we present various supplemental experiments that further compare the PAVI-E and -F between themselves and against baselines. Overall, we found the PAVI-F scheme faster to train and to yield better inference quality than the PAVI-E scheme. When its parameterization is affordable, PAVI-F should be preferred. PAVI-E nevertheless opens up promising research directions, with the potential for parameterization-constant, time-constant sample-amortized inference as the problem cardinality augments. Though degraded compared to PAVI-F’s, PAVI-E’s performance is still on par or beats baselines in a variety of inference tasks —see experiments 5.3&B.4. B.1 Tabular results for experiment 5.3 For convenience, we reproduce the results of our comparative experiment from Figure 3 in Table B.1. B.2 GRE results sanity check As exposed in the introduction of Section 5, in this work we focused on the usage of the ELBO as an inference performance metric (Blei et al., 2017): ELBO(q) = log p(X) − KL(q(Θ)||p(Θ|X)) (B.21) Given that the likelihood term log p(X) does not depend on the variational family q, differences in ELBOs directly transfer to differences in KL divergence, and provide a straightforward metric to compare different variational posteriors. Nonetheless, the ELBO doesn’t provide an absolute metric of quality. As a sanity check, we want to assert the quality of the results presented in Section 5.3 —that are transferable to Sec- tion 5.1&5.2, based on the same model. In Figure B.1 we plot the posterior samples of various methods against approximate closed-form ground truths, using the Card(P1) = 20 case. All the method’s results are aligned with the closed-form ground truth, with differences in ELBO translating meaningful qualitative differences in terms of inference quality. B.3 Effect of the reduced model cardinalities on the training efficiency In Figure B.2 we show the impact of the augmentation of Cardr(P1) on the efficiency of the variational posterior’s optimization. In practice, we noticed that the most efficient choice in the case of the PAVI-F scheme was to maximize the cardinalities of the reduced model given the memory constraints of the GPU. Indeed, training over larger cardinalities does make each optimization step slightly slower but also makes the ELBO gradient estimates less noisy and allows to train more encoding vectors Ei,n at a given optimization step. In the case of the PAVI-E scheme, the training speed is constant with respect to Cardr(P1). This is due to the computing of the encodings being vectorized across plates in our deep-set encoder —see Appendix A.1.3 (Lee et al., 2019). We observe a slight if barely noticeable reduction of the inference bias when augmenting Cardr(P1). Similar to the PAVI-F scheme, Cardr(P1) should be maximized with respect to the GPU memory constraint. The intuition behind this choice is that the generalization of the learning of summary statistics 28Architecture ELBO (101) Parameterization Optimization time (s) Card(P1) = 2 ADAVI (sa) 1.88 (± 0.21) 12,280 503 (± 50) CF (sa) 1.48 (± 0.05) 2,275 228 (± 23) CF 1.98 (± 0.02) 355 63 (± 6) UIVI 2.11 (± 0.05) 2,764 220 (± 22) PAVI-F 2.14 (± 0.02) 4,058 65 (± 6) PAVI-E (sa) 2.03 (± 0.03) 27,394 140 (± 14) PAVI-E 1.74 (± 0.18) 27,394 37 (± 4) Asymptotic ELBO 2.20 Card(P1) = 20 ADAVI (sa) 21.6 (± 1.36) 12,280 800 (± 80) CF (sa) 17.3 (± 0.60) 21,751 1,612 (± 161) CF 22.6 (± 0.01) 2,551 404 (± 40) UIVI 23.9 (± 0.02) 14,676 230 (± 23) PAVI-F 23.8 (± 0.02) 4,202 72 (± 7) PAVI-E (sa) 22.3 (± 0.30) 27,394 145 (± 14) PAVI-E 18.4 (± 2.63) 27,394 50 (± 5) Asymptotic ELBO 24.1 Card(P1) = 200 ADAVI (sa) 221.7 (± 10.93) 12,280 1,600 (± 160) CF (sa) 175.7 (± 0.52) 216,511 12,000 (± 1,200) CF 228.6 (± 0.52) 24,511 2,600 (± 260) UIVI 241.2 (± 0.14) 139,300 240 (± 24) PAVI-F 237.7 (± 0.95) 5,642 81 (± 8) PAVI-E (sa) 227.0 (± 4.76) 27,394 150 (± 15) PAVI-E 224.2 (± 15.34) 27,394 85 (± 9) Asymptotic ELBO 243.4 Table B.1: Scaling with plate cardinalities Reproduction of the results from Figure 3 29Figure B.1: GRE Sanity check Inference methods present qualitatively correct results, making ELBO comparisons relevant in our experiments. On the topmost line, we represent 4 different X samples for the GRE model described in Equation (10) with Card(P1) = 20. Each set of colored points represents the Xn1,• points belonging to one of the 20 groups. Bottom lines represent the posterior samples for the methods used in Section 5.3. Colored points are sampled from the posterior of the groups means θ1, whereas black points are samples from the population mean θ2. We represent as black circles a closed-form ground truth, centered on the correct posterior mean, and with a radius equal to 2 times the closed-form posterior’s standard deviation. Correct posterior samples should be centered on the same point as the corresponding black circle, and 95% of the points should fall within the black circle. Associated with each posterior sample is a Wasserstein Distance (WD) computed with a sample from an approximate closed-form posterior (Flamary et al., 2021). PAVI is represented on the 3 last lines. Some minor bias can be observed for the PAVI-E scheme, but this approximation error is marginal compared to the optimization error that can be observed for unbiased methods, such as CF(sa) (Bottou & Bousquet, 2007). We can observe a superior quality for the PAVI-F scheme, rivaling ADAVI and CF’s performance with orders of magnitude fewer parameters and training time, as visible in Figure 3. 30101 102 Time (s) 160 180 200 220 240 ELBO PAVI-F Card_redu=1 PAVI-F Card_redu=2 PAVI-F Card_redu=8 PAVI-F Card_redu=16 PAVI-F no stochasticity 102 Times (s) 160 180 200 220 240 ELBO PAVI-E Card_redu=1 PAVI-E Card_redu=2 PAVI-E Card_redu=8 PAVI-E Card_redu=16 PAVI-E no stochasticity Figure B.2: Both panels: Effect of Cardr(P1) on the training efficiency Experiment performed on the GRE model (see Equation (10)). We increase the cardinality of the reduced model Cardr(P1) from 1 to 20 while keeping Card(P1) = 20 fixed. At Cardr(P1) = 20, there is no stochasticity in the training, meaning we train directly on M. This experiment is interesting to evaluate the bias introduced in the stochastic training, as detailed in Appendix A.2. Left panel: PAVI-F scheme As Cardr(P1) augments, the training gets faster and less noisy, likely due to less stochasticity in the gradient estimates and more encodings vectors Ei,n being trained at once. This increase in speed quickly caps, and the speed is approximately the same between Cardr(P1) = 8 and Cardr(P1) = 16. This experiment also illustrates the unbiasedness of the stochastic training: at Cardr(P1) = 20 there is no stochasticity in the training, and the asymptotic performance is the same as for the stochastic training (see Appendix A.2). Right panel: PAVI-E scheme The case of Cardr(P1) = 1 is pathological: the encoder "learns" to collect summary statistics across a set of 1 element, and —not surprisingly— the learned function doesn’t generalize well on sets of size 20. In all of the other cases, the ELBO converges approximately to the same values as with Cardr(P1) = 20, that is to say when there is no stochasticity in the training (black curve). We furthermore observe a slight reduction of the inference bias as Cardr(P1) augments. This illustrates how the theoretical bias of the PAVI- E scheme identified in Appendix A.2 does not translate in significantly worse empirical results —albeit in the pathological case of Cardr(P1) = 1. Interestingly, the training speed is approximately constant across all values of Cardr(P1). This feature is essential in the PAVI-E scheme: we can train over a reduced version of our model —with a light memory footprint— and apply the obtained architecture to the full model. Note that this feature wouldn’t necessarily be present when training on CPU: computing summary statistics over larger sets would make the training slower when Cardr(P1) augments. 31Architecture ELBO (101) Optimization time (s) CF - 16.2 (± 2.9) 3,100 ADAVI - 20.0 (± 2.3) 3,000 UIVI - 26.3 (± 1.2) 500 PAVI-E (ours) - 15.2 (±2.8) 470 PAVI-F (ours) - 11.1 (±1.1) 300 Table B.2: Analytical performance over a Gaussian mixture HBM PAVI shows superior performance. PAVI converges in a fraction of CF’s optimization time —the second-best-performing architecture. across sets of data points is easier the closer the reduced set size is to the full set size. This constant training speed allows for a controlled memory footprint of the stochastic training: contrary to the PAVI-F scheme, Cardr(P1) could be maintained at a value manageable by the GPU without reducing the training speed. This example also displays a pathological choice for Cardr(P1): the encoder fails to learn to compute the correct summary statistics over sets of size 1. However trivial, this example underlines that Cardr(P1) should be chosen at a value logical with respect to the posterior’s sufficient statistics. When generalizing to moments of a higher order —such as the variance— there is therefore a theoretical lower bound to consider when fixating the value of Cardr(P1). Our intuition is that the more complex to estimate the statistic is, the larger Cardr(P1) should be. Note that in practice non-stochastic VI is intractable for large-scale models, because of its memory require- ment. In large-scale experiments such as the one presented in Figure 4, stochastic training is necessary. Our claim is to be faster than non-plate-amortized stochastic VI in those large-scale contexts —but not necessarily to be faster than non-stochastic VI in the small-scale regime of this experiment. For PAVI-F we nonetheless obtain similar training speed compared to non-stochastic VI with Cardr(P1) as small as 8. For PAVI-E, the stochastic training is as fast as the non-stochastic one. B.4 Additional comparative experiments B.4.1 Gaussian Mixture model In this experiment, we test out various baselines over a challenging model: a Gaussian mixture with random effects. D, Card(P1), Card(P0) = 2, 20, 10 ∀n1=1.. Card(P1) ∀n0=1.. Card(P0) Xn1,n0|θ1,n1, πn1 ∼ Mixture( � N(θ1 1,n1, σ2 x), . . . , N(θL 1,n1, σ2 x) � , πn1) ∀n1 = 1.. Card(P1) πn1 ∼ Dirichlet(1 ×⃗1L) ∀n1=1.. Card(P1) ∀l=1..L θl 1,n1|θl 2,0 ∼ N(θl 2,0, σ2 1) ∀l = 1..L θl 2,0 ∼ N(⃗0D, σ2 2) , (B.22) where Mixture([D1, . . . , DL], π) denotes a mixture between the distributions [D1, . . . , DL] with mixture weights π. Results are visible in Table B.2, where PAVI displays the best asymptotic ELBO, as well as the shortest optimization time. On that note, we underline that stochastic training over a mixture distri- bution is challenging, as —due to the sub-sampling of points— only a fraction of the mixture components could be expressed at a given step, requiring the architecture to dynamically cluster the data points across time. B.4.2 Hierarchical Variance model In this experiment, we test out our architectures over a non-canonical model, in which parent RVs play the role of variance for the distribution of their children. Our goal is in particular to evaluate a potential empirical bias for the PAVI-E scheme —as studied in Appendix A.2. We will refer to the following HBM as 32Architecture ELBO (102) Optimization time (s) CF - 11.2 (± 2.9) 500 UIVI -6.7 (± 0.8) 100 PAVI-E (ours) -6.7 (±1.0)1 90 PAVI-F (ours) - 6.7 (±0.8) 20 Table B.3: Analytical performance over a Hierarchical Variance HBM In this setting with hard- to-estimate summary statistics, PAVI-E doesn’t show any empirical bias. 1: PAVI-E suffers from numerical instability in this example, with runs degenerating into NaN results. Architecture ELBO (104) Optimization time (s) CF - 139.7 (± 21) 3,200 ADAVI — — UIVI — — PAVI-F (ours) - 8.2 (± 0.1) 1,600 Table B.4: Analytical performance over our Neuroimaging HBM This HBM is not pyramidal and consequently cannot be processed by the ADAVI architecture. Due to the large parameter space (approx- imately 2k parameters) and the complex density involved, despite intensive efforts we did not manage to obtain a numerically stable UIVI optimization. PAVI yields a higher ELBO than CF, in half of CF’s opti- mization time. the Hierarchical Variance model: D, Card(P1), Card(P0) = 2, 15, 15 ∀n1=1.. Card(P1) ∀n0=1.. Card(P0) log X(n1,n0)|θ1,n1 ∼ N(0, θ1,n1) ∀n1 = 1.. Card(P1) log θ1,(n1)|θ2,0 ∼ N(0, θ0,0) log θ2,0 ∼ N(⃗0D, 1) (B.23) In this model, the encoder —used in the PAVI-E scheme— has to collect non-trivial summary statistics: em- pirical variances across variable subsets of the observed data. This is the context in which we would expect to observe the most bias due to the approximation in Equation (A.13). The performance of PAVI-E re- mains nonetheless competitive. This illustrates how PAVI-E’s theoretical bias —introduced in the stochastic training— does not result in significantly worse inference compared to state-of-the-art architectures. PAVI also displays the best ELBO, in conjunction with UIVI, but in a 5 times shorter optimization time. B.4.3 Small dimension version of our Neuroimaging model In this experiment, we validate the performance of our architecture on synthetic data generated using a small dimension version of the model presented in Equation (B.24). To allow the use of the comparative baselines, we fixate S, Sr = 5, 3, T, T r = 6, 3, N, N r = 12, 3, D = 4, L = 7. Results are visible in Table B.4: PAVI can deal with HBMs unavailable to the ADAVI architecture (Rouillard & Wassermann, 2022), and so so with a performance superior to the CF baseline (Ambrogioni et al., 2021b). Despite intensive efforts, we did not manage to obtain a numerically stable UIVI optimization. This is likely due to the complexity of this inference problem: a large dimensionality (2k parameters) combined with complex dependencies between RVs in the posterior. In this context, automatic structured VI baselines such as CF or PAVI exploit the parametric form of the prior p to help with inference. PAVI’s plate amortization likely facilitates the inference, resulting in a reduced optimization error (Bottou & Bousquet, 2007). 33B.5 Experimental details - analytical examples All experiments were performed in Python, using the Tensorflow Probability library (Dillon et al., 2017). Through this section, we refer to Masked Autoregressive Flows (Papamakarios et al., 2018) as MAF. All experiments are performed using the Adam optimizer (Kingma & Ba, 2015). At training, the ELBO was estimated using a Monte Carlo procedure with 8 samples. All architectures were evaluated over a fixed set of 20 samples X, with 5 seeds per sample. Non-sample-amortized architectures were trained and evaluated on each of those points. Sample amortized architectures were trained over a dataset of 20, 000 samples separate from the 20 validation samples, then evaluated over the 20 validation samples. Termination condition Our termination condition amounts to plateau detection across the statistics of multiple runs. In details, we ran the inference with a large, conservative number of epochs for all methods and collected the evolution of the ELBO for each run. We then took as runtime the time to reach an ELBO close to the one at convergence for the method. We averaged over 100 repetitions to avoid stochastic noise due to Monte Carlo estimation of the ELBO. The order of magnitude difference between the results is clearly above the potential noise effects due to our procedure. B.5.1 Plate amortization and convergence speed (5.1) All 3 architectures (baseline, PAVI-F, PAVI-E) used: • for the flows Fi, a MAF with [32, 32] hidden units; • as encoding size, 128 For the encoder f in the PAVI-E scheme, we used a multi-head architecture with 4 heads of 32 units each, 2 ISAB blocks with 64 inducing points. B.5.2 Impact of encoding size (5.2) All architectures used: • for the flows Fi, a MAF with [32, 32] hidden units, after an affine block with triangular scaling matrix. • as encoding size, a value varying from 2 to 16 B.5.3 Scaling with plate cardinalities (5.3) ADAVI (Rouillard & Wassermann, 2022) we used: • for the flows Fi, a MAF with [32, 32] hidden units, after an affine block with triangular scaling matrix. • for the encoder, an encoding size of 8 with a multi-head architecture with 2 heads of 4 units each, 2 ISAB blocks with 32 inducing points. Cascading Flows (Ambrogioni et al., 2021b) we used: • a mean-field distribution over the auxiliary variables r • as auxiliary size, a fixed value of 8 • as flows, Highway Flows as designed by the Cascading Flows authors PAVI-F we used: 34• for the flows Fi, a MAF with [32, 32] hidden units, after an affine block with triangular scaling matrix. • an encoding size of 8 PAVI-E we used: • for the flows Fi, a MAF with [32, 32] hidden units, after an affine block with triangular scaling matrix. • for the encoder, an encoding size of 16 with a multi-head architecture with 2 heads of 8 units each, 2 ISAB blocks with 64 inducing points. UIVI we used as Card(P1) = 2 → 20 → 200: • as base distribution, a standard Gaussian with dimensionality 6 → 42 → 402 • as transform h, an affine transform with diagonal scale • as embedding distribution, a standard Gaussian with dimensionality 3 → 6 → 9 • as transform weights regressor, an MLP with hidden units [32, 32] → [64, 64] → [128, 128] • to sample uncorrelated samples ϵ, an HMC run with 5 burn-in steps and 5 samples • an Adam optimizer with exponential learning rate decay, starting at 1e − 2, ×0.9 every 300 steps B.5.4 Gaussian mixture (B.4.1) ADAVI (Rouillard & Wassermann, 2022) we used: • for the flows Fi, a MAF with [32] hidden units, after an affine block with diagonal scaling matrix. • for the encoder, an encoding size of 16 with a multi-head architecture with 2 heads of 8 units each, 2 ISAB blocks with 8 inducing points. Cascading Flows (Ambrogioni et al., 2021b) we used: • a mean-field distribution over the auxiliary variables r • as auxiliary size, a fixed value of 16 • as flows, Highway Flows as designed by the Cascading Flows authors PAVI-F we used: • for the flows Fi, a MAF with [32] hidden units, after an affine block with diagonal scaling matrix. • an encoding size of 16 • Cardr(P1) = 5 PAVI-E we used: • for the flows Fi, a MAF with [128, 128] hidden units, after an affine block with triangular scaling matrix. 35• for the encoder, an encoding size of 128 with a multi-head architecture with 4 heads of 32 units each, 2 ISAB blocks with 128 inducing points. • Cardr(P1) = 5 UIVI we used: • as base distribution, a standard Gaussian with dimensionality 82 • as transform h, an affine transform with diagonal scale • as embedding distribution, a standard Gaussian with dimensionality 6 • as transform weights regressor, a MLP with hidden units [64, 64] • to sample uncorrelated samples ϵ, an HMC run with 5 burn-in steps and 5 samples • an Adam optimizer with exponential learning rate decay, starting at 1e − 2, ×0.9 every 300 steps B.5.5 Hierarchical Variances (B.4.2) Cascading Flows (Ambrogioni et al., 2021b) we used: • a mean-field distribution over the auxiliary variables r • as auxiliary size, a fixed value of 32 • as flows, Highway Flows as designed by the Cascading Flows authors PAVI-F we used: • for the flows Fi, a MAF with [32, 32] hidden units, after an affine block with diagonal scaling matrix. • an encoding size of 32 • Cardr(P1) = 3 • Cardr(P0) = 3 PAVI-E we used: • for the flows Fi, a MAF with [128, 128] hidden units, after an affine block with diagonal scaling matrix. • for the encoder, an encoding size of 128 with a multi-head architecture with 4 heads of 32 units each, 2 ISAB blocks with 128 inducing points. • Cardr(P1) = 3 • Cardr(P0) = 3 UIVI we used: • as base distribution, a standard Gaussian with dimensionality 32 • as transform h, an affine transform with diagonal scale • as embedding distribution, a standard Gaussian with dimensionality 3 • as transform weights regressor, an MLP with hidden units [64, 64] • to sample uncorrelated samples ϵ, an HMC run with 5 burn-in steps and 5 samples • an Adam optimizer with exponential learning rate decay, starting at 1e − 2, ×0.9 every 300 steps 36B.5.6 Small Neuroimaging example (B.4.3) Cascading Flows (Ambrogioni et al., 2021b) we used: • a mean-field distribution over the auxiliary variables r • as auxiliary size, a fixed value of 32 • as flows, Highway Flows as designed by the Cascading Flows authors PAVI-F we used: • for the flows Fi, a MAF with [32, 32] hidden units, after an affine block with diagonal scaling matrix. • an combination of encoding sizes of 32 and 8 UIVI we used: • as base distribution, a standard Gaussian with dimensionality 1, 802 • as transform h, an affine transform with diagonal scale • as embedding distribution, a standard Gaussian with dimensionality 16 • as transform weights regressor, an MLP with hidden units [128, 256, 512, 1024] • to sample uncorrelated samples ϵ, an HMC run with 5 burn-in steps and 5 samples • an Adam optimizer with exponential learning rate decay, starting at 1e − 3, ×0.9 every 300 steps Optimization systematically degenerated into NaN results after around 200 optimization steps. B.6 Details about our Neuroimaging experiment (5.4) B.6.1 Data description In this experiment, we use data from the Human Connectome Project (HCP) dataset (Van Essen et al., 2012). We randomly select a cohort of S = 1, 000 subjects from this dataset, each subject is associated with T = 2 resting-state fMRI sessions (Smith et al., 2013). We minimally pre-process the signal using the nilearn python library (Abraham et al., 2014): 1. removing high variance confounds 2. detrending the data 3. band-filtering the data (0.01 to 0.1 Hz), with a repetition time of 0.74 seconds 4. spatially smoothing the data with a 4mm Full-Width at Half Maximum For every subject, we extract the surface Blood Oxygenation Level Dependent (BOLD) signal of N = 59, 412 vertices across the whole cortex. We compare this signal with the extracted signal of D = 64 DiFuMo components: a dictionary of brain spatial maps allowing for an effective fMRI dimensionality reduction (Dadi et al., 2020). Specifically, we compute the one-to-one Pearson’s correlation coefficient of every vertex with every DiFuMo component. The resulting connectome, with S subjects, T sessions, N vertices and a connectivity signal with D dimensions, is of shape (S × T × N × D). We project this data —correlation coefficients lying in ]−1; 1[— in an unbounded space using an inverse sigmoid function. 37B.6.2 Model description We use a model inspired by the work of Kong et al. (2019). We hypothesize that every vertex in the cortex belongs to either one of L = 7 functional networks. This number is inspired by the work of Thomas Yeo et al. (2011) and Kong et al. (2019), in which authors identify 7 general networks on resting-state fMRI data. Each network corresponds to a pattern of connectivity with the brain, represented as the correlation of the BOLD signal with the signal from the D = 64 DiFuMo components. We define L = 7 functional networks at the population level. Each network l corresponds to the connectivity fingerprint µl. At the population level, for a vertex n we denote as logitsn ∈ RL the logits of its probability to belong to each network l. Each subject s is associated with the logits logitss,n, which are considered as a perturbation of the population logits. This creates a regularization across subjects: the same vertex n is encouraged to have the same label across all subjects. The variable γl controls the inter-subject spatial variability across all subjects, for the network l. For every subject s, session t, and vertex n, we denote as Xs,t,n the observed connectivity. This connectivity is modeled via a mixture model: Xs,t,n is a perturbation of either one of the µl’s connectivity fingerprints. labels,n denotes the label in the mixture (that depends on the subject-specific logits). κl controls the variability between Xs,t,n and the mixture component µlabels,n. The resulting model can be described as: S, T, N, D, L = 1000, 2, 59412, 64, 7 ∀l=1..L : µl ∼ N(0 ×⃗1D, 6) ∀l=1..L : log κl ∼ N(0 ×⃗1L, 1) ∀n=1..N : logitsn ∼ N(0 ×⃗1L, 6) ∀l=1..L : log γl ∼ N(0 ×⃗1L, 1) ∀s=1..S ∀n=1..N : logitss,n | logitsn, [γl]l=1..L ∼ N(logitsn, [γ1...γL]) ∀s=1..S ∀n=1..N : labelss,n | logitss,n ∼ Categorical(logitss,n) ∀s=1..S ∀t=1..T ∀n=1..N : Xs,t,n|[µl]l=1..L, [κl]l=1..L, labels,n ∼ N(µlabels,n, κlabels,n) (B.24) The model contains 4 plates: the network plate of full cardinality L (that we did not exploit in our imple- mentation), the subject plate of full cardinality S, the session plate of full cardinality T and the vertex plate of full cardinality N. Our goal is to recover the posterior distribution of the networks µ and the labels label —represented as the parcellation in Figure 4— given the observed connectome described in Appendix B.6.1. B.6.3 PAVI implementation We used in this experiment the PAVI-F scheme, using: • for the RVs µl: – for the flows Fi, a MAF with [64, 128, 64] hidden units, following an affine block with diagonal scale – for the encoding size: 16 • for the RVs κl, γl: – a MAP estimator using a regressor with [64, 128, 64] hidden units – for the encoding size: 8 • for the RVs logitsn, logitss,n: – for the flows Fi, an affine block with diagonal scale regressed using a [64, 128, 64] MLP 38– for the encoding size: 8 • for the reduced model, we used Sr = 128, T r = 1 and N r = 128. To allow for the optimization over the discrete labels,n RV, we used the Gumbell-Softmax trick, using a fixed temperature of 1.0 (Jang et al., 2017; Maddison et al., 2016). B.6.4 Cognitive scores prediction For the cognitive scores prediction in Figure 4, we reproduced the standard methodology from Kong et al. (2019). We perform a 20-fold cross-validation across 1,000 subjects. We start from the logitss,n associated with each subject. We use PCA to project the features from the 19 training folds to their 100 first components (with 33% explained variance). We then train a linear regression to predict each of the 13 cognitive scores from the training-fold PCA features. We compute the test performance on the test fold —using the training set PCA and linear regression— averaged across the 13 cognitive measures. This process is reproduced 100 times. The reported scores are the triple average across folds, measures, and repetition, while the standard deviation is computed across the 100 repetitions only. C Supplemental discussion C.1 Plate amortization as a generalization of sample amortization In Section 4.1, we introduced plate amortization as applying the generic concept of amortization to the granularity of plates. There is actually an even stronger connection between sample amortization and plate amortization. An HBM p models the distribution of a given observed RV X —jointly with the parameters Θ. Different samples X0, X1, ... of the model p are i.i.d. draws from the distribution p(X). p can thus be considered as the model for "one sample". Instead of p, consider a "macro" model for the whole population of samples one could draw from p. The observed RV of that macro model would be the infinite collection of samples drawn from the same distribution p(X). In that light, the i.i.d. sampling of different X values from p could be interpreted as a plate of the macro model. Thus, we could consider sample amortization as an instance of plate amortization for the "sample plate". Or equivalently: plate amortization can be seen as the generalization of amortization beyond the particular case of sample amortization. C.2 Alternate formalism for SVI — PAVI-E(sa) scheme In this work, we propose a different formalism for SVI, based around the concept of full HBM Mfull versus reduced HBM Mredu sharing the same template T . This formalism is helpful to set up GPU-accelerated stochastic VI (Dillon et al., 2017), as it entitles a fixed computation graph -with the cardinality of the reduced model Mredu- in which encodings are "plugged in" -either sliced from larger encoding arrays or as the output of an encoder applied to a data slice, see Section 4.1&4.2. Particularly, our formalism doesn’t entitle a control flow over models and distributions, which can be hurtful in the context of compiled computation graphs such as in Tensorflow (Abadi et al., 2015). The reduced model formalism is also meaningful in the PAVI-E(sa), where we train and amortized variational posterior over Mredu and obtain "for free" a variational posterior for the full model Mfull —see Section 4.2. In this context, our scheme is no longer a different take on hierarchical, batched SVI: the cardinality of the full model is truly independent of the cardinality of the training and is only simulated as a scaling factor in the stochastic training —see Section 4.2. We have the intuition that fruitful research directions could stem from this concept. 39C.3 Benefiting from structure in inference Our contributions can be abstracted through the concept of plate amortization -see Section 4.1. Plate amortization is particularly useful in the context of heavily parameterized density approximators such as normalizing flows, but is not tied to it: plate-amortized Mean Field (Blei et al., 2017), ASVI (Ambrogioni et al., 2021a), or implicit (Yin & Zhou, 2018; Titsias & Ruiz, 2019) schemes are also possible to use. Plate amortization can be viewed as the amortization of common density approximators across different sub- structures of a problem. This general concept could have applications in other highly-structured problem classes such as graphs or sequences (Wu et al., 2020; Salehinejad et al., 2018). Central to our design, PAVI adjoins an encoding structure to an inference problem. The encodings Ei,n embed all the "individualized" information in the problem, while the shared conditional density estimators Fi translate those encodings into posterior distributions. We believe there is potential in externalizing part of the inference problem into an embedding. For instance, embeddings could be learned separately from density estimators, opening up the possibility for transfer learning (Weiss et al., 2016). Or the encodings could integrate some known symmetry of the problem, such as convolution-based encodings in the case of Random Fields (Bishop, 2006). C.4 Connection with meta-learning In Section 4.1, we introduced plate amortization: sharing the parameterization and learning across a model’s plates. This section discusses the connection between plate amortization and meta-learning(Ravi & Beatson, 2019; Iakovleva et al., 2020; Yao et al., 2019). Supervised learning can be seen as the mapping from a given context set C = {(x, y)} to a predictive function f (Bishop, 2006) such that f(x) = y. Meta-learning —or "learning to learn"— instead recovers this mapping C �→ f in the general case. Once the meta-training is completed, a predictive function f conditioned by an unseen context C can be obtained in a single forward pass —without any training done on C. As an instance of meta-learning, the Neural Process Family encodes the context C via a deep set encoder (Garnelo et al., 2018; Dubois et al., 2020; Zaheer et al., 2018). The encoded context, along with the data point x is then used to condition an estimator for the density q(y|x, C). This framework is similar to the PAVI-E scheme, where a combination of a deep set encoder and a normalizing-flow-based density estimator output the posterior probability of a ground RV θi,n. This encoder- estimator pair is repeatedly used across a plate. This is analogous to meta-learning to solve the inference problem across the different elements of a plate —such as the different subjects of a population study. In PAVI-E, the encoding Ei,n at a lower hierarchy plays a role similar to the context C in meta-leaning. A few differences, however, exist between the two frameworks: • meta-learning is typically concatenated to the 1-plate regime, whereas PAVI is designed for the multi-hierarchy scenario; • meta-learning typically considers a set of i.i.d. tasks, whereas in PAVI the inference of different ground RVs θi,n are conditionally dependent through the hierarchical model p; • meta-learning is trained using the forward KL loss, that is to say in the sample-amortized regime, maximizing the probability q(y) of samples from the underlying generative process. In contrast, PAVI —though possible to train using the forward KL loss— is trained via the reverse KL, which requires to evaluate the density of the generative process explicitly. We suspect there would be interesting applications for the PAVI architecture in hierarchical meta-learning scenarios. C.5 Conditional dependencies modeled in the variational family Consider an inference problem with three latent parameters θ1, θ2, θ3, and a model p(Θ, X) that factorizes as p(θ1, θ2, θ3) = p(θ1)p(θ2)p(θ3|θ1, θ2)p(X|θ3). To approximate the posterior p(Θ|X), different conditional 40dependencies can be modeled in the chosen variational family Q. The mean-field (MF) approximation fully factorizes the variational distribution as q(θ1, θ2, θ3) = q(θ1)q(θ2)q(θ3). MF was originally introduced in VI to facilitate computation, allowing dedicated optimization schemes (Blei et al., 2017). Nonetheless, this approximation assumes independence between RVs in the posterior and ultimately limits the expressivity of the variational family. To remove this approximation, modeling complex conditional dependencies in the variational family is an open research subject (Ambrogioni et al., 2021b; Webb et al., 2018). In the PAVI design, we inherit our statistical dependency structure from the prior distribution p, as detailed in Equation (3). This amounts to factorizing q(θ1, θ2, θ3) = q(θ1)q(θ2)q(θ3|θ1, θ2). This choice of dependencies follows the line of research from Hoffman & Blei (2014) and Ambrogioni et al. (2021a). As pointed out by Ambrogioni et al. (2021b), when modeling only those forward dependencies, the modeling of colliders can be an issue. More generally, one would like to model arbitrary dependencies in the variational posterior. As an example, the factorization q(θ1, θ2, θ3) = q(θ3)q(θ2|θ3)q(θ1|θ2, θ3) would more faithfully represent the conditional dependencies that can arise in the posterior. Modeling arbitrary dependencies reduces the inference gap but can become computationally intractable as the number of RVs increases. In PAVI, we strike a particular balance between expressivity and computational tractability, as further motivated in this section. In the case of a single plate —the 2-level case— Agrawal & Domke (2021) demonstrate that modeling only the forward dependencies does not result in reduced expressivity compared to the modeling of the full dependencies. Yet this result does not hold in the n-level case, as Webb et al. (2018) shows that faithful inversion features conditional dependencies in the posterior between ground RVs of the same template. We can dub those dependencies as horizontal dependencies across RVs in the same plate. Like Structured SVI (Hoffman & Blei, 2014) or Automatic SSVI (Ambrogioni et al., 2021a), the PAVI design thus results in reduced expressivity when stacking multiple plates in the model p. This issue can be partially alleviated with the usage of a backward encoding scheme —going in the reverse direction compared to the prior’s dependencies— as in the PAVI-E design (see Section 4.2) or in Cascading Flows (Ambrogioni et al., 2021b). In practice, though limiting our expressivity, horizontal dependencies are difficult to inject back into our architecture. Critically, in the PAVI design, the use of a common density estimator across the ground RVs of the same template (see Section 4.1) and the stochastic training over batches of those RVs (see Section 4.2) prevent the direct modeling horizontal dependencies. Put differently, the fact that we consider the inference over different ground RVs as conditionally independent inference problems is central to our design and adverse to the modeling of horizontal dependencies. Injecting horizontal dependencies back into our variational family is therefore a non-trivial research direction, that is not at the core of this paper. This opens up promising research directions: how could arbitrary conditional dependencies be modeled in the variational posterior in the context of stochastic training? C.6 Towards user-friendly Variational Inference By re-purposing the concept of amortization at the plate level, we aim to propose clear computation versus precision trade-offs in VI. Hyper-parameters such as the encoding size —as illustrated in Figure 2 (right)— allow to clearly trade inference quality in exchange for a reduced memory footprint. On the contrary, in clas- sical VI, changing Q’s parametric form —for instance, switching from Gaussian to Student distributions— can have a strong and complex impact on the number of weights and inference quality (Blei et al., 2017). By allowing the usage of normalizing flows in very large cardinality regimes, our contribution aims at dis- entangling approximation power and computational feasibility. In particular, having access to expressive density approximators for the posterior can help experimenters diversify the proposed HBMs, removing the need for properties such as conjugacy to obtain meaningful inference (Gelman et al., 2004). Combining clear hyper-parameters and scalable yet universal density approximators, we tend towards a user-friendly methodology in the context of large population studies VI. In its current implementation, PAVI requires to set up a rather large number of hyper-parameters. This includes choosing a normalizing flow architecture, the size of the encodings at different plate levels, and the reduced model cardinalities. Those hyper-parameters are in addition to generic ones such as the number of samples from the variational family used to estimate the ELBO, or the choice of an optimizer. However, 41our experiments suggest that those hyper-parameters could default to conservative values. As shown in Section 5.2, the encoding size should be maximized with respect to the available memory, as should the reduced cardinalities, as detailed in Appendix B.3. As for the normalizing flow architecture, since the latter is shared across plates, the user can always default to the most expressive architecture available without incurring an exploding number of weights. As an example, in our experiments, we stuck to the MAF architecture (Papamakarios & Murray, 2018). Since all hyper-parameters can be set conservatively, a user- friendly automatic VI API can be designed, where the user only provides the system with the generative model p, and some observed value X. The variational family q can then automatically be derived from p, and directly optimized over X. This is the design principle we implemented in the codebase adjoined to this publication. 42
Interactive and Concentrated Differential Privacy for Bandits Achraf Azize and Debabrota Basu Équipe Scool, Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189- CRIStAL F-59000 Lille, France {achraf.azize,debabrota.basu}@inria.fr Abstract Bandits play a crucial role in interactive learning schemes and modern recom- mender systems. However, these systems often rely on sensitive user data, making privacy a critical concern. This paper investigates privacy in bandits with a trusted centralized decision-maker through the lens of interactive Differential Privacy (DP). While bandits under pure ϵ-global DP have been well-studied, we contribute to the understanding of bandits under zero Concentrated DP (zCDP). We provide minimax and problem-dependent lower bounds on regret for finite-armed and linear bandits, which quantify the cost of ρ-global zCDP in these settings. These lower bounds reveal two hardness regimes based on the privacy budget ρ and suggest that ρ-global zCDP incurs less regret than pure ϵ-global DP. We propose two ρ- global zCDP bandit algorithms, AdaC-UCB and AdaC-GOPE, for finite-armed and linear bandits respectively. Both algorithms use a common recipe of Gaussian mechanism and adaptive episodes. We analyze the regret of these algorithms to show that AdaC-UCB achieves the problem-dependent regret lower bound up to multiplicative constants, while AdaC-GOPE achieves the minimax regret lower bound up to poly-logarithmic factors. Finally, we provide experimental validation of our theoretical results under different settings. 1 Introduction Multi-armed bandit (in brief, bandits) (Lattimore and Szepesvári, 2020) is the archetypal setting of reinforcement learning consisting of K actions corresponding to K unknown reward distributions {νa}a∈[K]. We call {νa}a∈[K] ≜ ν an environment or a bandit instance. For T time steps, a bandit algorithm (or policy) π chooses an action (or arm) at ∈ [K] and receives a reward rt from the reward distribution νat. The goal of the policy is to maximise the cumulative reward �T t=1 rt or equivalently minimise the regret, i.e. the cumulative reward that π cannot achieve since it does not know the optimal reward distribution a priori. Bandits are increasingly used in a wide range of sequential decision-making tasks under uncertainty, such as recommender systems (Silva et al., 2022), strategic pricing (Bergemann and Välimäki, 1996), clinical trials (Thompson, 1933) to name a few. These applications often involve individuals’ sensitive data, such as personal preferences, financial situation, and health conditions, and thus, naturally, invoke data privacy concerns in bandits. Example 1 (DoctorBandit). Let us consider a bandit algorithm recommending one of K medicines with distributions of outcomes {νa}a∈[K]. Specifically, on the t-th day, a new patient ut arrives, and medicine at ∈ [K] is recommended to her by a policy π. To recommend a medicine at, the policy might either consider the specific medical conditions (or context) ct of patient ut, or ignore it. Then, the patient’s reaction to the medicine is observed. If the medicine cures the patient, the observed reward rt = 1, otherwise rt = 0. This observed reward can reveal sensitive information about the health condition of patient ut. Thus, the goal of a privacy-preserving bandit algorithm is 16th European Workshop on Reinforcement Learning (EWRL 2023). arXiv:2309.00557v1 [stat.ML] 1 Sep 2023Algorithm 1 Sequential interaction between a policy and users 1: Input: A policy π = {πt}T t=1 and Users {ut}T t=1 represented by the table d ≜ {x1, . . . , xT } ∈ (RK)T 2: Output: A sequence of actions a1, . . . , aT 3: for t = 1, . . . , T do 4: π recommends action at ∼ πt(. | a1, r1, . . . , at−1, rt−1) 5: ut sends the sensitive reward rt ≜ xt,at to π 6: end for to recommend a sequence of medicines (actions) that cures the maximum number of patients while protecting the privacy of these patients. We present this interactive process in Algorithm 1. Differential privacy for bandits. Motivated by such data-sensitive scenarios, privacy issues are widely studied for bandits for different settings, such as stochastic bandits (Mishra and Thakurta, 2015; Tossou and Dimitrakakis, 2016; Sajed and Sheffet, 2019; Azize and Basu, 2022; Hu and Hegde, 2022), adversarial bandits (Tossou and Dimitrakakis, 2017), and linear contextual bandits (Shariff and Sheffet, 2018; Neel and Roth, 2018; Hanna et al., 2022). All these works adhere to Differential Privacy (DP) Dwork et al. (2014) as the framework to ensure the data privacy of users, which is presently the gold-standard of privacy-preserving data analysis. DP dictates that an algorithm’s output has a limited dependency on the presence of any single user. Also, multiple formulations of DP, namely local and global, are extended to bandits Basu et al. (2019). Here, we focus on the global DP formulation, where users trust the centralised decision-maker, i.e. the policy, and provide it access to the raw sensitive rewards. The goal of the policy is to reveal the sequence of actions while protecting the privacy of the users and achieving minimal regret. The existing works on global DP preserving bandits consider pure ϵ-DP and assume that the action sequence is published non-interactively in one-shot. In this paper, we extend the study of privacy in bandits to the settings, where an adversary interacts with a policy at each step (Vadhan and Wang, 2021), and the algorithm aims to achieve popular relaxations of pure DP, e.g. zero Concentrated DP (zCDP) (Dwork and Rothblum, 2016). Interactive DP. A bandit algorithm induces an interactive process (Algorithm 1). At each step of this interaction, an adversary can manipulate the arm suggested by the algorithm and return it a reward from another arm. This situation is invoked in non-compliant bandits, where the user deploys an arm other than the recommended one, and in poisoning attacks, where a manipulated version of reward is sent to the policy either to leak information or to destroy its performance. This motivates us to define Interactive DP for bandits. A bandit policy π satisfying Interactive DP protects all possible outcomes corresponding to a user by making the view of the adversary indistinguishable when interacting with the policy on neighbouring reward datasets. Our effort resonates with the recent works in DP (Vadhan and Wang, 2021; Vadhan and Zhang, 2022; Lyu, 2022), where an analyst interacts with an offline dataset through an adaptive sequence of queries. The goal is to preserve privacy while responding to these adaptive queries. Our work extends the study of Interactive DP to the online setting, where a bandit algorithm generates its data by sequentially interacting with the environment (Section 2). Relaxations of pure DP for bandits. Pure DP is widely studied for different settings of bandits. Recently, lower bounds on regret for finite-armed and linear bandits preserving pure global DP, and algorithm design techniques to match the lower bounds are proposed (Azize and Basu, 2022). This still leaves open the question that what will be the minimal cost of preserving relaxations of pure DP in bandits as stated in (Shariff and Sheffet, 2018; Azize and Basu, 2022). Additionally, pure DP is often achieved by using Laplace noise to perturb the statistics computed on history. While in practice, Gaussian noise is widely used to perturb statistics computed on the dataset that leads to preserving relaxations of pure DP, namely (ϵ, δ)-DP (Dwork et al., 2014), Rényi DP (RDP) (Mironov, 2017), and zero Concentrated DP (zCDP) (Dwork and Rothblum, 2016), but not pure DP. Our goal is to provide a complete picture of regret’s lower and upper bounds for a relaxation of pure DP. In private bandits, proving regret lower bounds often rely on coupling arguments where group privacy is a central property (Azize and Basu, 2022). Since zCDP scales well under group privacy, we adopt zCDP as the relaxation of pure DP. In this work, we investigate zCDP in two settings of bandits: stochastic bandits with finitely many arms, and stochastic linear bandits with (fixed) finitely many arms. To our knowledge, we are the first to study the complexity of zCDP for bandits with global DP. The central questions that we aim to address are: 1. What is the minimal cost to pay in terms of regret to achieve ρ-global zCDP for bandits? 2. How to design bandit algorithms that can achieve these regret lower bounds order-optimally? 2Table 1: The complexity of bandits with ρ-global zCDP. Each lower bound is the maximum of the classical non-private bound and the corresponding bound in the third column. Setting Type Regret Lower Bound due to ρ zCDP 1 Regret Upper Bound Finite-armed Minimax ρ−1/2K (Thm 2, a) O �� KT log(T) � + O � ρ−1/2K � log(T) � (Thm 5, a) Problem Dependent 2 ρ−1/2 � a:∆a>0 � ∆ata−1� � log(T) (Thm 2, b) O �� a log T ∆a � + O � ρ−1/2K � log(T) � (Thm 5, b) Linear Minimax ρ−1/2d (Thm 3) O �� dT log(KT) � + O � ρ−1/2d log 3 2 (KT) � (Thm 7) Our contributions. Answering these questions leads us to: 1. Hardness as regret lower bounds: First, addressing the open problem of (Shariff and Sheffet, 2018; Azize and Basu, 2022), we prove minimax and problem-dependent lower bounds for finite-armed bandits, and minimax lower bound for linear bandits with ρ-global zCDP that quantify the cost to ensure ρ-global zCDP in these settings (Section 3). The minimax lower bounds show the existence of two privacy regimes depending on the privacy budget ρ and the horizon T. Specifically, for ρ = Ω(T −1), an optimal algorithm does not have to pay any cost to ensure privacy in both settings. In the problem-dependent analysis, the additional regret due to ρ-global zCDP in finite-armed bandits appears as a lower order term, i.e. Ω �� (log T)/ρ � , with respect to the non-private lower bound Ω(log T). In contrast, the regret due to ϵ-global DP, Ω ((log T)/ϵ), is not a lower order term. 2. Order-optimal algorithm design: We propose two algorithms, AdaC-UCB and AdaC-GOPE, that preserves ρ-global zCDP for finite-armed and linear bandits, respectively (Section 4). Both algorithms share the same algorithmic blueprint. First, they add a calibrated Gaussian noise to reward statistics. Second, they run in adaptive episodes, with the number of episodes being logarithmic in T. We analyse the regret of both algorithms and show that they match the lower bounds up to multiplicative factors. AdaC-UCB achieves the problem-dependent lower up to multiplicative constants, while both AdaC-UCB and AdaC-GOPE match the corresponding minimax lower bounds up to poly-logarithmic factors. We summarise all the lower and upper bounds in Table 1. In Section 5, we numerically validate their performances in different settings. 3. Technical tools: We propose a novel technique to generate lower bounds for bandits with ρ-zCDP using coupling arguments. We adapt this technique to the sequential bandit setup and use it to derive regret lower bounds with a generic proof. We also discuss in depth the effect of partial information (bandit feedback) on the definition of DP for bandits (Appendix A). We also prove a lower bound on the cost of reward poisoning against an Interactive DP bandit algorithm (Theorem 8). This opens up a direction to bridge privacy defences and attacks for bandits. 2 Bandits with Interactive DP: The formulation First, we formalise Interactive DP for bandits with a centralised decision-maker. We adopt the Interactive DP definition as studied in (Vadhan and Zhang, 2022; Lyu, 2022), where a mechanism M is viewed as a party in an interactive protocol, interacting with a possibly adversarial analyst or users. We represent each user ut by the vector xt ≜ (xt,1, . . . , xt,K) ∈ RK, where xt,a represents the potential reward observed if action a was recommended to user ut. Due to the bandit feedback, if at step t action qt ∈ [K] was queried from the environment, only the reward rt = xt,qt is observed at step t. Thus, the set of users {ut}T t=1 is represented by the table of potential rewards d ≜ {x1, . . . , xT } ∈ (RK)T . We view the policy π as an interactive mechanism, taking as input the table of potential rewards d while interacting with a possibly adversarial analyst B. The interactive protocol is described in Definition 1. Definition 1 (The bandit-adversary interactive protocol). A policy π = {πt}T t=1, with its input d the table of potential rewards, and the adversary B = {Bt}T t=1 follow the interactive process: For t = 1, . . . , T: 1. The bandit algorithm selects an action ot ∼ π(d; q1, q2, . . . , qt−1), 2. The adversary returns a query action qt = Bt(o1, o2, . . . , ot). 3. The bandit algorithm observes the reward corresponding to qt for user ut, i.e. xt,qt. 1Here, we only express the private part of the lower bound 2For Bernoulli bandits, ta = ∆a and the lower bound reduces to Kρ−1/2� log(T). 3Here, we denote π(d; q1, q2, . . . , qt−1) ≜ πt(· | q1, x1,q1, . . . , qt−1, xt−1,qt−1). At each step t, the policy π recommends an action ot. The adversary B observes the recommended action ot, and chooses (adversarially) a query action qt, based on the history of recommended actions (os)t−1 s=1. At step t + 1, the policy recommends the next action ot+1 based only on its private input d containing all the sensitive rewards information about the users, and the adversarially chosen query actions (qs)t s=1. Following the Interactive DP framework, the policy π is a differentially private interactive mechanism if the view of adversary B, i.e. View(B ↔ π(d)) ≜ (o1, . . . , oT ), is indistinguishable when the interaction is run on two neighbouring tables of rewards d and d′. They represent two sets of users differing by only one individual, i.e. one row. Formally, it implies that the Hamming distance between the two tables d and d′ is one, dHam(d, d′) ≜ �T t=1 1 {xt ̸= x′ t}. We denote them by d ∼ d′. Definition 2 (Interactive DP policy). A policy π = {πt}T t=1 is said to be a. Interactive (ϵ, δ)-DP policy for a given δ ∈ [0, 1) if, for every pair of neighboring table of potential rewards datasets d, d′ ∈ X, every adversary B ∈ B, and every subset of possible views S ⊆ [K]T , Pr[View(B ↔ π(d)) ∈ S] ≤ exp(ϵ) · Pr[View(B ↔ π(d′)) ∈ S] + δ. b. Interactive (α, ϵ)-RDP policy for an α > 1 if, for every adversary B ∈ B, supd∼d′ Dα(View(B ↔ π(d))∥View(B ↔ π(d′))) ≤ ϵ. c. Interactive (ξ, ρ)-zCDP policy if, for every α ∈ (1, ∞), and every adversary B ∈ B, sup d∼d′ Dα(View(B ↔ π(d))∥View(B ↔ π(d′))) ≤ ξ + ρα. (1) Here, Dα(P∥Q) ≜ 1 α−1 log EQ �� dP dQ �α� denotes the Rényi divergence of order α between P and Q. We define ϵ-pure global DP as (ϵ, 0)-DP and ρ-global zCDP to be (0, ρ)-zCDP. Implications of ensuring Interactive DP for bandits: We elaborate on three interesting implications of the interactive definition of privacy in bandits, compared to the non-interactive definition adopted in the literature. We recall the non-interactive definition in detail in Appendix A. 1. Interactive adversarial hypothesis testing: Interactive DP (Def. 2) defends against an online adversary, who can manipulate the actions recommended by the policy. The adversary participates in the interaction between the algorithm and environment and can run an interactive (or sequential) hypothesis testing (Wald, 1992) to distinguish between two neighbouring datasets based on its view of the interaction. In contrast, the adversary in the non-interactive definition only observes the sequence of actions a = (at)T t=1 that the policy π recommends without any interference. Based on the sequence a, the adversary runs a one-shot hypothesis testing to distinguish between two neighbouring datasets (Kairouz et al., 2015). Hence, Interactive DP allows us to defend against a stronger and more realistic adversary. 2. Protecting non-compliant users: Interactive DP (Def. 2) protects the privacy of the users even if they are non-compliant (Kallus, 2018; Stirn and Jebara, 2018), i.e. the users decide to ignore the recommendations of the policy and choose a different arm. Specifically, the policy recommends an action ot at step t, but the adversary could choose another query action qt, different than ot. The reward rt = dt,qt and the decision of the policy ot+1 in the next step depend on the query action qt and not ot. In contrast, the non-interactive definition only protects the privacy of compliant users. 3. Defending against online poisoning attacks: In the Interactive DP definition, the policy rec- ommends action ot at step t and expects to receive the corresponding reward dt,ot. However, an adversary may intentionally query a different action qt, resulting in the observed reward dt,qt. This can be viewed as poisoning the reward from dt,ot to dt,qt (Liu and Shroff, 2019). The interactive definition inherently provides robustness against online reward poisoning. In Appendix A.4, we show that if a policy π is consistent and ρ-global zCDP, an online oracle attacker has to incur Ω( � (log T)/ρ) cost to make the policy choose a non-optimal target arm linearly. Thus, for a smaller privacy budget ρ, i.e. high privacy, an attacker has to poison further more to succeed. Remark: Handling bandit feedback. Ensuring privacy in bandit settings requires careful consid- eration of how the bandit feedback, or partial information, is handled. When a policy selects arms at, it only observes the reward rt = dt,at associated with arm at, while the other rewards dt,a for a ̸= at remain unobserved. A fundamental question in defining privacy for bandits is whether the private input dataset of the policy π should be considered as the set of observed rewards {rt}T t=1 (referred to as View DP), or the entire table of potential rewards d (referred to as Table DP). In 4App. A, we compare both definitions and demonstrate that Table DP is a stronger notion, as being Table DP implies being View DP. This intuition stems from the fact that Table DP protects users ut by safeguarding all their potential responses. We also establish that, for pure DP, the two notions are equivalent. However, for approximate DP and its variations, transitioning from View DP to Table DP causes a significant loss in the privacy budget due to group privacy considerations. For a detailed comparison and discussion, we refer to App. A. In Definition 2, we adopt the Table DP framework. Goal: Regret minimisation. Hereafter, we adopt ρ-global zCDP (Eq. (1)) as the privacy definition. The goal is to design a ρ-global zCDP policy that minimises regret. To define regret, we adhere to the classic interaction of bandits as explained in Algorithm 1. Specifically, the adversary is no longer part of the interaction. The policy π directly interacts with the set of users {u1, . . . , uT } as in Algorithm 1, and the goal is to maximize the expected cumulative reward, or equivalently minimize the expected regret. We study two settings: finite-armed stochastic bandits and stochastic linear bandits. Now, we formally define regrets for them. Finite-armed stochastic bandits. The environment ν ≜ (νa : a ∈ [K]) consists of K arms (or reward distributions) with finite means (µa)a∈[K]. For any horizon T, regret is defined as RegT (π, ν) ≜ Tµ⋆ − E � T � t=1 rt � = K � a=1 ∆aE [Na(T)] , (2) where µ⋆ ≜ maxa∈[K] µa is the mean of the optimal arm a⋆, ∆a ≜ µ⋆ − µa is the sub-optimality gap of the arm a, and Na(T) ≜ �T t=1 1 {at = a} is the number of times the arm a is played till T in the interaction of Algorithm 1. The expectation is taken both on the randomness of the environment ν and the policy π, using the canonical bandit model (Chapter 4.6 of (Lattimore and Szepesvári, 2020)). Stochastic linear bandits. We consider that a fixed set of actions A ⊂ Rd is available at each round, such that |A| = K. The rewards are generated by a linear structural equation. Specifically, at step t, the observed reward is rt ≜ ⟨θ⋆, at⟩ + ηt, where θ⋆ ∈ Rd is the unknown parameter, and ηt is a conditionally 1-subgaussian noise, i.e. E [exp (ληt) | a1, η1, . . . , at−1] ≤ exp � λ2/2 � almost surely for all λ ∈ R. For any horizon T > 0, the regret of a policy π is RegT (π, A, θ⋆) ≜ Eθ⋆ � T � t=1 ∆At � , (3) where suboptimality gap ∆a ≜ maxa′∈A ⟨a′ − a, θ⋆⟩. Eθ⋆[·] is the expectation with respect to the measure of outcomes induced by the interaction of π and the linear bandit environment (A, θ⋆). Remark. There are two interaction protocols: The bandit-adversary interactive protocol of Defini- tion 1 and the sequential interaction between a policy and users of Algorithm 1. The bandit-adversary interactive protocol is used to analyse the privacy of the policy. Specifically, we want to design a policy for which the view of an adversary is “similar" when only one user changes in the interaction of Definition 1. On the other hand, to analyse the accuracy of the policy, we adhere to the "classic" sequential interaction between a policy and users. In this interaction (Algorithm 1), the policy recommends at each time-step an action at and observes the reward rt corresponding to the user ut in the table d, i.e. rt = xt,at. There is no adversary in this interaction, and the goal of the policy is to maximize the expected cumulative reward or equivalently minimize the expected regret, when interacting with users, without the presence of the adversary. In brief, we want to design a policy that verifies the “adversarial" privacy constraint and minimizes the classic “expected" regret. 3 Lower bounds on regret of bandits with ρ-global zCDP In this section, we quantify the cost of ρ-global zCDP for bandits by providing regret lower bounds for any ρ-global zCDP policy. These lower bounds on regret provide valuable insight into the inherent hardness of the problem and establish a target for optimal algorithm design. We first derive a ρ-global zCDP version of the KL-decomposition Lemma using a sequential coupling argument. The regret lower bounds are then retrieved by plugging the KL upper bound in classic regret lower bound proofs. A summary of the lower bounds is in Table 1, while the proof details are deferred to Appendix C. KL decomposition lemma. To proceed with the lower bounds, first, we are interested to control the Kullback-Leibler (KL) divergence between marginal distributions induced by a ρ-zCDP mechanism 5when the datasets are generated using two different distributions. In particular, if P1 and P2 are two data-generating distributions over X n, we define the marginals over the output of mechanism M as Mν(A) ≜ � d∈X n M (A | d) dPν (d) , (4) when the inputs are generated from P1 and P2, i.e. for ν ∈ {1, 2} and A ∈ F. Theorem 1 (KL decomposition for ρ-zCDP). Let P1 and P2 be two product distributions over X n, i.e. P1 = �n i=1 p1,i and P2 = �n i=1 p2,i, where pν,i for ν ∈ {1, 2}, i ∈ [1, n] are distributions over X. Let ti ≜ TV (p1,i ∥ p2,i). If M is ρ-zCDP, then KL (M1 ∥ M2) ≤ ρ � n � i=1 ti �2 + ρ n � i=1 ti(1 − ti) (5) This is a centralised ρ-zCDP version of the KL-decomposition lemma under local DP (Duchi et al., 2013, Theorem 1), and a ρ-zCDP version of the Sequential Karwa-Vadhan lemma (Azize and Basu, 2022). In Appendix B, we elaborate on the new proof technique, which can be of parallel interest. Leveraging this decomposition, now, we derive two flavours of regret lower bounds, namely minimax and problem-dependent. The minimax lower bound expresses the best regret achievable by a policy on the corresponding worst-case environment. The problem-dependent lower bound controls the regret of a ‘reasonable’ (consistent) policy for a specific environment that the policy interacts with. Lower bounds on regret for finite-armed bandits Theorem 2 (Minimax and problem-dependent lower bounds for finite-armed bandits). (a) Minimax. Let Πρ be the set of ρ-zCDP policies. For any K > 1, T ≥ K − 1, and 0 < ρ ≤ 1, Regminimax T,ρ ≜ inf π∈Πρ sup ν∈EK RegT (π, ν) ≥ max � 1 27 � � T(K − 1) �� � without ρ-global zCDP , 1 44 K − 1 √ρ � �� � with ρ-global zCDP � . (b) Problem-dependent. Let E = M1 × · · · × MK be a class of environments with K arms, where Ma is a set of reward distributions with finite means. Let π be a consistent policy3 over E satisfying ρ-global zCDP . Then, for all ν = (Pa)K a=1 ∈ E, (i.e. Pa ∈ Ma ), it holds that lim inf T →∞ RegT (π, ν) � log(T) ≥ � a:∆a>0 ∆a √ρ tinf (Pa, µ∗, Ma). where tinf (P, µ∗, M) ≜ infP ′∈M {TV (P ∥ P ′) : µ (P ′) > µ∗} Comments on the minimax bound. The minimax regret lower bound suggests the existence of two hardness regimes depending on ρ and T. When ρ < (27/44)(K − 1)/T, i.e. the high-privacy regime, the lower bound becomes Ω � K/√ρ � , and ρ-global zCDP bandits incur more regret than non-private ones. When ρ > (27/44)(K − 1)/T, i.e. in the low-privacy regime, the lower bound retrieves the non-private lower bound, i.e. Ω( √ KT), and thus, we can achieve privacy for free. Comments on the problem-dependent bound. The problem-dependent lower bound shows that the price of privacy is a lower order term Ω �� log(T)/ρ � . For a fixed privacy budget ρ and asymptotically in T, this is negligible compared to the non-private problem-dependent regret lower bound of Ω (� a log(T)/∆a). In contrast, for pure ϵ-global DP, the price of privacy in the problem- dependent regret is Ω (log(T)/ϵ), which is not a second-order term (Azize and Basu, 2022). Thus, in a problem-dependent perspective, privacy is ‘free’ only for ρ-global zCDP, but not for ϵ-global DP. Lower bound on regret for linear bandits Theorem 3 (Minimax Lower Bounds for Linear Bandits). Let A = [−1, 1]d and Θ = Rd. Then, for any ρ-global zCDP policy, we have that Regminimax T (A, Θ) ≥ max          exp(−2) 8 d √ T � �� � without ρ-global zCDP , exp(−2.25) 4 d √ρ � �� � with ρ-global zCDP          . 3A policy π is called consistent over a class of environments E, if ∀ν ∈ E and p > 0, limT →∞ RT (π,ν) T p = 0. 6Algorithm 2 AdaC-UCB. Changes due to privacy are in blue. 1: Input: Privacy budget ρ, an environment ν with K arms, optimism parameter β > 3 2: Output: Actions satisfying ρ-global zCDP 3: Initialisation: Choose each arm once and let t = K 4: for ℓ = 1, 2, . . . do 5: Let tℓ = t + 1 6: Compute aℓ = argmaxa Iρ a(tℓ − 1, β) (Eq. (6)) 7: Choose arm aℓ until round t such that Naℓ(t) = 2Naℓ(tℓ − 1) 8: end for Two privacy regimes. Similar to the finite-arm case, the minimax regret lower bound for linear bandits suggests the existence of two hardness regimes for ρ ≥ 4 exp(−0.5) T and ρ ≤ 4 exp(−0.5) T . A generic proof technique. In order to prove the lower bounds, we deploy the KL upper bound of Theorem 1 in the classic proof scheme of regret lower bounds (Lattimore and Szepesvári, 2020). The high-level idea of proving bandit lower bounds is selecting two hard environments, which are hard to statistically distinguish but are conflicting, i.e. actions that may be optimal in one is sub-optimal in other. The KL upper bound of Theorem 1 allows us to quantify the extra-hardness to statistically distinguish environments due to the additional ‘blurriness’ created by the ρ-zCDP constraint. 4 Algorithm design: AdaC-UCB and AdaC-GOPE In this section, we propose AdaC-UCB and AdaC-GOPE, two algorithms that satisfy ρ-global zCDP for finite-armed and linear bandits respectively. The two algorithms share a similar blueprint: the Gaussian mechanism and adaptive episodes. For each setting, we present the algorithm, provide a privacy and a regret analysis, and compare the regret upper bounds to the regret lower bounds. 4.1 Stochastic finite-armed bandits Now, we study the setting of finite-armed bandits under ρ-zCDP as detailed in Section 2. Algorithm. AdaC-UCB is an extension of the generic algorithmic wrapper proposed by Azize and Basu (2022) for bandits with ρ-global zCDP. Following (Azize and Basu, 2022), AdaC-UCB relies on three ingredients: arm-dependent doubling, forgetting, and adding calibrated Gaussian noise. First, the algorithm runs in episodes. The same arm is played for a whole episode, and double the number of times it was last played. Second, at the beginning of a new episode, the index of arm a, as defined in Eq. (6), is computed only using samples from the last episode, where arm a was played, while forgetting all the other samples. In a given episode, the arm with the highest index is played for all the steps. Due to these two ingredients, namely doubling and forgetting, each empirical mean computed in the index of Eq. (6) only needs to be ρ-zCDP for the algorithm to be ρ-global zCDP, avoiding the need of composition theorems. We formalise this intuition in Lemma 7 of Appendix D. For AdaC-UCB, we use the private index to select the arms (Line 6 of Algorithm 2) as Iρ a(tℓ − 1, β) ≜ ˆµℓ a + N � 0, σ2 a,ℓ � + Ba(tℓ − 1, β). (6) Here, ˆµℓ a is the empirical mean of rewards collected in the last episode in which arm a was played, σ2 a,ℓ ≜ 1 2ρ×( 1 2 Na(tℓ−1)) 2 is the variance of the Gaussian noise. Finally, the exploration bonus is defined as Ba(tℓ − 1, β) ≜ �� 1 2× 1 2 Na(tℓ−1) + 1 ρ×( 1 2 Na(tℓ−1)) 2 � β log(tℓ). The term in blue rectifies the non-private confidence bound of UCB for the added Gaussian noise. Theorem 4 (Privacy of AdaC-UCB). For rewards in [0, 1], AdaC-UCB satisfies ρ-global zCDP. Proof sketch. The main idea is that a change in one user only affects the empirical mean calculated in one episode, which is made private using the Gaussian Mechanism and Lemma 7. Since the actions are computed only using the private empirical means, AdaC-UCB is ρ-global zCDP thanks to the post-processing lemma. We refer to Appendix D for the complete proof. Theorem 5 (Regret analysis of AdaC-UCB). For rewards in [0, 1] and β > 3, AdaC-UCB yields (a) a problem-dependent regret upper bound � a:∆a>0 � 8β ∆a log(T) + 8 � β ρ � log(T) + 2β β−3 � , and 7Algorithm 3 AdaC-GOPE. Changes due to privacy are in blue. 1: Input: Privacy budget ρ, A ⊂ Rd and δ 2: Output: Actions satisfying ρ-global zCDP 3: Initialisation: Set ℓ = 1, t1 = 1 and A1 = A 4: for ℓ = 1, 2, . . . do 5: βℓ ← 2−ℓ 6: Step 1: Find the G-optimal design πℓ for Aℓ: max π∈P(Aℓ) |Supp(π)|≤d(d+1)/2 log det V (π). (7) 7: Step 2: Sℓ ← Supp (πℓ) 8: Choose each action a ∈ Sℓ for Tℓ(a) ≜ ⌈cℓπℓ(a)⌉ times where cℓ is defined by Eq (8). 9: Observe rewards {rt} tℓ+� a Tℓ(a) t=tℓ 10: Tℓ ← � a∈Sℓ Tℓ(a) and tℓ+1 ← tℓ + Tℓ + 1 11: Step 3: Estimate the parameter as ˆθℓ = V −1 ℓ �tℓ+1−1 t=tℓ atrt with Vℓ = � a∈Sℓ Tℓ(a)aa⊤ 12: Step 4: Make the parameter estimate private ˜θℓ = ˆθℓ +V − 1 ℓ 2 Nℓ, where Nℓ ∼ N � 0, 2d ρcℓ Id � . 13: Step 4: Eliminate low rewarding arms: Aℓ+1 = � a ∈ Aℓ : maxb∈Aℓ � ˜θℓ, b − a � ≤ 2βℓ � . 14: end for (b) a minimax regret upper bound O �� KT log(T) � + O � Kρ−1/2� log(T) � . Order-optimality of AdaC-UCB. The problem-dependent regret upper bound of AdaC-UCB matches the problem-dependent regret lower bound of Theorem 2 up to multiplicative constants for Bernoulli Bandits. On the other hand, the minimax regret upper bound of AdaC-UCB achieves the minimax regret lower bound of Theorem 2 up to an extra √log T term, which is usually the extra cost to pay in minimax regret for the UCB algorithm. Discussion on related bounds. Under a distributed setting and for (α, ϵ)-RDP, Chowdhury and Zhou (2022) propose a variant of Successive Elimination (SE) with Skellam noise, which achieves a O(K � log(T)/ϵ) private regret. However, for non-private bandits, optimism-based strategies achieve optimality and have better performance than SE. This is shown by Azize and Basu (2022) while comparing their adaptive mechanism, AdaP-UCB, with DP-SE in the case of ϵ-pure DP. Similar reasoning follows here. Second, Skellam Noise is less practical to sample from than Gaussian Noise. 4.2 Stochastic Linear Bandits Here, we study ρ-global zCDP for stochastic linear bandits with a finite number of arms, as in Sec. 2. Algorithm. We propose AdaC-GOPE (Algorithm 3), which is a ρ-global zCDP extension of the G-Optimal design-based Phased Elimination (GOPE) algorithm (Lattimore and Szepesvári, 2020, Algorithm 12). AdaC-GOPE is a phased elimination algorithm. At the end of each episode ℓ, AdaC-GOPE eliminates the arms that are likely to be sub-optimal, i.e. the ones with an empirical gap exceeding the current threshold (βℓ = 2−ℓ). The elimination criterion only depends on the samples collected in the current episode. In addition, the actions to be played during an episode are chosen based on the solution of an optimal design problem (Equation (7)) that helps to exploit the structure of arms and to minimise the number of samples needed to eliminate a suboptimal arm. In particular, if πℓ is the G-optimal solution for Aℓ at phase ℓ, then each action a ∈ Aℓ is played Tℓ(a) ≜ cℓπℓ(a) times, where for δ′ ≜ δ Kℓ(ℓ+1), cℓ ≜ 8d β2 ℓ log � 4 δ′ � + 2d βℓ �2 ρ � d + 2 � d log � 2 δ′ � + 2 log � 2 δ′ ��1/2 . (8) The term in blue is the additional length of the episode to compensate for the noisy statistics used to ensure privacy. The samples collected in the current episode do not influence which actions are played in it. This decoupling allows: (a) the use of the tighter confidence bounds available in the fixed design setting (Appendix F.2), and (b) avoiding privacy composition theorems and using, therefore, Lemma 7 to make the algorithm private. Note that AdaC-GOPE can be seen as a generalisation of DP-SE (Sajed and Sheffet, 2019) to the linear bandit setting. 810−1 101 103 Privacy budget ρ 0 100 200 300 Regpriv − Regnon−priv T = 105 T = 106 T = 107 0.0 0.2 0.4 0.6 0.8 1.0 Step t ×107 0.0 0.2 0.4 0.6 0.8 1.0 PoP ρ =0.1 ρ = 0.5 ρ = 1 (a) Finite-armed Bandits 10−1 101 103 Privacy budget ρ 0 500 1000 Regpriv − Regnon−priv T = 104 T = 105 T = 106 0.0 0.5 1.0 Step t ×107 0.0 0.1 0.2 0.3 PoP ρ = 0.01 ρ = 0.1 ρ = 1 (b) Linear Bandits Figure 1: For each bandit setting, the left figure represents the evolution of the difference between the private and non-private regret with respect to the privacy budget ρ. The right figure represents the evolution of the price of privacy (PoP) with respect to the time step. Assumption 1 (Boundedness). We assume that: (1) actions are bounded: ∥a∥2 ≤ 1 for all a in A, (2) rewards are bounded: |rt| ≤ 1, and (3) the unknown parameter is bounded: ∥θ⋆∥2 ≤ 1. Theorem 6 (Privacy of AdaC-GOPE). Under Assumption 1, AdaC-GOPE satisfies ρ-global zCDP. Proof sketch. Similar to Theorem 4, a change in one user only affects the estimate ˆθℓ in one episode. Thanks to Lemma 7, it is enough that each ˆθℓ is ρ-zCDP with respect to the sequence of rewards collected in the corresponding episode. Since the actions only depend on the estimates { ˆθℓ}ℓ, the algorithm is ρ-global zCDP by the post-processing lemma. We refer to Appendix D for the proof. Theorem 7 (Regret analysis of AdaC-GOPE). Under Assumption 1 and for δ ∈ (0, 1), with probability at least 1 − δ, the regret RT of AdaC-GOPE (Algorithm 3) is upper-bounded by A � dT log � K log(T ) δ � + Bd √ρ � log � K log(T ) δ � log(T), where A and B are universal constants. If δ = 1 T , then E(RT ) ≤ O �� dT log(KT) � + O � d √ρ(log(KT)) 3 2 � . Order-optimality of AdaC-GOPE. The minimax regret upper bound of AdaC-GOPE matches with the minimax regret lower bound of Theorem 3 up to an extra (log KT) 3 2 factor. Related algorithms and bounds. Hanna et al. (2022) and Li et al. (2022) study private variants of the GOPE algorithm for pure ϵ-global DP and (ϵ, δ)-DP, respectively. However, both algorithms differ in how they privatize the estimated parameter ˆθ compared to AdaC-GOPE. They add noise to each sum of rewards �tℓ+1−1 t=tℓ rt (Line 11, Alg. 3), whereas we add noise in ˆθl (Line 12, Alg. 3). As a result, though we achieve linear dependence on the dimension d as suggested by the lower bound, others do not (d2 for (Hanna et al., 2022) and d3/2 for (Li et al., 2022)). In Appendix F, we analyse in detail the impact of adding noise at different steps of GOPE, both theoretically and experimentally. 5 Experimental analysis Now, we empirically verify whether AdaC-UCB and AdaC-GOPE can achieve privacy for free. Experimental setup. For finite-armed bandits, we test AdaC-UCB with β = 1 and compare it to its non-private counterpart, i.e. a UCB algorithm with adaptive episodes and forgetting. We test the algorithms for Bernoulli bandits with 5-arms and means {0.75, 0.625, 0.5, 0.375, 0.25} (as in (Sajed and Sheffet, 2019)). For linear bandits, we implement AdaC-GOPE and compare it to GOPE. We set the failure probability to δ = 0.001 and the noise to be ρt = N(0, 1). We use the Frank-Wolfe algorithm to solve the G-optimal design problem (Lattimore and Szepesvári, 2020). We chose K = 10 actions randomly on the unit tri-dimensional sphere (d = 3). The true parameter θ⋆ is also chosen randomly on the tri-dimensional sphere. For both settings, we run the private and non-private algorithms 100 times for a horizon T = 107, and compare the average regret between the private and non-private algorithms in Figure 1. Results and analysis. We reach two conclusions from the results of both settings. 1. Free-privacy in low-privacy regime. For a fixed horizon T, the difference between the private and non-private regret, Regpriv − Regnon−priv, converges to zero as the privacy budget ρ → ∞. Thus, our algorithms achieve the same regret as their non-private counterparts in the low-privacy regime. 2. Asymptotic no price of privacy. For a fixed privacy budget ρ, the Price of Privacy (PoP), i.e. PoP ≜ Regpriv−Regnon−priv Regnon−priv converges to zero as the horizon T increases. This observation resonates with both the theoretical regret upper bounds of the algorithms and the hardness suggested by the lower bounds, where cost due to privacy appears as lower-order terms. 96 Conclusion and future works We study bandits with interactive ρ-global zCDP. First, we demonstrate the benefits of adopting the Interactive DP definition for bandits. Then, we prove the minimax and problem-dependent regret lower bounds for finite-armed and linear bandits, showing that the additional regret due to ρ-global zCDP is less compared to pure ϵ-global DP. The minimax bound additionally shows the existence of two hardness regimes and privacy can be achieved for free in the low-privacy regime. We propose AdaC-UCB and AdaC-GOPE, which satisfy ρ-global zCDP using a generic algorithmic blueprint, and match the regret lower bounds up to constants and poly-logarithmic factors respectively. A possible future direction is to derive regret lower bounds for bandits with (ϵ, δ)-DP. Both pure ϵ-DP and ρ-zCDP enjoy a (‘tight’) group privacy property that gives meaningful lower bounds for bandits, when applied with coupling arguments. These arguments fail to adapt to (ϵ, δ)-DP. An interesting technical challenge would be to adapt, for bandits, the fingerprinting lemma, which is a technique used for proving (ϵ, δ)-DP lower bounds (Bun et al., 2014; Kamath et al., 2022). For the algorithm design, it would be also interesting to see how to close the multiplicative gaps. Acknowledgments and Disclosure of Funding This work is supported by the AI_PhD@Lille grant. D. Basu acknowledges the Inria-Kyoto University Associate Team “RELIANT” for supporting the project, and the ANR JCJC for the REPUBLIC project (ANR-22-CE23-0003-01). We also thank Philippe Preux for his support. References Azize, A. and Basu, D. (2022). When privacy meets partial information: A refined analysis of differentially private bandits. Advances in Neural Information Processing Systems, 35:32199– 32210. Basu, D., Dimitrakakis, C., and Tossou, A. (2019). Differential privacy for multi-armed bandits: What is it and what is its cost? arXiv preprint arXiv:1905.12298. Bergemann, D. and Välimäki, J. (1996). Learning and strategic pricing. Econometrica: Journal of the Econometric Society, pages 1125–1149. Bun, M. and Steinke, T. (2016). Concentrated differential privacy: Simplifications, extensions, and lower bounds. In Theory of Cryptography, pages 635–658, Berlin, Heidelberg. Springer Berlin Heidelberg. Bun, M., Ullman, J., and Vadhan, S. (2014). Fingerprinting codes and the price of approximate differential privacy. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing, pages 1–10. Chowdhury, S. R. and Zhou, X. (2022). Distributed differential privacy in multi-armed bandits. arXiv preprint arXiv:2206.05772. Duchi, J. C., Jordan, M. I., and Wainwright, M. J. (2013). Local privacy and statistical minimax rates. In Proc. of IEEE Foundations of Computer Science (FOCS). Dwork, C., Roth, A., et al. (2014). The algorithmic foundations of differential privacy. Foundations and Trends® in Theoretical Computer Science, 9(3–4):211–407. Dwork, C. and Rothblum, G. N. (2016). Concentrated differential privacy. ArXiv, abs/1603.01887. Hanna, O. A., Girgis, A. M., Fragouli, C., and Diggavi, S. (2022). Differentially private stochastic linear bandits:(almost) for free. arXiv preprint arXiv:2207.03445. Hu, B. and Hegde, N. (2022). Near-optimal thompson sampling-based algorithms for differentially private stochastic bandits. In Uncertainty in Artificial Intelligence, pages 844–852. PMLR. Jun, K.-S., Li, L., Ma, Y., and Zhu, J. (2018). Adversarial attacks on stochastic bandits. Advances in Neural Information Processing Systems, 31. 10Kairouz, P., Oh, S., and Viswanath, P. (2015). The composition theorem for differential privacy. In International conference on machine learning, pages 1376–1385. PMLR. Kallus, N. (2018). Instrument-armed bandits. In Algorithmic Learning Theory, pages 529–546. PMLR. Kamath, G., Mouzakis, A., and Singhal, V. (2022). New lower bounds for private estimation and a generalized fingerprinting lemma. arXiv preprint arXiv:2205.08532. Lalanne, C., Garivier, A., and Gribonval, R. (2022). On the statistical complexity of estimation and testing under privacy constraints. arXiv preprint arXiv:2210.02215. Lattimore, T. and Szepesvári, C. (2020). Bandit algorithms. Cambridge University Press. Li, F., Zhou, X., and Ji, B. (2022). Differentially private linear bandits with partial distributed feedback. In 2022 20th International Symposium on Modeling and Optimization in Mobile, Ad hoc, and Wireless Networks (WiOpt), pages 41–48. IEEE. Liu, F. and Shroff, N. (2019). Data poisoning attacks on stochastic bandits. In International Conference on Machine Learning, pages 4042–4050. PMLR. Lyu, X. (2022). Composition theorems for interactive differential privacy. In Advances in Neural Information Processing Systems. Mironov, I. (2017). Rényi differential privacy. In Proceedings of 30th IEEE Computer Security Foundations Symposium (CSF), pages 263–275. Mishra, N. and Thakurta, A. (2015). (Nearly) optimal differentially private stochastic multi-arm bandits. In UAI. Neel, S. and Roth, A. (2018). Mitigating bias in adaptive data gathering via differential privacy. In International Conference on Machine Learning, pages 3720–3729. PMLR. Sajed, T. and Sheffet, O. (2019). An optimal private stochastic-MAB algorithm based on an optimal private stopping rule. Shariff, R. and Sheffet, O. (2018). Differentially private contextual linear bandits. In Advances in Neural Information Processing Systems, pages 4296–4306. Silva, N., Werneck, H., Silva, T., Pereira, A. C., and Rocha, L. (2022). Multi-armed bandits in recommendation systems: A survey of the state-of-the-art and future directions. Expert Systems with Applications, 197:116669. Stirn, A. and Jebara, T. (2018). Thompson sampling for noncompliant bandits. arXiv preprint arXiv:1812.00856. Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3-4):285–294. Tossou, A. C. and Dimitrakakis, C. (2016). Algorithms for differentially private multi-armed bandits. In Thirtieth AAAI Conference on Artificial Intelligence. Tossou, A. C. and Dimitrakakis, C. (2017). Achieving privacy in the adversarial multi-armed bandit. In Thirty-First AAAI Conference on Artificial Intelligence. Vadhan, S. and Wang, T. (2021). Concurrent composition of differential privacy. In Theory of Cryptography: 19th International Conference, TCC 2021, Raleigh, NC, USA, November 8–11, 2021, Proceedings, Part II 19, pages 582–604. Springer. Vadhan, S. and Zhang, W. (2022). Concurrent composition theorems for all standard variants of differential privacy. arXiv preprint arXiv:2207.08335. Wald, A. (1992). Sequential tests of statistical hypotheses. Springer. 11Appendix Table of Contents A Privacy definitions for bandits 13 A.1 Non-interactive DP for bandits . . . . . . . . . . . . . . . . . . . . . . . . . . 13 A.2 Interactive DP for bandits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 A.3 Consequences of the Interactive DP definition . . . . . . . . . . . . . . . . . . 17 A.4 Poisoning attacks against Interactive DP . . . . . . . . . . . . . . . . . . . . . 19 B Lower bounds via couplings for concentrated DP 20 B.1 From the KL to a transport problem . . . . . . . . . . . . . . . . . . . . . . . . 20 B.2 Proxy solution to the transport Problem . . . . . . . . . . . . . . . . . . . . . . 21 C Regret lower bounds for bandits under ρ-global zCDP 23 C.1 Stochastic finite-armed bandits: Minimax lower bound . . . . . . . . . . . . . . 23 C.2 Stochastic finite-armed bandits: Problem-dependent lower bound . . . . . . . . 24 C.3 Stochastic linear bandits: Minimax lower bound . . . . . . . . . . . . . . . . . 25 D Privacy proofs 30 D.1 The privacy lemma of non-overlapping sequences . . . . . . . . . . . . . . . . 30 D.2 Generic privacy proof of AdaC-UCB and AdaC-GOPE . . . . . . . . . . . . . 31 E Stochastic bandits with global zCDP 35 E.1 Concentration inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 E.2 Regret analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 E.3 Extensions to (ϵ, δ)-global DP and (α, ϵ)-global RDP . . . . . . . . . . . . . . 37 F Linear Bandits with global zCDP 38 F.1 Basic definitions of optimal design . . . . . . . . . . . . . . . . . . . . . . . . 38 F.2 Concentration inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 F.3 Regret analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 F.4 Extensions to (ϵ, δ)-global DP and (α, ϵ)-global RDP . . . . . . . . . . . . . . 42 F.5 Adding noise at different steps of AdaC-GOPE . . . . . . . . . . . . . . . . . . 43 G Existing technical results and definitions 45 H Extended experimental analysis 46 12A Privacy definitions for bandits In this section, we discuss different ways to adopt Differential Privacy (DP) in bandits. The main ingredients to specify in order to have a complete definition are (1) the mechanism in question, (2) the input dataset, (3) the neighbouring relationship between the input datasets, and (4) the output of the mechanism. For all the adaptations of DP for bandits studied in this section, the output of the mechanism is the same, i.e. a sequence of actions in [K]T . The mechanism in question is always induced by the policy. For completeness, we recall the definition of the policy in Definition 3. The main differences in the adaptations of DP originate from two sources: 1. Considering the policy as an interactive or non-interactive mechanism. 2. Considering the input of the mechanism to be the sequence of observed rewards, i.e. r = {r1, . . . , rT } ∈ RT , that we call View DP. Alternatively, considering the input of the mechanism to be the full table of potential rewards, i.e. d = {d1, . . . , dT } ∈ (RK)T , that we call Table DP. Before starting, we recall the definition of the policy. Let T ∈ N be the horizon. For each t ∈ [T], let Ωt = ([K] × R)t and Ft = B(Ωt) with B being the Borel set. Definition 3. A policy π is a sequence of rules (πt)T t=1 , where each πt is a probability kernel from the histories (Ωt−1, Ft−1) to arms ([K], 2[K]). Since [K] is discrete, we adopt the convention that for i ∈ [K], πt(i | a1, r1, . . . , at−1, rt−1) = πt({i} | a1, r1, . . . , at−1, rt−1) In the following, we first consider the non-interactive definition of privacy in bandits, which is the definition usually adopted in the private bandit literature. There, we mainly discuss the relationship between View and Table DP for different variants of DP. Then, we formalize the interactive definition of privacy in bandits. We state several consequences of the interactive definition. Also, we discuss its relation to the non-interactive definition, as well as to poisoning attacks. A.1 Non-interactive DP for bandits In this section, we define View and Table DP. Then, we discuss their relations. We prove that Table DP always implies View DP, with the same privacy parameters. However, the converse may not be always true. The equivalence can only be shown for pure ϵ-DP. For other variants of DP, there could be a huge increase in the privacy budget. To commence, we recall the defintions of variants of DP for a non-interactive mechansim M. Definition 4 (Variants of Approximate DP (ADP) for non-interactive mechanisms). A non-interactive mechanism M, that assigns to each dataset d a probability distribution Md on some measurable space (X, F), satisfies 1. (ϵ, δ)-DP ( Dwork et al. (2014)) for a given δ ∈ [0, 1) if sup A∈F,d∼d′ Md(A) − eϵMd′(A) ≤ δ. 2. (α, ϵ)-Rényi DP (RDP) ( Mironov (2017)) for an α > 1 if sup d∼d′ Dα(Md∥Md′) ≤ ϵ. 3. (ξ, ρ)-zero Concentrated DP (zCDP) ( Bun and Steinke (2016)) if, for all α ∈ (1, ∞), sup d∼d′ Dα(Md∥Md′) ≤ ξ + ρα. Here, two datasets d and d′ are said to be neighbouring (denoted by d ∼ d′) if their Hamming distance is one. Dα(P∥Q) ≜ 1 α−1 log EQ �� dP dQ �α� denotes the Rényi divergence of order α between P and Q. We define ϵ-pure DP as (ϵ, 0)-DP and ρ-zCDP to be (0, ρ)-zCDP. 13A.1.1 View DP This is the definition usually adopted in the literature of private bandits (Mishra and Thakurta, 2015; Tossou and Dimitrakakis, 2016; Sajed and Sheffet, 2019; Azize and Basu, 2022). We formalise it by stating its main ingredients, and coin the term "View DP". Input. The input considered is only the sequence of observed rewards r = (r1, . . . , rT ) and the neighbouring is a change in one reward in this sequence. Mechanism. The induced mechanism from the interaction of π and a list of rewards r ≜ (rt)t∈[T ] ∈ RT is Vπ such that Vπ : RT → P([K]T ) r → Vπ r The mechanism Vπ, when applied to a sequence of observed rewards r, outputs (in one shot) a sequence of actions aT ≜ (a1, . . . , aT ) ∈ [K]T , with probability Vπ r (aT ) = �T t=1 πt(at−1|a1, r1, . . . at, rt−1). Neighbouring input. The Hamming distance between two lists of rewards r, r′ ∈ RT is the number of different elements in r and r′, i.e. dHam(r, r′) ≜ T � t=1 1 {rt ̸= r′ t} Privacy definition. A policy π is • (ϵ, δ)-view DP if Vπ is (ϵ, δ)-DP • (α, ϵ)-view RDP if Vπ is (α, ϵ)-RDP • (ξ, ρ)-view zCDP if Vπ is (ξ, ρ)-zCDP A.1.2 Table DP To formalise the intuition of Figure 2, Table DP protects the patients by considering the input of the mechanism as the table which represents all the possible outcomes of the bandit interaction. Again in the following, we explain the mechanism to be made DP, its inputs, outputs and the neighbouring relation between its input. Interaction protocol. Let ν ≜ {Pa : a ∈ [K]} a bandit instance with K arms. The policy π interacts with the environment ν up to a given time horizon T to produce a history HT ≜ {(at, rt)}T t=1. The iterative steps of this interaction process yielding HT verify two conditions: 1. the conditional probability of choosing an action at = a at time t is dictated only by the policy πt(a|Ht−1), 2. the conditional distribution of reward rt given (Ht−1, at) is Pat. Thus, the policy can be seen as a set of adaptively chosen queries, applied to an adaptively-gathered data set of rewards. Conceived this way, it is hard to decouple inputs from outputs to extend DP correctly and protect the privacy of users. Input. To overcome this problem, we will adhere to the random table model of bandits (Lattimore and Szepesvári, 2020). Each user ut is represented by the row vector xt ≜ (xt,1, . . . , xt,K) ∈ RK, where xt,a represents the potential reward observed, if action a was recommended to user ut. Due to the bandit feedback, only rt = xt,at is observed at step t. One can verify that defined this way, the induced history HT ≜ {(at, rt)}T t=1 from the interaction between π and ν still verifies the two conditions 1. and 2. as defined above. The distribution of HT depends both on the stochasticity of the environment ν and the randomness of the policy π and is denoted by Pν,π. 14Using the random table model, the input corresponding to the set of users {u1, . . . , uT } is the fixed dataset d = {x1, . . . , xT } ∈ (RK)T . This way, the bandit interaction can be seen as applying a set of adaptively chosen queries on a fixed dataset. Mechanism. The induced mechanism from the interaction of the policy π and a table of rewards d ≜ {(xt,i)i∈[K]}t∈[T ] ∈ (RK)T is Mπ such that Mπ : (RK)T → P([K]T ) d → Mπ d The mechanism Mπ when applied to a dataset d outputs (in one shot) a sequence of actions aT ≜ (a1, . . . , aT ) ∈ [K]T with probability Mπ d(aT ) = �T t=1 πt(at|a1, x1,a1, . . . at−1, xt−1,at−1). Neighbouring Input. A change in one user reflects as a change in one row in the table d, so we define dHam(d, d′) ≜ �T t=1 1 {xt ̸= x′ t} = �T t=1 1 � ∃i ∈ [K], xt,i ̸= x′ t,i � . Privacy definition. A policy π is • (ϵ, δ)-Table DP if Mπ is (ϵ, δ)-DP • (α, ϵ)-Table RDP if Mπ is (α, ϵ)-RDP • (ξ, ρ)-Table zCDP if Mπ is (ξ, ρ)-zCDP Figure 2: An example of the interaction of a policy with two sets of patients, that differ in one user only. Each row represents the potential reactions of the patient to each medicine, but only one reaction is observed by the policy, i.e. the framed one. A change in one patient reflects as a change in one row in this table of potential rewards. A.1.3 Relation Between Table ADP and View ADP Definitions Table DP is a stronger notion of privacy than View DP. Lemma 1. For a fixed policy π, we have that Mπ is ADP ⇒ Vπ is ADP. Proof. Suppose Mπ is ADP. Let r, r′ ∈ RT be two lists of rewards such that dHam(r, r′) = 1. Define d such that dt,i = rt for all i ∈ [K] and all t ∈ [T], i.e. d is the table of rewards where r is concatenated colon-wise K times. We define d′ similarly with respect to r′. For d, d′ defined this way, we have that dHam(d, d′) = 1 , Vπ r = Mπ d and Vπ r′ = Mπ d′. This means that • If Mπ is (ϵ, δ)-DP, then ∀A ∈ P([K]T ) Vπ r (A) − eϵVπ r′(A) = Mπ d(A) − eϵMπ d′(A) ≤ δ and Vπ is (ϵ, δ)-DP. 15• If Mπ is (α, ϵ)-RDP, then Dα(Vπ r ∥Vπ r′) = Dα(Mπ d∥Mπ d′) ≤ ϵ and Vπ is (α, ϵ)-RDP • If Mπ is (ξ, ρ)-zCDP, then Dα(Vπ r ∥Vπ r′) = Dα(Mπ d∥Mπ d′) ≤ ξ + ρα and Vπ is (ξ, ρ)-zCDP. For pure ϵ-DP, View DP and Table DP are equivalent. Lemma 2. For a fixed policy π, we have that Mπ is ϵ-DP ⇔ Vπ ϵ-DP. Proof. (Proving ⇒) Mπ is ϵ-DP → Vπ ϵ-DP is true by Lemma 1, because an ϵ-DP mechanism is also (ϵ, δ)-DP for δ = 0. (Proving ⇐) Suppose Vπ is ϵ-DP. We want to show that Mπ is ϵ-DP too. Let d, d′ ∈ (RK)T such that dHam(d, d′) = 1. For aT ∈ [K]T , let daT ≜ (d1,a1, d2,a2, . . . , dT,aT ) ∈ RT be the trajectory of reward induced by aT in d. Since dHam(d, d′) = 1, we have that ∀aT ∈ [K]T dHam(daT , d′ aT ) ≤ 1. Let aT ∈ [K]T . We have that Mπ d(aT ) = Vπ daT (aT ) ≤ eϵVπ d′ aT (aT ) = eϵMπ d′(aT ) where the inequality is because Vπ is ϵ-DP and daT ∼ d′ aT . Thus, Mπ is ϵ-DP. Remark 1. The crux of the reciprocal proof comes from the fact that to prove ϵ-DP, you only need to check the atomic events aT . In that case, we can link Mπ and Vπ easily. This is not the case for approximate DP. For example, for (ϵ, δ), there is a huge loss in parameters. Lemma 3. For a fixed policy π, we have that Vπ is (ϵ, δ)-DP ⇒ Mπ is (ϵ, KT δ)-DP. Proof. Suppose Vπ is (ϵ, δ)-DP. Let d, d′ ∈ (RK)T such that dHam(d, d′) = 1. We have that, for every a ∈ [K]T , dHam(daT , d′ aT ) ≤ 1. Let E ⊂ [K]T be an event, i.e a set of sequences. We have that Mπ d(E) = � aT ∈E Mπ d(aT ) = � aT ∈E Vπ daT (aT ) ≤ (a) � aT ∈E (eϵVπ d′ aT (aT ) + δ) ≤ (b) eϵMπ d′(E) + KT δ , where (a) holds true because Vπ is (ϵ, δ)-DP, and (b) is true because card(E) ≤ KT . This means that Mπ is (ϵ, KT δ)-DP All in all, Table DP is the notion of privacy that we adhere to in this paper, since it protects all the potential responses of an individual rather than just the observed one. 16A.2 Interactive DP for bandits The classic non-interactive definition of DP (Definition 4) considers only mechanisms M that release answers in one shot. However, data analysts often interact with a database in an adaptive fashion. This motivates the study of interactive mechanisms to capture full-featured privacy-preserving data analytics. Here, we adopt the Interactive DP definition as expressed in (Vadhan and Zhang, 2022). The mechanism M is viewed as a party in an interactive protocol, interacting with a possibly adversarial analyst. We recall the complete definition here. Definition 5 (Interactive protocol). An interactive protocol (A, B) is any pair of functions on tuples of binary strings. The interaction between A with input xA and B with input xB is the following random process (denoted (A(xA), B(xB))): 1. Uniformly choose random coins rA and rB for A and B, respectively. 2. Repeat the following for i = 0, 1, . . .. (a) If i is even, let mi = A(xA, m1, m3, . . . , mi−1; rA). (b) If i is odd, let mi = B(xB, m0, m2, . . . , mi−1; rB). (c) If mi = halt, then exit loop. The view of a party in an interactive protocol captures everything the party “observes” dur- ing the execution. If (A, B) is an interactive protocol, A’s view of the interaction is the tuple ViewA(A(xA) ← B(xB)) = (rA, xA, m1, m3, . . .) consisting of all the messages received by A in the execution of the protocol together with the private input xA and random coins rA. B’s view of (A(xA; rA), B(xB; rB)) is defined symmetrically. In the setting of DP, Party A is the mechanism, where the input xA is the dataset. Party B is the adversary that does not have an input xB. Since we only care about the view of the adversary, we will drop the subscript and denote the view of the adversary as View(B ↔ M(x)). With this notation, interactive differential privacy is defined by asking for the views of an adversary on any pair of neighbouring datasets View(B ↔ M(x)) and View(B ↔ M(x′)) satisfying the same closeness notion as in non-interactive differential privacy. Definition 6 (Variants of Approximate DP (ADP) for Interactive mechanisms). A mechanism M is said to be an 1. (ϵ, δ)-DP interactive mechanism for a given δ ∈ [0, 1) if, for every pair of neighboring datasets d, d′ ∈ X, every adversary B ∈ B, and every subset of possible views S ⊆ Range(View), we have Pr[View(B ↔ M(x)) ∈ S] ≤ exp(ϵ) · Pr[View(B ↔ M(x′)) ∈ S] + δ. 2. (α, ϵ)-RDP interactive mechanism for an α > 1 if, for every adversary B ∈ B sup d∼d′ Dα(View(B ↔ M(d))∥View(B ↔ M(d′))) ≤ ϵ. 3. (ξ, ρ)-zCDP interactive mechanism if, for every α ∈ (1, ∞), and every adversary B ∈ B sup d∼d′ Dα(View(B ↔ M(d))∥View(B ↔ M(d′))) ≤ ξ + ρα. The interactive protocol Definition 5 is adapted to bandits in Definition 1. Similarly, the interactive definitions of Definition 6 are formalised for bandits in Definition 2. A.3 Consequences of the Interactive DP definition Here, we state different corollaries and lemmas, obtained as consequences of Interactive DP. First, we recall that to check the interactive DP condition, it is enough to only consider deterministic adversaries (Lemma 2.2 in Vadhan and Wang (2021)). Second, it is easy to see that interactive DP implies non-interactive DP. Lemma 4. If π is Interactive b-ADP, π is b-global ADP. Proof. This is direct by taking the identity-adversary Bid defined by Bid t (o1, . . . , ot) = ot. 17We also provide the following lemma, that relates the interactive and non-interactive definitions, using an interactive post-processing. Lemma 5 (Relation between interactive and non-interactive DP for bandits). π is Interactive b-ADP if and only if, for every deterministic adversary B, πB is b-Table ADP, where πB = {πB t }T t=1 and πB t (a | a1, r1 . . . , at−1, rt−1) ≜ πt(a | B(a1), r1, B(a1, a2), r2 . . . , B(a1, . . . , at−1), rt−1) (9) Remark 2. We use b-ADP as a shorthand for properties that are true for the three variants of DP. Here, b is the budget, namely b = (ϵ, δ), (α, ϵ), (ξ, ρ). Proof. This is direct by observing that for every deterministic adversary B, the view of adversary B reduces to View(B ↔ M(d)) = MπB. This means that any interactive policy could be simulated by interactive post-processing of a mech- anism verifying non-interactive DP. If a policy is "closed" under interactive post-processing, both interactive and non-interactive DP definitions are equivalent. Finally, we provide a "group privacy" property, verified by any Interactive DP policy. Corollary 1. If π is a ρ-global zCDP policy then, for any sequence of actions (a1, . . . , aT ) and any two sequence of rewards r ≜ {r1, . . . , rT } and r’ ≜ {r′ 1, . . . , r′ T }, we have that T � t=1 KL � πt(. | a1, r1, . . . , at−1, rt−1) �� πt(. | a1, r′ 1, . . . , at−1, r′ t−1) � ≤ ρdHam(r, r’)2 Proof. Let a ≜ (a1, . . . , aT ) be a fixed sequence of actions. Let r ≜ {r1, . . . , rT } and r’ ≜ {r′ 1, . . . , r′ T } be two sequences of rewards. Step 1: The constant adversary. We consider the constant adversary Ba defined as Ba(o1, . . . , ot) ≜ at i.e. Ba is the adversary that always queries at step t the action at, independently of the actions recommended by the policy. Let πa ≜ πBa as defined in Eq. (9). Since π is ρ-global zCDP, using Lemma 5, then Mπa is ρ-zCDP. And Lemma 1 gives that Vπa is ρ-zCDP. Step 2: Group privacy of zCDP. Using the group privacy property of ρ-zCDP i.e. Theorem 10 with α = 1, we get that KL (Vπa r ∥ Vπa r’ )) ≤ ρ dHam(r, r’)2. (10) Step 3: Decomposing the view of the constant adversary. On the other hand, we have that Vπa r (o1, . . . , oT ) = T � t=1 πt(ot | a1, r1, . . . , at−1, rt−1). In other words Vπa r = �T t=1 πt(. | a1, r1, . . . , at−1, rt−1). Similarly, Vπa r’ = �T t=1 πt(. | a1, r′ 1, . . . , at−1, r′ t−1). Hence, we get KL (Vπa r ∥ Vπa r’ )) = T � t=1 KL � πt(. | a1, r1, . . . , at−1, rt−1) �� πt(. | a1, r′ 1, . . . , at−1, r′ t−1) � (11) Plugging Equaion (11) in Inequality (10) concludes the proof. 18A.4 Poisoning attacks against Interactive DP We recall the setting of poisoning attacks for bandits (Jun et al., 2018; Liu and Shroff, 2019). A poisoning attacker B sits between a policy π and the real environment ν. When the policy pulls the action at, the environment generates the real reward r0 t ∼ νat and the attacker decides on an attack αt. The reward observed by the policy π is then rt = r0 t − αt. The goal of the attacker B is to manipulate π to choose a sub-optimal target arm (call it K without loss of generality) while spending a minimum cumulative attack cost �T t=1 |αt| in expectation. The attack is successful if the number of pulls of the target arms NK(T) = T − o(T). The Oracle attack (Jun et al., 2018) is a trivial attack when the attacker knows the real means of the environment ν. The attack proceeds by attacking any round t where a non-target arm at ̸= K is pulled by π. The Oracle attacker pulls down the reward of the corresponding arm by αt = ∆γ at = max {µat − µK + γ, 0} for a small parameter γ > 0. The Oracle attack transforms the original bandit problem into one where all non-target arms have an expected reward of less than µk. Theorem 8 (Defense against Oracle attack). If π is a consistent ρ-global zCDP policy, the Oracle attacker needs Ω �� log(T ) ρ � expected cumulative cost to succeed. Proof. The oracle attack targets the arm K and makes it appear optimal for the policy π. Since π is a consistent policy, π will linearly pull the ‘optimal arm’ in the transformed bandit, which is arm K. Thus, the Oracle attacker can succeed. On the other hand, the Oracle attack, defined by αt = ∆γ at1 {at ̸= K} = max {µat − µK + γ, 0} 1 {at ̸= K} has an expected cumulative cost of E � T � t=1 |αt| � = K−1 � a=1 E[Na(T)]∆γ at Since π is a consistent ρ-global zCDP policy, the problem-dependent regret lower bound (Theorem 2) gives that for a ̸= K, E[Na(T)] = Ω �� log(T ) ρ � which concludes the proof. 19B Lower bounds via couplings for concentrated DP In this section, we are interested in controlling the distance (the Kullback-Leibler, i.e. KL) between marginal distributions induced by a differentially private mechanism, when the datasets are generated using two different distributions. This type of information-theoretic bounds is generally the main step for many standard methods for obtaining minimax lower bounds. Our main theorem in this section relates the effect of Concentrated DP on this information-theoretic quantity. In particular, if P1 and P2 are two data-generating distributions over X n, we are interested in the marginals over the output of the mechanism M when the inputs are generated from P1 and P2, i.e. for ν ∈ {1, 2} and A ∈ F Mν(A) ≜ � d∈X n M (A | d) dPν (d) (12) In the following, we will provide general results to bound the KL divergence between the distributions M1 and M2 defined in (12), when the mechanism M is ρ-zCDP. The upper bound depends on the privacy budget ρ and the per-step total variation distance between the data-generating distributions P1 and P2. We recall the definition of an f-divergence. Definition 7 (f-divergence). Let f : (0, ∞) → R be a convex function with f(1) = 0. Let P and Q be two probability distributions on a measurable space (X, F). If P ≪ Q then the f-divergence is defined as Df(P∥Q) ≜ EQ � f �dP dQ �� where dP dQ is a Radon-Nikodym derivative and f(0) ≜ f(0+). B.1 From the KL to a transport problem Let P1 and P2 two distributions over X n. Define C as a coupling of (P1, P2), i.e. the marginals of C are P1 and P2. We denote by Π(P1, P2) the set of all the couplings between P1 and P2. Let M1 and M2 be defined as in (12). We recall the definition of an f-divergence. Theorem 9. We have that Df(M1∥M2) ≤ inf C∈Π(P1,P2) E(d,d′)∼C[Df(Md∥Md′)]. (13) Proof. Let C be a coupling of P1 and P2. We provide a visual proof of the theorem. First, we recall Theorem 15. If PX PY |X −→ PY and PX QY |X −→ QY , then Df (PY ∥QY ) ≤ EX∼PX � Df � PY |X∥QY |X �� . The idea is to use Theorem 15, where the input is a pair of datasets (d, d′) sampled from the coupling C, the first channel applies the private mechanism to the first dataset, the second channel applies the mechanism to the second dataset. In other words, • X = (d, d′) a pair of datasets in X n 20• the input distribution is PX = C the coupling distribution. • the first channel is the mechanism applied to the first dataset PY |X = M(Y | d). • the second channel is the mechanism applied to the second dataset QY |X = M(Y | d′). • Y is the output of the mechanism Using this notation, we have that • PY = M1 • QY = M2 • Df � PY |X∥QY |X � = Df(Md∥Md′). Using Theorem 15, we have that Df(M1∥M2) ≤ E(d,d′)∼C[Df(Md∥Md′)]. which is true for every coupling C. Taking the infimum over the couplings concludes the proof. We will use the group privacy to upper bound the RHS of Equation 13. Theorem 10 (Group Privacy for ρ-zCDP, Proposition 27, Bun and Steinke (2016)). If M is ρ-CDP, then ∀d, d′ ∈ X n, ∀α ≥ 1, Dα(Md∥Md′) ≤ ρdHam(d, d′)2α. Combining the last two theorems gives the following corollary. Corollary 2. If M is ρ-CDP, then KL (M1 ∥ M2) ≤ ρ inf C∈Π(P1,P2) E(d,d′)∼C[dHam(d, d′)2]. Proof. Let M be ρ-CDP. Applying Theorem 9, with f(x) = x log(x) gives that KL (M1 ∥ M2) ≤ ρ inf C∈Π(P1,P2) E(d,d′)∼C[KL (Md ∥ Md′)]. Applying Theorem 10 with α = 1 gives that KL (Md ∥ Md′) ≤ ρdHam(d, d′)2 Combining both inequalities gives the final bound. B.2 Proxy solution to the transport Problem Deriving the sharpest upper bound for the KL would require solving the transport problem inf C∈Π(P1,P2) E(d,d′)∼C[dHam(d, d′)2]. As a proxy, we will use maximal couplings. Proposition 1. Let P1 and P2 be two probability distributions that share the same σ-algebra. There exists a coupling c∞(P1, P2) ∈ Π(P1, P2) called a maximal coupling, such that E(X1,X2)∼c∞(P1,P2) [1 {X1 ̸= X2}] = TV (P1 ∥ P2) Using maximal coupling for data-generating distributions that are product distributions yields the following bound. 21Theorem 1 (KL decomposition for ρ-zCDP). Let P1 and P2 be two product distributions over X n, i.e. P1 = �n i=1 p1,i and P2 = �n i=1 p2,i, where pν,i for ν ∈ {1, 2}, i ∈ [1, n] are distributions over X. Let ti ≜ TV (p1,i ∥ p2,i). If M is ρ-zCDP, then KL (M1 ∥ M2) ≤ ρ � n � i=1 ti �2 + ρ n � i=1 ti(1 − ti) Proof. Let ci ∞ be a maximal coupling between p1,i and p2,i for all i ∈ [1, n]. We define the coupling C∞ ≜ �n i=1 ci ∞. Then C∞ is a coupling of P1 and P2. Since dHam(d, d′) = �n i=1 1 {di ̸= d′ i} we get that, for (d, d′) ∼ C∞, dHam(d, d′) ∼ n � i=1 Bernoulli(ti), where ti ≜ TV (p1,i ∥ p2,i). This further yields E(d,d′)∼C∞[dHam(d, d′)2] = � n � i=1 ti �2 + n � i=1 ti(1 − ti). Corollary 2 concludes the proof. Comments on the bound of Theorem 1. This is a centralised ρ-zCDP version of the KL- decomposition lemma under local DP (Duchi et al., 2013, Theorem 1), and a ρ-zCDP version of the Sequential Karwa-Vadhan lemma (Azize and Basu, 2022). We also refer to (Lalanne et al., 2022) that uses similar coupling ideas to derive ρ-zCDP variants of LeCam and Fano inequalities. 22C Regret lower bounds for bandits under ρ-global zCDP In this section, we will use the result of Theorem 1 in classic regret lower bounds for bandits to generate multiple lower bounds, namely minimax and problem dependent for stochastic and minimax for linear bandits. C.1 Stochastic finite-armed bandits: Minimax lower bound Theorem 2 (Part a: Minimax lower bound for finite-armed bandits). Let Πρ be the set of ρ-zCDP policies. For any K > 1, T ≥ K − 1, and 0 < ρ ≤ 1, Regminimax T,ρ ≜ inf π∈Πρ sup ν∈EK RegT (π, ν) ≥ max � 1 27 � � T(K − 1) �� � without ρ-global zCDP , 1 44 K − 1 √ρ � �� � with ρ-global zCDP � . Proof. The non-private part of the lower bound is due to Theorem 15.2 in Lattimore and Szepesvári (2020). To prove the private part of the lower bound, we plug our KL decomposition theorem into the proofs of regret lower bounds for bandits. Step 1: Choosing the ‘hard-to-distinguish’ environments. First, we fix a ρ-zCDP policy π . Let ∆ be a constant (to be specified later), and ν be a Gaussian bandit instance with unit variance and mean vector µ = (∆, 0, 0, ..., 0). To choose the second bandit instance, let a ≜ arg mini∈[2,K] Eν,π[Ni(T)] be the least played arm in expectation other than the optimal arm 1. The second environment ν′ is then chosen to be a Gaussian bandit instance with unit variance and mean vector µ′ = (∆, 0, 0, . . . 0, 2∆, 0 . . . , 0), where µ′ j = µj for every j except for µ′ a = 2∆. The first arm is optimal in ν and the arm i is optimal in ν′. Since T = Eνπ [N1(T)] + � i>1 Eνπ [Ni(T)] ≥ (K − 1)Eνπ [Na(T)], we observe that na ≜ Eνπ [Na(T)] ≤ T K − 1 Step 2: From lower bounding regret to upper bounding KL-divergence. Now by the classic regret decomposition and Markov inequality (Lemma 11), we get4 RegT (π, ν) = (T − Eνπ [N1(T)]) ∆ ≥ Mνπ (N1(T) ≤ T/2) T∆ 2 , and RegT (π, ν′) = ∆Eν′π [N1(T)] + � a/∈{1,i} 2∆Eν′π [Na(T)] ≥ Mν′π (N1(T) > T/2) T∆ 2 . Let us define the event A ≜ {N1(T) ≤ T/2} = {(a1, a2, . . . , aT ) : card({j : aj = 1}) ≤ T/2}. By applying the Bretagnolle–Huber inequality, we have: RegT (π, ν) + RegT (π, ν′) ≥ T∆ 2 (Mνπ(A) + Mν′π(Ac)) ≥ T∆ 4 exp(−KL (Mνπ ∥ Mν′π)) Step 3: KL-divergence decomposition with ρ-global zCDP. Now, we apply Theorem 1 along with an oracle argument similar to (Shariff and Sheffet, 2018). Since ν and ν′ only differ in the distribution of arm a, the oracle coupling induces a maximal coupling only on the samples coming from arm a. Specifically, we build the following oracle coupling O. When π samples an action i ̸= a, the oracle O provides the same sample twice, i.e. ri ∼ νi and r′ i = ri. Otherwise, for the samples coming from 4In all regret lower bound proofs, we are under the probability space over sequence of actions, produced when π interacts with ν for T time-steps. We do this to use the KL-divergence decomposition of Mνπ 23arm a, the oracle provides, in expectation, na fresh iid samples from the maximal coupling between νa and ν′ a. Using Theorem 1 with the oracle coupling O, n = na and ti = ta ≜ TV (νa ∥ ν′ a), we get that KL (Mνπ ∥ Mν′π) ≤ ρ(n2 at2 a + nata(1 − ta)) ≤ ρ(n2 at2 a + nata) . The last inequality is due to the fact that 1 − ta ≤ 1. Finally, using Pinsker’s Inequality (Lemma 13), we obtain ta = TV (νa ∥ ν′ a) ≤ � 1 2KL (N(0, 1) ∥ N(2∆, 1)) = ∆ Step 4: Choosing the worst ∆. Plugging back in the regret expression, we find RegT (π, ν) + RegT (π, ν′) ≥ T∆ 4 exp � −ρ � n2 a∆2 + na∆ �� ≥ T∆ 4 exp � −ρ � na∆ + 1 2 �2� ≥ T∆ 4 exp � −ρ � T K − 1∆ + 1 2 �2� By optimising for ∆, we choose ∆ = K−1 T � 1 √ρ − 1 2 � > 0, since ρ ≤ 1. This gives that RegT (π, ν) + RegT (π, ν′) ≥ K − 1 4 � 1 √ρ − 1 2 � exp (−1) ≥ K − 1 8√ρ exp (−1) We conclude the proof by lower bounding 1 8 exp(−1) ≥ 1 22, and using 2 max(a, b) ≥ a + b. C.2 Stochastic finite-armed bandits: Problem-dependent lower bound Theorem 2 (Part b: Problem-dependent lower bounds for finite-armed bandits). Let E = M1 × · · · × MK be a class of environments with K arms, where Ma is a set of reward distributions with finite means. Let π be a consistent policy5 over E satisfying ρ-global zCDP . Then, for all ν = (Pa)K a=1 ∈ E, (i.e. Pa ∈ Ma ), it holds that lim inf T →∞ RegT (π, ν) � log(T) ≥ � a:∆a>0 ∆a √ρ tinf (Pa, µ∗, Ma). where tinf (P, µ∗, M) ≜ infP ′∈M {TV (P ∥ P ′) : µ (P ′) > µ∗} Proof. Let π be a consistent policy satisfying ρ-global zCDP. Let µa be the mean of the a-th arm in ν, ta = tinf (Pa, µ∗, Ma). Fix a suboptimal arm a, and let β > 0 be an arbitrary constant. Step 1: Choosing the ‘hard-to-distinguish’ environment. Let ν′ ≜ � P ′ j �K j=1 ∈ E be a bandit with P ′ j = Pj for j ̸= a and P ′ a ∈ Ma be such that TV (Pa ∥ P ′ a) ≤ ta + β and µ (P ′ a) > µ∗, which exists by the definition of ta. Let µ′ ∈ RK be the vector of means of distributions of ν′. 5A policy π is called consistent over a class of environments E, if ∀ν ∈ E and p > 0, limT →∞ RT (π,ν) T p = 0. 24Step 2: From lower bounding regret to upper bounding KL-divergence. For simplicity of notations, we use RegT = RegT (π, ν), Reg′ T = RegT (π, ν), and A = {(a1, a2, . . . , aT ) : card({j : aj = 1}) ≤ T/2}. Then, by regret decomposition and Markov Inequality 11, we obtain RegT + Reg′ T ≥ T 2 (Mνπ(A)∆a + Mν′π (Ac) (µ′ a − µ∗)) (14) ≥ T 2 min {∆a, µ′ a − µ∗} (Mνπ(A) + Mν′π (Ac)) ≥ T 4 min {∆a, µ′ a − µ∗} exp(−KL (Mνπ ∥ Mν′π)) Step 3: KL-divergence decomposition with ρ-global zCDP. Similar to Step 3 in the previous minimax proof, we build the oracle coupling O that provides a maximal coupling only on the samples coming from arm a. Using Theorem 1 with the oracle coupling O, n = na and ti = TV (νa ∥ ν′ a) = ta + β, we get that KL (Mνπ ∥ Mν′π) ≤ ρ[n2 a(ta + β))2 + na(ta + β))] where na ≜ Eνπ [Na(T)] . Step 4: Rearranging and taking the limit inferior. Thus, we get RegT + Reg′ T ≥ T 4 min {∆a, µ′ a − µ∗} exp � −ρ � n2 a (ta + β)2 + na (ta + β) �� Solving for na gives that na ≥ � 4c(T) + 1 − 1 2(ta + β) where c(T) ≜ 1 ρ log � T min{∆a,µ′ a−µ∗} 4(RegT +Reg′ T) � . Now, taking the limit on both sides leads to lim inf T →∞ Eνπ [Na(T)] � log(T) ≥ 1 (ta + β) lim inf T →∞ � c(T) log(T) = 1 (ta + β) � � � �1 ρ � 1 − lim sup T →∞ log � RegT + Reg′ T � log(T) � = 1 (ta + β) �1 ρ The last equality follows from the definition of consistency, which says that for any p > 0, there exists a constant Cp such that for sufficiently large T, RegT + Reg′ T ≤ CpT p. This property implies that lim sup T →∞ log � RegT + Reg′ T � log(T) ≤ lim sup T →∞ p log(T) + log (Cp) log(T) = p, which gives the result since p > 0 was an arbitrary constant. We arrive at the claimed result by taking the limit as β tends to zero. C.3 Stochastic linear bandits: Minimax lower bound First, we give a specific coupling lemma for the linear case and plug it in the minimax lower bound proofs. 25Let ν = {Pa, a ∈ [K]} and ν′ = {P ′ a, a ∈ [K]} be two bandit instances. When the policy π interacts with the bandit instance ν, it induces a marginal distribution mν,π over the sequence of actions, i.e. mν,π(a1, . . . , aT ) ≜ � r1,...,rT T � t=1 πt(at | a1, r1, . . . , at−1, rt−1)pat(rt) drt. We define mν′,π similarly. Lemma 6. If π is ρ-global zCDP, then KL (mν,π ∥ mν′,π) ≤ ρ � Eν,π � T � t=1 tat ��2 + ρ Eν,π � T � t=1 tat(1 − tat) � + ρ Vν,π � T � t=1 tat � (15) where tat ≜ TV � Pat �� P ′ at � and Eν,π and Vν,π are the expectation and variance under mν,π respectively. Proof. We adapt the proofs of Appendix B to the bandit case, by creating a coupled bandit instance. Let ν = {Pa : a ∈ [K]} and ν′ = {P ′ a : a ∈ [K]} be two bandit instances. Define ca as the maximal coupling between Pa and P ′ a. Let π = {πt}T t=1 be a ρ-global zCDP policy. Here, we build a coupled environment γ of ν and ν′. The policy π interacts with the coupled environment γ up to a given time horizon T to produce a history {(At, Rt, R′ t)}T t=1. The iterative steps of this interaction process are: 1. the probability of choosing an action At = a at time t is dictated only by the policy πt and A1, R1, A2, R2, . . . , At−1, Rt−1, i.e. ignores {R′ s}t−1 s=1. 2. the distribution of rewards (Rt, R′ t) is cAt and is conditionally independent of the previous observed history {(As, Rs, R′ s)}t−1 t=1. This interaction is similar to the interaction process of policy π with the first bandit instance ν, with the addition of sampling an extra R′ t from the coupling of Pat and P ′ at. The distribution of the history induced by the interaction of π and the coupled environment can be defined as pγπ(a1, r1, r′ 1 . . . , aT , rT , r′ T ) ≜ T � t=1 πt(at | a1, r1, . . . , at−1, rt−1)cat(rt, r′ t) To simplify the notation, let a ≜ (a1, . . . , aT ), r ≜ (r1, . . . , rT ) and r’ ≜ (r′ 1, . . . , r′ T ). Also, let ca(r, r’) ≜ �T t=1 cat(rt, r′ t) and π(a | r) ≜ �T t=1 πt(at | a1, r1, . . . , at−1, rt−1). We put h ≜ (a, r, r’). With the new notation pγπ(a, r, r’) ≜ π(a | r)ca(r, r’) Similarly, we define qγπ(a, r, r’) ≜ π(a | r’)ca(r, r’) It follows that mν,π is the marginal of pγπ when integrated over (r, r’), and mν′,π is the marginal of qγπ when integrated over (r, r’), i.e. mν,π(a) = � r,r’ pγπ(a, r, r’) dr dr’ and mν′,π(a) = � r,r’ qγπ(a, r, r’) dr dr’ By the data-processing inequality, we get that KL (mν,π ∥ mν′,π) ≤ KL (pγπ ∥ qγπ) (16) 26In the following, upper case variables refer to random variables. We have that KL (pγπ ∥ qγπ) (a) = EH≜(A,R,R’)∼pγπ � log � π(A | R)cA(R, R’) π(A | R’)cA(R, R’) �� (b) = T � t=1 EH∼pγπ � log �πt(At | A1, R1, . . . At−1, Rt−1) πt(At | A1, R′ 1, . . . At−1, R′ t−1) �� (c) = T � t=1 EH∼pγπ � EH∼pγπ � log �πt(At | A1, R1, . . . At−1, Rt−1) πt(At | A1, R′ 1, . . . At−1, R′ t−1) � �� A1, R1, . . . At−1, Rt−1 �� (d) = T � t=1 EH∼pγπ � EAt∼πt(.|A1,R1,...At−1,Rt−1) � log �πt(At | A1, R1, . . . At−1, Rt−1) πt(At | A1, R′ 1, . . . At−1, R′ t−1) ��� (e) = T � t=1 EH∼pγπ � KL � πt(. | A1, R1, . . . At−1, Rt−1) �� πt(. | A1, R′ 1, . . . At−1, R′ t−1) �� , where we obtain (a): by definition of pγπ, qγπ and the KL divergence (b): by definition of π(A | R) and π(A | R’) (c): using the towering property of the expectation (d): using that, conditioned on the history (A1, R1, . . . At−1, Rt−1), the distribution of At is πt(. | A1, R1, . . . At−1, Rt−1). (e): by definition of the KL divergence On the other hand, Corollary 1, we have that T � t=1 KL � πt(. | A1, R1, . . . At−1, Rt−1) �� πt(. | A1, R′ 1, . . . At−1, R′ t−1) � ≤ ρd2 Ham(R, R′) which means that KL (pγπ ∥ qγπ) ≤ EH∼pγπ � ρd2 Ham(R, R′) � (a) = EH∼pγπ � EH∼pγπ � ρd2 Ham(R, R′) �� A �� (b) = ρ EH∼pγπ � EH∼pγπ � dHam(R, R′) �� A �2 + ρ V � dHam(R, R′) �� A �� (c) = ρ Eν,π   � T � t=1 tat �2  + ρ Eν,π � T � t=1 tat(1 − tat) � (d) = ρ � Eν,π � T � t=1 tat ��2 + ρ Eν,π � T � t=1 tat(1 − tat) � + ρ Vν,π � T � t=1 tat � , where we obtain (a): using the towering property of the expectation (b) and (d): by definition of the variance (c): using that dHam(R, R′) = �T t=1 1 {Rt ̸= R′ t} where 1 {Rt ̸= R′ t} | At ∼ Bernoulli(tat) by the definition of the maximal coupling and the sum is iid given A. Finally, plugging the upper bound in Inequality (16) concludes the proof. 27Theorem 3 (Minimax lower bounds for linear bandits). Let A = [−1, 1]d and Θ = Rd. Then, for any ρ-global zCDP policy, we have that Regminimax T (A, Θ) ≥ max          exp(−2) 8 d √ T � �� � without ρ-global zCDP , exp(−2.25) 4 d √ρ � �� � with ρ-global zCDP          . Proof. For the non-private lower bound, Theorem 24.1 of (Lattimore and Szepesvári, 2020) gives that, Regminimax T (A, Θ) ≥ exp(−2)d 8 √ T. Now, we focus on proving the ρ-global zCDP part of the lower bound. Let Θ = � − 1 T √ρ, 1 T √ρ �d . For θ, θ′ ∈ Θ, let ν and ν′ be the bandit instances corresponding resp. to θ and θ′. We denote Mθ = Mν,π and Mθ′ = Mν′,π. Let Eθ and Eθ′ the expectations under Mθ and Mθ′ respectively. Step 1: From lower bounding regret to upper bounding KL-divergence We begin with RegT (A, θ) = Eθ � T � t=1 d � i=1 (sign (θi) − Ati) θi � ≥ 1 T√ρ d � i=1 Eθ � T � t=1 I {sign (Ati) ̸= sign (θi)} � ≥ 1 √ρ d � i=1 Mθ � T � t=1 I {sign (Ati) ̸= sign (θi)} ≥ T/2 � In this derivation, the first equality holds because the optimal action satisfies a∗ i = sign (θi) for i ∈ [d]. The first inequality follows from an observation that (sign (θi) − Ati) θi ≥ |θi| I {sign (Ati) ̸= sign (θi)}. The last inequality is a direct application of Markov’s inequality 11. For i ∈ [d] and θ ∈ Θ, we define pθ,i ≜ Mθ � T � t=1 I {sign (Ati) ̸= sign (θi)} ≥ T/2 � . Now, let i ∈ [d] and θ ∈ Θ be fixed. Also, let θ′ j = θj for j ̸= i and θ′ i = −θi. Then, by the Bretagnolle-Huber inequality, pθ,i + pθ′,i ≥ 1 2 exp (−KL (Mθ ∥ Mθ′)) . Step 2: KL-divergence decomposition with ρ-global zCDP. Define pt ≜ TV (N (⟨At, θ⟩ , 1) ∥ N (⟨At, θ′⟩ , 1)). From Lemma 6, we obtain that KL (Mθ ∥ Mθ′) ≤ ρ � Eνπ � T � t=1 pt ��2 + ρ � Eνπ � T � t=1 pt �� + ρ Vν,π � T � t=1 pt � On the other hand, using Pinsker’s inequality (Lemma 13), we have that T � t=1 pt ≤ T � t=1 � 1 2KL (N (⟨At, θ⟩ , 1) ∥ N (⟨At, θ′⟩ , 1)) 28≤ T � t=1 � 1 4 � ⟨At, θ − θ′⟩2� ≤ 1 2 � T � t=1 |⟨At, θ − θ′⟩| � ≤ 1 2 � T � t=1 |At,i| (2 |θi|) � ≤ 1 2 � T × 2 1 T√ρ � = 1 √ρ. The last inequality holds true because At ∈ [−1, 1]d and θ, θ′ ∈ � − 1 T √ρ, 1 T √ρ �d . This gives that Eνπ � T � t=1 pt � ≤ 1 √ρ and Vνπ � T � t=1 pt � ≤ 1 4ρ Plugging back in the KL decomposition, we get that, KL (Mθ ∥ Mθ′) ≤ ρ � 1 √ρ �2 + ρ � 1 √ρ � + ρ � 1 4ρ � = 1 + √ρ + 1 4 ≤ 9 4 where the last inequality is due to ρ ≤ 1. Step 3: Choosing the ‘hard-to-distinguish’ θ. Now, we have that pθ,i + pθ′,i ≥ 1 2 exp (−9/4) Now, we apply an ‘averaging hammer’ over all θ ∈ Θ, such that |Θ| = 2d, to obtain � θ∈Θ 1 |Θ| d � i=1 pθ,i = 1 |Θ| d � i=1 � θ∈Θ pθ,i ≥ d 4 exp(−9 4). This implies that there exists a θ ∈ Θ such that �d i=1 pθ,i ≥ d exp(− 9 4)/4. Step 4: Plugging back θ in the regret decomposition. With this choice of θ, we conclude that RegT (A, θ) ≥ 1 √ρ d � i=1 pθ,i ≥ exp(− 9 4) 4 d √ρ 29D Privacy proofs In this section, we give a complete proof of the privacy of both AdaC-UCB and AdaC-GOPE. Both algorithms share the same blueprint. We first formalise the intuition behind the blueprint in Lemma 7, then give a generic proof of privacy and specify the minor differences to complete the proofs in the last section. D.1 The privacy lemma of non-overlapping sequences Remark 3. The Privacy Lemma shows that when the mechanism M is applied to non-overlapping subsets of the input dataset, there is no need to use the composition theorems. Plus, there is no additional cost in the privacy budget. Lemma 7 (Privacy Lemma). Let M be a mechanism that takes a set as input. Let ℓ < T and t1, . . . tℓ, tℓ+1 be in [1, T] such that 1 = t1 < · · · < tℓ < tℓ+1 − 1 = T. Let’s define the following mechanism G : {x1, . . . , xT } → ℓ � i=1 M{xti,...,xti+1−1} (17) In other words, G is the mechanism we get by applying M to the partition of the input dataset {x1, . . . , xT } according to t1 < · · · < tℓ < tℓ+1, i.e.     x1 x2 ... xT     G→    o1 ... oℓ    where oi ∼ M{xti,...,xti+1−1}. We have that (a) If M is (ϵ, δ)-DP then G is (ϵ, δ)-DP (b) If M is (α, ϵ)-RDP then G is (α, ϵ)-RDP (c) If M is (ξ, ρ)-zCDP then G is (ξ, ρ)-zCDP Proof. Let x ≜ {x1, . . . , xT } and x′ ≜ {x′ 1, . . . , x′ T } be two neighboring datasets. This implies that ∃j ∈ [1, T] such that xj ̸= x′ j and ∀t ̸= j, xt = x′ t. Let ℓ′ be such that tℓ′ ≤ j ≤ tℓ′+1 − 1. (a) Suppose M is (ϵ, δ)-DP. For every output event E = E1 × · · · × Eℓ, we have that Gx(E) = ℓ � i=1 M{xti,...,xti+1−1}(Ei) = M{xtℓ′ ,...,xtℓ′+1−1}(Eℓ′) ℓ � i=1,i̸=ℓ′ M{xti,...,xti+1−1}(Ei) ≤ � eϵM{x′ tℓ′ ,...,x′ tℓ′+1−1}(Eℓ′) + δ � ℓ � i=1,i̸=ℓ′ M{xti,...,xti+1−1}(Ei) = eϵGx′(E) + δ × ℓ � i=1,i̸=ℓ′ M{xti,...,xti+1−1}(Ei) 30≤ eϵGx′(E) + δ since �ℓ i=1,i̸=ℓ′ M{xti,...,xti+1−1}(Ei) ≤ 1 Which gives that G is (ϵ, δ)-DP. (b) M is (α, ϵ)-RDP. We have that Dα(Gx∥Gx′) = 1 α − 1 log �� o=(o1,...,oℓ) Gx′(o) � Gx(o) Gx′(o) �α� Since Gx(o) = ℓ � i=1 M{xti,...,xti+1−1}(oi) and Gx′(o) = ℓ � i=1 M{x′ ti,...,x′ ti+1−1}(oi) we get Gx(o) Gx′(o) = M{xtℓ′ ,...,xtj ,...,xtℓ′+1−1}(oi) M{xtℓ′ ,...,x′ tj ,...,xtℓ′+1−1}(oi) Thus, Dα(Gx∥Gx′) = Dα(M{xtℓ′ ,...,xtj ,...,xtℓ′+1−1}∥M{xtℓ′ ,...,x′ tj ,...,xtℓ′+1−1}) ≤ ϵ Which gives that G is (α, ϵ)-RDP. (c) M is (ξ, ρ)-zCDP. Similarly, we have that Dα(Gx∥Gx′) = Dα(M{xtℓ′ ,...,xtj ,...,xtℓ′+1−1}∥M{xtℓ′ ,...,x′ tj ,...,xtℓ′+1−1}) ≤ ξ + ρα . Thus, G is (ξ, ρ)-zCDP. For each of the three algorithms proposed, the final actions can be seen as a post-processing of some private quantity of interest (empirical means for AdaC-UCB or the parameter ˆθ for linear and contextual bandits). However, we cannot directly conclude the privacy of the proposed algorithms using just a post-processing argument and Lemma 7. This is because the steps corresponding to the start of an episode in the algorithms t1 < · · · < tℓ < tℓ+1 are adaptive and depend on the dataset itself, while for Lemma 7, those have been fixed before. To deal with the adaptive episode, we propose a generic privacy proof. D.2 Generic privacy proof of AdaC-UCB and AdaC-GOPE In this section, we give one generic proof that works for the two proposed algorithms. First, we give a summary of the intuition of the proof for dealing with adaptive episodes. By fixing two neighbouring tables of rewards d and d′ that only differ at some user uj, and a deterministic adversary B, we have that • the view of the adversary B from the beginning of the interaction until step j will be the same • the adaptive episodes generated by the policy in the first j steps will be the same, which means that step j will fall in the same episode in the view of B when interacting with π(d) or π(d′) • for these fixed similar episodes, we use the privacy Lemma 7 31• the view of B from step j + 1 until T will be private by post-processing Let d = {d1, . . . , dT } and d′ = {d′ 1, . . . , d′ T } two neighbouring reward tables in (RK)T . Let j ∈ [1, T] such that, for all t ̸= j, dt = d′ t. Let B be a deterministic adversary. We want to show that Dα(View(B ↔ π(d))∥View(B ↔ π(d′))) ≤ αρ. Step 1. Sequential decomposition of the view of the adversary B We observe that due to the sequential nature of the interaction, the view of B can be decomposed to a part that depends on d<j ≜ {d1, . . . , dj−1}, which is identical for both d and d′ and a second conditional part on the history. First, let us denote View(B ↔ π(d)) ≜ PB,π d . We have that, for every sequence of actions o ≜ (o1, . . . , oT ) ∈ [K]T PB,π d (o) = T � t=1 πt � ot | B(o1), d1,B(o1), . . . , B(o1, . . . , ot−1), dt−1,B(o1,...,ot−1) � ≜ PB,π d<j (o≤j)PB,π d (o>j | o≤j) where • o≤j ≜ (o1, . . . , oj) and o>j ≜ (oj+1, . . . , oT ) • PB,π d<j (o≤j) ≜ �j t=1 πt � ot | B(o1), d1,B(o1), . . . , B(o1, . . . , ot−1), dt−1,B(o1,...,ot−1) � • PB,π d (o>j | o≤j) ≜ �T t=j+1 πt � ot | B(o1), d1,B(o1), . . . , B(o1, . . . , ot−1), dt−1,B(o1,...,ot−1) � Similarly PB,π d′ (o) = PB,π d<j (o≤j)PB,π d′ (o>j | o≤j) since d′ <j = d<j. Step 2. Decomposing the Rényi divergence. We have that e(α−1)Dα(PB,π d ∥PB,π d′ ) = � o∈[K]T PB,π d′ (o) � PB,π d (o) PB,π d′ (o) �α = � o∈[K]T PB,π d′ (o) � PB,π d (o>j | o≤j) PB,π d′ (o>j | o≤j) �α = � o≤j∈[K]j PB,π d<j (o≤j) � o>j∈[K]T −j PB,π d′ (o>j | o≤j) � PB,π d (o>j | o≤j) PB,π d′ (o>j | o≤j) �α = � o≤j∈[K]j PB,π d<j (o≤j)e(α−1)Dα(PB,π d (.|o≤j)∥PB,π d′ (.|o≤j)) = Eo≤j∼PB,π d<j � e(α−1)Dα(PB,π d (.|o≤j)∥PB,π d′ (.|o≤j))� Step 3. The adaptive episodes are the same, before step j. Let ℓ such that tℓ ≤ j < tℓ+1 in the view of B when interacting with d. Let us call it ψπ d (j) ≜ ℓ. Similarly, let ℓ′ such that tℓ′ ≤ j < tℓ′+1 in the view of B when interacting with d. Let us call it ψπ d′(j) ≜ ℓ′. Since ψπ d (j) only depends on d<j, which is identical for d and d′, we have that ψπ d (j) = ψπ d′(j) with probability 1. 32We call ξj the last time-step of the episode ψπ d (j), i.e ξj ≜ tψπ d (j)+1 − 1. Step 4. Private sufficient statistics. Fix o≤j. Let rs ≜ ds,B(o1,...,os), for s ∈ [1, j], be the reward corresponding to the action chosen by B in the table d. Similarly, r′ s ≜ d′ s,B(o1,...,os) for d′. Let us define Lj ≜ G{r1,...,rξj } and L′ j ≜ G{r′ 1,...,r′ ξj }, where G is defined as in Eq. 17, using the same episodes for d and d′. The underlying mechanism M, used to define G, will be specified for each algorithm in Section D.2.1. In addition, the specified mechanism M will verify ρ-zCDP with respect to its set input. Using the structure of the policy π, there exists a randomised mapping fdξj +1,...,dT such that PB,π d (. | o≤j) = fdξj +1,...,dT (Lj) and PB,π d′ (. | o≤j) = fdξj +1,...,dT (L′ j). In other words, the view of the adversary B from step ξj + 1 until T only depends on the sufficient statistics Lj and the new inputs dξj+1, . . . , dT , which are the same for d and d′. For example, the sufficient statistics are the private mean estimate of the active arm in each episode for AdaC-UCB and the noisy parameter estimate ˆθ for AdaC-GOPE. Step 5. Concluding with Lemma 7 and post-processing. Using Lemma 7, we have that Dα(Lj, L′ j) ≤ αρ Using the post-processing property of Dα (Lemma 10), we get that Dα(PB,π d (. | o≤j)∥PB,π d′ (. | o≤j)) = Dα(fdξj +1,...,dT (Lj)∥fdξj +1,...,dT (L′ j)) ≤ Dα(Lj, L′ j) ≤ αρ Finally, we conclude by taking the expectation with respect to o≤j ∼ PB,π d<j e(α−1)Dα(PB,π d ∥PB,π d′ ) = Eo≤j∼PB,π d<j � e(α−1)Dα(PB,π d (.|o≤j)∥PB,π d′ (.|o≤j))� ≤ e(α−1)αρ Thus, we conclude Dα(PB,π d ∥PB,π d′ ) ≤ αρ Remark 4. The same proof could be adapted to (α, ϵ)-RDP, by just showing that the Rényi divergence is smaller than ϵ rather than αρ. For (ϵ, δ)-DP, the same proof follows by changing Dα to the Hockey- Stick Divergence i.e Dfϵ(P, Q) = E[fϵ( dP dQ)] where fϵ = max(t − eϵ, 0). Otherwise, just rewriting the proof using the probability of events is straightforward too. D.2.1 Instantiating the specifics of privacy proof for each algorithm In this section, we instantiate Step 4 of the generic proof for each algorithm, by specifying the mechanism G and M in the proof and showing that they are ρ-zCDP. • For AdaC-UCB, the mechanism M is the private empirical mean statistic, i.e M{r1,...,rt} ≜ 1 t �t s=1 rs + N � 0, 1 2ρt2 � . Since rewards are in [0, 1], by the Gaussian Mechanism (i.e. Theorem 14) M is ρ-DP. • For AdaC-GOPE, the mechanism M is a private estimate of the linear parameter θ, i.e M{rtℓ,...,rtℓ+1−1} ≜ V −1 ℓ ��tℓ+1−1 t=tℓ asrs � + V − 1 ℓ 2 Nℓ where Vℓ = � a∈Sℓ Tℓ(a)aa⊤, Nℓ ∼ N � 0, 2 ρg2 ℓId � and gℓ = maxb∈Aℓ ∥b∥V −1 ℓ . 33To show that M is ρ-zCDP, we rewrite ˆθℓ = V −1 ℓ ��tℓ+1−1 t=tℓ asrs � = V − 1 ℓ 2 ϕℓ where ϕℓ ≜ V − 1 ℓ 2 ��tℓ+1−1 t=tℓ asrs � . Let {rs}tℓ+1−1 s=tℓ and {r′ s}tℓ+1−1 s=tℓ two neighbouring sequence of rewards that differ at only step j ∈ [tℓ, tℓ+1 − 1]. We have that ∥ϕℓ − ϕ′ ℓ∥2 = ∥V − 1 ℓ 2 [aj(rs − r′ s)] ∥2 ≤ 2∥V − 1 ℓ 2 aj∥2 ≤ 2gℓ since rj, r′ j ∈ [−1, 1]. Using the Gaussian Mechanism (i.e. Theorem 14), this means that ϕℓ + Nℓ is ρ-zCDP and M is too by post-processing. 34E Stochastic bandits with global zCDP E.1 Concentration inequalities Lemma 8. Assume that (Xi)1≤i≤n are iid random variables in [0, 1], with E(Xi) = µ. Then, for any δ ≥ 0, P � ˆµn + Zn − �� 1 2n + 1 ρn2 � log �1 δ � ≥ µ � ≤ δ, (18) and P � ˆµn + Zn + �� 1 2n + 1 ρn2 � log �1 δ � ≤ µ � ≤ δ, (19) where ˆµn = 1 n �n t=1 Xt and Zn ∼ N � 0, 1 2ρn2 � . Proof. Let Y = (ˆµn + Zn − µ). Using Properties 2 and 3 of Lemma 15, we get that Y is � 1 4n + 1 2ρn2 -subgaussian. We conclude using the concentration on subgaussian random variables, i.e. Lemma 14. E.2 Regret analysis Theorem 5 (Part a: Problem-dependent regret). For rewards in [0, 1] and β > 3, AdaC-UCB yields a regret upper bound of � a:∆a>0 � 8β ∆a log(T) + 8 � β ρ � log(T) + 2β β − 3 � . Proof. By the generic regret decomposition of Theorem 11 in Azize and Basu (2022), for every suboptimal arm a, we have that E[Na(T)] ≤ 2ℓ+1 + P � Gc a,ℓ,T � T + β β − 3, where Ga,ℓ,T = � ˆµa,2ℓ + Zℓ + �� 1 2 × 2ℓ + 1 ρ × (2ℓ)2 � β log(T) < µ1 � . such that Zℓ ∼ N � 0, 1 2ρ×(2ℓ)2 � Step 1: Choosing an ℓ. Now, we observe that P(Gc a,ℓ,T ) = P � ˆµa,2ℓ + Zℓ + �� 1 2 × 2ℓ + 1 ρ × (2ℓ)2 � β log(T) ≥ µ1 � = P � ˆµa,2ℓ + Zℓ − �� 1 2 × 2ℓ + 1 ρ × (2ℓ)2 � β log(T) ≥ µa + ϵ � for ϵ = � ∆a − 2 �� 1 2×2ℓ + 1 ρ×(2ℓ)2 � β log(T) � . The idea is to choose ℓ big enough so that ϵ ≥ 0. Let us consider the contrary, i.e. ϵ < 0 ⇒ 2ℓ < 2β log(T) ∆2a � 1 + ∆a � 1 ρβ log(T) � 35⇒ 2ℓ < 2β ∆2a log(T) + 2 � β ρ∆2a � log(T) (20) Thus, by choosing ℓ = � 1 log(2) log � 2β ∆2a log(T) + 2 � β ρ∆2a � log(T) �� we ensure ϵ > 0. This also implies that P(Gc a,ℓ,T ) ≤ P � ˆµa,2ℓ + Zℓ − �� 1 2 × 2ℓ + 1 ρ × (2ℓ)2 � β log(T) ≥ µa � ≤ 1 T β The last inequality is due to Equation 18 of Lemma 8. Step 2: The regret bound. Combining Steps 1 and 2, we get that E[Na(T)] ≤ β β − 3 + 2ℓ+1 + T × 1 T β ≤ 8β ∆2a log(T) + 8 � β ρ∆2a � log(T) + 2β β − 3. (21) Plugging this upper bound back in the definition of problem-dependent regret RegT (AdaC-UCB, ν) ≤ � a:∆a>0 � 8β ∆a log(T) + 8 � β ρ � log(T) + 2β β − 3 � . Theorem 5 (Part b: Minimax regret). For rewards in [0, 1] and β > 3, AdaC-UCB yields a regret upper bound of O �� KT log(T) � + O � K �1 ρ log(T) � . Proof. Let ∆ be a value to be tuned later. We observe that RegT (AdaP-UCB, ν) = � a ∆aE[Na(T)] = � a:∆a≤∆ ∆aE[Na(T) + � a:∆a>∆ ∆aE[Na(T)] ≤ T∆ + � a:∆a>∆ ∆a � 8β ∆2a log(T) + 8 � β ρ∆2a � log(T) + 2β β − 3 � (Eq. 21) ≤ T∆ + 8βK log(T) ∆ + 8K � β log(T) ρ + 3β β − 3 � a ∆a ≤ 4 � 2βKT log(T) + 8K � β log(T) ρ + 3β β − 3 � a ∆a . Here, the last step is tuning ∆ = � 8βK log(T ) T . 36E.3 Extensions to (ϵ, δ)-global DP and (α, ϵ)-global RDP In this section, we specify the modifications required to make AdaC-UCB (ϵ, δ)-global DP and (α, ϵ)-global RDP. Also, we give the corresponding regret upper bounds. The difference comes from the different calibrations of the Gaussian Mechanism (Thm 14). Adapting the analysis from ρ-zCDP reduces to changing the 1 2ρ factor to 2 ϵ2 log( 1.25 δ ) for (ϵ, δ)-DP and to α 2ϵ for (α, ϵ)-RDP, i.e. varying the constant b in Theorem 14. (ϵ, δ)-global DP. The private index to select the arms (Line 6 of Algorithm 2) becomes Iρ a(tℓ − 1, β) ≜ ˆµℓ a + N � 0, σ2 a,ℓ � + Ba(tℓ − 1, β). where σ2 a,ℓ ≜ 2 log( 1.25 δ ) ϵ2×( 1 2 Na(tℓ−1)) 2 , and the exploration bonus is Ba(tℓ − 1, β) ≜ � � � � � 1 2 × 1 2Na(tℓ − 1) + 4 log( 1.25 δ ) ϵ2 × � 1 2Na(tℓ − 1) �2 � β log(tℓ) . Thus, the regret upper bounds become: Problem-dependent: � a:∆a>0 � 8β ∆a log(T) + 8 � 4 βϵ2 log �1.25 δ �� log(T) + 2β β − 3 � . Problem-independent: O �� KT log(T) � + O  K � log � 1 δ � ϵ � log(T)   . (α, ϵ)-global RDP. The private index to select the arms (Line 6 of Algorithm 2) becomes Iρ a(tℓ − 1, β) ≜ ˆµℓ a + N � 0, σ2 a,ℓ � + Ba(tℓ − 1, β). where σ2 a,ℓ ≜ α 2ϵ×( 1 2 Na(tℓ−1)) 2 , and the exploration bonus is Ba(tℓ − 1, β) ≜ � � � � � 1 2 × 1 2Na(tℓ − 1) + α ϵ × � 1 2Na(tℓ − 1) �2 � β log(tℓ) . The regret upper bounds become: Problem-dependent: � a:∆a>0 � 8β ∆a log(T) + 8 � βα ϵ � log(T) + 2β β − 3 � . Problem-independent: O �� KT log(T) � + O � K �α ϵ log(T) � . 37F Linear Bandits with global zCDP F.1 Basic definitions of optimal design Definition 8 (Optimal design). Let A ⊂ Rd and π : A → [0, 1] be a distribution on A so that � a∈A π(a) = 1. Let V (π) ∈ Rd×d and f(π), g(π) ∈ R be given by V (π) = � a∈A π(a)aaT , f(π) = log det V (π), g(π) = max a∈A ∥a∥V (π)−1 • π is called a design • The set Supp (π) ≜ {a ∈ π : π(a) ̸= 0} is called the core set of A • A design that maximises f is known as a D-optimal design • A design that minimises g is known as G-optimal design Theorem 11 (Kiefer–Wolfowitz theorem). Assume that A is compact and span(A) = Rd. The following are equivalent: • π⋆ is a minimiser of g. • π⋆ is a maximiser of f. • g(π⋆) = d Furthermore, there exists a minimiser π⋆ of g such that |Supp (π)| ≤ d(d+1) 2 F.2 Concentration inequalities Let a1, . . . , at be deterministically chosen without the knowledge of r1, . . . , rt. Let π be an optimal design for A. Let Vt ≜ �t s=1 asaT s = � a∈A Na(t)aaT be the design matrix, ˆθt = V −1 t �t s=1 asrs be the least square estimate and ˜θt = ˆθt + V − 1 t 2 Nt where Nt ∼ N � 0, 2 ρg2 t Id � , where gt ≜ maxb∈A ∥b∥V −1 t . Theorem 12. Let δ ∈ [0, 1] and βt ≜ gt � 2 log � 4 δ � + g2 t � 2 ρ � d + 2 � d log � 2 δ � + 2 log � 2 δ �� . For every a ∈ A, we have that P ���� � ˜θt − θ⋆, a ���� ≥ βt � ≤ δ. Proof. For every a ∈ A � ˜θt − θ⋆, a � = � ˆθt − θ⋆, a � + aT V − 1 t 2 Nt = � ˆθt − θ⋆, a � + Zt where Zt ≜ aT V − 1 t 2 Nt. Step 1: Concentration of the least square estimate. Using Eq.(20.2) from Chapter 20 of Lattimore and Szepesvári (2020), we have that P ���� � ˆθt − θ⋆, a ���� ≥ gt � 2 log �4 δ �� ≤ δ 2 Step 2: Concentration of the injected Gaussian noise. On the other hand, using Cauchy-Schwartz, we have that |Zt| = ���aT V − 1 t 2 Nt ��� ≤ ∥V − 1 t 2 a∥.∥Nt∥ ≤ gt∥Nt∥ 38using that ∥V − 1 t 2 a∥ = ∥a∥V −1 t ≤ gt. Here, Nt = � 2 ρgtN(0, Id). Thus, using Lemma 16, we get P   |Zt| ≥ g2 t � � � �2 ρ � d + 2 � d log �2 δ � + 2 log �2 δ ��   ≤ δ 2 Steps 1 and 2 together conclude the proof. Corollary 3. Let β be a confidence level. If each action a ∈ A is chosen for Na(t) ≜ ctπ(a) where ct ≜   8d β2 log �4 δ � + 2d β � � � �2 ρ � d + 2 � d log �2 δ � + 2 log �2 δ ��  then, for t = � a∈Supp(π) Na(t), we get that P ���� � ˜θt − θ⋆, a ���� ≥ β � ≤ δ . Proof. We have that Vt = � a∈Supp(π) Na(t)aaT ≥ ctV (π) This means g2 t = max b∈A ∥b∥2 V −1 t ≤ 1 ct max b∈A ∥b∥2 V (π)−1 = g(π) ct = d ct , where the last equality is because π is an optimal design for A. Recall that βt ≜ gt � 2 log � 4 δ � + g2 t � 2 ρ � d + 2 � d log � 2 δ � + 2 log � 2 δ �� . Thus, βt ≤ � d ct � 2 log �4 δ � + d ct � � � �2 ρ � d + 2 � d log �2 δ � + 2 log �2 δ �� ≤ � 2d log � 4 δ � � 8d β2 log � 4 δ � + d � 2 ρ � d + 2 � d log � 2 δ � + 2 log � 2 δ �� 2d β � 2 ρ � d + 2 � d log � 2 δ � + 2 log � 2 δ �� = β 2 + β 2 = β The final inequality is due to ct ≥ 8d β2 log � 4 δ � , and ct ≥ 2d β � 2 ρ � d + 2 � d log � 2 δ � + 2 log � 2 δ �� . We conclude the proof using Theorem 12. F.3 Regret analysis Theorem 13. Under Assumption 1 and for δ ∈ (0, 1), with probability at least 1 − δ, the regret RT of AdaC-GOPE (Algorithm 3) is upper-bounded by A � dT log �K log(T) δ � + Bd √ρ � log �K log(T) δ � log(T) 39where A and B are universal constants. If δ = 1 T , then E(RT ) ≤ O �� dT log(KT) � + O �� 1 ρd(log(KT)) 3 2 � Proof. Step 1: Defining the good event E. Let E ≜ ∞ � ℓ=1 � a∈Aℓ ���� � ˜θℓ − θ∗, a ���� ≤ βℓ � . Using Corollary 3, we get that P(¬E) ≤ ∞ � ℓ=1 � a∈Aℓ P ���� � ˜θℓ − θ∗, a ���� > βℓ � ≤ ∞ � ℓ=1 � a∈Aℓ δ kℓ(ℓ + 1) ≤ δ Step 2: Good properties under E. We have that under E • The optimal arm a⋆ ∈ arg maxa∈A ⟨θ∗, a⟩ is never eliminated. Proof. for every episode ℓ and b ∈ Aℓ, we have that under E, � ˜θℓ, b − a⋆� = � ˜θℓ − θ⋆, b − a⋆� + ⟨θ⋆, b − a⋆⟩ ≤ � ˜θℓ − θ⋆, b − a⋆� ≤ ��� � ˜θℓ − θ∗, a⋆���� + ��� � ˜θℓ − θ∗, b ���� ≤ 2βℓ where the first inequality is because ⟨θ⋆, b − a⋆⟩ ≤ 0 by definition of the optimal arm a⋆. This means that a⋆ is never eliminated. • Each sub-optimal arm a will be removed after ℓa rounds where ℓa ≜ min{ℓ : 4βℓ < ∆a}. Proof. We have that under E, � ˜θℓa, a⋆ − a � ≥ ⟨θ⋆, a⋆⟩ − βℓa − ⟨θ⋆, a⟩ − βℓa = ∆a − 2βℓa > 2βℓa which means that a get eliminated at the round ℓa. • for a ∈ Aℓ+1, we have that ∆a ≤ 4βℓ. Proof. If ∆a > 4βℓ, then by the definition of ℓa, ℓ ≥ ℓa and arm a is already eliminated, i.e. a /∈ Aℓ+1 Step 3: Regret decomposition under E. Fix ∆ to be optimised later. Under E, each sub-optimal action a such that ∆a > ∆ will only be played for the first ℓ∆ rounds where ℓ∆ ≜ min{ℓ : 4βℓ < ∆} = � log2 � 4 ∆ �� We have that RT = � a∈A ∆aNa(T) 40= � a:∆a>∆ ∆aNa(T) + � a:∆a≤∆ ∆aNa(T) = ℓ∆∧ℓ(T ) � ℓ=1 � a∈Aℓ ∆aTℓ(a) + T∆ ≤ ℓ∆∧ℓ(T ) � ℓ=1 4βℓ−1Tℓ + T∆ where the last inequality is thanks to the third bullet point in Step 2, i.e. ∆a ≤ 4βℓ−1 for a ∈ Aℓ. Also ℓ(T) is the total number of episodes played until timestep T. Step 4: Upper-bounding Tℓ and ℓ(T) under E. We have that Tℓ = � a∈Sℓ Tℓ(a) = � a∈Sℓ   8dπℓ(a) β2 ℓ log �4kℓ(ℓ + 1) δ � + 2dπℓ(a) βℓ � � � �2 ρ � d + 2 � d log �2kℓ(ℓ + 1) δ � + 2 log �2kℓ(ℓ + 1) δ ��  ≤ d(d + 1) 2 + 8d β2 ℓ log �4kℓ(ℓ + 1) δ � + 2d βℓ � � � �2 ρ � d + 2 � d log �2kℓ(ℓ + 1) δ � + 2 log �2kℓ(ℓ + 1) δ �� . since βℓ+1 = 1 2βℓ and �ℓ(T ) ℓ=1 Tℓ = T, there exists a constant C such that ℓ(T) ≤ C log(T). In other words, the length of the episodes is at least doubling so their number is logarithmic. Which means that, for ℓ ≤ ℓ(T), there exists a constant C′ such that log �4kℓ(ℓ + 1) δ � ≤ C′ log �k log(T) δ � hence Tℓ ≤ d(d + 1) 2 + 8d β2 ℓ C′ log �k log(T) δ � + 4d βℓ � 1 ρC′ log �k log(T) δ � Step 5: Upper-bounding regret under E. Under E ℓ∆∧ℓ(T ) � ℓ=1 4βℓ−1Tℓ ≤ ℓ∆∧ℓ(T ) � ℓ=1 8βℓ � d(d + 1) 2 + 8d β2 ℓ C′ log �k log(T) δ � + 4d βℓ � 1 ρC′ log �k log(T) δ �� ≤ 4d(d + 1) + 64dC′ log �k log(T) δ � � ℓ∆ � ℓ=1 2ℓ � + 32d � 1 ρC′ log �k log(T) δ � ℓ(T) ≤ 4d(d + 1) + 16dC′ log �k log(T) δ � �16 ∆ � + 32d � 1 ρC′ log �k log(T) δ � ℓ(T) ≤ 4d(d + 1) + C1d log �k log(T) δ � 1 ∆ + C2d � 1 ρ log �k log(T) δ � log(T) All in all, we have that RT ≤ 4d(d + 1) + C2d � 1 ρ log �k log(T) δ � log(T) + C1d log �k log(T) δ � 1 ∆ + T∆ 41Step 6: Optimizing for ∆. Taking ∆ = � C1d T log � k log(T ) δ � , we get that RT ≤ A � dT log �k log(T) δ � + Bd � 1 ρ log �k log(T) δ � log(T) Step 7: Upper-bounding the expected regret. For δ = 1 T , we get that E(RT ) ≤ (1 − δ)RT (δ) + δT ≤ RT (δ) + 1 ≤ C′ 1 � dT log(kT) + C′ 2 �1 ρd log(kT) 3 2 F.4 Extensions to (ϵ, δ)-global DP and (α, ϵ)-global RDP In this section, we specify the modifications required to make AdaC-GOPE (ϵ, δ)-global DP and (α, ϵ)-global RDP, and provide the corresponding regret upper bounds. The difference comes from the different calibrations of the Gaussian Mechanism (Thm 14). Adapting the analysis from ρ-zCDP reduces to changing the 1 2ρ factor to 2 ϵ2 log( 1.25 δ ) for (ϵ, δ)-DP and to α 2ϵ for (α, ϵ)-RDP, i.e. varying the constant b in Theorem 14. (ϵ, δ)-global DP. The number of times each action a is played at episode ℓ for AdaC-GOPE is Tℓ(a) ≜ cℓπℓ(a) times, where for δ′ ≜ δ Kℓ(ℓ+1), cℓ ≜ 8d β2 ℓ log � 4 δ′ � + 2d βℓ � 8 ϵ2 log(1.25 δ ) � d + 2 � d log � 2 δ′ � + 2 log � 2 δ′ ��1/2 The added Gaussian noise in Step 4 of AdaC-GOPE becomes Nℓ ∼ N � 0, 8d ϵ2cℓ log( 1.25 δ )Id � . Thus, the regret upper-bound becomes O �� dT log(KT) � + O �� 1 ϵ2 log �1.25 δ � d(log(KT)) 3 2 � (α, ϵ)-global RDP. The number of times each action a is played at episode ℓ for AdaC-GOPE is Tℓ(a) ≜ cℓπℓ(a) times, where for δ′ ≜ δ Kℓ(ℓ+1), cℓ ≜ 8d β2 ℓ log � 4 δ′ � + 2d βℓ � 2α ϵ � d + 2 � d log � 2 δ′ � + 2 log � 2 δ′ ��1/2 The added Gaussian noise in Step 4 of AdaC-GOPE becomes Nℓ ∼ N � 0, 2dα ϵcℓ Id � . Thus, the regret upper-bound becomes O �� dT log(KT) � + O ��α ϵ d(log(KT)) 3 2 � . 42F.5 Adding noise at different steps of AdaC-GOPE In order to make the GOPE algorithm differentially private, the main task is to derive a private estimate of the linear parameter θ at each phase ℓ, i.e. ˆθℓ. If the estimate is private with respect to the samples used to compute it, i.e. ˆθℓ = V −1 ℓ ��tℓ+1−1 t=tℓ asrs � w.r.t {rs}tℓ+1−1 s=tℓ , then due to forgetting and post-processing, the algorithm turns private too. We discuss three different ways to make the empirical estimate ˆθℓ private. 1. Adding noise in the end. A first attempt would be to analyse the L2 sensitivity of ˆθℓ directly, and adding Gaussian noise calibrated by the L2 sensitivity of ˆθℓ. Let {rs}tℓ+1−1 s=tℓ and {r′ s}tℓ+1−1 s=tℓ two neighbouring sequence of rewards that differ at only step j ∈ [tℓ, tℓ+1 − 1]. Then, we have that ∥ˆθℓ − ˆθ′ℓ∥2 = ∥V −1 ℓ [aj(rs − r′ s)] ∥2 ≤ 2∥V −1 ℓ aj∥2 since rj, r′ j ∈ [−1, 1]. However, it is hard to control the quantity ∥V −1 ℓ aj∥2 without additional assumptions. The G-optimal design permits only to control another related quantity, i.e. ∥aj∥V −1 ℓ = ∥V − 1 ℓ 2 aj∥2. Thus, it is better to add noise at a step before if one does not want to add further assumption. 2. Adding noise in the beginning. Since ˆθℓ = V −1 ℓ ��tℓ+1−1 t=tℓ asrs � , another way to make ˆθℓ private is by adding noise directly to the sum of observed rewards. Specifically, one can rewrite the sum tℓ+1−1 � t=tℓ asrs = � a∈Sℓ a � at=a,t∈[tℓ,tℓ+1−1] rt . Since rewards are in [−1, 1], the L2 sensitivity of � at=a,t∈[tℓ,tℓ+1−1] rt is 2. Thus, by Theorem 14, this means that the noisy sum of rewards � at=a,t∈[tℓ,tℓ+1−1] rt + N � 0, 2 ρ � is ρ-zCDP. Hence, by post-processing lemma, the corresponding noisy estimate ˆθℓ + V −1 ℓ �� a∈Sℓ aN � 0, 2 ρ �� is a ρ-zCDP estimate of ˆθℓ. This is exactly how both Hanna et al. (2022) and Li et al. (2022) derive a private version of GOPE for different privacy definitions, i.e. pure ϵ-DP for Hanna et al. (2022) and (ϵ, δ)-DP for Li et al. (2022), respectively. The drawback of this approach is that the variance of the noise depends on the size of the support Sℓ of the G-optimal design. To deal with this, both Hanna et al. (2022) and Li et al. (2022) solve a variant of the G-optimal design to get a solution where |Sℓ| ≤ 4d log log d + 16 rather than the full d(d + 1)/2 support of AdaC-GOPE’s optimal design. And still, the dependence on d in the private part of the regret achieved by both these algorithms are d2 in (Hanna et al., 2022, Eq (18)), and d 3 2 in (Li et al., 2022, Eq (56)), respectively. Thus, both of these existing algorithms do not achieve to the linear dependence on d in the regret term due to privacy, as suggested by the minimax lower bound. 3. Adding noise at an intermediate level. In contrast, AdaC-GOPE adds noise to the statistic ϕℓ = V − 1 2 ℓ �tℓ+1−1 � t=tℓ asrs � . ϕℓ is an intermediate quantity between the sum of rewards �tℓ+1−1 t=tℓ asrs, and the parameter ˆθℓ, whose L2 sensitivity can be controlled directly using the G-optimal Design. Due to this subtle observation, the private estimation ˜θℓ of AdaC-GOPE is independent of the size of the support Sℓ. 43Hence, the regret term of AdaC-GOPE due to privacy enjoys a linear dependence on d, as suggested by the minimax lower bound. Conclusion. In brief, to achieve the same DP guarantee with the same budget, one may arrive at it by adding noise at different steps, and the resulting algorithms may have different utilities. In general, adding noise at an intermediate level of computation (not directly to the input, i.e. local and not output perturbation) generally gives the best results. Remark 5. We also compare the empirical performance of AdaC-GOPE with a variant where the noise is added to the sum statistic i.e. ˜θℓ ≜ ˆθℓ+V −1 ℓ �� a∈Sℓ aN � 0, 2 ρ �� . The results are presented in Appendix H validating that AdaC-GOPE yields the lowest regret with respect to the other noise perturbation strategy. 44G Existing technical results and definitions In this section, we summarise the existing technical results and definitions required to establish our proofs. Lemma 9 (Post-processing Lemma (Proposition 2.1, (Dwork et al., 2014))). If a randomised algo- rithm A satisfies (ϵ, δ)-Differential Privacy and f is an arbitrary randomised mapping defined on A’s output, then f ◦ A satisfies (ϵ, δ)-DP. Theorem 14 (The Gaussian Mechanism ( Dwork et al. (2014), Mironov (2017), Bun and Steinke (2016))). Let f : X → Rd be a mechanism with L2 sensitivity s(f) ≜ max d∼d′∥f(d) − f(d′)∥2. Let g ≜ f + Z, such that Z ∼ N(0, b × s(f)2Id). Here, N(µ, Σ) denotes the Gaussian distribution with mean µ and co-variance matrix Σ, and ∥ · ∥2 denotes the L2 norm on Rd. Then, for b = 2 ϵ2 log( 1.25 δ ), α 2ϵ, 1 2ρ, g satisfies (ϵ, δ)-DP, (α, ϵ)-RDP and ρ-zCDP respectively. Lemma 10 (Post-processing property of Renyi Divergence, Lemma 2.2 Bun and Steinke (2016)). Let P and Q be distributions on Ω and let f : Ω → Θ be a function. Let f(P) and f(Q) denote the distributions on Θ induced by applying f to P and Q respectively. Then Dα(f(P)∥f(Q)) ≤ Dα(P∥Q). Lemma 11 (Markov’s Inequality). For any random variable X and ε > 0, P(|X| ≥ ε) ≤ E[|X|] ε . Definition 9 (Consistent Policies). A policy π is called consistent over a class of bandits E if for all ν ∈ E and p > 0, it holds that lim T →∞ RegT (π, ν) T p = 0. The class of consistent policies over E is denoted by Πcons (E). Lemma 12 (Bretagnolle-Huber inequality). Let P and Q be probability measures on the same measurable space (Ω, F), and let A ∈ F be an arbitrary event. Then, P(A) + Q (Ac) ≥ 1 2 exp(−D(P, Q)), where Ac = Ω\A is the complement of A. Lemma 13 (Pinsker’s Inequality). For two probability measures P and Q on the same probability space (Ω, F), we have KL (P ∥ Q) ≥ 2(TV (P ∥ Q))2. Definition 10 (Sub-Gaussianity). A random variable X is σ-subgaussian if for all λ ∈ R, it holds that E[exp(λX)] ≤ exp(λ2σ2/2) Lemma 14 (Concentration of Sub-Gaussian random variables). If X is σ-sub-Gaussian, then for any ϵ ≥ 0, P (X ≥ ϵ) ≤ exp � − ϵ2 2σ2 � Lemma 15 (Properties of Sub-Gaussian Random Variables). Suppose that X1 and X2 are indepen- dent and σ1 and σ2-sub-Gaussian, respectively, then 1. cX is |c| σ-sub-Gaussian for all c ∈ R. 2. X1 + X2 is � σ2 1 + σ2 2-sub-Gaussian. 3. If X has mean zero and X ∈ [a, b] almost surely, then X is b−a 2 -sub-Gaussian. Lemma 16 (Concentration of the χ2-Distribution, Claim 17 of Shariff and Sheffet (2018)). If X ∼ N(0, Id) and δ ∈ (0, 1), then P � ∥X∥2 ≥ d + 2 � d log �1 δ � + 2 log �1 δ �� ≤ δ Theorem 15 (Conditioning Increases f-divergence). Let PX PY |X −→ PY and PX QY |X −→ QY . Then, Df (PY ∥QY ) ≤ EX∼PX � Df � PY |X∥QY |X �� . 45H Extended experimental analysis In this section, we add an experimental comparison between AdaC-GOPE and a variant of AdaC-GOPE where the way of making the estimate ˆθℓ private is different (Section F.5). In AdaR- GOPE-Var, Step 4 changes to ˜θAdaR-GOPE-Var ℓ = ˆθℓ + V −1 ℓ � � a∈Sℓ aN � 0, 2 ρ �� . We compare AdaC-GOPE and AdaR-GOPE-Var in the same experimental setup and instances as in Section 5, for different privacy budgets ρ and report the results in Figure 3. 0.0 0.5 1.0 Step t ×106 101 103 105 Regret AdaC-GOPE AdaC-GOPE-Var (a) ρ = 0.001 0.0 0.5 1.0 Step t ×106 101 103 105 Regret AdaC-GOPE AdaC-GOPE-Var (b) ρ = 0.01 0.0 0.5 1.0 Step t ×106 101 103 Regret AdaC-GOPE AdaC-GOPE-Var (c) ρ = 0.1 0.0 0.5 1.0 Step t ×106 101 103 Regret AdaC-GOPE AdaC-GOPE-Var (d) ρ = 1 Figure 3: Evolution of the regret over time for AdaC-GOPE and Adar-GOPE-Var for different values of the privacy budget ρ As suggested by the regret analysis, AdaC-GOPE achieves less regret, especially in the high privacy regime where the private part of the regret has more impact. 46
Learning multi-modal generative models with permutation-invariant encoders and tighter variational bounds Marcel Hirt1,2 Domenico Campolo1 Victoria Leong1 Juan-Pablo Ortega1 1Nanyang Technological University, Singapore 2 Corresponding author: marcelandre.hirt@ntu.edu.sg Abstract Devising deep latent variable models for multi-modal data has been a long- standing theme in machine learning research. Multi-modal Variational Autoen- coders (VAEs) have been a popular generative model class that learns latent rep- resentations which jointly explain multiple modalities. Various objective func- tions for such models have been suggested, often motivated as lower bounds on the multi-modal data log-likelihood or from information-theoretic consid- erations. In order to encode latent variables from different modality subsets, Product-of-Experts (PoE) or Mixture-of-Experts (MoE) aggregation schemes have been routinely used and shown to yield different trade-offs, for instance, regard- ing their generative quality or consistency across multiple modalities. In this work, we consider a variational bound that can tightly lower bound the data log- likelihood. We develop more flexible aggregation schemes that generalise PoE or MoE approaches by combining encoded features from different modalities based on permutation-invariant neural networks. Our numerical experiments illustrate trade-offs for multi-modal variational bounds and various aggregation schemes. We show that tighter variational bounds and more flexible aggregation models can become beneficial when one wants to approximate the true joint distribution over observed modalities and latent variables in identifiable models. 1 Introduction Multi-modal data sets where each sample has features from distinct sources have grown in re- cent years. For example, multi-omics data such as genomics, epigenomics, transcriptomics and metabolomics can provide a more comprehensive understanding of biological systems if multiple modalities are analysed in an integrative framework [7, 74, 90]. However, annotations or labels in such data sets are often rare, making unsupervised or semi-supervised generative approaches partic- ularly attractive as such methods can be used in these settings to (i) generate data, such as missing modalities, and (ii) learn latent representations that are useful for down-stream analyses or that are of scientific interest themselves. The availability of heterogenous data for different modalities promises to learn generalizable rep- resentations that can capture shared content across multiple modalities in addition to modality- specific information. A promising class of weakly-supervised generative models is multi-modal VAEs [117, 138, 110, 115] that combine information across modalities in an often-shared low- dimensional latent representation. Other classes of generative models, such as denoising diffusion or energy-based models, have achieved impressive generative quality. However, these models are not naturally learning multi-modal latent representations and commonly resort to different guidance techniques [24, 47] to generate samples that are coherent across multiple modalities. Non-linear latent variable models often lack identifiability, even up to indeterminacies, which makes it hard to interpret inferred latent representations or model parameters. However, utilizing auxiliary Preprint. Under review. arXiv:2309.00380v1 [stat.ML] 1 Sep 2023variables or additional modalities, recent work [62, 63, 139] has shown that such models can become identifiable up to known indeterminacies with such models adapted, for instance, to neuroscience applications: [151] model neural activity conditional on non-neural labels using VAEs, while [107] model neural recordings conditional on behavioural variables using self-supervised learning. A common route for learning the parameters of latent variable models is via maximization of the marginal data likelihood with various lower bounds thereof suggested in previous work. Setup. We consider a set of M random variables {X1, . . . , XM} with empirical density pd, where each random variable Xs, s ∈ M = {1, . . . , M}, can be used to model a different data modality taking values in Xs. With some abuse of notation, we write X = {X1, . . . , XM} and for any subset S ⊂ M, we set X = (XS, X\S) for two partitions of the random variables into XS = {Xs}s∈S and X\S = {Xs}s∈M\S. We pursue a latent variable model setup, analogous to uni- modal VAEs [66, 100]. For a latent variable Z ∈ Z with prior density pθ(z), we posit a joint generative model1 pθ(z, x) = pθ(z) �M s=1 pθ(xs|z), where pθ(xs|z) is commonly referred to as the decoding distribution for modality s. Observe that all modalities are independent given the latent variable z shared across all modalities. One can introduce modality-specific latent variables by making sparsity assumptions for the decoding distribution. We assume throughout that Z = RD, and that pθ(z) is a Lebesgue density, although the results can be extended to more general settings such as discrete random variables Z with appropriate adjustments, for instance, regarding the gradient estimators. Multi-modal variational bounds and mutual information. Popular approaches to train multi- modal models are based on a mixture-based variational bound [22, 110] given by LMix(θ, ϕ, β) = � ρ(S)LMix S (x, θ, ϕ, β)dS, where LMix S (x, θ, ϕ, β) = � qϕ(z|xS) [log pθ(x|z)] dz − βKL(qϕ(z|xS)|pθ(z)) (1) and ρ is some distribution on the power set P(M) of M and β > 0. For β = 1, one obtains the bound LMix S (x, θ, ϕ, β) ≤ log pθ(x). Variations of (1) have been suggested [114], for example, by replacing the prior density pθ in the KL-term by a weighted product of the prior density pθ and the uni-modal encoding distributions qϕ(z|xs), for all s ∈ M. Maximizing LMix S can be seen as minimizing � H(X|ZS) + β Iqϕ(XS, ZS) = H(X) − Iqϕ(X, ZS) + β Iqϕ(XS, ZS) � , (2) where Iq(X, Y ) = � q(x, y) log q(x,y) q(x)q(y) is the mutual information of random variables X and Y having marginal and joint densities q, whilst H(X|Y ) = − � q(x, y) log q(x|y)]dxdy is the conditional entropy of X given Y . Likewise, the multi-view variational information bottle- neck approach developed in [74] for predicting x\S given xS can be interpreted as minimizing − Iqϕ(X\S, Z) + β Iqϕ(XS, Z). [52] suggested a related bound motivated by a conditional vari- ational bottleneck perspective that aims to maximize the reduction of total correlation of X when conditioned on Z, as measured by the conditional total correlation, see [136, 127, 32], i.e., minimizing � TC(X|Z) = TC(X) − TC(X, Z) = TC(X) + Iqϕ(X, Z) − M � s=1 Iqϕ(Xs, Z) � , (3) where TC(X) = KL(p(x)| �d i=1 p(xi)) for d-dimensional X. Resorting to variational lower bounds and using a constant β > 0 that weights the contributions of the mutual information terms, approxi- mations of (3) can be optimized by maximizing LTC(θ, ϕ, β) = � ρ(S) � {qϕ(z|x) [log pθ(x|z)] dz − βKL(qϕ(z|x)|qϕ(z|xS))} dS, where ρ is concentrated on the uni-modal subsets of M. Similar bounds have been suggested in [114] and [117] by considering different KL-regularisation terms, see also [116]. [111] add a con- trastive term to the maximum likelihood objective and minimize − log pθ(x) − β Ipθ(XS, X\S). 1We usually denote random variables using upper-case letters, and their realizations by the corresponding lower-case letter. 2Multi-modal aggregation schemes. In order to optimize the variational bounds above or to allow for flexible conditioning at test time, we need to learn encoding distributions qϕ(z|xS) for any S ∈ P(M). The typical aggregation schemes that are scalable to a large number of modalities are based on a choice of uni-modal encoding distributions qϕs(z|xs) for any s ∈ M, which are then used to define the multi-modal encoding distributions as follows: • Mixture of Experts (MoE), see [110], qMoE ϕ (z|xS) = 1 |S| � s∈S qϕs(z|xs). • Product of Experts (PoE), see [137], qPoE ϕ (z|xS) = 1 Z pθ(z) � s∈S qϕs(z|xs), for some Z ∈ R. Contributions. This paper contributes (i) a new variational bound that addresses known limita- tions of previous variational bounds. For instance, mixture-based bounds (1) may not provide tight bounds on the joint log-likelihood if there is considerable modality-specific variation [22]. In con- trast, the novel variational bound becomes a tight lower bound of both the marginal log-likelihood log pθ(xS) as well as the conditional log pθ(x\S|xS) for any choice of S ∈ P(M), provided that we can learn a flexible multi-modal encoding distribution. This paper then contributes (ii) new multi-modal aggregation schemes that yield more expressive multi-modal encoding distributions when compared to MoEs or PoEs. These schemes are motivated by the flexibility of permutation- invariant architectures such as DeepSets [144] or attention models [125, 75]. We illustrate that these innovations (iii) are beneficial when learning identifiable models, aided by using flexible prior and encoding distributions consisting of mixtures and (iv) yield higher log-likelihoods in experiments. Further related work. Canonical Correlation Analysis [49] is a classical approach for multi- modal data that aims to find projections of two modalities by maximally correlating them and has been interpreted in a probabilistic or generative framework [9]. Furthermore, it has been extended to include more than two modalities [6, 119] or to allow for non-linear transformations [2, 42, 132, 61]. Probabilistic CCA can also be seen as multi-battery factor analysis (MBFA) [17, 69], wherein a shared latent variable models the variation common to all modalities with modality-specific latent variables capturing the remaining variation. Likewise, latent factor regression or classification mod- els [113] assume that observed features and response are driven jointly by a latent variable. [126] considered a tiple-ELBO for two modalities, while [115] introduced a generalised variational bound that involves a summation over all modality subsets. A series of work has developed multi-modal VAEs based on shared and private latent variables [133, 76, 84, 85, 97]. [123] proposed a hybrid generative-discriminative objective and minimized an approximation of the Wasserstein distance between the generated and observed multi-modal data. [60] consider a semi-supervised setup of two modalities that requires no explicit multi-modal aggregation function. Extending the Info-Max principle [81], maximizing mutual information Iq(g1(X1), g(X2)) ≤ Iq((X1, X2), (Z1, Z2)) based on representations Zs = gs(Xs) for modality-specific encoders gs from two modalities has been a motivation for approaches based on (symmetrised) contrastive objectives [120, 148, 23] such as InfoNCE [96, 98, 131] as a variational lower bound on the mutual information between Z1 and Z2. 2 A tighter variational bound with arbitrary modality masking For S ⊂ M and β > 0, we define LS(xS, θ, ϕ, β) = � qϕ(z|xS) [log pθ(xS|z)] dz − βKL(qϕ(z|xS)|pθ(z)). (4) This is simply a standard variational lower bound [59, 14] restricted to the subset S for β = 1, and therefore LS(xS, θ, ϕ, 1) ≤ log pθ(xS). To obtain a lower bound on the log-likelihood of all modalities, we introduce an (approximate) conditional lower bound L\S(x, θ, ϕ, β) = � qϕ(z|x) � log pθ(x\S|z) � dz − βKL(qϕ(z|x)|qϕ(z|xS)). (5) For some fixed density ρ on P(M), we suggest the overall bound L(x, θ, ϕ, β) = � ρ(S) � LS(xS, θ, ϕ, β) + L\S(x, θ, ϕ, β) � dS, which is a generalisation of the bound suggested in [138] to an arbitrary number of modalities. This bound can be optimised using standard Monte Carlo techniques, for example, by computing unbi- ased pathwise gradients [66, 100, 122] using the reparameterisation trick. For variational families 3such as Gaussian mixtures2, one can employ implicit reparameterisation [29]. It is straightforward to adapt variance reduction techniques such as ignoring the score term of the multi-modal encoding densities for pathwise gradients [101], see Algorithm 1 in Appendix K for pseudo-code. Neverthe- less, a scalable approach requires an encoding technique that allows to condition on any masked modalities with a computational complexity that does not increase exponentially in M. Remark 1 (Optimization, multi-task learning and the choice of ρ). For simplicity, we have chosen to sample S ∼ ρ in our experiments via the hierarchical construction γ ∼ U(0, 1), mj ∼ Bern(γ) iid for all j ∈ [M] and setting S = {s ∈ [M]: mj = 1}. The distribution ρ for masking the modali- ties can be adjusted to accommodate various weights for different modality subsets. Indeed, (2) can be seen as a linear scalarisation of a multi-task learning problem [30, 109]. We aim to optimise a loss vector (LS + L\S)S⊂M, where the gradients for each S ⊂ M can point in different directions, making it challenging to minimise the loss for all modalities simultaneously. Consequently, [56] used multi-task learning techniques (e.g., as suggested in [19, 142]) for adjusting the gradients in mixture based VAEs. Such improved optimisation routines are orthogonal to our approach. Simi- larly, we do not analyse optimisation issues such as initialisations and training dynamics that have been found challenging for multi-modal learning [134, 51]. Multi-modal distribution matching. Likelihood-based learning approaches aim to match the model distribution pθ(x) to the true data distribution pd(x). Variational approaches achieve this by matching in the latent space the encoding distribution to the true posterior as well as maximizing a tight lower bound on log pθ(x), see for instance [103]. We show here analogous results for the multi- modal variational bound. Consider therefore the densities pθ(z, x) = pθ(z)pθ(xS|z)pθ(x\S|z) and qϕ(z, x) = pd(x)qϕ(z|x) = pd(xS)qϕ(z|xS)qϕ(x\S|z, xS). The standard interpretation is that the former is the generative density, while the latter is the encoding path consisting of the conditional variational approximation qϕ and the empirical density pd. The following Proposition, proven in Appendix A, shows that maximizing the variational lower bound L leads to a joint distribution matching of qϕ(z, x) and pθ(z, x), analogously to the uni-modal setting [150]. Proposition 2 (Joint distribution matching). For any S ∈ P(M), we have that � pd(x) � LS(xS, θ, ϕ, 1) + L\S(x, θ, ϕ, 1) � dx + H(pd(x)) = −KL(qϕ(z, x)|pθ(z, x)). In particular, LS(xS, θ, ϕ, 1) + L\S(x, θ, ϕ, 1) is a lower bound on log pθ(x). Moreover, Proposition 12 in Appendix A illustrates that maximizing � pd(xS)LS(xS, θ, ϕ)dxS drives (i) the joint inference distribution qϕ(z, xS) = pd(xS)qϕ(z|xS) of the S submodalities to the joint generative distribution pθ(z, xS) = pθ(z)pθ(xS|z) and (ii) the generative marginal pθ(xS) to its empirical counterpart pd(xS). Analogously, maximizing � pd(x\S|xS)L\S(x, θ, ϕ)dx\S drives (i) the distribution pd(x\S|xS)qϕ(z|x) to the distribution pθ(x\S|z)qϕ(z|xS) and (ii) the conditional pθ(x\S|xS) to its empirical counterpart pd(x\S|xS), provided that qϕ(z|xS) approximates pθ(z|xS) exactly. In this case, Proposition 12 implies that L\S(x, θ, ϕ) is a lower bound of log pθ(x\S|xS). Furthermore, it shows that L\S(x, θ, ϕ) contains a Bayes-consistency matching term for the multi- modal encoders where a mismatch can yield poor cross-generation, as an analogue of the prior not matching the aggregated posterior [87] leading to poor unconditional generation, see Remark 13. Our problem setup recovers meta-learning with (latent) Neural processes [34] when only optimizing the variational term L\S, where S is determined by context-target splits, cf. Appendix B. Information-theoretic perspective. Beyond generative modelling, β-VAEs [46] have been pop- ular for representation learning and data reconstruction. [3] suggest learning a latent representation that achieves certain mutual information with the data based on upper and lower variational bounds of the mutual information. A Legendre transformation thereof recovers the β-VAE objective and allows a trade-off between information content or rate versus reconstruction quality or distortion. We show that the proposed variational objective gives rise to an analogous perspective for multiple modalities. We recall first that mutual information Iqϕ(XS, Z) can be bounded by standard [11, 4, 3] lower and upper bounds using the rate and distortion: HS − DS ≤ HS − DS + ∆1 = Iqϕ(XS, Z) = RS − ∆2 ≤ RS, (6) 2For MoE aggregation schemes, [110] considered a stratified ELBO estimator as well as a tighter bound based on importance sampling, see also [93], that we do not pursue here for consistency with other aggregation schemes that can likewise be optimised based on importance sampling ideas. 4with ∆1, ∆2 ≥ 0 for the rate RS = � pd(xS)KL(qϕ(z|xS)|pθ(z))dxS measuring the information content that is encoded by qϕ into the latents, and the distortion DS = − � qϕ(xS, z) log pθ(xS|z)dzdxS given as the negative reconstruction log-likelihood. Observe that − � pd(xS)L(xS)dxS = DS + βRS and for any β > 0, it holds that HS ≤ RS + DS. To arrive at a similar interpretation for the conditional bound L\S, we set R\S = � pd(x)KL(qϕ(z|x)|qϕ(z|xS))dx for a conditional or cross rate. Similarly, set D\S = − � pd(x)qϕ(z|x) log pθ(x\S|z)dzdx. One obtains the following bounds, see Appendix A. Lemma 3 (Variational bounds on the conditional mutual information). It holds that − � L\S(x, θ, ϕ, β)pd(dx) = D\S + βR\S and for ∆\S,1, ∆\S,2 ≥ 0, H\S − D\S + ∆\S,1 = Iqϕ(X\S, ZM|XS) = R\S − ∆\S,2. Using the chain rules for entropy, we obtain that the suggested bound can be seen as a relaxation of bounds on marginal and conditional mutual information. Corollary 4 (Lagrangian relaxation). It holds that H − DS − D\S ≤ Iqϕ(XS, ZS) + Iqϕ(X\S, ZM|XS) ≤ RS + R\S and minimizing L for fixed β = ∂(DS+D\S) ∂(RS+R\S) minimizes the rates RS+R\S and distortions DS+D\S. Remark 5 (Mixture based variational bound). Rephrasing the arguments in [22], we can write − � pd(dx)LMix S (x) = DS + Dc \S + βRS, where Dc \S = � pd(xS)qϕ(z|xS) log pθ(x\S|z)dzdxS is a cross-distortion term. Due to H(XM|ZS) = −H(XM) + Iqϕ(XM, ZS) ≤ DS + Dc \S, we can view minimizing LMix S as minimizing H(XM) − Iqϕ(XM, ZS) + β Iqϕ(XS, ZS), see (2). Optimal variational distributions. Consider the annealed likelihood ˜pβ,θ(xS|z) ∝ pθ(xS|z)1/β as well as the adjusted posterior ˜pβ,θ(z|xS) ∝ ˜pβ,θ(xS|z)pθ(z). The minimum of the bound � pd(dx)LS(x) is attained at any xS for the variational density q⋆(z|xS) ∝ exp � 1 β [log pθ(xS|z) + β log pθ(z)] � ∝ ˜pβ,θ(z|xS), (7) see also [50]. Similarly, if (7) holds, then it is readily seen that the minimum of the bound � pd(dx)L\S(x) is attained at any x for the variational density q⋆(z|x) = ˜pβ,θ(z|x). In contrast, as shown in Appendix D, the optimal variational density for the mixture-based (1) multi-modal objective is attained at q⋆(z|xS) ∝ ˜pβ,θ(z|xS) exp �� pd(x\S|xS) log ˜pβ,θ(x\S|z)dx\S � . 3 Permutation-invariant modality encoding In order to optimize multi-modal bounds, we need to learn variational densities with different con- ditioning sets. To unify the presentation, let hs,φ : Xs �→ RDE be some modality-specific feature function that maps into a shared parameter space RDE. Fixed multi-modal aggregation schemes. We recall the following multi-modal encoding func- tions suggested in previous work where usually hs,φ(xs) = � µs,φ(xs)⊤, vec(Σs,φ(xs))⊤�⊤ with µs,φ and Σs,φ being the mean, respectively the (often diagonal) covariance, of a uni-modal encoder of modality s. Accommodating more complex variational families, such as mixture distributions for the uni-modal encoding distributions, can be more challenging for these approaches. • Mixture of Experts (MoE), see [110], qMoE φ (z|xS) = 1 |S| � s∈S qN (z|µs,φ(xs), Σs,φ(xs)), where qN (z|µ, Σ) is a Gaussian density with mean µ and covariance Σ. • Product of Experts (PoE), see [137], qPoE φ (z|xS) = 1 Z pθ(z) � s∈S qN (z|µs,φ(xs), Σs,φ(xs)), for some normalising constant Z. Under the assumption that the prior is Gaussian pθ(z) = qN (z|µθ, Σθ) with mean µθ ∈ RD and covariance matrix Σθ, the multi-modal encod- ing distribution qPoE φ (z|xS) is Gaussian with mean (µθΣθ + � s∈S µs,φ(xs)Σs,φ(xs))(Σ−1 1,θ + � s∈S Σs,φ(xs)−1)−1 and covariance (Σ−1 1,θ + � s∈S Σs,φ(xs)−1)−1. 5Learnable multi-modal aggregation schemes. We aim to learn a more flexible aggregation scheme under the constraint that the encoding distribution is invariant [15] with respect to the or- dering of encoded features of each modality. Put differently, for all (Hs)s∈S ∈ R|S|×DE and all permutations π ∈ SS of S, we assume that the conditional distribution is SS-invariant, i.e. q′ ϑ(z|h) = q′ ϑ(z|π · h) for all z ∈ RD, where π acts on H = (Hs)s∈S via π · H = (Hπ(s))s∈S. We set qϕ(z|xS) = q′ ϑ(z|hs,φ(xs)s∈S), ϕ = (φ, ϑ) and remark that the encoding distribution is not invariant with respect to the modalities, but becomes only invariant after applying modality-specific encoder functions hs,φ. Observe that such a constraint is satisfied by the aggregation schemes above for hs,φ being the encoding parameters for the uni-modal variational approximation. A variety of invariant (or equivariant) functions along with their approximation properties have been considered previously, see for instance [106, 144, 99, 75, 108, 94, 88, 105, 143, 18, 129, 147, 78, 12], and applied in different contexts such as meta-learning [27, 34, 64, 45, 38], reinforcement learning [118, 146] or generative modeling of (uni-modal) sets [77, 80, 65, 13, 79]. We can use such constructions to parameterise more flexible encoding distributions that allow for applying a reparameterisation trick [67, 100, 122]. Indeed, the results from [15] imply that for an exchangable sequence HS = (Hs)s∈S ∈ R|S|×DE and random variable Z, the distribution q′(z|hS) is SS- invariant if and only if there is a measurable function3 f ⋆ : [0, 1] × M(RDE) → RD such that (HS, Z) a.s. = (HS, f ⋆(Ξ, MHS)), where Ξ ∼ U[0, 1] and Ξ ⊥⊥ HS with MHS(·) = � s∈S δHs(·) being the empirical measure of hS, which retains the values of hS, but discards their order. For variational densities from a location-scale family such as a Gaussian or Laplace distribution, we find it more practical to consider a different reparameterisation in the form Z = µ(hS) + σ(hS) ⊙ Ξ, where Ξ is a sample from a parameter-free density p such as a standard Gaussian and Laplace distribution, while [µ(hS), log σ(hS)] = f(hS) for a permutation-invariant function f : R|S|×DE → R2D. Likewise, for mixture distributions thereof, we assume that [µ1(hS), log σ1(hS), . . . , µK(hS), log σK(hS), log ω(hS)] = f(hS) for a permutation-invariant function f : RDE → R2DK+K and Z = µL(hS) + σL(hS) ⊙ Ξ with L ∼ Cat(ω(hS)) denoting the sampled mixture component out of K mixtures. For simplicity, we consider here only two examples of permutation-invariant functions f that have representations with parameter ϑ in the form fϑ(hS) = ρϑ �� s∈S gϑ(hS)s � for a function ρϑ : RDP → RDO and permutation-equivariant function gϑ : RN×DE → RN×DP . Example 6 (Sum Pooling Encoders). The Deep Set [144] construction fϑ(hS) = ρϑ �� s∈S χϑ(hs) � applies the same neural network χϑ : RDE → RDP to each encoded feature hs. For simplicity, we assume that χϑ is a feed-forward neural network, and remark that pre-activation ResNets [43] have been advocated in [147] when χϑ contains multiple layers. For exponential fam- ily models, the optimal natural parameters of the posterior solve an optimisation problem where the dependence on the generative parameters from the different modalities decomposes as a sum, see Appendix G. Example 7 (Set Transformer Encoders). Let MTBϑ be a multi-head pre-layer-norm transformer block [130, 140], see Appendix E for precise definitions. For some neural network χϑ : RDE → RDP , set g0 S = χϑ(hS) and for k ∈ {1, . . . , L}, set gk S = MTBϑ(gk−1 S ). We then consider fϑ(hS) = ρϑ �� s∈S gL s � . This can be seen as a Set Transformer [75, 146] model without any inducing points as for most applications, a computational complexity that scales quadratically in the number of modalities can be acceptable. In our experiments, we use layer normalisation [8] within the transformer model, although, for example, set normalisation [146] could be used alternatively. Remark 8 (Pooling expert opinions). Combining expert distributions has a long tradition in decision theory and Bayesian inference, see [35] for early works, with popular schemes being linear pooling (i.e., MoE) or log-linear pooling (i.e., PoE with tempered densities). These are optimal schemes for minimizing different objectives, namely a weighted (forward or reverse) KL-divergence between the pooled distribution and the inidividual experts [1]. Log-linear pooling operators are externally Bayesian, that is, they allow for consistent Bayesian belief updates when each expert updates her belief with the same likelihood function [36]. 3The function f ⋆ generally depends on the cardinality of S. Finite-length exchangeable sequences imply a de Finetti latent variable representation only up to approximation errors [25]. 6Permutation-equivariance and private latent variables. Suppose that the generative model factorises as pθ(z, x) = p(z) � s∈M pθ(xs|z′, ˜zs) with z = (z′, ˜z1, . . . , ˜zM), for shared la- tent variables Z′ and private latent variable ˜Zs, s ∈ M. For s ̸= t ∈ [M], it holds that hφ,s(Xs) ⊥⊥ ˜Zt | Z′, ˜Zs. Under the assumption that we have modality-specific feature functions hφ,s such that {Hs = hφ,s(Xs)}s∈S is exchangeable, the results from [15] imply a permutation- equivariant representation of the private latent variables, conditional on the shared latent variables. This suggests to consider encoders for the private latent variables that satisfy q′ ϕ(˜zS|π·hφ(xS), z′) = q′ ϕ(π · ˜zS|hφ(xS), z′) for any permutation π ∈ SS. Details are given in Appendix F, including permutation-equivariant versions of PoEs, SumPooling and SelfAttention aggregations. 4 Identifiability and model extensions 4.1 Identifiability Identifiability of parameters and latent variables in latent structure models is a classic problem [70, 72, 5], that has been studied increasingly for non-linear latent variable models, e.g., for ICA [53, 40, 41], VAEs [62, 151, 135, 92, 82], EBMs [63], flow-based [112] or mixture models [68]. Non-linear generative models such as ICA are generally unidentifiable without imposing some struc- ture [54, 139]. However, identifiability up to some ambiguity can be achieved in some conditional models based on observed auxiliary variables and injective decoder functions wherein the prior den- sity is conditional on auxiliary variables. In our multi-modal setup, observations from different modalities can act as auxiliary variables to obtain identifiability of conditional distributions given some modality subset under an analogous assumption, see Appendix H. Example 9 (Auxiliary variable as a modality). In the iVAE model [62], the latent variable distribu- tion pθ(z|x1) is independently modulated via an auxiliary variable X1 = U. Instead of interpreting this distribution as a (conditional) prior density, we view it as a posterior density given the first modality X1. Assuming observations X2 from another modality, [62] estimate the model by lower bounding log pθ(x2|x1) via L\{1} under the assumption that qϕ(z|x1) is given by the prior density pθ(z|x1). Similarly, [91] optimise log pθ(x1, x2) by a double VAE bound that reduces to L for a masking distribution ρ(s1, s2) = (δ1 ⊗ δ0)(s1, s2) that always masks the modality X2 and choos- ing to parameterise separate encoding functions for different conditioning sets. Our bound thus generalises these procedures to multiple modalities in a scalable way. 4.2 Mixture models An alternative to the choice of uni-modal prior densities pθ has been to use Gaussian mixture priors [58, 57, 26] or more flexible mixture models [28]. Following previous work, we include a latent cluster indicator variable c ∈ [K] that indicates the mixture component out of K possible mixtures with augmented prior pθ(c, z) = pθ(c)pθ(z|c). The classic example is pθ(c) being a categorical distribution and pθ(z|c) a Gaussian with mean µc and covariance matrix Σc. Similar to [28] that use an optimal variational factor in a mean-field model, we use an optimal factor of the cluster indicator in a structured variational density qϕ(c, z|xS) = qϕ(z|xS)qϕ(c|z, xS) with qϕ(c|z, xS) = pθ(c|z) We show in greater detail in Appendix J how one can optimize an augmented multi-modal bound. 5 Experiments 5.1 Linear multi-modal VAEs The relationship between uni-modal VAEs and probabilistic principle component analysis [121] has been studied in previous work [21, 83, 102]. [50] considered a variational rate-distortion analysis for linear VAEs and [89] illustrated that varying β simply scales the inferred latent factors. Our focus will be the analysis of different multi-modal fusion schemes and multi-modal variational bounds in this setting. We perform a simulation study based on two different data generation mechanisms of multi-modal (M = 5) linear Gaussian models wherein i) all latent variables are shared across all modalities and ii) only parts of the latent variables are shared across all modalities with the re- maining latent variables being modality specific. The latter setting can be incorporated by imposing 7sparsity structures on the decoders and allows us to analyse scenarios with considerable modality- specific variation described through private latent variables. We refer to Appendix M for details about the data generation mechanisms. We assess the learned generative models and inferred latent representations by computing the true marginal log-likelihood of the multi-modal data, and addi- tionally assess the tightness of the variational bound. Results for case (i) of shared latent variables are given in Table 1, with the corresponding results for modality-specific latent variables found in Table 5 in Appendix M. In order to evaluate the (weak) identifiability of the method, we follow [62, 63] to compute the mean correlation co-efficient (MCC) between the true latent variables Z and samples from the variational distribution qϕ(·|xM) after an affine transformation using CCA. Our results suggest that first, more flexible aggregation schemes improve the log-likelihood, the tightness of the variational bound and the identifiability for both variational objectives. Second, our new bound provides a tighter approximation to the log-likelihood for different aggregation schemes. Additionally, we compute different rate and distortion terms in Appendix M, Figures 3 and 4 and the KL-divergence between the encoding distribution and the true posterior. Table 1: Multi-modal Gaussian model with dense decoders: LLH Gap is the relative difference of the log-likelihood of the learned model relative to the log-likelihood based on the exact MLE. Bound gap is the relative difference of the variational bound to the log-likelihood based on the MLE. Our bound Mixture bound Aggregation LLH Gap Bound Gap MCC LLH Gap Bound Gap MCC PoE 0.03 (0.058) 0.12 (0.241) 0.75 (0.20) 0.04 (0.074) 0.13 (0.220) 0.77 (0.21) MoE 0.01 (0.005) 0.02 (0.006) 0.82 (0.04) 0.02 (0.006) 0.11 (0.038) 0.67 (0.03) SumPooling 0.00 (0.000) 0.00 (0.000) 0.84 (0.00) 0.00 (0.002) 0.02 (0.003) 0.84 (0.02) SelfAttention 0.00 (0.003) 0.00 (0.003) 0.84 (0.00) 0.02 (0.007) 0.03 (0.007) 0.83 (0.00) 5.2 Non-linear models (a) Observed data X (b) True latents Z (c) Our bound with SumPooling (d) Our bound with PoE (e) Mixture bound with PoE Figure 1: Bi-modal model with label (colour-coded) and continuous modality in (a) with true latent variables in (b). Inferred latent variables in (c) - (e) with a linear transformation inditerminancy. Auxiliary labels as modalities. We construct artificial data following [62], with the latent vari- ables Z ∈ RD being conditionally Gaussian having means and variances that depend on an ob- served index value X2 ∈ [K]. More precisely, pθ(z|x2) = N(µx2, Σx2), where µc ∼ ⊗ U(−5, 5) and Σc = diag(Λc), Λc ∼ ⊗ U(0.5, 3) iid for c ∈ [K]. The marginal distribution over the labels is uniform U([K]) so that the prior density pθ(z) = � [K] pθ(z|x2)pθ(x2)dx2 becomes a Gaussian mixture. We choose an injective decoding function f1 : RD → RD1, D ≤ D1, as a composition of MLPs with LeakyReLUs and full rank weight matrices having monotonically increasing row di- mensions [63] and with iid randomly sampled entries. We assume X1|Z ∼ N(f1(Z), σ2 I). We set σ = 0.1, D = D1 = 2, and f1 has a single hidden layer of size D1 = 2. One realisation of bi-modal data X, the true latent variable Z, as well as inferred latent variables for a selection of different bounds and aggregation schemes, are shown in Figure 1, with more examples given in Figures 6 and 7. Results over multiple repetitions in Table 7 indicate that both a tighter variational bound and more flexible aggregation schemes improve the identifiability of the latent variables and the log-likelihood as estimated using importance sampling with 64 particles. 8Multiple modalities. Considering the same generative model for Z with a Gaussian mixture prior, suppose now that instead of observing the auxiliary label, we observe multiple modalities Xs ∈ RDs, Xs|Z ∼ N(fs(Z), σ2 I), for injective MLPs fs constructed as above, with D = 10, Ds = 25, σ = 0.5 and K = M = 5. We consider a semi-supervised setting where modali- ties are missing completely at random, as in [145], with a missing rate η as the sample average of 1 |M| � s∈M(1 − Ms). Our bound and the suggested permutation-invariant aggregation schemes can naturally accommodate this partially observed setting, see Appendix I for details. Table 2 shows that using the new variational bound improves the log-likelihood and the identifiability of the latent representation. Furthermore, using learnable aggregation schemes benefits both variational bounds. Table 2: Partially observed (η = 0.5) non-linear identifiable model with 5 modalities: The first four rows use a fixed standard Gaussian prior, while the last four rows use a Gaussian mixture prior with 5 components. Mean and standard deviation over 4 repetitions. Our bound Mixture Aggregation LLH Lower Bound MCC LLH Lower Bound MCC PoE -250.9 (5.19) -256.1 (5.43) 0.94 (0.015) -288.4 (8.53) -328.8 (9.17) 0.93 (0.018) MoE -250.1 (4.77) -255.3 (4.90) 0.92 (0.022) -286.2 (7.63) -325.1 (8.03) 0.90 (0.019) SumPooling -249.6 (4.85) -253.1 (4.84) 0.95 (0.016) -275.6 (7.35) -317.7 (8.72) 0.92 (0.031) SelfAttention -249.7 (4.83) -253.1 (4.84) 0.95 (0.014) -275.5 (7.45) -317.6 (8.68) 0.93 (0.022) SumPooling -247.3 (4.23) -251.9 (4.31) 0.95 (0.009) -269.6 (7.42) -311.5 (8.47) 0.94 (0.018) SelfAttention -247.5 (4.22) -252.1 (4.21) 0.95 (0.013) -269.9 (6.06) -311.6 (7.72) 0.93 (0.022) SumPoolingMixture -244.8 (4.44) -249.5 (5.84) 0.95 (0.011) -271.9 (6.54) -313.4 (7.30) 0.93 (0.021) SelfAttentionMixture -245.4 (4.55) -248.2 (4.80) 0.96 (0.010) -270.3 (5.96) -312.1 (7.61) 0.94 (0.016) 5.3 MNIST-SVHN-Text Following previous work [114, 115, 56], we consider a tri-modal dataset based on augmenting the MNIST-SVHN dataset [110] with a text-based modality comprised of the string with the English name of the digit at different starting positions. Herein, SVHN consists of relatively noisy images, whilst MNIST and text are clearer modalities. Multi-modal VAEs have been shown to exhibit dif- fering performances relative to their multi-modal coherence, latent classification accuracy or test log-likelihood, see Appendix L for definitions. Previous works often differ in their hyperparam- eters, from neural network architectures, latent space dimensions, priors and likelihood families, modality-specific likelihood weightings, fixed decoder variances, etc. However, we have chosen the same hyperparameters for all models, thereby providing a clearer disentanglement of how either the variational objective or the aggregation scheme affect different multi-modal evaluation measures. In particular, we consider multi-modal generative models with (i) shared latent variables and (ii) private and shared latent variables. As an additional benchmark we also consider PoE or MoE schemes (de- noted PoE+, resp., MoE+) with additional neural network layers in their modality-specific encoding functions so that the number of parameters matches or exceeds those of the introduced permutation- invariant models, see Appendix P.5 for details. For models without private latent variables, estimates of the test log-likelihoods in Table 3 suggest that our bound improves the log-likelihood across dif- ferent aggregation schemes for all modalities and differnet βs (Table 9), with similar results for permutation-equivariant schemes, except for a Self-Attention model. Furthermore, more flexible fusion schemes yield higher log-likelihoods for both bounds. We provide qualitative results for the reconstructed modalities in Figures 10 - 12. We believe that the clearest observation here is that realistic cross-generation of the SVHN modality is challenging for the mixture-based bound with all aggregation schemes. In contrast, our bound, particularly when combined with the learnable aggre- gation schemes, improves the cross-generation of SVHN. No bound or aggregation scheme performs best across all modalities by the generative coherence measures (see Table 4 for uni-modal inputs, Table 10 for bi-modal ones and Tables 11 - 14 for models with private latent variables and differ- ent βs). Overall, our bound is slightly more coherent for cross-generating SVHN or Text, but less coherent for MNIST. Furthermore, mixture based bounds tend to improve the unsupervised latent classifcation accuracy across different fusion approaches and modalities, see Table 15. To provide complementary insights into the trade-offs for the different bounds and fusion schemes, we consider a multi-modal rate-distortion evaluation in Figure 2. Ignoring MoE where reconstructions are simi- lar, observe that our bound improves the full reconstruction, with higher full rates, and across various fusion schemes. In contrast, mixture-based bounds yield improved cross-reconstructions for all ag- 9gregation models, with increased cross-rates terms. Flexible permutation-invariant architectures for our bound improve the full reconstruction, even at lower full rates. (a) Full Reconstr. −DM (b) Cross Reconstr. −Dc \S (c) Full Rates RM (d) Cross Rates R\S Figure 2: Rate and distortion terms for MNIST-SVHN-Text with shared latent variables (β = 1). Table 3: Test log-likelihood estimates for the joint data (M+S+T) and marginal data (importance sampling with 512 particles). The first part of the table is based on the same generative model with shared latent variable Z ∈ R40, while the second part of the table is based on a restrictive generative model with a shared latent variable Z′ ∈ R10 and modality-specific latent variables ˜Zs ∈ R10. Our bound Mixture bound Aggregation M+S+T M S T M+S+T M S T PoE+ 6872 (9.62) 2599 (5.6) 4317 (1.1) -9 (0.2) 5900 (10) 2449 (10.4) 3443 (11.7) -19 (0.4) PoE 6775 (54.9) 2585 (18.7) 4250 (8.1) -10 (2.2) 5813 (1.2) 2432 (11.6) 3390 (17.5) -19 (0.1) MoE+ 5428 (73.5) 2391 (104) 3378 (92.9) -74 (88.7) 5420 (60.1) 2364 (33.5) 3350 (58.1) -112 (133.4) MoE 5597 (26.7) 2449 (7.6) 3557 (26.4) -11 (0.1) 5485 (4.6) 2343 (1.8) 3415 (5.0) -17 (0.4) SumPooling 7056 (124) 2478 (9.3) 4640 (114) -6 (0.0) 6130 (4.4) 2470 (10.3) 3660 (1.5) -16 (1.6) SelfAttention 7011 (57.9) 2508 (18.2) 4555 (38.1) -7 (0.5) 6127 (26.1) 2510 (12.7) 3621 (8.5) -13 (0.2) PoE+ 6549 (33.2) 2509 (7.8) 4095 (37.2) -7 (0.2) 5869 (29.6) 2465 (4.3) 3431 (8.3) -19 (1.7) SumPooling 6337 (24.0) 2483 (9.8) 3965 (16.9) -6 (0.2) 5930 (23.8) 2468 (16.8) 3491 (18.3) -7 (0.1) SelfAttention 6662 (20.0) 2516 (8.8) 4247 (31.2) -6 (0.4) 6716 (21.8) 2430 (26.9) 4282 (49.7) -27 (1.1) Table 4: Conditional coherence with shared latent variables and uni-modal inputs. The letters on the second line represent the generated modality based on the input modalities on the line below it. Our bound Mixture bound M S T M S T Aggregation M S T M S T M S T M S T M S T M S T PoE 0.97 0.22 0.56 0.29 0.60 0.36 0.78 0.43 1.00 0.96 0.83 0.99 0.11 0.57 0.10 0.44 0.39 1.00 PoE+ 0.97 0.15 0.63 0.24 0.63 0.42 0.79 0.35 1.00 0.96 0.83 0.99 0.11 0.59 0.11 0.45 0.39 1.00 MoE 0.96 0.80 0.99 0.11 0.59 0.11 0.44 0.37 1.00 0.94 0.81 0.97 0.10 0.54 0.10 0.45 0.39 1.00 MoE+ 0.93 0.77 0.95 0.11 0.54 0.10 0.44 0.37 0.98 0.94 0.80 0.98 0.10 0.53 0.10 0.45 0.39 1.00 SumPooling 0.97 0.48 0.87 0.25 0.72 0.36 0.73 0.48 1.00 0.97 0.86 0.99 0.10 0.63 0.10 0.45 0.40 1.00 SelfAttention 0.97 0.44 0.79 0.20 0.71 0.36 0.61 0.43 1.00 0.97 0.86 0.99 0.10 0.63 0.11 0.45 0.40 1.00 6 Conclusion Limitations. A drawback of our bound is that computing a gradient step is more expensive as it requires drawing samples from two encoding distributions. Similarly, learning aggregation functions is more computationally expensive compared to fixed schemes. Mixture-based bounds might be preferred if one is interested primarily in cross-modal reconstructions. Outlook. Using modality-specific encoders to learn features and aggregating them with a permutation-invariant function is clearly not the only choice for building multi-modal encoding distributions. However, it allows us to utilize modality-specific architectures for the encoding func- tions. Alternatively, our bounds could also be used, e.g., when multi-modal transformer architectures [141] encode a distribution on a shared latent space. Our approach applies to general prior densities if we can compute its cross-entropy relative to the multi-modal encoding distributions. An extension would be to apply it with more flexible prior distributions, e.g., as specified via score-based genera- tive models [124]. The ideas in this work might also be of interest for other approaches that require flexible modeling of conditional distributions, such as in meta-learning with Neural processes. 10Acknowledgements This work is supported by funding from the Wellcome Leap 1kD Program and by the RIE2025 Human Potential Programme Prenatal/Early Childhood Grant (H22P0M0002), administered by A*STAR. The computational work for this article was partially performed on resources of the Na- tional Supercomputing Centre, Singapore (https://www.nscc.sg). References [1] A. E. Abbas. A Kullback-Leibler view of linear and log-linear pools. Decision Analysis, 6 (1):25–37, 2009. [2] S. Akaho. A kernel method for canonical correlation analysis. In International Meeting of Psychometric Society, 2001, 2001. [3] A. Alemi, B. Poole, I. Fischer, J. Dillon, R. A. Saurous, and K. Murphy. Fixing a broken elbo. In International conference on machine learning, pages 159–168. PMLR, 2018. [4] A. A. Alemi, I. Fischer, J. V. Dillon, and K. Murphy. Deep Variational Information Bottle- neck. arXiv preprint arXiv:1612.00410, 2016. [5] E. S. Allman, C. Matias, and J. A. Rhodes. Identifiability of parameters in latent structure models with many observed variables. The Annals of Statistics, 37(6A):3099–3132, 2009. [6] C. Archambeau and F. Bach. Sparse probabilistic projections. Advances in neural information processing systems, 21, 2008. [7] R. Argelaguet, B. Velten, D. Arnol, S. Dietrich, T. Zenz, J. C. Marioni, F. Buettner, W. Huber, and O. Stegle. Multi-Omics Factor Analysis—a framework for unsupervised integration of multi-omics data sets. Molecular systems biology, 14(6):e8124, 2018. [8] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [9] F. R. Bach and M. I. Jordan. A Probabilistic Interpretation of Canonical Correlation Analysis. 2005. [10] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014. [11] D. Barber and F. Agakov. The IM Algorithm: a variational approach to Information Maxi- mization. Advances in neural information processing systems, 16(320):201, 2004. [12] S. Bartunov, F. B. Fuchs, and T. P. Lillicrap. Equilibrium aggregation: Encoding sets via optimization. In Uncertainty in Artificial Intelligence, pages 139–149. PMLR, 2022. [13] M. Biloˇs and S. G¨unnemann. Scalable normalizing flows for permutation invariant densities. In International Conference on Machine Learning, pages 957–967. PMLR, 2021. [14] D. M. Blei, A. Kucukelbir, and J. D. McAuliffe. Variational inference: A review for statisti- cians. Journal of the American Statistical Association, 112(518):859–877, 2017. [15] B. Bloem-Reddy and Y. W. Teh. Probabilistic symmetries and invariant neural networks. J. Mach. Learn. Res., 21:90–1, 2020. [16] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable trans- formations of Python+NumPy programs, 2018. URL http://github.com/google/jax. [17] M. Browne. Factor analysis of multiple batteries by maximum likelihood. British Journal of Mathematical and Statistical Psychology, 1980. [18] A. Bruno, J. Willette, J. Lee, and S. J. Hwang. Mini-batch consistent slot set encoder for scalable set encoding. Advances in Neural Information Processing Systems, 34:21365–21374, 2021. 11[19] Z. Chen, V. Badrinarayanan, C.-Y. Lee, and A. Rabinovich. Gradnorm: Gradient normaliza- tion for adaptive loss balancing in deep multitask networks. In International conference on machine learning, pages 794–803. PMLR, 2018. [20] J. Chung, K. Kastner, L. Dinh, K. Goel, A. C. Courville, and Y. Bengio. A recurrent latent variable model for sequential data. In Advances in neural information processing systems, pages 2980–2988, 2015. [21] B. Dai, Y. Wang, J. Aston, G. Hua, and D. Wipf. Connections with robust PCA and the role of emergent sparsity in variational autoencoder models. The Journal of Machine Learning Research, 19(1):1573–1614, 2018. [22] I. Daunhawer, T. M. Sutter, K. Chin-Cheong, E. Palumbo, and J. E. Vogt. On the Limitations of Multimodal VAEs. In International Conference on Learning Representations, 2022. [23] I. Daunhawer, A. Bizeul, E. Palumbo, A. Marx, and J. E. Vogt. Identifiability results for multimodal contrastive learning. arXiv preprint arXiv:2303.09166, 2023. [24] P. Dhariwal and A. Nichol. Diffusion models beat GANs on image synthesis. Advances in Neural Information Processing Systems, 34:8780–8794, 2021. [25] P. Diaconis and D. Freedman. Finite exchangeable sequences. The Annals of Probability, pages 745–764, 1980. [26] N. Dilokthanakul, P. A. Mediano, M. Garnelo, M. C. Lee, H. Salimbeni, K. Arulkumaran, and M. Shanahan. Deep unsupervised clustering with Gaussian Mixture Variational Autoen- coders. arXiv preprint arXiv:1611.02648, 2016. [27] H. Edwards and A. Storkey. Towards a neural statistician. arXiv preprint arXiv:1606.02185, 2016. [28] F. Falck, H. Zhang, M. Willetts, G. Nicholson, C. Yau, and C. C. Holmes. Multi-facet clus- tering Variational Autoencoders. Advances in Neural Information Processing Systems, 34: 8676–8690, 2021. [29] M. Figurnov, S. Mohamed, and A. Mnih. Implicit reparameterization gradients. In Advances in Neural Information Processing Systems, pages 441–452, 2018. [30] J. Fliege and B. F. Svaiter. Steepest descent methods for multicriteria optimization. Mathe- matical methods of operations research, 51:479–494, 2000. [31] A. Foong, W. Bruinsma, J. Gordon, Y. Dubois, J. Requeima, and R. Turner. Meta-learning stationary stochastic process prediction with convolutional neural processes. Advances in Neural Information Processing Systems, 33:8284–8295, 2020. [32] S. Gao, R. Brekelmans, G. Ver Steeg, and A. Galstyan. Auto-encoding total correlation explanation. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1157–1166. PMLR, 2019. [33] M. Garnelo, D. Rosenbaum, C. Maddison, T. Ramalho, D. Saxton, M. Shanahan, Y. W. Teh, D. Rezende, and S. A. Eslami. Conditional neural processes. In International conference on machine learning, pages 1704–1713. PMLR, 2018. [34] M. Garnelo, J. Schwarz, D. Rosenbaum, F. Viola, D. J. Rezende, S. Eslami, and Y. W. Teh. Neural processes. arXiv preprint arXiv:1807.01622, 2018. [35] C. Genest and J. V. Zidek. Combining probability distributions: A critique and an annotated bibliography. Statistical Science, 1(1):114–135, 1986. [36] C. Genest, K. J. McConway, and M. J. Schervish. Characterization of externally Bayesian pooling operators. The Annals of Statistics, pages 487–501, 1986. [37] S. Ghalebikesabi, R. Cornish, L. J. Kelly, and C. Holmes. Deep generative pattern-set mixture models for nonignorable missingness. arXiv preprint arXiv:2103.03532, 2021. 12[38] G. Giannone and O. Winther. Scha-vae: Hierarchical context aggregation for few-shot gen- eration. In International Conference on Machine Learning, pages 7550–7569. PMLR, 2022. [39] Y. Gong, H. Hajimirsadeghi, J. He, T. Durand, and G. Mori. Variational selective autoen- coder: Learning from partially-observed heterogeneous data. In International Conference on Artificial Intelligence and Statistics, pages 2377–2385. PMLR, 2021. [40] H. H¨alv¨a and A. Hyvarinen. Hidden markov nonlinear ica: Unsupervised learning from nonstationary time series. In Conference on Uncertainty in Artificial Intelligence, pages 939– 948. PMLR, 2020. [41] H. H¨alv¨a, S. Le Corff, L. Leh´ericy, J. So, Y. Zhu, E. Gassiat, and A. Hyvarinen. Disentangling identifiable features from noisy data with structured nonlinear ICA. Advances in Neural Information Processing Systems, 34:1624–1633, 2021. [42] D. R. Hardoon, S. Szedmak, and J. Shawe-Taylor. Canonical correlation analysis: An overview with application to learning methods. Neural computation, 16(12):2639–2664, 2004. [43] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV 14, pages 630–645. Springer, 2016. [44] J. Heek, A. Levskaya, A. Oliver, M. Ritter, B. Rondepierre, A. Steiner, and M. van Zee. Flax: A neural network library and ecosystem for JAX, 2023. URL http://github.com/ google/flax. [45] L. B. Hewitt, M. I. Nye, A. Gane, T. Jaakkola, and J. B. Tenenbaum. The variational homoen- coder: Learning to learn high capacity generative models from few examples. arXiv preprint arXiv:1807.08919, 2018. [46] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Ler- chner. β-VAE: Learning basic visual concepts with a constrained variational framework. In International conference on learning representations, 2017. [47] J. Ho and T. Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [48] M. D. Hoffman and M. J. Johnson. ELBO surgery: yet another way to carve up the variational evidence lower bound. In Workshop in Advances in Approximate Bayesian Inference, NIPS, 2016. [49] H. Hotelling. Relations between two sets of variates. Biometrika, 28(3/4):321–377, 1936. [50] S. Huang, A. Makhzani, Y. Cao, and R. Grosse. Evaluating lossy compression rates of deep generative models. arXiv preprint arXiv:2008.06653, 2020. [51] Y. Huang, J. Lin, C. Zhou, H. Yang, and L. Huang. Modality competition: What makes joint training of multi-modal network fail in deep learning?(provably). arXiv preprint arXiv:2203.12221, 2022. [52] H. Hwang, G.-H. Kim, S. Hong, and K.-E. Kim. Multi-view representation learning via total correlation objective. Advances in Neural Information Processing Systems, 34:12194–12207, 2021. [53] A. Hyvarinen and H. Morioka. Unsupervised feature extraction by time-contrastive learning and nonlinear ICA. Advances in neural information processing systems, 29, 2016. [54] A. Hyv¨arinen and P. Pajunen. Nonlinear Independent Component Analysis: Existence and uniqueness results. Neural networks, 12(3):429–439, 1999. [55] N. B. Ipsen, P.-A. Mattei, and J. Frellsen. not-MIWAE: Deep Generative Modelling with Missing not at Random Data. In ICLR 2021-International Conference on Learning Repre- sentations, 2021. 13[56] A. Javaloy, M. Meghdadi, and I. Valera. Mitigating Modality Collapse in Multimodal VAEs via Impartial Optimization. arXiv preprint arXiv:2206.04496, 2022. [57] Z. Jiang, Y. Zheng, H. Tan, B. Tang, and H. Zhou. Variational deep embedding: an unsuper- vised and generative approach to clustering. In Proceedings of the 26th International Joint Conference on Artificial Intelligence, pages 1965–1972, 2017. [58] M. J. Johnson, D. Duvenaud, A. B. Wiltschko, S. R. Datta, and R. P. Adams. Structured vaes: Composing probabilistic graphical models and variational autoencoders. arXiv preprint arXiv:1603.06277, 2016. [59] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for graphical models. Machine learning, 37(2):183–233, 1999. [60] T. Joy, Y. Shi, P. H. Torr, T. Rainforth, S. M. Schmon, and N. Siddharth. Learning multimodal VAEs through mutual supervision. arXiv preprint arXiv:2106.12570, 2021. [61] M. Karami and D. Schuurmans. Deep probabilistic canonical correlation analysis. In Pro- ceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 8055–8063, 2021. [62] I. Khemakhem, D. Kingma, R. Monti, and A. Hyvarinen. Variational Autoencoders and nonlinear ICA: A unifying framework. In International Conference on Artificial Intelligence and Statistics, pages 2207–2217. PMLR, 2020. [63] I. Khemakhem, R. Monti, D. Kingma, and A. Hyvarinen. ICE-BeeM: Identifiable Conditional Energy-Based Deep Models Based on Nonlinear ICA. Advances in Neural Information Pro- cessing Systems, 33:12768–12778, 2020. [64] H. Kim, A. Mnih, J. Schwarz, M. Garnelo, A. Eslami, D. Rosenbaum, O. Vinyals, and Y. W. Teh. Attentive neural processes. In International Conference on Learning Representations, 2018. [65] J. Kim, J. Yoo, J. Lee, and S. Hong. Setvae: Learning hierarchical composition for generative modeling of set-structured data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15059–15068, 2021. [66] D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [67] D. P. Kingma and M. Welling. Auto-Encoding Variational Bayes. Proceedings of the 2nd International Conference on Learning Representations (ICLR), 2014. [68] B. Kivva, G. Rajendran, P. K. Ravikumar, and B. Aragam. Identifiability of deep genera- tive models without auxiliary information. In Advances in Neural Information Processing Systems, 2022. [69] A. Klami, S. Virtanen, and S. Kaski. Bayesian canonical correlation analysis. Journal of Machine Learning Research, 14(4), 2013. [70] T. C. Koopmans and O. Reiersol. The identification of structural characteristics. The Annals of Mathematical Statistics, 21(2):165–181, 1950. [71] D. Kramer, P. L. Bommer, D. Durstewitz, C. Tombolini, and G. Koppe. Reconstructing nonlinear dynamical systems from multi-modal time series. In International Conference on Machine Learning, pages 11613–11633. PMLR, 2022. [72] J. B. Kruskal. More factors than subjects, tests and treatments: An indeterminacy theorem for canonical decomposition and individual differences scaling. Psychometrika, 41(3):281–293, 1976. [73] T. A. Le, H. Kim, M. Garnelo, D. Rosenbaum, J. Schwarz, and Y. W. Teh. Empirical eval- uation of neural process objectives. In NeurIPS workshop on Bayesian Deep Learning, vol- ume 4, 2018. 14[74] C. Lee and M. van der Schaar. A variational information bottleneck approach to multi-omics data integration. In International Conference on Artificial Intelligence and Statistics, pages 1513–1521. PMLR, 2021. [75] J. Lee, Y. Lee, J. Kim, A. Kosiorek, S. Choi, and Y. W. Teh. Set Transformer: A framework for attention-based permutation-invariant neural networks. In International conference on machine learning, pages 3744–3753. PMLR, 2019. [76] M. Lee and V. Pavlovic. Private-shared disentangled multimodal vae for learning of latent representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1692–1700, 2021. [77] C.-L. Li, M. Zaheer, Y. Zhang, B. Poczos, and R. Salakhutdinov. Point cloud GAN. arXiv preprint arXiv:1810.05795, 2018. [78] Q. Li, T. Lin, and Z. Shen. Deep neural network approximation of invariant functions through dynamical systems. arXiv preprint arXiv:2208.08707, 2022. [79] Y. Li and J. Oliva. Partially observed exchangeable modeling. In International Conference on Machine Learning, pages 6460–6470. PMLR, 2021. [80] Y. Li, H. Yi, C. Bender, S. Shan, and J. B. Oliva. Exchangeable neural ode for set modeling. Advances in Neural Information Processing Systems, 33:6936–6946, 2020. [81] R. Linsker. Self-organization in a perceptual network. Computer, 21(3):105–117, 1988. [82] C. Lu, Y. Wu, J. M. Hern´andez-Lobato, and B. Sch¨olkopf. Invariant causal representation learning for out-of-distribution generalization. In International Conference on Learning Rep- resentations, 2022. [83] J. Lucas, G. Tucker, R. B. Grosse, and M. Norouzi. Don’t Blame the ELBO! A Linear VAE Perspective on Posterior Collapse. In Advances in Neural Information Processing Systems, pages 9408–9418, 2019. [84] Q. Lyu and X. Fu. Finite-sample analysis of deep CCA-based unsupervised post-nonlinear multimodal learning. IEEE Transactions on Neural Networks and Learning Systems, 2022. [85] Q. Lyu, X. Fu, W. Wang, and S. Lu. Understanding latent correlation-based multiview learn- ing and self-supervision: An identifiability perspective. arXiv preprint arXiv:2106.07115, 2021. [86] C. Ma, S. Tschiatschek, K. Palla, J. M. Hernandez-Lobato, S. Nowozin, and C. Zhang. EDDI: Efficient Dynamic Discovery of High-Value Information with Partial VAE. In International Conference on Machine Learning, pages 4234–4243. PMLR, 2019. [87] A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and B. Frey. Adversarial Autoencoders. In ICLR, 2016. [88] H. Maron, E. Fetaya, N. Segol, and Y. Lipman. On the universality of invariant networks. In International conference on machine learning, pages 4363–4371. PMLR, 2019. [89] E. Mathieu, T. Rainforth, N. Siddharth, and Y. W. Teh. Disentangling disentanglement in Variational Autoencoders. In International Conference on Machine Learning, pages 4402– 4412. PMLR, 2019. [90] K. Minoura, K. Abe, H. Nam, H. Nishikawa, and T. Shimamura. A mixture-of-experts deep generative model for integrated analysis of single-cell multiomics data. Cell reports methods, 1(5):100071, 2021. [91] G. Mita, M. Filippone, and P. Michiardi. An identifiable double VAE for disentangled rep- resentations. In International Conference on Machine Learning, pages 7769–7779. PMLR, 2021. [92] G. E. Moran, D. Sridhar, Y. Wang, and D. M. Blei. Identifiable deep generative models via sparse decoding. arXiv preprint arXiv:2110.10804, 2021. 15[93] W. Morningstar, S. Vikram, C. Ham, A. Gallagher, and J. Dillon. Automatic differentiation variational inference with mixtures. In International Conference on Artificial Intelligence and Statistics, pages 3250–3258. PMLR, 2021. [94] R. Murphy, B. Srinivasan, V. Rao, and B. Riberio. Janossy pooling: Learning deep permutation-invariant functions for variable-size inputs. In International Conference on Learning Representations (ICLR 2019), 2019. [95] A. Nazabal, P. M. Olmos, Z. Ghahramani, and I. Valera. Handling incomplete heterogeneous data using VAEs. Pattern Recognition, 107:107501, 2020. [96] A. v. d. Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. [97] E. Palumbo, I. Daunhawer, and J. E. Vogt. Mmvae+: Enhancing the generative quality of mul- timodal vaes without compromises. In The Eleventh International Conference on Learning Representations, 2023. [98] B. Poole, S. Ozair, A. Van Den Oord, A. Alemi, and G. Tucker. On variational bounds of mutual information. In International Conference on Machine Learning, pages 5171–5180. PMLR, 2019. [99] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 652–660, 2017. [100] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1278–1286, 2014. [101] G. Roeder, Y. Wu, and D. Duvenaud. Sticking the landing: An asymptotically zero-variance gradient estimator for variational inference. arXiv preprint arXiv:1703.09194, 2017. [102] M. Rolinek, D. Zietlow, and G. Martius. Variational Autoencoders pursue PCA directions (by accident). In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12406–12415, 2019. [103] M. Rosca, B. Lakshminarayanan, and S. Mohamed. Distribution matching in variational inference. arXiv preprint arXiv:1802.06847, 2018. [104] D. B. Rubin. Inference and missing data. Biometrika, 63(3):581–592, 1976. [105] A. Sannai, Y. Takai, and M. Cordonnier. Universal approximations of permutation invari- ant/equivariant functions by deep neural networks. arXiv preprint arXiv:1903.01939, 2019. [106] A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski, R. Pascanu, P. Battaglia, and T. Lilli- crap. A simple neural network module for relational reasoning. Advances in neural informa- tion processing systems, 30, 2017. [107] S. Schneider, J. H. Lee, and M. W. Mathis. Learnable latent embeddings for joint behavioural and neural analysis. Nature, pages 1–9, 2023. [108] N. Segol and Y. Lipman. On universal equivariant set networks. In International Conference on Learning Representations, 2019. [109] O. Sener and V. Koltun. Multi-task learning as multi-objective optimization. Advances in neural information processing systems, 31, 2018. [110] Y. Shi, B. Paige, P. Torr, et al. Variational Mixture-of-Experts Autoencoders for Multi-Modal Deep Generative Models. Advances in Neural Information Processing Systems, 32, 2019. [111] Y. Shi, B. Paige, P. Torr, and N. Siddharth. Relating by Contrasting: A Data-efficient Frame- work for Multimodal Generative Models. In International Conference on Learning Repre- sentations, 2020. 16[112] P. Sorrenson, C. Rother, and U. K¨othe. Disentanglement by nonlinear ICA with general incompressible-flow networks (GIN). arXiv preprint arXiv:2001.04872, 2020. [113] J. H. Stock and M. W. Watson. Forecasting using principal components from a large number of predictors. Journal of the American statistical association, 97(460):1167–1179, 2002. [114] T. Sutter, I. Daunhawer, and J. Vogt. Multimodal generative learning utilizing Jensen- Shannon-divergence. Advances in Neural Information Processing Systems, 33:6100–6110, 2020. [115] T. M. Sutter, I. Daunhawer, and J. E. Vogt. Generalized multimodal elbo. In 9th International Conference on Learning Representations (ICLR 2021), 2021. [116] M. Suzuki and Y. Matsuo. Mitigating the Limitations of Multimodal VAEs with Coordination- based Approach. 2022. [117] M. Suzuki, K. Nakayama, and Y. Matsuo. Joint multimodal learning with deep generative models. arXiv preprint arXiv:1611.01891, 2016. [118] Y. Tang and D. Ha. The sensory neuron as a transformer: Permutation-invariant neural net- works for reinforcement learning. Advances in Neural Information Processing Systems, 34: 22574–22587, 2021. [119] A. Tenenhaus and M. Tenenhaus. Regularized generalized Canonical Correlation Analysis. Psychometrika, 76:257–284, 2011. [120] Y. Tian, D. Krishnan, and P. Isola. Contrastive multiview coding. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XI 16, pages 776–794. Springer, 2020. [121] M. E. Tipping and C. M. Bishop. Probabilistic Principal Component Analysis. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):611–622, 1999. [122] M. Titsias and M. L´azaro-Gredilla. Doubly stochastic variational bayes for non-conjugate inference. In Proceedings of the 31st International Conference on Machine Learning (ICML- 14), pages 1971–1979, 2014. [123] Y.-H. H. Tsai, S. Bai, P. P. Liang, J. Z. Kolter, L.-P. Morency, and R. Salakhutdinov. Mul- timodal transformer for unaligned multimodal language sequences. In Proceedings of the conference. Association for Computational Linguistics. Meeting, volume 2019, page 6558. NIH Public Access, 2019. [124] A. Vahdat, K. Kreis, and J. Kautz. Score-based generative modeling in latent space. Advances in Neural Information Processing Systems, 34, 2021. [125] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [126] R. Vedantam, I. Fischer, J. Huang, and K. Murphy. Generative models of visually grounded imagination. In International Conference on Learning Representations, 2018. [127] G. Ver Steeg and A. Galstyan. Maximally informative hierarchical representations of high- dimensional data. In Artificial Intelligence and Statistics, pages 1004–1012. PMLR, 2015. [128] S. Virtanen, A. Klami, S. Khan, and S. Kaski. Bayesian group factor analysis. In Artificial Intelligence and Statistics, pages 1269–1277. PMLR, 2012. [129] E. Wagstaff, F. B. Fuchs, M. Engelcke, M. A. Osborne, and I. Posner. Universal approxima- tion of functions on sets. Journal of Machine Learning Research, 23(151):1–56, 2022. [130] Q. Wang, B. Li, T. Xiao, J. Zhu, C. Li, D. F. Wong, and L. S. Chao. Learning deep transformer models for machine translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1810–1822, 2019. 17[131] T. Wang and P. Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Machine Learning, pages 9929–9939. PMLR, 2020. [132] W. Wang, R. Arora, K. Livescu, and J. Bilmes. On deep multi-view representation learning. In International conference on machine learning, pages 1083–1092. PMLR, 2015. [133] W. Wang, X. Yan, H. Lee, and K. Livescu. Deep Variational Canonical Correlation Analysis. arXiv preprint arXiv:1610.03454, 2016. [134] W. Wang, D. Tran, and M. Feiszli. What makes training multi-modal classification networks hard? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog- nition, pages 12695–12705, 2020. [135] Y. Wang, D. Blei, and J. P. Cunningham. Posterior collapse and latent variable non- identifiability. Advances in Neural Information Processing Systems, 34:5443–5455, 2021. [136] S. Watanabe. Information theoretical analysis of multivariate correlation. IBM Journal of research and development, 4(1):66–82, 1960. [137] M. Wu and N. Goodman. Multimodal generative models for scalable weakly-supervised learning. Advances in Neural Information Processing Systems, 31, 2018. [138] M. Wu and N. Goodman. Multimodal generative models for compositional representation learning. arXiv preprint arXiv:1912.05075, 2019. [139] Q. Xi and B. Bloem-Reddy. Indeterminacy in latent variable models: Characterization and strong identifiability. arXiv preprint arXiv:2206.00801, 2022. [140] R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y. Lan, L. Wang, and T. Liu. On layer normalization in the transformer architecture. In International Conference on Machine Learning, pages 10524–10533. PMLR, 2020. [141] P. Xu, X. Zhu, and D. A. Clifton. Multimodal learning with transformers: A survey. arXiv preprint arXiv:2206.06488, 2022. [142] T. Yu, S. Kumar, A. Gupta, S. Levine, K. Hausman, and C. Finn. Gradient surgery for multi- task learning. Advances in Neural Information Processing Systems, 33:5824–5836, 2020. [143] C. Yun, S. Bhojanapalli, A. S. Rawat, S. Reddi, and S. Kumar. Are transformers universal approximators of sequence-to-sequence functions? In International Conference on Learning Representations, 2019. [144] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. R. Salakhutdinov, and A. J. Smola. Deep Sets. Advances in neural information processing systems, 30, 2017. [145] C. Zhang, Z. Han, H. Fu, J. T. Zhou, Q. Hu, et al. CPM-Nets: Cross partial multi-view networks. Advances in Neural Information Processing Systems, 32, 2019. [146] F. Zhang, B. Liu, K. Wang, V. Y. Tan, Z. Yang, and Z. Wang. Relational Reasoning via Set Transformers: Provable Efficiency and Applications to MARL. arXiv preprint arXiv:2209.09845, 2022. [147] L. Zhang, V. Tozzo, J. Higgins, and R. Ranganath. Set Norm and Equivariant Skip Connec- tions: Putting the Deep in Deep Sets. In International Conference on Machine Learning, pages 26559–26574. PMLR, 2022. [148] Y. Zhang, H. Jiang, Y. Miura, C. D. Manning, and C. P. Langlotz. Contrastive learning of medical visual representations from paired images and text. In Machine Learning for Healthcare Conference, pages 2–25. PMLR, 2022. [149] S. Zhao, C. Gao, S. Mukherjee, and B. E. Engelhardt. Bayesian group factor analysis with structured sparsity. The Journal of Machine Learning Research, 2016. 18[150] S. Zhao, J. Song, and S. Ermon. InfovVAE: Balancing Learning and Inference in Variational Autoencoders. In Proceedings of the aaai conference on artificial intelligence, volume 33, pages 5885–5892, 2019. [151] D. Zhou and X.-X. Wei. Learning identifiable and interpretable latent models of high- dimensional neural activity using pi-VAE. Advances in Neural Information Processing Sys- tems, 33:7234–7247, 2020. 19Contents A Multi-modal distribution matching 21 B Meta-learning and Neural processes 23 C Information-theoretic perspective 24 D Optimal variational distributions 25 E Permutation-invariant architectures 25 F Permutation-equivariance and private latent variables 27 G Multi-modal posterior in exponential family models 29 H Identifiability 29 I Missing modalities 31 J Mixture model extensions for different variational bounds 31 K Algorithm and STL-gradient estimators 32 L Evaluation of multi-modal generative models 32 M Linear models 33 N Non-linear identifiable models 35 N.1 Auxiliary labels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 N.2 Five continuous modalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 O MNIST-SVHN-Text 38 O.1 Training hyperparamters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 O.2 Multi-modal rates and distortions . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 O.3 Log-likelihood estimates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 O.4 Generated modalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 O.5 Conditional coherence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 P Encoder Model architectures 42 P.1 Linear models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 P.2 Linear models with private latent variables . . . . . . . . . . . . . . . . . . . . . . 42 P.3 Nonlinear model with auxiliary label . . . . . . . . . . . . . . . . . . . . . . . . . 42 P.4 Nonlinear model with five modalities . . . . . . . . . . . . . . . . . . . . . . . . . 42 P.5 MNIST-SVHN-Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 P.6 MNIST-SVHN-Text with private latent variables . . . . . . . . . . . . . . . . . . 42 20Q MNIST-SVHN-Text Decoder Model architectures 42 R Compute resources and existing assets 43 A Multi-modal distribution matching Proof of Proposition 2. Our proof extends the arguments in [138]. Observe first that for any S ⊂ M, the encoding distribution is marginally consistent in the sense that it holds that � X\S qϕ(z, x)dx\S = � X\S pd(xS)qϕ(x\S|z, xS)qϕ(z|xS)dx\S = pd(xS)qϕ(z|xS). Consequently, KL(qϕ(z, x)|pθ(z, x)) = � X×Z log pd(xS)pd(x\S|xS)qϕ(z|x)qϕ(z|xS) pθ(z)pθ(xS|z)pθ(x\S|z)qϕ(z|xS) pd(x)qϕ(z|x)dxdz = � XS×Z log pd(xS)qϕ(z|xS) pθ(z)pθ(xS|z) � X\S � qϕ(z, x)dx\S � dxSdz + � X×Z pd(x)qϕ(z|x) log pd(x\S|xS)qϕ(z|x) pθ(x\S|z)qϕ(z|xS) dxdz = � XS×Z pd(xS)qϕ(z|xS) log qϕ(z|xS) pθ(z)pθ(xS|z)dxSdz − H(pd(xS)) + � X×Z pd(x)qϕ(z|x) log qϕ(z|x) pθ(x\S|z)qϕ(z|xS)dxdz − H(pd(x\S|xS)). The claim follows by the chain rule for the entropy. Following the same arguments as for uni-modal VAEs, this establishes a lower bound on the log- likelihood. Corollary 10 (Tight lower bound on multi-modal log-likelihood). For any modality mask S, we have � pd(x) � LS(xS, θ, ϕ, 1) + L\S(x, θ, ϕ, 1) � dx = � pd(x) [log pθ(x) − KL(qϕ(z|x)|pθ(z|x))] dx. Proof. Recall that qϕ(z, x) = pd(x)qϕ(z|x). Proposition 12 implies then that � pd(x) � LS(xS, θ, ϕ, 1) + L\S(x, θ, ϕ, 1) � dx = − KL(qϕ(z, x)|pθ(z, x)) − H(pd(x)) = − � pd(x) � qϕ(z|x) log qϕ(z|x) − log pθ(z, x))dzdx = � pd(x) log pθ(x)dx − � qϕ(z|x) (log pθ(z|x) − log qϕ(z|x)) dzdx. Remark 11. Corollary 10 shows that the variational bound becomes tight if the encoding distri- bution closely approximates the true posterior distribution. A similar result does not hold for the mixture-based multi-modal bound. Indeed, as shown in [22], there is a gap between the variational bound and the log-likelihood given by the conditional entropies that cannot be reduced even for flexible encoding distributions. Moreover, our bound can be tight for an arbitrary number of modal- ities. In contrast, [22] show that for mixture-based bounds, this variational gap increases with each additional modality, if the new modality is ’sufficiently diverse’. 21Proposition 12 (Marginal and conditional distribution matching). For any S ∈ P(M), we have � pd(xS)LS(xS, θ, ϕ)dxS + H(pd(xS)) = − KL(qϕ(z, xS)|pθ(z, xS)) (ZXmarginal) = − KL(pd(xS)|pθ(xS)) − � pd(xS)KL(qϕ(z|xS)|pθ(z|xS))dxS (Xmarginal) = − KL(qagg ϕ,S(z)|pθ(z)) − � qagg ϕ,S(z)KL(q⋆(xS|z)|pθ(xS|z))dxS, (Zmarginal) where qagg ϕ,S(z) = � pd(xS)qϕ(z|xS)dxS is the aggregated prior [87] restricted on modalities from S and q⋆(xS|z) = qϕ(xS, z)/qagg ϕ (z). Moreover, for fixed xS, � pd(x\S|xS)L\S(x, θ, ϕ)dx\S + H(pd(x\S|xS)) = − KL � qϕ(z|x)pd(x\S|xS) ��pθ(x\S|z)qϕ(z|xS) � (ZXconditional) = − KL(pd(x\S|xS)|pθ(x\S|xS)) (Xconditional) − � pd(x\S|xS) � KL(qϕ(z|x)|pθ(z|x)) + � qϕ(z|x) log qϕ(z|xS) pθ(z|xS)dz � dx\S = − KL(qagg ϕ,\S(z|xS)|qϕ(z|xS)) − � qagg ϕ,\S(z|xS) � KL(q⋆(x\S|z, xS)|pθ(xS|z)) � dz, (Zconditional) where qagg ϕ,\S(z|xS) = � pd(x\S|xS)qϕ(z|x)dx\S can be seen as an aggregated encoder conditioned on xS and q⋆(x\S|z, xS) = qϕ(z, x\S|xS)/qagg ϕ,\S(z|xS) = pd(x\S|xS)qϕ(z|x)/qagg ϕ,\S(z|xS). Proof of Proposition 12. The equations for LS(xS) are well known for uni-modal VAEs, see for ex- ample [150]. To derive similar representations for the conditional bound, note that the first equation (ZXconditional) for matching the joint distribution of the latent and the missing modalities conditional on a modality subset follows from the definition of L\S, � pd(x\S|xS)L\S(x, θ, ϕ)dx\S = � pd(x\S|xS) � qϕ(z|x) � log pθ(x\S|z) − log qϕ(z|x) + log qϕ(z|xS)) � dzdx\S = � pd(x\S|xS) log pd(x\S|xS)dx\S + � pd(x\S|xS) � qϕ(z|x) � log pθ(x\S|z)qϕ(z|xS)) qϕ(z|x)pd(x\S|xS) � dzdx\S = − H(pd(x\S|xS)) − KL � qϕ(z|x)pd(x\S|xS) ��pθ(x\S|z)qϕ(z|xS) � . To obtain the second representation (Xconditional) for matching the conditional distributions in the data space, observe that pθ(x\S|xS, z) = pθ(x\S|z) and consequently, − � pd(x\S|xS)L\S(x, θ, ϕ)dx\S − H(pd(x\S|xS)) = � pd(x\S|xS)qϕ(z|x) log pd(x\S|xS)qϕ(z|x) pθ(x\S|z)qϕ(z|xS) dzdx\S = � pd(x\S|xS)qϕ(z|x) log pd(x\S|xS)qϕ(z|x)pθ(z|xS) pθ(x\S|z)pθ(z|xS)qϕ(z|xS) dzdx\S = � pd(x\S|xS)qϕ(z|x) log pd(x\S|xS)qϕ(z|x)pθ(z|xS) pθ(x\S|z, xS)pθ(z|xS)qϕ(z|xS)dzdx\S = � pd(x\S|xS)qϕ(z|x) log pd(x\S|xS)qϕ(z|x)pθ(z|xS) pθ(x\S|xS)pθ(z|xS, x\S)qϕ(z|xS)dzdx\S =KL(pd(x\S|xS)|pθ(x\S|xS)) + � pd(x\S|xS) � qϕ(z|x) � log qϕ(z|x) pθ(z|x) + log pθ(z|xS) qϕ(z|xS) � dzdx\S. 22Lastly, the representation (Zconditional) for matching the distributions in the latent space given a modal- ity subset follows by recalling that pd(x\S|xS)qϕ(z|x) = qagg ϕ,\S(z|xS)q⋆(x\S|z, xS) and consequently, − � pd(x\S|xS)L\S(x, θ, ϕ)dx\S − H(pd(x\S|xS)) = � pd(x\S|xS)qϕ(z|x) log pd(x\S|xS)qϕ(z|x) pθ(x\S|z)qϕ(z|xS) dzdx\S = � qagg ϕ,\S(z|xS)q⋆(x\S|z, xS) log qagg ϕ,\S(z|xS)q⋆(x\S|z, xS) pθ(x\S|z)qϕ(z|xS) dzdx\S =KL(qagg ϕ,\S(z|xS)|qϕ(z|xS)) − � qagg ϕ,\S(z|xS) � KL(q⋆(x\S|z, xS)|pθ(xS|z)) � dz. Remark 13 (Prior-hole problem and Bayes or conditional consistency). In the uni-modal setting, the mismatch between the prior and the aggregated prior can be large and can lead to poor uncondi- tional generative performance, because this would lead to high-probability regions under the prior that have not been trained due to their small mass under the aggregated prior [48, 103]. Equation (Zmarginal) extents this to the multi-modal case and we expect that unconditional generation can be poor if this mismatch is large. Moreover, (Zconditional) extends this conditioned on some modality subset and we expect that cross-generation for x\S conditional on xS can be poor if the mismatch between qagg ϕ,\S(z|xS) and qϕ(z|xS) is large for xS ∼ pd, because high-probability regions under qϕ(z|xS) will not have been trained - via optimizing L\S(x) - to model x\S conditional on xS, due to their small mass under qagg ϕ,\S(z|xS). The mismatch will vanish when the encoders are consistent and correspond to a single Bayesian model where they approximate the true posterior distributions. B Meta-learning and Neural processes Meta-learning. We consider a standard meta-learning setup but use slightly non-standard nota- tions to remain consistent with notations used in other parts of this work. We consider a compact input or covariate space A and output space X. Let D = ∪∞ M=1(A × X)M be the collection of all input-output pairs. In meta-learning, we are given a meta-dataset, i.e., a collection of elements from D. Each individual data set D = (a, x) = Dc ∪ Dt ∈ D is called a task and split into a context set Dc = (ac, xc), and target set Dt = (at, xt). We aim to predict the target set from the context set. Consider, therefore, the prediction map π: Dc = (ac, xc) �→ p(xt|at, Dc) = p(xt, xc|at, ac)/p(xc|ac), mapping each context data set to the predictive stochastic process conditioned on Dc. Variational lower bounds for Neural processes. Latent Neural processes [34, 31] approximate this prediction map by using a latent variable model with parameters θ in the form of z ∼ pθ, pθ(xt|at, z) = � (a,x)∈Dt pϵ(x − fθ(a, z)) for a prior pθ, decoder fθ and a parameter free density pϵ. The model is then trained by (approxi- mately) maximizing a lower bound on log pθ(xt|at, ac, xc). Note that for an encoding density qϕ, we have that log pθ(xt|at, ac, xx) = � qϕ(z|x, a) log pθ(xt|at, z)dz − KL(qϕ(z|a, x)|pθ(z|ac, xc)). 23Since the posterior distribution pθ(z|ac, xc) is generally intractable, one instead replaces it with a variational approximation or learned conditional prior qϕ(z|ac, xc), and optimizes the following objective LLNP \C (x, a) = � qϕ(z|x, a) log pθ(xt|at, z)dz − KL(qϕ(z|a, x)|qϕ(z|ac, xc)). Note that this objective coincides with L\C conditioned on the covariate values a and where C comprises the indices of the data points that are part of the context set. Using this variational lower bound can yield subpar performance compared to other biased log- likelihood objectives [64, 31], possibly because the variational approximation qϕ(z|ac, xc) needs not to be close the posterior distribution pθ(z|ac, xc). It would therefore be interesting to analyze in future work if one can alleviate such issues if one optimizes additionally the variational objective corresponding to LC, i.e., LLNP C (xc, ac) = � qϕ(z|xc, ac) log pθ(xc|ac, z)dz − KL(qϕ(z|ac, xc)|pθ(z)), as we do in this work for multi-modal generative models. Note that the objective LLNP C alone can be seen as a form of a neural statistician model [27] where C coincides with the indices of the target set, while a form of the mixture-based bound corresponds to a neural process bound similar to variational homoencoders [45], see also the discussion in [73]. C Information-theoretic perspective We recall first that the mutual information on the inference path4 is given by Iqϕ(XS, ZS) = � qϕ(xS, z) log qϕ(xS, z) pd(xS)qagg ϕ,S(z)dzdxS, where qagg ϕ,S(z) = � pd(xS)qϕ(z|xS)dxS is the aggregated prior [87]. It can be bounded by standard [11, 4, 3] lower and upper bounds using the rate and distortion: HS − DS ≤ HS − DS + ∆1 = Iqϕ(XS, ZS) = RS − ∆2 ≤ RS, with ∆1 = � qagg ϕ (z)KL(q⋆(xS|z)|pθ(xS|z))dz > 0, ∆2 = KL(qagg ϕ,S(z)|pθ(z)) > 0 and q⋆(xS|z) = qϕ(xS, z)/qagg ϕ (z). Moreover, if the bounds in (6) become tight with ∆1 = ∆2 = 0 in the hypothetical scenario of infinite-capacity decoders and encoders, one obtains � pdLS = (1 − β) Iqϕ(XS, ZS) + HS. For β > 1, maximizing LS yields an auto-decoding limit that minimizes Iqϕ(xS, z) for which the latent representations do not encode any information about the data, whilst β < 1 yields an auto-encoding limit that maximizes Iqϕ(XS, Z) and for which the data is perfectly encoded and decoded. To arrive at a similar interpretation for the conditional bound L\S, recall that we have de- fined R\S = � pd(x)KL(qϕ(z|x)|qϕ(z|xS)dx for a conditional or cross rate term and D\S = − � pd(x)qϕ(z|x) log pθ(x\S|z)dzdx for the distortion term. Bounds on the conditional mutual information Iqϕ(X\S, ZM|XS) = � pd(xS)KL(pd(x\S, z|xS))|pd(x\S|xS)qagg ϕ,\S(z|xS))dxS with qagg ϕ,\S(z|xS) = � pd(x\S|xS)qϕ(z|x)dx\S can be established as follows. Proof of Lemma 3. The proof follows by adapting the arguments in [3]. The law of X\S and Z conditional on XS on the encoder path can be written as qϕ(z, x\S|xS) = pd(x\S|xS)qϕ(z|x) = qagg ϕ,\S(z|xS)q⋆(x\S|z, xS) 4We include the conditioning modalities as an index for the latent variable Z. 24with q⋆(x\S|z, xS) = qϕ(z, x\S|xS)/qagg ϕ,\S(z|xS). To prove a lower bound on the conditional mutual information, note that Iqϕ(X\S, ZM|XS) = � pd(xS) � qagg ϕ,\S(z|xS) � q⋆(x\S|z, xS) log qagg ϕ,\S(z|xS)q⋆(x\S|z, xS) qagg ϕ,\S(z|xS)pd(x\S|x\S) dzdx\SdxS = � pd(xS) � qagg ϕ,\S(z|xS) � q⋆(x\S|z, xS) log pθ(x\S|z)) + KL(q⋆(x\S|z, xS)|pθ(x\S|z)) � dzdxS − � pd(xS) � pd(x\S|xS) log pd(x\S|xS)dx = � pd(x) � qϕ(z|x) log pθ(x\S|z)dzdx − � pd(xS) � pd(x\S|xS) log pd(x\S|xS)dx � �� � =−H\S=−H(X\S|XS) + � pd(xS) � qagg ϕ,\S(z|xS)KL(q⋆(x\S|z, xS)|pθ(x\S|z))dxS � �� � =∆\S,1≥0 =∆\S,1 + D\S + H\S. The upper bound follows by observing that Iqϕ(X\S, ZM|XS) = � pd(xS) � pd(x\S|x\) log qϕ(z|x)pd(x\S|xS) qagg ϕ,\S(z|xS)pd(x\S|xS)dzdx = � pd(x)KL(qϕ(z|x)|qϕ(z|xS))dx − � pd(xS)KL(qagg ϕ,\S(z|xS)|qϕ(z|xS))dxS � �� � =∆\S,2≥0 =R\S − ∆\S,2. D Optimal variational distributions The optimal variational density for the mixture-based (1) multi-modal objective, � pd(dx)LMix S (x) = � pd(xS) � qϕ(z|xS) � pd(x\S|xS) � log pθ(xS|z) + log pθ(x\S|z) − β log pθ(z) − β log qϕ(z|xS) � dx\SdzdxS is attained at q⋆(z|xS) ∝ exp � 1 β � pd(x\S|xS) � log pθ(xS|z) + log pθ(x\S|z) − β log pθ(z) � dx\S � ∝ ˜pβ,θ(z|xS) exp �� pd(x\S|xS) log ˜pβ,θ(x\S|z)dx\S � . E Permutation-invariant architectures Multi-head attention and masking. We introduce here a standard multi-head attention [10, 125] mapping MHAϑ : RI×DX × RS×DY → RI×DY given by MHAϑ(X, Y ) = W O � Head1(X, Y, Y ), . . . , HeadH(X, Y, Y ) � , ϑ = (WQ, WK, WV , WO), with output matrix WO ∈ RDA×DY , projection matrices WQ, WK, WV ∈ RDY ×DA and Headh(Q, K, V ) = Att(QW h Q, KW h K, V W h V ) ∈ RI×D (8) 25where we assume that D = DA/H ∈ N is the head size. Here, the dot-product attention function is Att(Q, K, V ) = σ(QK⊤)V, where σ is the softmax function applied to each column of Q and K⊤, respectively. Masked multi-head attention. In practice, it is convenient to consider masked multi-head atten- tion models MMHAϑ,M : RI×DX × RT ×DY → RI×DY for mask matrix M ∈ {0, 1}I×T that operate on key or value sequences of fixed length T where the h-th head (8) is given by Headh(Q, K, V ) = � M ⊙ σ(QW h Q(KW h K)⊤) � Vt′W h V ∈ RT ×D. Using the softmax kernel function SMD(q, k) = exp(q⊤k/ √ D), we set MMHAϑ,M(X, Y )i = T � t=1 H � h=1 MitSMD(W Q h Xi, W K h Yt) �T t′=1 Mit′SMD(XiW Q h , Yt′W K h ) YtW V h W O h (9) which does not depend on Yt if M·t = 0. Masked self-attention. For mask matrix M = mm⊤ with m = (1{s∈S})s∈M, we write MHAϑ(YS, YS) = MMHAϑ,M(i(YS), i(YS))S. where MMHAϑ,M operates on sequences with fixed length and i(YS))t = Yt if t ∈ S and 0 other- wise. LayerNorm and SetNorm. Let h ∈ RT ×D and consider the normalisation N(h) = h − µ(h) σ(h) ⊙ γ + β where µ and σ standardise the input h by computing the mean, and the variance, respectively, over some axis of h, whilst γ and β define a transformation. LayerNorm [8] standardises inputs over last axis, e.g., µ(h) = 1 D �D d=1 µ·,d, i.e., separately for each element. In contrast, SetNorm [147] stan- dardises inputs over both axes, e.g., µ(h) = 1 T D �T t=1 �D d=1 µt,d, thereby losing the global mean and variance only. In both cases, γ and β share their values across the first axis. Both normalisations are permutation-equivariant. Transformer. We consider a masked pre-layer-norm [130, 140] multi-head transformer block (MMTBϑ,M(iS(YS)))S = (Z + σReLU(LN(Z)))S with σReLU being a ReLU non-linearity and Z = iS(YS) + MMHAϑ,M(LN(iS(YS)), LN(iS(YS))) where M = mm⊤ for m = (1{s∈S})s∈M. Set-Attention Encoders. Set g0 = iS(χϑ(hS)) and for k ∈ {1, . . . , L}, let gk = MMTBϑ,M(gk−1 S ). Then, we can express the self-attention multi-modal aggregation mapping via fϑ(hS) = ρϑ �� s∈S gL s � . Remark 14 (Mixture-of-Product-of-Experts or MoPoEs). [115] introduced a MoPoE aggregation scheme that extends MoE or PoE schemes by considering a mixture distribution of all 2M modality subsets, where each mixture component consists of a PoE model, i.e., qMoPoE ϕ (z|xM) = 1 2M � xS∈P(xM) qPoE ϕ (z|xS). This can also be seen as another permutation-invariant model. While it does not require learning sep- arate encoding models for all modality subsets, it however becomes computationally expensive to evaluation for large M. Our mixture models using components with a SumPooling or SelfAttention aggregation can be seen as an alternative that allows one to choose the number of mixture compo- nents K to be smaller than 2M, with non-uniform weights, while the individual mixture components are not constrained to have a PoE form. 26Remark 15 (Multi-modal time series models). We have introduced our generative model in a gen- eral form that also applies to the time-series setup, such as when a latent Markov process drives multiple time series. For example, consider a latent Markov process Z = (Zt)t∈N with prior dy- namics pθ(z1, . . . , zT ) = pθ(z1) �T t=2 pθ(zt|zt−1) for an initial density pθ(z1) and homogeneous Markov kernels pθ(zt|zt−1). Conditional on Z, suppose that the time-series (Xs,t)t∈N follows the dynamics pθ(xs,1, . . . , xs,T |z1, . . . , zT ) = �T t=2 pθ(xs,t|zt) for decoding densities pθ(xs,t|zt). A common choice [20] for modeling the encoding distribution for such sequential VAEs is to assume the factorisation qϕ(z1, . . . zT |x1, . . . xT ) = qϕ(z1|x1) �T t=2 qϕ(zt|zt−1, xt) for xt = (xs,t)s∈M, with initial encoding densities qϕ(z1|x1) and encoding Markov kernels qϕ(zt|zt−1, xt). One can again consider modality-specific encodings hs = (hs,1, . . . , hs,T ), hs,t = hs,φ(xs,t), now applied separately at each time step that are then used to construct Markov kernels that are permutation- invariant in the form of q′ ϕ(zt|zt−1, πhφ(xt,S)) = q′ ϕ(zt|zt−1, hφ(xt,S)) for permutations π ∈ SS. Alternatively, in absence of the auto-regressive encoding structure with Markov kernels, one could also use transformer models that use absolute or relative positional embeddings across the last tem- poral axis, but no positional embeddings across the first modality axis, followed by a sum-pooling operation across the modality axis. Note that previous works using multi-modal time series such as [71] use a non-amortized encoding distribution for the full multi-modal posterior only. A numerical evaluation of permutation-invariant schemes for time series models is however outside the scope of this work. F Permutation-equivariance and private latent variables In principle, the general permutation invariant aggregation schemes that have been introduced could also be used for learning multi-modal models with private latent variables. For example, suppose that the generative model factorises as pθ(z, x) = p(z) � s∈M pθ(xs|z′, ˜zs) (10) for z = (z′, ˜z1, . . . , ˜zM) ∈ Z, for shared latent variables Z′ and private latent variable ˜Zs for each s ∈ M. Note that for s ̸= t ∈ [M], Xs ⊥⊥ ˜Zt | Z′, ˜Zs. (11) Consequently, pθ(z′, ˜zS, ˜z\S|xS) = pθ(z′, ˜zS, |xS)pθ(˜z\S|z′, ˜zS, xS) = pθ(z′, ˜zS, |xS)pθ(˜z\S|z′, ˜zS). (12) An encoding distribution qϕ(z|xS) that approximates pθ(z|xS) should thus be unaffected by the inputs xS when encoding ˜zs for s /∈ S, provided that, a priori, all private and shared latent variables are independent. Observe that for fϑ with the representation fϑ(hS) = ρϑ �� s∈S gϑ(hS)s � , where ρϑ has aggregated inputs y, the gradients of its i-th dimension with respect to the modality values xs is ∂ ∂xs [fϑ(hS(xS))i] = ∂ρϑ,i ∂y �� s∈S gϑ(hS(xS)) � ∂ ∂xs �� t∈S gϑ(hS(xS))t � . In the case of a SumPooling aggregation, the gradient simplifies to ∂ρϑ,i ∂y �� s∈S χϑ(hS(xS)) � ∂χϑ ∂h (hs(xs)) ∂hs(xs) ∂xs . Notice that only the first factor depends on i so that ρϑ,i has to be constant around y = � s∈S χϑ(hS(xS)) if some other components have a non-zero gradient with respect to xs. However, the specific generative model also lends itself to an alternative parameterisation. The assumption of private latent variables suggests an additional permutation-equivariance into the en- coding distribution that approximates the posterior in (12), in the sense that for any permutation π ∈ SS, it holds that q′ ϕ(˜zS|π · hφ(xS), z′) = q′ ϕ(π · ˜zS|hφ(xS), z′), 27assuming that all private latent variables are of the same dimension D.5 Indeed, suppose we have modality-specific feature functions hφ,s such that {Hs = hφ,s(Xs)}s∈S is exchangeable. Clearly, (11) implies for any s ̸= t that hφ,s(Xs) ⊥⊥ ˜Zt | Z′, ˜Zs. The results from [15] then imply, for fixed |S|, the existence of a function f ⋆ such that for all s ∈ S, almost surely, (HS, ˜Zs) = (HS, f ⋆(Ξs, Z′, Hs, MHS)), where Ξs ∼ U[0, 1] iid and Ξs ⊥⊥ HS. (13) This fact suggests an alternative route to approximate the posterior distribution in (12): First, pθ(˜z\S|z′, ˜zS) can often be computed analytically based on the learned or fixed prior distribu- tion. Second, a permutation-invariant scheme can be used to approximate pθ(z′|xS). Finally, a permutation-equivariant scheme can be employed to approximate pθ(˜zS|xS, z′) with a reparame- terisation in the form of (13). Three examples of such permutation-equivariant schemes are given below with pseudocode for optimising the variational bound given in Algorithm 2. Example 16 (Permutation-equivariant PoE). Similar to previous work [133, 76, 114], we consider an encoding density of the form qϕ(z′, ˜zM|xS) = qPoE φ (z′|xS) � s∈S qN (˜zs|˜µs,φ(xs), ˜Σs,φ(xs)) � s∈M\S pθ(˜zs), where qPoE φ (z′|xS) = 1 Z pθ(z′) � s∈S qN (z′|µ′ s,φ(xs), Σ′ s,φ(xs)) is a (permutation-invariant) PoE aggregation, and we assumed that the prior density factorises over the shared and different private variables. For each modality s, we encode different features h′ s,φ = (µ′ s,φ, Σ′ s,φ) and ˜hs,φ = (˜µs,φ, ˜Σs,φ) for the shared, respectively, private, latent variables. Example 17 (Permutation-equivariant Sum-Pooling). We consider an encoding density that writes as qϕ(z′, ˜zM|xS) = qSumP ϕ (z′|xS)qEquiv-SumP ϕ (˜zS|z′, xS) � s∈M\S pθ(˜zs|z′). Here, we use a (permutation-invariant) Sum-Pooling aggregation scheme for constructing the shared latent variable Z′ = µ′(hS)+σ′(hS)⊙Ξ′ ∼ qSumP ϕ (z′|xS), where Ξ′ ∼ p and fϑ : R|S|×DE → RD given as in Exanoke (6) with [µ′(h), log σ′(h)] = fϑ(h). To sample ˜ZS ∼ qEquiv-SumP ϕ (˜zS|z′, xS), consider functions χj,ϑ : RDE → RDP , j ∈ [3], and ρϑ : RDP → RDO, e.g., fully-connected neural networks. We define f Equiv-SumP ϑ : Z × R|S|×DE → R|S|×DO via f Equiv-SumP ϑ (z′, hS)s = ρϑ ��� t∈S χ0,ϑ(ht) � + χ1,ϑ(z′) + χ2,ϑ(hs) � . With � ˜µ(hS)⊤, log ˜σ(hS)⊤�⊤ = f Equiv-SumP ϑ (z′, hS), we then set ˜Zs = ˜µ(hS)s + ˜σ(hS)s ⊙ ˜Ξs for ˜Ξs ∼ p iid, hs = hφ,s(xs) for modality-specific feature functions hφ,s : Xs → RDE. Example 18 (Permutation-equivariant Self-Attention). Similar to a Sum-Pooling approach, we con- sider an encoding density that writes as qϕ(z′, ˜zM|xS) = q SA ϕ (z′|xS)qEquiv-SA ϕ (˜zS|z′, xS) � s∈M\S pθ(˜zs|z′). Here, the shared latent variable Z′ is sampled via the permutation-invariant aggregation above by summing the elements of a permutation-equivariant transformer model of depth L′. For encoding the private latent variables, we follow the example above but set � ˜µ(hS)⊤, log ˜σ(hS)⊤�⊤ = f Equiv-SA ϑ (z′, hS)s = gL S, with gk S = MTBϑ(gk−1 S ) an g0 = (χ1,ϑ(hs) + χ2,ϑ(z′))s∈S. 5The effective dimension can vary across modalities in practice if the decoders are set to mask redundant latent dimensions. 28Remark 19 (Cross-modal context variables). In contrast to the PoE model, where the private encod- ings are independent, the private encodings are dependent in the Sum-Pooling model by conditioning on a sample from the shared latent space. The shared latent variable Z′ can be seen as a shared cross- modal context variable, and similar probabilistic constructions to encode such context variables via permutation-invariant models have been suggested in few-shot learning algorithms [27, 38] or neural process models [34, 33, 64]. Remark 20 (Variational bounds with private latent variables). To compute the multi-modal varia- tional bounds, notice that the required KL-divergences can be written as follows: KL(qϕ(z′, ˜z|xS)|pθ(z′, ˜z)) = KL(qϕ(z′|xS)|pθ(z′))+ � qϕ(z′|xS)KL(qϕ(˜zS|z′, xS)|pθ(˜zS|z′))dz′ and KL(qϕ(z′, ˜z|xM)|qϕ(z′, ˜z|xS)) =KL(qϕ(z′|xM)|(qϕ(z′|xS)) + � qϕ(z′|xM)KL(qϕ(PS ˜z|z′, xM)|qϕ(PS ˜z|z′, xS))dz′ + � qϕ(z′|xM)KL(qϕ(P\S ˜z|z′, xS)|pθ(P\S ˜z|z′))dz′ where PS : (˜z1, . . . ˜zM) �→ (˜zs)s∈S projects all private latent variables to those contained in S. G Multi-modal posterior in exponential family models Consider the setting where the decoding and encoding distributions are of the exponential family form, that is pθ(xs|z) = µs(xs) exp [⟨Ts(xs), fs,θ(z)⟩ − log Zs(fs,θ(z))] for all s ∈ M, while for all S ⊂ M, qϕ(z|xS) = µ(z) exp [⟨V (z), λϕ,S(xS)⟩ − log ΓS(λϕ,S(xS))] where µs and µ are base measures, Ts(xs) and V (z) are sufficient statistics, while the natural pa- rameters λϕ,S(xS) and fs,θ(z) are parameterised by the decoder or encoder networks, respectively, with Zs and ΓS being normalising functions. Note that we made a standard assumption that the multi-modal encoding distribution has a fixed base measure and sufficient statistics for any modality subset. For fixed generative parameters θ, we want to learn a multi-modal encoding distribution that minimises, see Remark 5, over xS ∼ pd, KL(qϕ(z|xS)|pθ(z|xS)) = � qϕ(z|xS) � log qϕ(z|xS) − log pθ(z) − � s∈S log pθ(xs|z) � dz − log pθ(xS) = � qϕ(z|xS) � ⟨V (z), λϕ,S(xS)⟩ − log ΓS(λϕ,S(xS)) − � s∈S log µs(xs) − � � s∈S ⟨Ts,θ(xs), fs,θ(z)⟩ + log pθ(z) − � s∈S Zs(fs,θ(z)) �� dz − log pθ(xS) = � qϕ,ϑ(z|xS) �� � V (z) 1 � , � λϕ,ϑ,S(xS) − log ΓS(λϕ,ϑ,S(xS)) � � − � s∈S � � Ts(xs) 1 � , � fθ,s(z) bθ,s(z) � �� dz, with bθ,s(z) = 1 |S|pθ(z) − log Zs(fs,θ(z)). H Identifiability We are interested in identifiability, conditional on having observed some non-empty modality subset S ⊂ M. For illustration, we translate an identifiability result from the uni-modal iVAE setting in [82], which does not require the conditional independence assumption from [62]. We assume 29that the encoding distribution qϕ(z|xS) approximates the true posterior pθ(z|xS) and belongs to a strongly exponential family, i.e., pθ(z|xS) = qϕ(z|xS) = pEF Vϕ,S,λϕ,S(z|xS), (14) with pEF VS,λS(z|xS) = µ(z) exp [⟨VS(z), λ(xS)⟩ − log ΓS(λS(xS))] , where µ is a base measure, VS : Z → Rk is the sufficient statistics, λS(xS) ∈ Rk the natural param- eters and ΓS a normalising term. Furthermore, one can only reduce the exponential component to the base measure on sets having measure zero. In this section, we assume that pθ(xs|z) = ps,ϵ(xs − fθ,s(z)) (15) for some fixed noise distribution ps,ϵ with a Lebesgue density, which excludes observation models for discrete modalities. Let ΘS be the domain of the parameters θS = (f\S, VS, λS) with f\S : Z ∋ z �→ (fs(z))s∈M\S ∈ ×s∈M\SXs = X\S. Assuming (14), note that pθS(x\S|xS) = � pVS,λS(z|xS)p\S,ϵ(x\S − f\S(z))dz, with p\S,ϵ = ⊗s∈M\Sps,ϵ. We define an equivalence relation on ΘS by (f\S, VS, λS) ∼AS ( ˜f\S, ˜VS, ˜λS) iff there exist invertible AS ∈ Rk×k and cS ∈ Rk such that VS(f −1 \S (x\S)) = AS ˜VS( ˜f −1 \S (x\S)) + cS for all x\S ∈ X\S. Proposition 21 (Weak identifiability). Consider the data generation mechanism pθ(z, x) = pθ(z) � s∈M pθ(xs|z) where the observation model satisfies (15) for an injective f\S. Sup- pose further that pθ(z|xS) is strongly exponential and (14) holds. Assume that the set {x\S ∈ X\S|φ\S,ϵ(x\S) = 0} has measure zero, where φ\S,ϵ is the characteristic function of the density p\S,ϵ. Furthermore, suppose that there exist k + 1 points x0 S, . . . , xk S ∈ XS such that L = � λS(x1 S) − λS(x0 S), . . . , λS(xk S) − λS(x0 S) � ∈ Rk×k is invertible. Then pθS(x\S|xS) = p˜θS(x\S|xS) for all x ∈ X implies θ ∼AS ˜θ. This result follows from Theorem 4 in [82]. Note that pθS(x\S|xS) = p˜θS(x\S|xS) for all x ∈ X implies with the regularity assumption on φ\S,ϵ that the transformed variables Z = f −1 \S (X\S) and ˜Z = ˜f −1 \S (X\S) have the same density function conditional on XS. Remark 22. The joint decoder function f\S can be injective, even if the individual modality-specific decoder functions are not, suggesting that the identifiability of latent variables can be improved when training a multi-modal model compared to separate uni-modal models. Remark 23. The identifiability result above is about conditional models and does not contradict the un-identifiability of VAEs: When S = ∅ and we view x = xM as one modality, then the parameters of pθ∅(x) characterised by the parameters V∅ and λ∅ of the prior pθ∅(z|x∅) and the encoders fM will not be identifiable as the invertibility condition will not be satisfied. Remark 24. Note that the identifiablity concerns parameters of the multi-modal posterior distri- bution. We believe that our inference approach is beneficial for this type of identifiability because (a) unlike some other variational bounds, the posterior is the optimal variational distribution with L\S(x) being a lower bound on log pθ(x\S|xS) for flexible encoders, and (b) the trainable aggrega- tion schemes can be more flexible for approximating the optimal encoding distribution. Remark 25. For models with private latent variables, we might not expect that conditioning on XS helps to identify ˜Z\S as pθ(z′, ˜zS, ˜z\S|xS) = pθ(z′, ˜zS|xS)pθ(˜z\S|z′, ˜z\S). Indeed, Proposition 21 will not apply in such models as f\S will not be injective. 30I Missing modalities In practical applications, modalities can be missing for different data points. We describe this missingness pattern by missingness mask variables ms ∈ {0, 1} where ms = 1 indicates that observe modality s, while ms = 0 means it is missing. The joint generative model that ex- tends (16) will be of the form pθ(z, x, m) = pθ(z) � s∈M pθ(xs|z)pθ(m|x) for some distribu- tion pθ(m|x) over the mask variables m = (ms)s∈M. For S ⊂ M, we denote by xo S = {xs : ms = 1, s ∈ S} and xm S = {xs : ms = 0, s ∈ S} the set of observed, respectively missing, modalities. The full likelihood of the observed and missingness masks becomes then pθ(xo S, m) = � pθ(z) � s∈S pθ(xs|z)pθ(m|x)dxm s dz. If pθ(m|x) does not depend on the obser- vations, that is, observations are missing completely at random [104], then the missingness mech- anisms pθ(m|x) for inference approaches maximizing pθ(xo, m) can be ignored. Consequently, one can instead concentrate on maximizing log pθ(xo) only, based on the joint generative model pθ(z, xo) = pθ(z) � {s∈M: ms=1} pθ(xs|z). In particular, one can employ the variational bounds above by considering only the observed modalities. Since masking operations are readily supported for the considered permutation-invariant models, appropriate imputation strategies [95, 86] for the encoded features of the missing modalities are not necessarily required. Settings allowing for not (completely) at random missingness have been considered in the uni-modal case, for instance, in [55, 37, 39], and we leave multi-modal extensions thereof for future work. J Mixture model extensions for different variational bounds We consider the optimization of an augmented variational bound L(x, θ, ϕ) = � ρ(S) � � qϕ(c, z|xS) [log pθ(c, xS|z)] dzdc − KL(qϕ(c, z|xS)|pθ(c, z)) + � qϕ(c, z|xS) � log pθ(x\S|z) � dzdc − KL(qϕ(c, z|x)|qϕ(c, z|xS)) � dS. We will pursue here an encoding approach that does not require modelling the encoding distribution over the discrete latent variables explicitly, thus avoiding large variances in score-based Monte Carlo estimators or resorting to advanced variance reduction techniques or alternatives such as continuous relaxation approaches. Assuming a structured variational density of the form qϕ(c, z|xS) = qϕ(z|xS)qϕ(c|z, xS), we can express the augmented version of (4) via LS(xS, θ, ϕ) = � qϕ(c, z|xS) [log pθ(c, xS|z)] dz − βKL(qϕ(c, z|xS)|pθ(c, z)) = � qϕ(z|xS) [fx(z, xS) + fc(z, xS)] dz, where fx(z, xS) = log pθ(xS|z) − β log qϕ(z|xS)) and fc(z, xS) = � qϕ(c|z, xS) [−β log qϕ(c|z, xS) + β log pθ(c, z)] dc. (16) We can also write the augmented version of (5) in the form of L\S(x, θ, ϕ) = � qϕ(c, z|xS) � log pθ(x\S|z) � dz − βKL(qϕ(c, z|x)|qϕ(c, z|xS)) = � qϕ(z|x)gx(z, x)dz where gx(z, x) = log pθ(x\S|z) − β log qϕ(z|x) + β log qϕ(z|xS) 31which does not depend on the encoding density of the cluster variable. To optimize the variational bound with respect to the cluster density, we can thus optimize (16), which attains its maximum value of f ⋆ c (z, xS) = β log � pθ(c)pθ(z|c)dc = β log pθ(z) at qϕ(c|z, xS) = pθ(c|z) due to Remark 26 below with g(c) = β log pθ(c, z). Remark 26 (Entropy regularised optimization). Let q be a density over C, exp(g) be integrable with respect to q and τ > 0. The maximum of f(q) = � C q(c) [g(c) − τ log q(c)] dc that is attained at q⋆(c) = 1 Z eg(c)/τ with normalising constant Z = � C eg(c)/τ dc is f ⋆ = f(q⋆) = τ log � C eg(c)/τ dc We can derive an analogous optimal structured variational density for the mixture-based and total- correlation-based variational bounds. First, we can write the mixture-based bound (1) as LMix S (x, θ, ϕ) = � qϕ(z|xS) [log pθ(c, x|z)] dz − βKL(qϕ(c, z|xS)|pθ(c, z)) = � qϕ(z|xS) � f Mix x (z, x) + fc(z, x) � dz, where f Mix x (z, x) = log pθ(x|z) − β log qϕ(z|xS) and fc(z, x) has a maximum value of f ⋆ c (z, x) = β log pθ(z). Second, we can express the corresponding terms from the total-correlation-based bound as LTC S (θ, ϕ) = � qϕ(z|x) [log pθ(x|z)] dz − βKL(qϕ(c, z|x)|qϕ(c, z|xS)) = � qϕ(z|x) � f TC x (z, x) � dz, where f TC x (z, x) = log pθ(x|z) − β log qϕ(z|x) + β log qϕ(z|xS). K Algorithm and STL-gradient estimators We consider a multi-modal extension of the sticking-the-landing (STL) gradient estimator [101] that has also been used in previous multi-modal bounds [110]. The gradient estimator ignores the score function terms when sampling qϕ(z|xS) for variance reduction purposes due to the fact that it has a zero expectation. For the bounds (2) that involves sampling from qϕ(z|xS) and qϕ(z|xM), we thus ignore the score terms for both integrals. Consider the reparameterisation with noise vari- ables ϵS, ϵM ∼ p and transformations zS = tS(ϕ, ϵS, xS) = finvariant-agg(ϑ, ϵS, S, hS), for hS = hφ,s(xs)s∈S and zM = tM(ϕ, ϵM, xM) = finvariant-agg(ϑ, ϵM, M, hM), for hM = hφ,s(xs)s∈M . We need to learn only a single aggregation function that applies that masks the modalities appropri- ately. Pseudo-code for computing the gradients are given in Algorithm 1. If the encoding distribution is a mixture distribution, we apply the stop-gradient operation also to the mixture weights. Notice that in the case of a mixture prior and an encoding distribution that includes the mixture component, the optimal encoding density over the mixture variable has no variational parameters and is given as the posterior density of the mixture component under the generative parameters of the prior. In the case of private latent variables, we proceed analogously and rely on reparameterisations z′ S = t′ S(ϕ, ϵ′ S, xS) for the shared latent variable z′ S ∼ qϕ(z′|xS) as above and ˜zS = ˜tS(ϕ, z′, ϵS, xS) = fequivariant-agg(ϑ, ˜ϵS, z′, S, hS) for the private latent variables ˜zS ∼ qϕ(˜zS|z′, xS). Moreover, we write PS for a projection on the S-coordinates. Pseudo-code for computing unbiased gradient esti- mates for our bound is given in Alorithm 2. L Evaluation of multi-modal generative models We evaluate models using different metrics suggested previously for multi-modal learning, see for example [110, 138, 115]. 32Algorithm 1 Single training step for computing unbiased gradients of L(x). Input: Multi-modal data point x, generative parameter θ, variational parameters ϕ = (φ, ϑ). Sample S ∼ ρ. Sample ϵS, ϵM ∼ p. Set zS = tS(ϕ, ϵS, xM) and zM = tM(ϕ, ϵM, xM). Stop gradients of variational parameters ϕ′ = stop grad(ϕ). Set �LS(θ, ϕ) = log pθ(xS|zS) + β log pθ(zS) − β log qϕ′(zS|xS). Set �L\S(θ, ϕ) = log pθ(x\S|zM) + β log qϕ(zM|xS) − β log qϕ′(zM|xM). Output: ∇θ,ϕ � �LS(θ, ϕ) + �L\S(θ, ϕ) � Algorithm 2 Single training step for computing unbiased gradients of L(x) with private latent vari- ables. Input: Multi-modal data point x, generative parameter θ, variational parameters ϕ = (φ, ϑ). Sample S ∼ ρ. Sample ϵ′ S, ϵS, ϵ\S, ϵ′ M, ϵM, ϵ\M ∼ p. Set z′ S = t′ S(ϕ, ϵ′ S, xS), ˜zS = ˜tS(ϕ, z′ S, ϵS, xS). Set z′ M = t′ M(ϕ, ϵ′ M, xM), ˜zM = ˜tM(ϕ, z′ M, ϵM, xM). Stop gradients of variational parameters ϕ′ = stop grad(ϕ). Set �LS(θ, ϕ) = log pθ(xS|z′ S, ˜zS) + β log pθ(z′ S) − β log qϕ′(z′ S|xS) + β log pθ(˜zS|z′ S) − β log qϕ′(˜zS|z′ S, xS). Set �L\S(θ, ϕ) = log pθ(x\S|z′ M) + β log qϕ(z′ M|xS) − β log qϕ′(˜zM|z′ M, xM) + β log qϕ(PS(˜zM)|z′ M, xS) + β log pθ(P\S(˜zM)|z′ M, ˜zM) − β log qϕ′(˜zM|z′ M, xM). Output: ∇θ,ϕ � �LS(θ, ϕ) + �L\S(θ, ϕ) � Marginal, conditional and joint log-likelihoods. We can estimate the marginal log-likelihood using classic importance sampling log pθ(xS) ≈ log 1 K K � k=1 pθ(zk, xS) qϕ(zk|xS) for zk ∼ qϕ(·|xS). This also allows to approximate the joint log-likelihood log pθ(x), and conse- quently also the conditional log pθ(x\S|xS) = log pθ(x) − log pθ(xS). Generative coherence with joint auxiliary labels. Following previous work [110, 115, 22, 56], we assess whether the generated data share the same information in the form of the class labels across different modalities. To do so, we use pre-trained classifiers clfs : Xs → [K] that classify values from modality s to K possible classes. More precisely, for S ⊂ M and m ∈ M, we compute the self- (m ∈ S) or cross- (m /∈ S) coherence CS→m as the empirical average of 1{clfm(ˆxm)=y}, over test samples x with label y where ˆzS ∼ qϕ(z|xS) and ˆxm ∼ pθ(xm|ˆzS). The case S = M \ {m} corresponds to a leave-one-out conditional coherence. Linear classification accuracy of latent representations. To evaluate how the latent represen- tation can be used to predict the shared information contained in the modality subset S based on a linear model, we consider the accuracy AccS of a linear classifier clfz : Z → [K] that is trained to predict the label based on latent samples zS ∼ qϕ(zS|xtrain S ) from the training values xtrain S and evaluated on latent samples zS ∼ qϕ(z|xtest S ) from the test values xtest S . M Linear models Generative model. Suppose that a latent variable Z taking values in RD is sampled from a standard Gaussian prior pθ(z) = N(0, I) generates M data modalities Xs ∈ RDs, D ≤ Ds, 33based on a linear decoding model pθ(xs|z) = N(Wsz + bs, σ2 I) for a factor loading matrix Ws ∈ RDs×D, bias bs ∈ RDs and observation scale σ > 0. Note that the annealed likelihood function ˜pβ,θ(xs|z) = N(Wsz + bs, βσ2 I) corresponds to a scaling of the observation noise, so that we consider only the choice σ = 1, set σβ = σβ1/2 and vary β > 0. It is obvious that for any S ⊂ M, it holds that ˜pβ,θ(xS|z) = N(WSz + bS, σ2 β IS), where WS and bS are given by concatenating row-wise the emission or bias matrices for modalities in S, while σ2 β IS is the diagonal matrix of the variances of the corresponding observations. By standard properties of Gaussian distributions, it follows that ˜pβ,θ(xS) = N(bS, CS) where CS = WSW ⊤ S + σ2 β IS is the data covariance matrix. Furthermore, with KS = W ⊤ S WS + σ2 β Id, the adjusted posterior is ˜pβ,θ(z|xS) = N(K−1 S W ⊤ S (xS − bS), σ2 β Id K−1 S ). We sample orthogonal rows of W so that the posterior covariance becomes diagonal so that it can – in principle – be well approximated by an encoding distribution with a diagonal covariance matrix. Indeed, the inverse of the posterior co- variance matrix is only a function of the generative parameters of the modalities within S and can be written as the sum σ2 β I +W ⊤ S WS = σ2 β I + � s∈S W ⊤ s Ws, while the posterior mean function is xS �→ (σ2 β I + � s∈S W ⊤ s Ws)−1 � s∈S Ws(xs − bs). Data generation. We generate 5 data sets of N = 5000 samples, each with M = 5 modalities. We set the latent dimension to D = 30, while the dimension Ds of modality s is drawn from U(30, 60). We set the observation noise to σ = 1, shared across all modalities, as is standard for a PCA model. We sample the components of bs independently from N(0, 1). For the setting without modality- specific latent variables, Ws is the orthonormal matrix from a QR algorithm applied to a matrix with elements sampled iid from U(−1, 1). The bias coefficients Wb are sampled independently from N(0, 1/d). Conversely, the setting with private latent variables in the ground truth model allows us to describe modality-specific variation by considering the sparse loading matrix WM =   W ′ 1 ˜W1 0 . . . 0 W ′ 2 0 ˜W2 . . . 0 ... ... ... ... ... W ′ M 0 . . . 0 ˜WM   . Here, W ′ s, ˜Ws ∈ RDs×D′ with D′ = D/(M + 1) = 5, Furthermore, the latent variable Z can be written as Z = (Z′, ˜Z1, . . . , ˜ZM) for private and shared latent variables ˜Zs, resp. Z′. We similarly generate orthonormal � W ′ s, ˜Ws � from a QR decomposition. Observe that the general generative model with latent variable Z corresponds to the generative model (10) with shared Z′ and private latent variables ˜Z with straightforward adjustments for the decoding functions. Similar models have been considered previously, particularly from a Bayesian standpoint with different sparsity assumptions on the generative parameters [6, 128, 149]. Maximum likelihood estimation. Assume now that we observe N data points {xn}n∈[N], con- sisting of stacking the views xn = (xs,n)s∈S for each modality in S and let S = 1 N �N n=1(xn − b)(xn − b)⊤ ∈ RDx×Dx, Dx = �M s=1 Ds, be the sample covariance matrix across all modalities. Let Ud ∈ RDx×D be the matrix of the first D eigenvectors of S with corresponding eigenvalues λ1, . . . λD stored in the diagonal matrix ΛD ∈ RD×D. The maximum likelihood estimates are then given by bML = 1 N �N n=1 xn, σ2 ML = 1 N−D �N j=D+1 λj and WML = UD(ΛD − σ2 ML I)1/2 with the loading matrix identifiable up to rotations. Model architectures. We estimate the observation noise scale σ based on the maximum likelihood estimate σML. We assume linear decoder functions pθ(xs|z) = N(W θ s z + bθ, σ2 ML), fixed standard Gaussian prior p(z) = N(0, I) and generative parameters θ = (W θ 1 , bθ 1, . . . , W θ M, bθ M). Details about the various encoding architectures are given in Table 17. The modality-specific encoding functions for the PoE and MoE schemes have a hidden size of 512, whilst they are of size 256 for the learnable aggregation schemes having additional aggregation parameters φ. Simulation results. We show different rate-distortion terms for the learned models where the true data generation mechanism has (see Figure 4 ) or has not (see Figure 3) private latent variables. 34In both settings, we use the general multi-modal model without private latent variables in order to compare different aggregation schemes and bounds. We find that our bound yields encoding distri- butions that are closer to the true posterior distribution across various aggregation schemes. Note that in the case of the mixture-based bound, the posterior distribution is only optimal as an encod- ing distribution that uses all modalities (Sub-figures (c)). The trade-offs between full reconstruction quality and full rates vary across ground truth models, bounds and aggregation. Cross-reconstruction terms are usually better for the mixture-based bound. Moreover, the mixture-based bound has lower cross-modal rates, i.e., the encoding distribution does not change as much if additional modalities are included. Table 5 shows the log-likelihood of the generative model and the value of the lower bound when the true data has private latent variables. Compared to the results in Table 1 with full decoder matrices, there appear to be smaller differences across different bounds and fusion schemes. Finally, we consider permutation-equivariant schemes for learning models with private latent vari- ables as detailed in Appendix F, applied to the setting with sparse variables in the data genera- tion mechanism. Figure 5 shows different rate-distortion terms for β ∈ {0.1, 1, 4.} for PoE and SumPooling and SelfAttention aggregation models. We find that our variational bound tends to obtain higher full reconstruction terms, while the full rates vary for different configurations. Con- versely, the mixture-based bound obtains better cross-model reconstruction, with less clear patterns in the cross-rate terms. Table 6 shows the log-likelihood values for the learned generative model that is similar across different configurations, apart from a PoE scheme that achieves lower log-likelihood for a mixture-based bound. (a) Full Reconstruction −DM (b) Full Rates RM (c) Full Posterior Approximation (d) Cross Reconstruction −Dc \S (e) Cross Rates R\S (f) Uni-Modal Posterior Approxima- tion Figure 3: Linear Gaussian models with dense decoder matrix: Rate and distortion terms and KL- divergence of encoding distributions to posterior distribution from learned generative model. N Non-linear identifiable models N.1 Auxiliary labels Table 19 illustrates first the benefits of our bound that obtain better log-likelihood estimates for different fusion schemes. Second, it demonstrates the advantages of our new fusion schemes that achieve better log-likelihoods for both bounds. Third, it shows the benefit of using aggregation schemes that have the capacity to accommodate prior distributions different from a single Gaussian. Observe also that MoE schemes lead to low MCC values, while PoE schemes had high MCC values. We also show in Figure 6 the reconstructed modality values and inferred latent variables for one realisation with our bound, with the corresponding results for a mixture-based bound in Figure 7. 35(a) Full Reconstruction −DM (b) Full Rates RM (c) Full Posterior Approximation (d) Cross Reconstruction −Dc \S (e) Cross Rates R\S (f) Uni-Modal Posterior Approxima- tion Figure 4: Linear Gaussian models with sparse decoder matrix: Rate and distortion terms and KL- divergence of encoding distributions to posterior distribution from learned generative model. Table 5: Multi-modal Gaussian model with sparse decoders in the ground truth model: LLH Gap is the relative difference of the log-likelihood of the learned model relative to the log-likelihood based on the exact MLE. Bound gap is the relative difference of the variational bound to the log-likelihood based on the MLE. Our bound Mixture bound Aggregation LLH Gap Bound Gap MCC LLH Gap Bound Gap MCC PoE 0.00 (0.000) 0.00 (0.000) 0.84 (0.004) 0.00 (0.007) 0.01 (0.001) 0.87 (0.004) MoE 0.01 (0.001) 0.01 (0.001) 0.81 (0.001) 0.01 (0.002) 0.01 (0.002) 0.83 (0.003) SumPooling 0.00 (0.000) 0.00 (0.000) 0.84 (0.015) 0.01 (0.001) 0.01 (0.002) 0.84 (0.013) SelfAttention 0.00 (0.001) 0.00 (0.000) 0.84 (0.005) 0.01 (0.002) 0.01 (0.002) 0.83 (0.004) Table 6: Multi-modal Gaussian model with sparse decoders in the ground truth model and permutation-equivariant encoders: LLH Gap is the relative difference of the log-likelihood of the learned model relative to the log-likelihood based on the exact MLE. Bound gap is the relative difference of the variational bound to the log-likelihood based on the MLE. Our bound Mixture bound Aggregation LLH Gap Bound Gap MCC LLH Gap Bound Gap MCC PoE (equivariant) 0.00 (0.000) 0.00 (0.000) 0.91 (0.016) 0.01 (0.001) 0.02 (0.001) 0.88 (0.011) SumPooling (equivariant) 0.00 (0.000) 0.00 (0.000) 0.85 (0.004) 0.00 (0.000) 0.00 (0.001) 0.82 (0.003) SelfAttention (equivariant) 0.00 (0.000) 0.00 (0.000) 0.83 (0.006) 0.00 (0.000) 0.00 (0.003) 0.83 (0.003) 36(a) Full Reconstruction −DM (b) Full Rates RM (c) Cross Reconstruction −Dc \S (d) Cross Rates R\S Figure 5: Linear Gaussian models with sparse decoder matrix and permutation-equivariant aggrega- tion: Rate and distortion terms for varying β. Table 7: Non-linear identifiable model with one real-valued modality and an auxiliary label acting as a second modality: The first four rows use a fixed standard Gaussian prior, while the last four rows use a Gaussian mixture prior with 5 components. Mean and standard deviation over 4 repetitions. Our bound Mixture bound Aggregation LLH (β = 1) MCC (β = 1) MCC (β = 0.1) LLH (β = 1) MCC (β = 1) MCC (β = 0.1) PoE -43.4 (10.74) 0.98 (0.006) 0.99 (0.003) -318 (361.2) 0.97 (0.012) 0.98 (0.007) MoE -20.5 (6.18) 0.94 (0.013) 0.93 (0.022) -57.9 (6.23) 0.93 (0.017) 0.93 (0.025) SumPooling -17.9 (3.92) 0.99 (0.004) 0.99 (0.002) -18.9 (4.09) 0.99 (0.005) 0.99 (0.008) SelfAttention -18.2 (4.17) 0.99 (0.004) 0.99 (0.003) -18.6 (3.73) 0.99 (0.004) 0.99 (0.007) SumPooling -15.4 (2.12) 1.00 (0.001) 0.99 (0.004) -18.6 (2.36) 0.98 (0.008) 0.99 (0.006) SelfAttention -15.2 (2.05) 1.00 (0.001) 1.00 (0.004) -18.6 (2.27) 0.98 (0.014) 0.98 (0.006) SumPoolingMixture -15.1 (2.15) 1.00 (0.001) 0.99 (0.012) -18.2 (2.80) 0.98 (0.010) 0.99 (0.005) SelfAttentionMixture -15.3 (2.35) 0.99 (0.005) 0.99 (0.004) -18.4 (2.63) 0.99 (0.007) 0.99 (0.007) N.2 Five continuous modalities Table 8 demonstrates that our bound can yield to higher log-likelihoods and tigher bounds compared to a mixture-based bound, as do more flexible fusion schemes. Similar results for the partially observed case (η = 0.5) have been illustrated in the main text in Table 2. 37Table 8: Fully observed (η = 0) non-linear identifiable model with 5 modalities: The first four rows use a fixed standard Gaussian prior, while the last four rows use a Gaussian mixture prior with 5 components. Mean and standard deviation over 4 repetitions. Our bound Mixture bound Aggregation LLH Lower Bound MCC LLH Lower Bound MCC PoE -473.6 (9.04) -476.9 (9.61) 0.98 (0.005) -497.7 (11.26) -559.0 (2.33) 0.97 (0.008) MoE -477.9 (8.50) -484.3 (8.88) 0.91 (0.014) -494.6 (9.20) -546.8 (7.02) 0.92 (0.004) SumPooling -471.4 (8.29) -472.3 (8.54) 0.99 (0.004) -480.5 (8.84) -530.5 (3.02) 0.98 (0.005) SelfAttention -471.4 (8.97) -472.3 (9.52) 0.99 (0.002) -482.8 (10.51) -532.7 (2.89) 0.98 (0.004) SumPooling -465.4 (8.16) -467.6 (8.25) 0.98 (0.002) -475.1 (7.54) -521.7 (3.55) 0.98 (0.003) SelfAttention -469.3 (4.76) -471.5 (4.99) 0.98 (0.003) -474.7 (8.20) -522.7 (2.79) 0.98 (0.002) SumPoolingMixture -464.5 (8.16) -466.3 (7.91) 0.99 (0.003) -474.2 (7.61) -521.2 (4.13) 0.98 (0.004) SelfAttentionMixture -464.4 (8.50) -466.0 (9.66) 0.99 (0.003) -473.6 (8.24) -520.6 (2.62) 0.98 (0.002) (a) Observed data x (b) True latents z (c) PoE (x) (d) PoE (z) (e) MoE (x) (f) MoE (z) (g) SumP, K = 1 (x) (h) SumP, K = 1 (z) (i) SumP, K = 5 (x) (j) SumP, K = 5 (z) (k) SumPM, K = 5 (z) (l) SumPM, K = 5 (z) Figure 6: Bi-modal non-linear model with label and continuous modality based on our bound. O MNIST-SVHN-Text O.1 Training hyperparamters The MNIST-SVHN-Text data set is taken from the code accompanying [115] with around 1.1 million train and 200k test samples. All models are trained for 100 epochs with a batch size of 250 using Adam [66] and a cosine decay schedule from 0.0005 to 0.0001. 38(a) Observed data x (b) True latents z (c) PoE (x) (d) PoE (z) (e) MoE (x) (f) MoE (z) (g) SumP, K = 1 (x) (h) SumP, K = 1 (z) (i) SumP, K = 5 (x) (j) SumP, K = 5 (z) (k) SumPM, K = 5 (z) (l) SumPM, K = 5 (z) Figure 7: Bi-modal non-linear model with label and continuous modality based on mixture bound. (a) Full Reconstr. −DM (b) Cross Reconstr. −Dc \S (c) Full Rates RM (d) Cross Rates R\S Figure 8: Rate and distortion terms for MNIST-SVHN-Text with shared and private latent variables. Table 9: Test log-likelihood estimates for varying β choices for the joint data (M+S+T) as well as for the marginal data of each modality based on importance sampling (512 particles). Multi-modal generative model with a 40-dimensional shared latent variable. Our bound Mixture bound (β, Aggregation) M+S+T M S T M+S+T M S T (0.1, PoE+) 5433 (24.5) 1786 (41.6) 3578 (63.5) -29 (2.4) 5481 (18.4) 2207 (19.8) 3180 (33.7) -39 (1.0) (0.1, SumPooling) 7067 (78.0) 2455 (3.3) 4701 (83.5) -9 (0.4) 6061 (15.7) 2398 (9.3) 3552 (7.4) -50 (1.9) (1.0, PoE+) 6872 (9.6) 2599 (5.6) 4317 (1.1) -9 (0.2) 5900 (10.0) 2449 (10.4) 3443 (11.7) -19 (0.4) (1.0, SumPooling) 7056 (124.4) 2478 (9.3) 4640 (113.9) -6 (0.0) 6130 (4.4) 2470 (10.3) 3660 (1.5) -16 (1.6) (4.0, PoE+) 7021 (13.3) 2673 (13.2) 4413 (30.5) -5 (0.1) 5895 (6.2) 2484 (5.5) 3434 (2.2) -13 (0.4) (4.0, SumPooling) 6690 (113.4) 2483 (9.9) 4259 (117.2) -5 (0.0) 5659 (48.3) 2448 (10.5) 3233 (27.7) -10 (0.2) 39(a) Full Reconstr. −DM (b) Cross Reconstr. −Dc \S (c) Full Rates RM (d) Cross Rates R\S Figure 9: Rate and distortion terms for MNIST-SVHN-Text with shared latent variables and different β. O.2 Multi-modal rates and distortions O.3 Log-likelihood estimates O.4 Generated modalities (a) Our bound (b) Mixture-based bound Figure 10: Conditional generation for different aggergation schemes and bounds and shared latent variables. The first column is the conditioned modality. The next three columns are the generated modalities using a SumPooling aggregation, followed by the three columns for a SelfAttention ag- gregation, followed by PoE+ and lastly MoE+. O.5 Conditional coherence Table 10: Conditional coherence for models with shared latent variables and bi-modal condition- als. The letters on the second line represent the modality which is generated based on the sets of modalities on the line below it. Our bound Mixture bound M S T M S T Aggregation M+S M+T S+T M+S M+T S+T M+S M+T S+T M+S M+T S+T M+S M+T S+T M+S M+T S+T PoE 0.98 0.98 0.60 0.75 0.58 0.77 0.82 1.00 1.00 0.96 0.97 0.95 0.61 0.11 0.61 0.45 0.99 0.98 PoE+ 0.97 0.98 0.55 0.73 0.52 0.75 0.83 1.00 0.99 0.97 0.97 0.96 0.64 0.11 0.63 0.45 0.99 0.97 MoE 0.88 0.97 0.90 0.35 0.11 0.35 0.41 0.72 0.69 0.88 0.96 0.89 0.32 0.10 0.33 0.42 0.72 0.69 MoE+ 0.85 0.94 0.86 0.32 0.10 0.32 0.40 0.71 0.67 0.87 0.96 0.89 0.32 0.10 0.32 0.42 0.72 0.69 SumPooling 0.97 0.97 0.86 0.78 0.30 0.80 0.76 0.99 1.00 0.97 0.97 0.95 0.65 0.10 0.65 0.45 0.99 0.97 SelfAttention 0.97 0.97 0.82 0.76 0.30 0.78 0.69 1.00 1.00 0.97 0.97 0.99 0.66 0.10 0.65 0.45 0.99 1.00 40(a) Our bound, β = 0.1 (b) Our bound, β = 4 (c) Mixture-based bound, β = 0.1 (d) Mixture-based bound, β = 4 Figure 11: Conditional generation for different β parameters. The first column is the conditioned modality. The next three columns are the generated modalities using a SumPooling aggregation, followed by the three columns for a PoE+ scheme. (a) Our bound (b) Mixture-based bound Figure 12: Conditional generation for permutation-equivariant schemes and private latent variable constraints. The first column is the conditioned modality. The next three columns are the gener- ated modalities using a SumPooling aggregation, followed by the three columns for a SelfAttention scheme and a PoE model. 41Table 11: Conditional coherence for models with private latent variables and uni-modal condition- als. The letters on the second line represent the modality which is generated based on the sets of modalities on the line below it. Our bound Mixture bound M S T M S T Aggregation M S T M S T M S T M S T M S T M S T PoE+ 0.97 0.12 0.13 0.20 0.62 0.24 0.16 0.15 1.00 0.96 0.83 0.99 0.11 0.58 0.11 0.44 0.39 1.00 SumPooling 0.97 0.42 0.59 0.44 0.67 0.40 0.65 0.45 1.00 0.97 0.86 0.99 0.11 0.62 0.11 0.45 0.40 1.00 SelfAttention 0.97 0.12 0.12 0.27 0.71 0.28 0.46 0.40 1.00 0.96 0.09 0.08 0.12 0.67 0.12 0.15 0.17 1.00 Table 12: Conditional coherence for models with private latent variables and bi-modal condition- als. The letters on the second line represent the modality which is generated based on the sets of modalities on the line below it. Our bound Mixture bound M S T M S T Aggregation M+S M+T S+T M+S M+T S+T M+S M+T S+T M+S M+T S+T M+S M+T S+T M+S M+T S+T PoE+ 0.97 0.97 0.14 0.66 0.33 0.67 0.18 1.00 1.00 0.97 0.97 0.94 0.63 0.11 0.63 0.45 0.99 0.96 SumPooling 0.97 0.97 0.54 0.79 0.43 0.80 0.57 1.00 1.00 0.97 0.97 0.93 0.64 0.11 0.63 0.45 0.99 0.97 SelfAttention 0.97 0.97 0.12 0.80 0.29 0.81 0.49 1.00 1.00 0.96 0.96 0.08 0.70 0.12 0.70 0.15 1.00 1.00 Latent classification accuracy. P Encoder Model architectures P.1 Linear models P.2 Linear models with private latent variables P.3 Nonlinear model with auxiliary label P.4 Nonlinear model with five modalities P.5 MNIST-SVHN-Text For SVHN and and Text, we use 2d- or 1d-convolutional layers, respectively, denoted as Conv(f, k, s) for feature dimension f, kernel-size k and stride s. We denote transposed convolu- tions as tConv. We use the neural network architectures as implemented in Flax [44]. P.6 MNIST-SVHN-Text with private latent variables Q MNIST-SVHN-Text Decoder Model architectures For models with private latent variables, we concatenate the shared and private latent variables. We use a Laplace likelihood as the decoding distribution for MNIST and SVHN, where the decoder function learns both its mean as a function of the latent and a constant log-standard-deviation at Table 13: Conditional coherence for models with shared latent variables for different βs and uni- modal conditionals. The letters on the second line represent the modality which is generated based on the sets of modalities on the line below it. Our bound Mixture bound M S T M S T (β, Aggregation) M S T M S T M S T M S T M S T M S T (0.1, PoE+) 0.98 0.11 0.12 0.12 0.62 0.14 0.61 0.25 1.00 0.96 0.83 0.99 0.11 0.58 0.11 0.45 0.39 1.00 (0.1, SumPooling) 0.97 0.48 0.81 0.30 0.72 0.33 0.86 0.55 1.00 0.97 0.86 0.99 0.11 0.64 0.11 0.45 0.40 1.00 (1.0, PoE+) 0.97 0.15 0.63 0.24 0.63 0.42 0.79 0.35 1.00 0.96 0.83 0.99 0.11 0.59 0.11 0.45 0.39 1.00 (1.0, SumPooling) 0.97 0.48 0.87 0.25 0.72 0.36 0.73 0.48 1.00 0.97 0.86 0.99 0.10 0.63 0.10 0.45 0.40 1.00 (4.0, PoE+) 0.97 0.29 0.83 0.41 0.60 0.58 0.76 0.38 1.00 0.96 0.82 0.99 0.10 0.57 0.10 0.44 0.38 1.00 (4.0, SumPooling) 0.97 0.48 0.88 0.35 0.66 0.44 0.83 0.53 1.00 0.96 0.85 0.99 0.11 0.57 0.10 0.45 0.39 1.00 42Table 14: Conditional coherence for models with shared latent variables for different βs and bi- modal conditionals. The letters on the second line represent the modality which is generated based on the sets of modalities on the line below it. Our bound Mixture bound M S T M S T (β, Aggregation) M+S M+T S+T M+S M+T S+T M+S M+T S+T M+S M+T S+T M+S M+T S+T M+S M+T S+T (0.1, PoE+) 0.98 0.98 0.15 0.70 0.14 0.72 0.66 1.00 1.00 0.96 0.96 0.93 0.62 0.11 0.62 0.45 0.99 0.95 (0.1, SumPooling) 0.97 0.97 0.86 0.83 0.31 0.84 0.85 0.99 1.00 0.97 0.97 0.94 0.66 0.11 0.65 0.45 0.99 0.96 (1.0, PoE+) 0.97 0.98 0.55 0.73 0.52 0.75 0.83 1.00 0.99 0.97 0.97 0.96 0.64 0.11 0.63 0.45 0.99 0.97 (1.0, SumPooling) 0.97 0.97 0.86 0.78 0.30 0.80 0.76 0.99 1.00 0.97 0.97 0.95 0.65 0.10 0.65 0.45 0.99 0.97 (4.0, PoE+) 0.97 0.98 0.84 0.76 0.66 0.78 0.82 1.00 1.00 0.97 0.97 0.96 0.62 0.10 0.62 0.45 0.99 0.98 (4.0, SumPooling) 0.97 0.97 0.89 0.77 0.40 0.78 0.86 0.99 1.00 0.97 0.97 0.96 0.61 0.10 0.60 0.45 0.99 0.97 Table 15: Unsupervised latent classification for β = 1 and models with shared latent variables only (top half) and shared plus private latent variables (bottom half). Accuracy is computed with a linear classifier (logistic regression) trained on multi-modal inputs (M+S+T) or uni-modal inputs (M, S or T). Our bound Mixture bound Aggregation M+S+T M S T M+S+T M S T PoE 0.988 (0.000) 0.940 (0.009) 0.649 (0.039) 0.998 (0.001) 0.991 (0.004) 0.977 (0.002) 0.845 (0.000) 1.000 (0.000) PoE+ 0.978 (0.002) 0.934 (0.001) 0.624 (0.040) 0.999 (0.001) 0.998 (0.000) 0.981 (0.000) 0.851 (0.000) 1.000 (0.000) MoE 0.841 (0.008) 0.974 (0.000) 0.609 (0.032) 1.000 (0.000) 0.940 (0.001) 0.980 (0.001) 0.843 (0.001) 1.000 (0.000) MoE+ 0.850 (0.039) 0.967 (0.014) 0.708 (0.167) 0.983 (0.023) 0.928 (0.017) 0.983 (0.002) 0.846 (0.001) 1.000 (0.000) SelfAttention 0.985 (0.001) 0.954 (0.002) 0.693 (0.037) 0.986 (0.006) 0.991 (0.000) 0.981 (0.001) 0.864 (0.003) 1.000 (0.000) SumPooling 0.981 (0.000) 0.962 (0.000) 0.704 (0.014) 0.992 (0.008) 0.994 (0.000) 0.983 (0.000) 0.866 (0.002) 1.000 (0.000) PoE+ 0.979 (0.009) 0.944 (0.000) 0.538 (0.032) 0.887 (0.07) 0.995 (0.002) 0.980 (0.002) 0.848 (0.006) 1.000 (0.000) SumPooling 0.987 (0.004) 0.966 (0.004) 0.370 (0.348) 0.992 (0.002) 0.994 (0.001) 0.982 (0.000) 0.870 (0.001) 1.000 (0.000) SelfAttention 0.990 (0.003) 0.968 (0.002) 0.744 (0.008) 0.985 (0.000) 0.997 (0.001) 0.974 (0.000) 0.681 (0.031) 1.000 (0.000) each pixel. Following previous works [110, 115], we re-weight the log-likelihoods for different modalities relative to their dimensions. R Compute resources and existing assets Our computations were performed on shared HPC systems. All experiments except Section 5.3 were run on a CPU server using one or two CPU cores. The experiments in Section 5.3 were run a GPU server using one NVIDIA A100. Our implementation is based on JAX [16] and Flax [44]. We compute the mean correlation co- efficient (MCC) between true and infered latent variables following [63], as in https://github. com/ilkhem/icebeem. In our MNIST-SVHN-Text experiments, we use code from [115], https: //github.com/thomassutter/MoPoE. Table 16: Unsupervised latent classification for different βs and models with shared latent variables only. Accuracy is computed with a linear classifier (logistic regression) trained on multi-modal inputs (M+S+T) or uni-modal inputs (M, S or T). Our bound Mixture bound (β, Aggregation) M+S+T M S T M+S+T M S T (0.1, PoE+) 0.983 (0.006) 0.919 (0.001) 0.561 (0.048) 0.988 (0.014) 0.992 (0.002) 0.979 (0.002) 0.846 (0.004) 1.000 (0.000) (0.1, SumPooling) 0.982 (0.004) 0.965 (0.002) 0.692 (0.047) 0.999 (0.001) 0.994 (0.000) 0.981 (0.002) 0.863 (0.005) 1.000 (0.000) (1.0, PoE+) 0.978 (0.002) 0.934 (0.001) 0.624 (0.040) 0.999 (0.001) 0.998 (0.000) 0.981 (0.000) 0.851 (0.000) 1.000 (0.000) (1.0, SumPooling) 0.981 (0.000) 0.962 (0.000) 0.704 (0.014) 0.992 (0.008) 0.994 (0.000) 0.983 (0.000) 0.866 (0.002) 1.000 (0.000) (4.0, PoE+) 0.981 (0.006) 0.943 (0.007) 0.630 (0.008) 0.993 (0.001) 0.998 (0.000) 0.981 (0.000) 0.846 (0.001) 1.000 (0.000) (4.0, SumPooling) 0.984 (0.004) 0.963 (0.001) 0.681 (0.009) 0.995 (0.000) 0.992 (0.002) 0.980 (0.001) 0.856 (0.001) 1.000 (0.000) 43Table 17: Encoder architectures for Gaussian models. (a) Modality-specific encoding functions hs(xs). Latent di- mension D = 30, modality dimension Ds ∼ U(30, 60). MoE/PoE SumPooling/SelfAttention Input: Ds Input: Ds Dense Ds × 512, ReLU Dense Ds × 256, ReLU Dense 512 × 512, ReLU Dense 256 × 256, ReLU Dense 512 × 60 Dense 256 × 60 (b) Model for outer aggregation function ρϑ for SumPooling and SelfAttention schemes. Outer Aggregation Input: 256 Dense 256 × 256, ReLU Dense 256 × 256, ReLU Dense 256 × 60 (c) Inner aggregation function χϑ. SumPooling SelfAttention Input: 256 Input: 256 Dense 256 × 256, ReLU Dense 256 × 256, ReLU Dense 256 × 256, ReLU Dense 256 × 256 Dense 256 × 256 (d) Transformer parameters. SelfAttention (1 Layer) Input: 256 Heads: 4 Attention size: 256 Hidden size FFN: 256 Table 18: Encoder architectures for Gaussian models with private latent variables. (a) Modality-specific encoding functions hs(xs). All private and shared latent variables are of dimension 10. Modality dimension Ds ∼ U(30, 60). PoE (hshared s and hprivate s ) SumPooling/SelfAttention (one hs) Input: Ds Input: Ds Dense Ds × 512, ReLU Dense Ds × 128, ReLU Dense 512 × 512, ReLU Dense 128 × 128, ReLU Dense 512 × 10 Dense 128 × 10 (b) Model for outer aggregation func- tion ρϑ for SumPooling scheme. Outer Aggregation (ρϑ) Input: 128 Dense 128 × 128, ReLU Dense 128 × 128, ReLU Dense 128 × 10 (c) Inner aggregation functions. SumPooling (χ0,ϑ, χ1,ϑ, χ2,ϑ) SelfAttention (χ1,ϑ, χ2,ϑ) Input: 128 Input: 128 Dense 128 × 128, ReLU Dense 128 × 128, ReLU Dense 128 × 128, ReLU Dense 128 × 128 Dense 128 × 128 (d) Transformer parameters. SelfAttention (1 Layer) Input: 128 Heads: 4 Attention size: 128 Hidden size FFN: 128 44Table 19: Encoder architectures for nonlinear model with auxiliary label. (a) Modality-specific encoding functions hs(xs). Modality dimension D1 = 2 (continuous modality) and D2 = 5 (la- bel). Embedding dimension DE = 4 for PoE and MoE and DE = 128 otherwise. Modality-specific encoders Input: Ds Dense Ds × 128, ReLU Dense 128 × 128, ReLU Dense 128 × DE (b) Model for outer aggregation function ρϑ for SumPooling and SelfAttention schemes and mixtures thereof. Output dimension is D0 = 25 for mixture densities and DO = 4 otherwise. Outer Aggregation Input: 128 Dense 128 × 128, ReLU Dense 128 × 128, ReLU Dense 128 × DO (c) Inner aggregation function χϑ. SumPooling SelfAttention Input: 128 Input: 128 Dense 128 × 128, ReLU Dense 128 × 128, ReLU Dense 128 × 128, ReLU Dense 128 × 128 Dense 128 × 128 (d) Transformer parameters. SelfAttention Input: 128 Heads: 4 Attention size: 128 Hidden size FFN: 128 Table 20: Encoder architectures for nonlinear model with five modalities. (a) Modality-specific encoding functions hs(xs). Modality dimensions Ds = 25. Latent dimension D = 25 MoE/PoE SumPooling/SelfAttention Input: Ds Input: Ds Dense Ds × 512, ReLU Dense Ds × 256, ReLU Dense 512 × 512, ReLU Dense 256 × 256, ReLU Dense 512 × 50 Dense 256 × 256 (b) Model for outer aggregation function ρϑ for SumPooling and SelfAttention schemes and mixtures thereof. Output dimension is D0 = 50 for mixture densities and DO = 25 otherwise. Outer Aggregation Input: 256 Dense 256 × 256, ReLU Dense 256 × 256, ReLU Dense 256 × DO (c) Inner aggregation function χϑ. SumPooling SelfAttention Input: 256 Input: 256 Dense 256 × 256, ReLU Dense 256 × 256, ReLU Dense 256 × 256, ReLU Dense ×256 Dense 256 × 256 (d) Transformer parameters. SelfAttention Input: 256 Heads: 4 Attention size: 256 Hidden size FFN: 256 45Table 21: Encoder architectures for MNIST-SVHN-Text. (a) MNIST-specific encoding functions hs(xs). Modality dimensions Ds = 28 × 28. Embed- ding dimension is DE = 2D for PoE/MoE and DE = 256 for SumPooling/SelfAttention. For PoE+/MoE+, we add four times a Dense layer of size 256 with ReLU layer before the last linear layer. MoE/PoE/SumPooling/SelfAttention Input: Ds, Dense Ds × 400, ReLU Dense 400 × 400, ReLU Dense 400 × DE (b) SVHN-specific encoding functions hs(xs). Modality dimensions Ds = 3 × 32 × 32. Embed- ding dimension is DE = 2D for PoE/MoE and DE = 256 for SumPooling/SelfAttention. For PoE+/MoE+, we add four times a Dense layer of size 256 with ReLU layer before the last linear layer. MoE/PoE/SumPooling/SelfAttention Input: Ds Conv(32, 4, 2), ReLU Conv(64, 4, 2), ReLU Conv(64, 4, 2), ReLU Conv(128, 4, 2), ReLU, Flatten Dense 2048 × DE (c) Text-specific encoding functions hs(xs). Modality dimensions Ds = 8 × 71. Embed- ding dimension is DE = 2D for PoE/MoE and DE = 256 for permutation-invariant mod- els (SumPooling/SelfAttention) and DE = 128 for permutation-equivariant models (SumPool- ing/SelfAttention). For PoE+/MoE+, we add four times a Dense layer of size 256 with ReLU layer before the last linear layer. MoE/PoE/SumPooling/SelfAttention Input: Ds Conv(128, 1, 1), ReLU Conv(128, 4, 2), ReLU Conv(128, 4, 2), ReLU, Flatten Dense 128 × DE (d) Model for outer aggregation function ρϑ for SumPooling and SelfAttention schemes. Output dimension is D0 = 2D = 80 for models with shared latent variables only and D0 = 10+10 for models with private and shared latent variables. DE = 256 for permutation-invariant and DI = 128 for permutation-invariant models. Outer Aggregation Input: DE Dense DE × DE, LReLU Dense DE × DE, LReLU Dense DE × DO (e) Inner aggregation function χϑ for permutation-invariant models (DE = 256) and permutaion-equivariant models (DE = 128). SumPooling SelfAttention Input: DE Input: DE Dense DE × DE, LReLU Dense DE × DE, LReLU Dense DE × DE, LReLU Dense ×DE Dense DE × DE (f) Transformer parameters for permutation- invariant models. DE = 256 for permutation- invariant and DI = 128 for permutation-invariant models. SelfAttention (2 Layers) Input: DE Heads: 4 Attention size: DE Hidden size FFN: DE 46Table 22: Decoder architectures for MNIST-SVHN-Text. (a) MNIST decoder. DI = 40 for models with shared latent variables only, and DI = 10 + 10 otherwise. MNIST Input: DI Dense 40 × 400, ReLU Dense 400 × 400, ReLU Dense 400 × Ds, Sigmoid (b) SVHN decoder. DI = 40 for models with shared latent variables only, and DI = 10 + 10 otherwise. SVHN Input: DI Dense DI × 128, ReLU tConv(64, 4, 3), ReLU tConv(64, 4, 2), ReLU tConv(32, 4, 2), ReLU tConv(3, 4, 2) (c) Text decoder. DI = 40 for models with shared latent variables only, and DI = 10+10 otherwise. Text Input: DI Dense DI × 128, ReLU tConv(128, 4, 3), ReLU tConv(128, 4, 2), ReLU tConv(71, 1, 1) 47
Differentially Private Functional Summaries via the Independent Component Laplace Process Haotian Lin hzl435@psu.edu Department of Statistics, The Pennsylvania State University, University Park, PA 16802, USA Matthew Reimherr mreimherr@psu.edu Department of Statistics, The Pennsylvania State University, University Park, PA 16802, USA and Amazon Science, Seattle, WA 98109, USA Abstract In this work we propose a new mechanism for releasing differentially private functional sum- maries called the Independent Component Laplace Process, or ICLP, mechanism. By treat- ing the functional summaries of interest as truly infinite-dimensional objects and perturbing them with the ICLP noise, this new mechanism relaxes assumptions on data trajectories and preserves higher utility compared to classical finite-dimensional subspace embedding approaches in the literature. We establish the feasibility of the proposed mechanism in multiple function spaces. Several statistical estimation problems are considered, and we demonstrate by slightly over smoothing the summary, the privacy cost will not dominate the statistical error and is asymptotically negligible. Numerical experiments on synthetic and real datasets demonstrate the efficacy of the proposed mechanism. Keywords: Differential Privacy, Functional Data Analysis, Hilbert Space, Reproducing Kernel Hilbert Space, Stochastic Processes, 1. Introduction Data security has garnered critical attention in the last decade as substantial individualized data are collected. The most widely used paradigm in formal data privacy is differential privacy (DP), introduced by Dwork et al. (2006). DP provides a rigorous and interpretable definition for data privacy as it bounds the amount of information that attackers can in- fer from publicly released database queries. Numerous mechanisms have been developed under conventional data settings like scalar or vector-valued data. However, advances in technologies enable us to collect and process densely observed data over some temporal or spatial domains, which are coined functional data to differentiate them from classic mul- tivariate data (Ramsay et al., 2005; Kokoszka and Reimherr, 2017; Ferraty and Romain, 2011). Even though functional data analysis, FDA, has been proven useful in various fields like economics, finance, genetics and etc., and has been researched widely in the statistical community, there are only a few works concerning privacy preservation within the realm of functional data. 1 arXiv:2309.00125v1 [stat.ML] 31 Aug 2023In this paper, we propose an additive noise mechanism for functional summaries, namely infinite dimensional summaries, to achieve ϵ-DP. Additive noise mechanisms are one of the most commonly used mechanisms to achieve DP, which sanitizes statistical summaries by adding calibrated noise from predetermined distributions, e.g. Laplace and Gaussian mech- anism (Dwork et al., 2006, 2014). With functional summaries, classic privacy tools embed the problem into a finite-dimensional subspace by using finite basis expansions to approx- imate summaries and perturb expansion coefficients with i.i.d. noise (Wang et al., 2013; Chandrasekaran et al., 2014; Alda and Rubinstein, 2017; Zhang et al., 2012). However, these mechanisms have potential weaknesses. First, determining the dimension of the sub- space is crucial as it plays a trade-off role between utility and privacy. While data-driven approaches might cause potential privacy leakage, a pre-determined dimension will lack adaption to the data, potentially failing to capture the shape of the functions or injecting excess noise. Second, in multivariate settings, privacy budget allocation over each com- ponent can affect a mechanism’s utility and robustness substantially. Some previous work has shown that capturing the covariance structure in the data can substantially reduce the amount of noise injected (Hardt and Talwar, 2010; Awan and Slavkovi´c, 2021). However, current mechanisms allocate an equal privacy budget to all components, failing to recognize the importance of different functional components, and thus injecting excess noise for “more important” components, degrading utility and robustness. Contribution: To overcome the downsides inherent in finite-dimensional subspaces embedding approaches, we treat the functional summary and privacy noise as truly infinite- dimensional. We propose a mechanism by perturbing functional summaries with a stochastic process called the Independent Component Laplace Process (ICLP), and name this mecha- nism as the ICLP mechanism. It ensures a full path-level release rather than a finite grid of evaluating points. We establish the feasibility of the ICLP mechanism in a general separable Hilbert space, H, by characterizing a subspace of H and show that the feasibility holds if and only if functional summaries reside in this subspace. We also show how the proposed mechanism applies to the space of the continuous functions, even though this space is not a Hilbert space. We provide two strategies based on regularization to restrict the functional summaries to be within a desired subspace and show one can gain privacy for “free” by slightly over smooth the summaries. We demonstrate the proposed mechanism for the mean function protection problem and show it can go beyond classic functional data settings, as it is also applicable to the realm of more classic non-parametric smoothing problems like kernel density estimation. To obtain a privacy-safe regularization parameter in the regular- ization term, we adopt the plug-in approach such that the regularization parameters are tied to the covariance structure of the ICLP noise. This approach not only overcomes the po- tential information leakage in conventional data-driven approaches, but also has theoretical utility guarantees. The proposed mechanism differentiates itself from current mechanisms in the following senses. First, the ICLP mechanism avoids finite-dimensional subspace em- beddings and frees the assumption that every dataset in the database shares the same finite-dimensional subspace. Second, the privacy budgets allocated to each component are not uniform but proportional to the global sensitivity of that component. Compared to uni- form allocation in current mechanisms, like the functional mechanism (Zhang et al., 2012) and Bernstein mechanism (Alda and Rubinstein, 2017), such allocation injects less noise into the more important components, which improves the utility of the released summaries. 2Related Work: In the overlap of functional summaries and differential privacy, the landmark paper is Hall et al. (2013) which provided a framework for achieving (ϵ, δ)-DP on infinite dimensional functional objects, but focussed on a finite grid of evaluation points. The follow-up work in Mirshani et al. (2019) pushed Hall et al. (2013)’s result forward, established (ϵ, δ)-DP over the full functional path for objects in Banach spaces. In more general spaces, Reimherr and Awan (2019) considered elliptical perturbations to achieve (ϵ, δ)-DP in locally convex vector spaces, including all Hilbert spaces, Banach spaces, and Frechet spaces. They also showed the impossibility of achieving ϵ-DP in infinite dimensional objects with elliptical distributions. Turning to ϵ-DP, a series of works have been proposed by resorting to finite-dimensional representations, like polynomial bases, trigonometric bases, or Bernstein polynomial bases to approximate target functional summaries (Wang et al., 2013; Chandrasekaran et al., 2014; Alda and Rubinstein, 2017) and loss functions (Zhang et al., 2012). One then per- turbs the expansion coefficients via the Laplace mechanism independently. In addition to additive noise mechanisms, Awan et al. (2019) extended the exponential mechanism (Mc- Sherry and Talwar, 2007) to any arbitrary Hilbert spaces and showed its application to functional principal component analysis. From the perspective of the robust noise injec- tion, a heterogeneous noise injection scheme (Phan et al., 2019) is proposed by assigning different weighted privacy budgets on each coordinate to further improve the robustness of private summaries. 2. Preliminaries and Notation 2.1 Differential Privacy Let D be the collection of all possible n-unit databases and D be an element of D. Denote the summary of interest as f : D → Ω, where (Ω, F) is a measurable space. We denote the sanitized version of fD by ˜fD, which is a random element of Ω indexed by D. We state the definition of differential privacy in terms of conditional distributions (Wasserman and Zhou, 2010). Definition 1 Let ˜fD be the sanitized version of the functional summary fD. Assume {PD : D ∈ D} is the family of probability measures over Ω induced by { ˜fD : D ∈ D}. We say ˜fD achieves ϵ-DP if for any two adjacent datasets (only different on one record) D, D′, and any measurable set A ∈ F, one has PD(A) ≤ eϵPD′(A). The definition implies the summaries of two adjacent datasets should have almost the same probability distribution. The privacy budget ϵ controls how much privacy will be lost while releasing the result, and a small ϵ implies a higher similarity between PD and PD′ and thus increased privacy. 2.2 Notation We introduce some notation that will be used now throughout the paper. Let H be a real separable Hilbert space with inner product ⟨·, ·⟩H. A random element X ∈ H is said to have 3mean µ ∈ H and (linear) covariance operator C : H → H if E[⟨X, h⟩H] = ⟨µ, h⟩H and Cov(⟨X, h1⟩H, ⟨X, h2⟩H) = ⟨Ch1, h2⟩H, for any h, h1, h2 in H. A linear operator, C, is a covariance operator if and only if it is symmetric, positive semidefinite, and trace class (Bosq, 2000). Denoted {λj, ϕj}j≥1 as the eigenvalues and eigenfunctions of C, then we define two norms associated with C: ∥h∥C = � � � � � j≥1 ⟨h, ϕj⟩2 H λj and ∥h∥1,C = � j≥1 |⟨h, ϕj⟩H| � λj . We also denote the two corresponding subspaces of H as HC = {h ∈ H : ∥h∥C < ∞} and H1,C = {h ∈ H : ∥h∥1,C < ∞}. Note ∥·∥C is the classic Cameron-Martin norm induced by C (Bogachev, 1998) and HC is called the Cameron-Martin space of C. Here ∥·∥1,C is analogous to a weighted ℓ1-norm, and h ∈ H1,C leads to h ∈ HC. Lastly, we will use some common notation when discussing asymptotic results, namely, let an ≍ bn and an ≲ bn(an = O(bn)) denote |an/bn| → c and |an/bn| ≤ c, respectively, for some constant c when n → ∞. 3. The ICLP Mechanism and its Feasibility In this section, we first define the Independent Component Laplace Process, and then formally propose the the ICLP mechanism, where the private summary takes the form of ˜fD = fD + σZ with Z as an ICLP noise. Initially, we assume fD lies in a real separable Hilbert space H. Then we also show that privacy protection can also hold for the space of continuous functions (which is not a Hilbert space) as well under certain assumptions on the covariance operator. The proofs of all the Lemmas and Theorems can be found in Appendix A. 3.1 Independent Component Laplace Process The proposed stochastic process is motivated by Mirshani et al. (2019), who achieved (ϵ, δ)- DP on functional summaries in Banach spaces. Formally, their mechanism can be written as ˜fD = fD + σZ where Z ∼ GP(0, C) is a centered Gaussian process with covariance operator C. There is a dual perspective of this mechanism. By applying the Karhunen- Lo´eve Theorem (Kosambi, 2016), the mechanism is equivalent to ˜fD = fD + σZ = ∞ � j=1 (⟨fD, ϕj⟩ + σ ⟨Z, ϕj⟩) ϕj, (1) where {λj}j≥1 and {ϕj}j≥1 are the eigenvalues and eigenfunctions of C and ⟨Z, ϕj⟩ indepen- dently follows N(0, λj), meaning that the mechanism perturbs each component’s coefficient with independent normal random variables. Unfortunately, the existing Laplace process cannot play a role analogous to the Gaussian process under such an expansion since it is an elliptical distribution and it has been proved that no elliptical distribution can be used to achieve ϵ-DP in infinite-dimensional spaces (Reimherr and Awan, 2019). Motivated by the dual perspective from the Karhunen-Lo´eve expansion and the Laplace mechanism in 4the scalar setting, we consider adding independent Laplace noises with heterogeneous vari- ances in the expansion (1), which is equivalent to perturbing the functional summary with a particular stochastic process. The desired stochastic process is defined as follows. Definition 2 Let X be a random element in H with E ∥X∥2 < ∞. Given a non-negative decreasing real sequence, {λj}j≥1, satisfying � j≥1 λj < ∞, and an orthonormal basis of H, {ϕj}j≥1, we say X is an Independent Component Laplace Process with mean µ if it admits the decomposition X = µ + ∞ � j=1 � λjZjϕj, where Zj are i.i.d. Laplace random variables with zero-mean and variance 1. The collection of square integrable random elements of H is itself a Hilbert space with inner product E⟨X, Y ⟩H. Thus, while the ICLP is given as an infinite sum, it is still well-defined, see Chapter 1 in Bosq (2000). Theorem 3 If the given non-negative decreasing real sequence, {λj}j≥1, satisfies � j≥1 λj < ∞, and the basis {ϕj}j≥1 are orthonormal in H, then the stochastic process defined in Def- inition 2 is well-defined in H. 3.2 Feasibility in Separable Hilbert Spaces To investigate the feasibility of a randomized mechanism, one can start with the equiv- alence/orthogonality of probability measures. As discussed in Awan et al. (2019) and Reimherr and Awan (2019), the probability measures induced by an ϵ-DP mechanism are necessarily equivalent (though this is not sufficient for DP) in a probabilistic sense; oth- erwise, it is impossible to be DP if the measures are orthogonal. More specifically, if the mechanism produces a private summary ˜fD that is probabilistically orthogonal to ˜fD′, i.e. there exists a A ∈ F so that PD(A) = 0 and PD′(A) = 1, then the mechanism cannot be DP since fD and fD′ can be distinguished with probability one on A. In the following, we use this perspective to develop the feasibility of the ICLP mechanism. Denote the probability measure family induced by the ICLP mechanism as {PD : D ∈ D}, in the following theorem, we provide necessary and sufficient conditions for pairwise equivalence in {PD : D ∈ D}. Theorem 4 Let D, D′ ∈ D be two adjacent datasets, ˜fD, ˜fD′ be the private summaries based on the ICLP mechanism, and denote the corresponding probability measures over H as PD and PD′. Then PD and PD′ are equivalent if and only if fD − fD′ ∈ HC = {h ∈ H : ∥h∥C < ∞} . (2) Theorem 4 shows that if the difference of fD and fD′ resides in the Cameron-Martin space of C then the probability family will be pairwise equivalent. An analogous result for the equivalence of elliptical distributions appears in Theorem 2 of Reimherr and Awan (2019) even though the ICLP is not an elliptical distribution. However, it turns out that, unlike el- liptical distributions, pairwise equivalence is not enough for the ICLP mechanism to achieve DP. To see the reason behind this, one needs to consider the density of PD in H. Since there is no common base measure in H that plays the same role as the Lebesgue measure 5in Rd, it is more complicated to consider the density in H. Fortunately, we are adding the same type of noise to functional summaries, and therefore we only need the density as the Radon-Nikodym derivative of PD w.r.t. P0, where P0 is the probability measure induced by σZ. Lemma 5 Let Ph and P0 be the probability measures induced by {h+σZ} and σZ. Suppose h ∈ HC, then the Radon–Nikodym derivative of PD w.r.t. P0 is given by dPh dP0 (z) = exp � − 1 σ � ∥z − h∥1,C − ∥z∥1,C �� , (3) P0 almost everywhere and is unique. Now we are ready to show why Condition (2) is not enough for ϵ-DP. Indeed, the pairwise equivalence only guarantees the density in Equation (3) is well-defined, but does not guar- antee that the density is bounded, which is a requirement for ϵ-DP however. Meanwhile, HC is enough for (ϵ, δ)-DP since it allows densities to be unbounded up to a set with P0 measure less than δ. In the next theorem, we will show the appropriate space that fD −fD′ should reside in is actually a subspace of HC. Theorem 6 Under the same conditions of Theorem 4, let H1,C = {f ∈ H : ∥f∥1,C < ∞} be a subspace of HC and if fD1 − fD2 ∈ HC \ H1,C, then there is no σ ∈ R+ such that the ICLP mechanism, ˜ fD = fD + σZ satisfies ϵ-DP. Indeed, if fD resides in the gap between HC and H1,C, the sensitivity of fD will be infinite and there is no possibility to calibrate the ICLP noise with any σ to achieve ϵ-DP. Now, with the proper space in Theorem 6 and the feasible density in Lemma 5, we can establish the ICLP mechanism formally. Theorem 7 (The ICLP Mechanism) Let fD ∈ H1,C be the functional summary and Z is an ICLP with covariance operator C. Define the global sensitivity (GS) as ∆ = sup D∼D′ ∥fD − fD′∥1,C and σ = ∆ ϵ . (4) Then the sanitized version of fD, ˜fD = fD + σZ, achieves ϵ-DP. 3.3 Extensions To Space of Continuous Functions Theorem 7 implies the ICLP mechanism provides privacy protection for a wide range of infinite-dimensional objects in separable Hilbert spaces. The DP post-processing inequality (Dwork et al., 2014), is an especially important property for functions since practically one may only be interested in a few scalar summaries. However, the post-processing inequality only applies to measurable mappings. If H = L2[0, 1], then this eliminates the possibility 6of releasing point-wise evaluations of the functional summary, since such mappings are not measurable operations in L2[0, 1]. Therefore, in this section, we extend the ICLP mechanism to the space of continuous functions, i.e. C(T) with T a compact set over Rd, where such operations are measurable (and thus protected). We show that the ICLP, under mild conditions, is also in C(T). Theorem 8 Let C : T × T → R be a symmetric, positive definite, bivariate function with compact domain T. If C is α-H¨older continuous in each coordinate, i.e. there exists a positive constant MC, α ∈ (0, 1] s.t. |C (t1, s) − C (t2, s)| ≤ MC |t1 − t2|α then there exists an ICLP, Z, with covariance function C and there exists a modification ˜Z : T × Ω → R of Z that is a continuous process, s.t. 1. ˜Z is sample continuous, i.e. ∀ω ∈ Ω, ˜Zω(t) is continuous w.r.t. t ∈ T; 2. For any t ∈ T, P( ˜Z(t) = Z(t)) = 1. Meaning that there exists a stochastic process in C(T) equally distributed as the ICLP except on a zero-measure set. All of the results of the ICLP mechanism for H in Section 3.2 are now applicable to sum- maries in C(T). Furthermore, the point-wise evaluation is now a measurable operation and thus is protected. We also note that the proof of Theorem 8 is not just a standard theorem from stochastic processes and relies heavily on the structure of the ICLP. 4. Methodology Based on Theorem 6 and 7, to achieve ϵ-DP one would need to guarantee the quantity, fD − fD′ lies in H1,C for any neighboring databases D and D′. This is a challenging task to do directly and it is easier to structure the problem by restricting the individual functional summary residing in H1,C, which automatically leads to fD −fD′ ∈ H1,C. In the rest of this section, we will provide different approaches to construct qualified summaries and derive upper bounds for their global sensitivity. We also apply the ICLP mechanism to achieve privacy protection over several statistical estimation problems. 4.1 Generalized Obtainment of Qualified Summaries IID Laplace: The first strategy utilizes a finite basis expansion (Kokoszka and Reimherr, 2017) and adds IID Laplace noise to each coefficient, which is in the same spirit as all current mechanisms, which rely on finite-dimensional representations. Let {ej}j≥1 be an orthonormal basis in H. Then we approximate the summary using M bases, i.e. ˆfD = M � j=1 fDjej with fDj = ⟨fD, ej⟩. Expanding the functional summaries via a finite basis facilitates dimension reduction so that classic privacy tools can be implemented to sanitize each coefficient. Since the functional summary resides in a finite-dimensional subspace, it naturally satisfies the requirement in 7Theorem 7 (here one takes ϕj = ej and λj = 1) and therefore leads to a finite GS. However, it inherits all the drawbacks from subspace embedding mechanisms. For example, even though the leading components are usually more important for the utility of the estimate, this approach treats all components equally during the privatizing process and thus degrades the utility of the released summary. Additionally, the truncation level M controls the trade- off between variance, bias, and privacy; this approach would force one to either introduce more noise or accept higher bias when more components are required to deal with complex sample trajectories. ℓ1 Regularization: To satisfy fD ∈ H1,C while avoiding a finite-dimensional representa- tion, we consider using a regularization approach with ∥ · ∥1,C as a penalty term. Formally, let L(f, D) : H × D → R be a loss function, consider the following minimization problem, ˆfD = argmin f∈H � L(f, D) + ψ ∥f∥1,Cη � for η ≥ 1, (5) where Cη shares the same eigenfunctions as C while the eigenvalues are raised to λη j and ψ is the regularization parameter. The benefit of using a power of the kernel, Cη, are twofold. First, the space corresponding to ∥ · ∥1,Cη is a subspace of H1,C, guaranteeing that ˆfD ∈ H1,C. Second, it allows more flexibility to control the smoothness of the constructed functional summaries. Late on, we will see even though η = 1 is a natural setting, a slightly over-smoothing summary can be helpful for utility and even make privacy error negligible compared to statistical error. RKHS Regularization: As we will see in Section 4.2, there are some serious drawbacks to using the ∥·∥1,C-norm as the penalty. We, therefore, consider an RKHS approach as our final strategy, which turns out to work quite well in our applications. Formally, for a given η > 1, by the Cauchy-Schwarz inequality, ∥h∥1,C = ∞ � j=1 |hj| � λj = ∞ � j=1 |hj| λ η 2 j λ η−1 j 2 ≤ ∥h∥Cη � trace(Cη−1). Therefore, by taking η > 1 such that Cη−1 is a trace-class operator, we get that ˆfD ∈ H1,C where ˆfD = argmin f∈H � L(f, D) + ψ ∥f∥2 Cη � for η > 1. (6) 4.2 Mean Function Privacy Protection In this part, we consider the problem of protecting the privacy of the mean function sum- mary. Assume X1, X2, · · · , Xn are i.i.d. elements of an arbitrary real separable Hilbert space H with E Xi = µ0 ∈ H. Our goal is to release a differentially private estimator of the true mean function µ0 that satisfies ϵ-DP. When using IID Laplace, one can start with ˆfD = 1 n �n i=1 Xi, which is an unbiased estimator of the mean function µ. While in the other two approaches we use a quadratic loss function in a regularized empirical risk minimization setup with penalty term P(θ) = 8∥θ∥1,Cη or ∥θ∥2 Cη, i.e., ˆµD = argmin θ∈H � 1 n n � i=1 ∥Xi − θ∥2 + ψP(θ) � . (7) In Theorem 9 and Theorem 11, we derive the close form of the estimators and provide their global sensitivity analysis for ℓ1 and RKHS regularization. Theorem 9 (ℓ1 regularization) Let P(θ) = ∥θ∥1,Cηl, then solution of (7) is ˆµl D = ∞ � j=1 sψ,2λ ηl/2 j �� ¯X, ϕj �� ϕj. (8) where sa,b(x) = sgn(|x|) (|x| − a/b)+ is the soft thresholding function with threshold a/b. Furthermore, assume ∥X∥L2 ≤ τ, then for any fixed ψ, there exists an integer Jτ,ψ such that the global sensitivity of ˆµl D satisfies sup D∼D′ ∥ˆµl D − ˆµl D′∥1,C ≤ 2τ n Jτ,ψ � j=1 λ − ηl j 2 , Remark 10 The integer Jτ,ψ := max{j ≥ 1 : τ ≤ ψ/ληl/2 j } indeed can be viewed as a truncation number as the coefficients after Jτ,ϕ will be shrunk to 0, i.e. the summation in (8) is indeed finite. The upper bound for global sensitivity is based on the fact that, in the worst case scenario, the coefficients are not shrunk to zero and thus the soft thresholding adjustments cancel out. Therefore, unfortunately, the ℓ1 regularization doesn’t produce a better sensitivity than the IID Laplace approach while the soft thresholding introduces extra bias into the summary. In the next theorem, we show that the RKHS regularized estimator is indeed infinite- dimensional and has a better sensitivity than the other two. Theorem 11 (RKHS regularization) Let P(θ) = ∥θ∥Cηr , then solution of (7) is ˆµr D = ∞ � j=1 ληr j ληr j + ψ � ¯X, ϕj � ϕj. (9) Furthermore, assume ∥X∥l1 := � j≥1 |⟨X, ϕj⟩| ≤ τ, then the global sensitivity of ˆµr D satisfies sup D∼D′ ∥ˆµr D − ˆµr D′∥1,C ≤ 2τ n sup j    λ ηr− 1 2 j ληr j + ψ.    Remark 12 The coefficients of the RKHS regularized estimator (9) will not be shrunk exactly to zero, and hence one is able to perturb the functional summary with the truly infinite-dimensional ICLP. Besides, other assumptions about the boundness of X can also be used, such as ∥Xi∥ ≤ τ, but they in general produce a substantially larger sensitivity. 9In the next Theorem, we provide a guarantee of the utility of the ICLP mechanism. We demonstrate that with some smoothness assumptions on the true mean function µ0 and non-private estimator ˆµD, the privacy cost, E ∥˜µD − ˆµD∥2 L2, will not dominate the total error, which will be O(n−1) and thus matches the optimal rate without privacy. Therefore, one can restrict the summary into a smoother class such that privacy is gained for “free”. Theorem 13 (Utility Analysis) Assume Xi are i.i.d. observations from X with mean function µ0 and L2-norm bounded by τ, λj ≍ j−2ν. 1. Let ˜µr D be the private summary from RKHS regularization, if ∥µ0∥Cηr < ∞ then E ∥˜µr D − µ0j∥2 L2 = O � ψ− 1 ηr � τ nϵ �2 + 1 n + ψ � . Taking ηr ≥ 1 + (2ν)−1 and � nϵ2 τ 2 �−ηr ≲ ψ ≲ n−1, E ∥˜µr D − ˆµ∥2 L2 = � o(n−1), ηr > 1 + (2ν)−1 O(n−1), ηr = 1 + (2ν)−1 . 2. Let ˜µl D be the private summary from ℓ1 regularization, let Jτ := max{j ≥ 1 : τ ≤ ψ/ληl/2 j }. If ∥µ0∥Cηl < ∞ E ���˜µl D − µ0 ��� 2 L2 = O � J2(r+1) τ � τ nϵ �2 + 1 n + ψ � . Taking ηl ≥ 2(1 + ν−1) and τ − (ηl−1)ν−1 ν+1 (ϵ2n)− ηlν 2(ν+1) ≲ ψ ≲ n−1, E ���˜µl D − ˆµ ��� 2 L2 = � o(n−1), ηl > 2(1 + ν−1) O(n−1), ηl = 2(1 + ν−1) . We note that if ηr ≥ 1+(2ν)−1, the privacy cost will not dominate the mean square error of ˜µr D and if the inequality holds, it is a lower order of n−1 and thus asymptotically negligible. The threshold of ηr depends on the decay rate of the eigenvalues and thus depends on the smoothness of the covariance kernel C. In particular, if C satisfies the Sacks–Ylvisaker conditions (Sacks and Ylvisaker, 1966, 1968, 1970) of order s, then λj ≍ j−2(s+1). For example, setting C equals to the Ornstein–Uhlenbeck covariance function results in ηr = 3/2 and equals to the reproducing kernel of the univariate Sobolev space Wm 2 ([0, 1]) results in ηr = 1 + (2m)−1, see Micchelli and Wahba (1979); Yuan and Cai (2010) for more instances. We also establish a similar utility analysis for ℓ1 regularization, and its mean square error can also reach O(n−1) under similar smoothness assumptions as well. However, RKHS regularization is able to gain “free” privacy with a less smoothness assumption and thus possesses higher utility, which provides a theoretical justification of the superiority of the RKHS method. We will see this reflected later on in Section 6 as well. 104.3 Privacy-Safe Regularization Parameter Selection Determining the regularization parameters in the mechanisms, like the truncation level M (for subspace embedding mechanisms) and ψ (for the ICLP mechanism), is crucial to guarantee a reasonable performance of the private releases. Parameter regularization in statistical modeling has been well studied and Cross Validation (CV), or one of its many variants, is a widely used approach. However, CV focuses on balancing variance and bias in the statistical error, which ignores the trade-off between the privacy cost and the statistical error. To fit CV into the DP framework, Private Cross Validation (PCV) is proposed in Mirshani et al. (2019), which aims to find out the “sweet spot” between privacy cost and statistical error. However, as data-driven approaches, both CV and PCV are not truly privacy-safe since the regularization parameters may contain information about the data. There are some approaches that one can get end-to-end privacy-guaranteed regularization parameters. For example, one can use out-sample public datasets (Zhang et al., 2012) or one can spend extra privacy budget on the tuning process (Chaudhuri et al., 2011; Chaudhuri and Vinterbo, 2013). Since the ICLP mechanism is tied to a kernel, one can obtain privacy-safe regularization parameters by picking kernels whose eigenvalues decay at a polynomial rate and therefore satisfy the conditions in Theorem 13, then the theoretical values for ηl, ηr and ψ in Theorem 13 can be directly used as regularization parameter inputs. We call this approach the plug- in approach. The plug-in approach doesn’t degrade the privacy guarantee, since the plug-in values only rely on sample size, privacy budget, and the noise’s covariance function without further information about the dataset. It also provides theoretical foundations to make sure the overall error will not be dominated by the privacy cost. In practice, the constant for the plug-in value can affect the performance of the ICLP mechanism. In our experiments, we observe that by appropriately normalizing the data trajectories and the trace of the covariance kernel, setting the constant to 1 usually leads to satisfactory performance. In the next section, we will compare the performance of data-driven approaches with plug-in approaches and numerically show that the plug-in approach performs as well as the data- driven approach while being end-to-end privacy safe. 4.4 Beyond Functional Data In this section, we demonstrate how the ICLP mechanism can be applied to more general learning problems where the summary of interest is a function. Kernel Density Estimation: Let D = {x1, · · · , xn} ⊆ T, where T is a compact set over Rd, be an i.i.d. sample from a distribution with density f0. For any given ICLP kernel K, we adopt the RKHS regu- larization by picking the density estimation kernel as Kη with η > 1. For a given d × d symmetric and positive definite bandwidth matrix H, the kernel density estimator under RKHS regularization takes the form of ˆKD(x) = 1 n n � i=1 Kη H (x − xi) = 1 n � det(H) n � i=1 Kη � H− 1 2 (x − xi) � . (10) We now provide the global sensitivity and utility analysis of ˆKD(x) in the following theorem. 11Theorem 14 Suppose Kη(·, ·) is pointwise bounded, then the global sensitivity ∆ of ˆKD(x) in (10) satisfies ∆ = sup D∼D′ ��� ˆKD − ˆKD′ ��� 1,K ≤ 2MK n � det(H) � tr(Kη−1). Furthermore, taking H to be a diagonal matrix with same entry, assume f ′′ 0 is absolutely continuous, � T (f ′′′ 0 (x))2dx < ∞ and � T Kη(x)dx = 1, the risk R satisfies R = E � T � ˜fD(x) − f0(x) �2 dx ≤ O � c1 n2h2d + h4 + c2 nhd � , for some constants c2 and c2. Remark 15 If h is taken to be h ≍ n 1 4+d , then R = O(n− 4 4+d ), which matches the optimal kernel density estimation risk (Wasserman, 2006). The connection between estimating kernel and the noise kernel also appeared in Hall et al. (2013) where they stated that one can achieve (ϵ, δ)-DP by adding a Gaussian process with its covariance function equal to the kernel used in estimation. For privacy-safe bandwidth, h, we can pick h ≍ n 1 4+d to ensure privacy is gained for free. But a private version of “rule of thumb”, see Rao and Scott (1992) and Hall et al. (2013), is also feasible. Regularized Functional Under Empirical Risk Minimization: The functional summaries one desires to release may come from learning algorithms like regularization-based algorithms. In section 4.1, we have proposed using such algorithms to obtain qualified functional summaries. Here, we generalized the approach to broader scenarios like non-parametric regression and classification. Let D = {d1, · · · , dn} be the collection of n samples, where di is a tuple with finite size. Given a loss function L, we consider the following minimization problem, ˆfD = argmin f∈H1,C � 1 n n � i=1 L(di, f) + ψ∥f∥Cη � for some η > 1. (11) When di’s are couples i.e. di = (yi, xi), (11) can be viewed as non-parametric classification (yi’s take discrete value) or regression (yi’s take continuous value) problem. The solution of (11) can be expressed as ˆfD = �n i=1 aiCη(·, di) by the Representer Theorem (Kimeldorf and Wahba, 1971). However, although the theorem provides an elegant solution for (11), it is ill for calculating the global sensitivity as all the elements in the vector (a1, · · · , an) change once we swap one individual in the dataset. In the following theorem, we provide sensitivity analysis for ˆfD under certain regularized conditions. Theorem 16 Suppose ˆfD is the solution of (11) and the loss function L in (11) is an M- admissible loss function (Bousquet and Elisseeff, 2002), then the global sensitivity for ˆfD satisfies ∆ = sup D∼D′ ��� ˆfD − ˆfD′ ��� 1,C ≤ M ψn � sup x Cη(x, x) � tr(Cη−1). 12Remark 17 One can also prove the privacy loss E ∥ ˜fD − ˆfD∥2 L2 is bounded by c1(ψn)−2. We don’t provide utility analysis for this case study as the statistical loss can vary based on different regularization conditions on f0 and is out of the scope of this paper. The application scenario is wide since the upper bound holds for any convex and locally M-admissible loss function and bounded kernel with finite tracer. For example, support vector mechanism with hinge-loss, non-parametric regression with square-loss, and logistic regression with log(1 + x)-loss are potentially applicable learning models. 5. Algorithm and Implementation Based on the definition of the ICLP, the generic implementation of the mechanism can be achieved by Karhunen-Lo´eve expansion. 1. Given any mercer kernel C, obtain its eigenvalues {λj}j≥1 and eigenfunctions{ϕj}j≥1. 2. Generate ICLP noise by Z = � j≥1 � λj 2 Zjϕj where Zj i.i.d. ∼ Lap(1). 3. Calibrate Z to desired privacy level by the global sensitivity ∆ and privacy budget ϵ. However, the summation in generating Z can not be implemented in finite time and usually is terminated at a large integer. Therefore, in practice, we utilize its approximated version Algorithm 1. Algorithm 1: Approximated ICLP mechanism 1 Given the covariance kernel C and K different points (x1, x2, · · · , xK) on the compact domain T , calculate the value of C on the grid expanded by (x1, x2, · · · , xK), i.e. ˆC =    C(x1, x1) · · · C(x1, xK) ... ... ... C(xK, x1) · · · C(xK, xK)    2 Obtain K estimated eigenvalues {ˆλk}K k=1 and eigenfunctions {ˆϕk}K k=1 of ˆC by eigendecomposition. 3 for k in 1,2,· · · , K do 4 Set ˆfDk = ⟨ ˆfD, ˆϕk⟩ and generate Zk from � ˆλk 2 Lap(1) 5 ˜fDk = ˆfDk + σZk where σ = √ 2∆ ϵ 6 Return ˜f = �K k=1 ˜fDkϕj. A natural question about Algorithm 1 is that does all the theoretical analyses still hold if the privacy noise is sampled in a finite approximation manner instead of the “true” infinite sum. The answer is yes as long as the same cutoff K is used both in constructing privacy noise and in expressing the original estimate, which is followed directly from post- processing inequality. However, a key advantage of our theoretical analyses is that the 13privacy guarantees will still hold regardless of what K is used. Another problem regarding the Algorithm 1 is that even though larger K will lead to more accurate estimates of eigenvalues and eigenfunctions, it also increases the computational burden as the algorithm relies on the Karhunen-Lo´eve decomposition. Following, we investigated how different cutoff values, K, will affect computational time by comparing the average computation time for generating 100 ICLPs to 100 Gaussian Processes. We choose the Gaussian Process as the competitor since it is the stochastic process used to achieve (ϵ, δ)-DP for functional data, and sampling Gaussian processes is nothing more than sampling a multivariate Gaussian with covariance ˆC. Theoretically, generating one ICLP and one Gaussian process are both in time complexity O(n3) since both Cholesky and the Eigen decomposition are O(n3). In Table 1, we report the average time to generate 100 ICLPs and 100 GPs under different settings. We found that, generating 100 Gaussian Processes is about 30% to 50% faster than generating 100 ICLPs in practice. Kernel Type K ICLP GP Kernel Type K ICLP GP Exponential 100 0.5676875 0.2734629 Matern(ν = 3 2) 100 0.5710142 0.2706821 200 2.778597 1.683808 200 2.636642 1.617183 500 29.98454 20.90210 500 29.38142 20.56870 Gaussian 100 0.5530248 0.2683786 Matern(ν = 5 2) 100 0.5512221 0.2714773 200 2.617800 1.610193 200 2.618398 1.615180 500 29.26284 20.41770 500 29.29077 20.53389 Table 1: Computation time (in second) for generating 100 ICLPs and GPs under different cutoff value K and covariance kernel C. 6. Numerical Experiments In this section, we numerically evaluate the effectiveness of the ICLP mechanism and some other comparable mechanisms like the IID Laplace and Bernstein mechanism. 6.1 Simulation for Mean Function Protection In this section, we conduct the simulation for the mean function privacy protection problem discussed in Section 4.2. We use the Mat´ern kernel (Cressie and Huang, 1999) as the covariance kernel for the ICLP since its resulting RKHS ties to a particular Sobolev space, allowing us to control the smoothness directly. It takes the form of Cα(s, t) = 1 Γ(ν)2α−1 �√ 2α|t − s| ρ �α Kα �√ 2α|t − s| ρ � where Kα is the modified Bessel function. Denote {λj}j≥1 and {ϕj}j≥1 as its eigenvalues and eigenfunctions respectively. Specifically, we set ρ = 0.1 and α = 1.5 such that λj ≍ j−4. For sample curves, we generate them via Xi(t) = µ0(t) + �100 j=1 j−2Uijϕj(t) where Uij are i.i.d. uniformly from [−2, 2] and t ∈ [0, 1]. We consider four true mean functions scenarios: S-1: µ0(t) = 10t ∗ exp(−t). 14S-2: µ0(t) = 0.3f0.3,0.05(t) + 0.7 ∗ f0.8,0.05(t). S-3: µ0(t) = 0.2 ∗ (f0,0.03(t) + f0.2,0.05(t) + f0.5,0.05(t) − f0.75,0.03(t) + f1,0.03(t)). S-4: µ0(t) = �25 j=1 Rijϕj(t), where Rij i.i.d. ∼ U[−1, 1]. where fa,b is the probability density function of normal distribution with mean a and vari- ance b2. The shape complexity of the curves rises sequentially, for example, S-1 is just a monotonically increasing function, S-2 is a bimodal function, and S-3 and S-4 are many fluc- tuating multimodal functions. For the Bernstein mechanism, the implementation is based on r package diffpriv and we use the sample mean ¯X(t) as the non-private summary by setting the cover size parameter as 20. We measure the performance by mean square er- ror(MSE), i.e. E ∥˜µ − µ0∥2 L2, which it is approximated via Monte-Carlo by generating 1000 private mean functions for each combination of n. 6.1.1 Comparison of PCV and Plug-In In Section 5, we discuss the method for a privacy-safe selection of regularization parameters. Here we demonstrate its effectiveness by comparing it with the PCV method. In ℓ1 and RKHS regularization, we set η and ψplug to be the values in Theorem 13 such that the privacy cost is the same order as the statistical error. For PCV, we obtain ψpcv by 10-fold PCV within the range of [0.1ψplug, 10ψplug]. In the IID Laplace, the plug-in approach is more ambiguous as the plug-in values for truncation level such that ˜µ reaches optimal rate is a collection of integers, i.e. M = {M1, · · · , MK}. We calculate MSE for each M ∈ M and take the smallest MSE as the plug-in result. While for PCV, we consider a wider range of M by adding and subtracting 3 to its maximum and minimum elements. 0 2000 4000 Sample Size n 10 3 10 2 10 1 100 MSE S - 1 0 2000 4000 Sample Size n 10 3 10 2 10 1 100 S - 2 0 2000 4000 Sample Size n 10 3 10 2 10 1 100 S - 3 0 2000 4000 Sample Size n 0.3 1.0 3.0 S - 4 IID Laplace ICLP- 1 ICLP-RKHS pcv plug pcv plug Figure 1: MSE for plug-in and PCV approaches to select regularization parameter ψ under different mechanisms, sample size and true mean functions. The values reported are averages over 100 independent repetitions. One standard error is on the order of graph line width. In Figure 1, we report the MSE for each mechanism for different sample size n with ϵ = 1. For the rather simple µ0 in S-1, the MSE curves of the plug-in almost line up with the PCV ones for IID Laplace, ℓ1, and RKHS regularization. In S-2 and S-3, the ICLP mechanism still has consistent curves between the plug-in and PCV while the plug-in MSE curve of the IID Laplace has a step-down pattern. This pattern is due to the fact that the maximum number in M is determined by the sample size, so with complex curves and small sample 15sizes, the IID Laplace does not have enough components to estimate the mean of the curve well, but as the sample size increases, more available components make the estimate better. In S-4, PCV does better than the plug-in approach for all mechanisms, which is expected since more components are required in IID Laplace and less penalty is required in RKHS regularization and PCV always did so. Since it has been shown that selecting regularization parameters via the plug-in approach has a reasonable and consistent performance to PCV, we use the plug-in approach in the following simulations to be fully privacy safe. 6.1.2 Comparison of Different Mechanisms Under the same settings, we compare the performance of different mechanisms under dif- ferent sample sizes n via MSE. The results are reported in Figure 2. 100 300 1000 3000 Sample Size n 10 3 10 2 10 1 100 101 MSE S - 1 100 300 1000 3000 Sample Size n 10 3 10 2 10 1 100 101 S - 2 100 300 1000 3000 Sample Size n 10 3 10 2 10 1 100 101 S - 3 IID Laplace Bernstein ICLP- 1 ICLP-RKHS 100 300 1000 3000 Sample Size n 0.01 0.10 1.00 S - 4 Figure 2: MSE for different mechanisms under different sample sizes n and true mean functions. The values reported are averages over 100 independent repetitions. One standard error is on the order of graph line width. It is known that the optimal non-private MSE under L2 norm is of order n−1, and it should be a straight line with slope −1 after taking log10. From Figure 2, it can be observed that the ICLP mechanism with the RKHS regularization approach always achieves the best performance under all scenarios. Its MSE curves also decrease in the same pattern as the desired optimal non-private MSE in S-1 to S-3 while in S-4, the MSE decreases slower than expected with small n as the plug-in value will over smooth summaries, but return parallel with the straight line as n increases. On the other hand, the ℓ1 regularization is almost the worse one as expected. The IID Laplace and Bernstein are quite close to the RKHS regularization in S-1, showing their effectiveness in simpler curve scenarios, but they fail to mimic the behaviors of the RKHS regularization where curves’ shapes are more complex i.e. S-2 to S-4. Although the IID Laplace can narrow the gap from the RKHS regularization, this only happens as n increases, and thus its effectiveness can be restrictive when limited samples are available. 6.2 Simulation for Density Kernel Estimator Protection To demonstrate the wide range of application scenarios of the ICLP mechanism, we conduct a simulation on private kernel density estimates. We consider the setting under R and R2 with samples generated from two mixture Gaussian distributions. 161. R setting: xi i.i.d. ∼ 2 � i=1 piN(µi, 0.1; 0, 1), where N(µ, σ; a, b) is a truncated normal distribution over [a, b] and p1 = 0.6, µ1 = 0.3, µ2 = 0.7. 2. R2 setting: xi i.i.d. ∼ 2 � i=1 piN � µi, � 1 0.5 0.5 1 � ; � 5 −5 � , � 5 −5 �� where N(µ, Σ; a, b) is a multivariate truncated normal distribution over [a1, b1] × [a2, b2]. Specifically, we take p1 = 0.6, µ1 = (−3, −3), µ2 = (3, 2). We compare the ICLP mechanism (RKHS regularization), IID Laplace, and the Bernstein mechanism. For the ICLP mechanism and the Bernstein mechanism, we pick multiple smoothing parameters η and lattice number K to demonstrate how they affect private curves and surfaces; while for IID Laplace, we select the truncated number that provides the best fit under PCV criteria. We generate 2000 samples under each scenario and use the Gaussian kernel in R and exponential kernel in R2. We use h ≍ n1/(4+d) where d = 1, 2 to ensure we gain privacy for free and being privacy safe. The results are reported in Figure 3 and Figure 4. First, for the univariate setting, we can see the ICLP mechanism performs similarly to the IID Laplace; a higher η produces less variability in the curves but tends to be over- smooth. The Bernstein mechanism needs over 30 lattice points in the interval to catch the shape of the bimodal curve but results in producing a messy tail at both ends. A lower lattice number produces better tails but fails to catch the bimodal pattern. For R2 we can see by slightly over smoothing, the ICLP mechanism releases private KDEs very close to non-private ones. A smaller η (Figure 4a) is more precise at peaks but will be ”noisy” around lower density regions; while a larger η (Figure 4c) produces smooth lower density regions but causes underestimating at peaks. Figure 4b shows that there is a clear ”sweet point” to tradeoff smoothness and underestimation. The IID Laplace performs similarly to the underestimating ICLP case, but the peaks of private KDE don’t fully line up with the non-private one. The Bernstein mechanism, on the other hand, fails to produce similar surfaces to the non-private estimator even though we increase the number of lattice points. 7. Real Applications This section presents two real data applications of the proposed methods to study releasing functional summaries for different types of functional datasets. 7.1 Application to Medical and Energy Usage Data In this section, we aim to release a private mean function that satisfies ϵ-DP for the fol- lowing two different type datasets: First, we consider medical data of Brain scans Diffusion 170.0 0.2 0.4 0.6 0.8 1.0 0.0 0.5 1.0 1.5 2.0 (a) 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.5 1.0 1.5 2.0 (b) 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.5 1.0 1.5 2.0 (c) 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.5 1.0 1.5 2.0 (d) x 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.5 1.0 1.5 2.0 (e) x 0.0 0.2 0.4 0.6 0.8 1.0 −0.5 0.0 0.5 1.0 1.5 2.0 2.5 (f) Figure 3: Non-private (Blue) and 10 random realization of private (Red) KDE curves. (a) ICLP mechanism with η = 1.25 (b) ICLP mechanism with η = 1.5 (c) IID Laplace and (d)-(f) Bernstein mechanism with K = 10, 30, 50. (a) (b) (c) (d) (e) (f) Figure 4: 3D plot of non-private (Blue) and private (Red) KDE over R2. (a)-(c) ICLP mechanism with η = 1.01, 1.05, 1.2 (d) IID Laplace and (e)-(f) Bernstein mechanism with K = 5, 10. Tensor Imaging (DTI) dataset available in the r package refund. The DTI dataset pro- vides fractional anisotropy (FA) tract profiles for the corpus callosum (CCA) of the right corticospinal tract (RCST) for patients with Multiple Sclerosis and for controls. Specifi- 18cally, we study the CCA data, with 382 patients measured at 93 equally spaced locations of the CCA. Second, we study the Electricity demand in the Adelaide dataset available in the r package fds. The dataset consists of half-hourly electricity demands from Sunday to Saturday in Adelaide between 6/7/1997 and 31/3/2007. Our analysis focuses on Monday specifically, meaning the dataset consists of 508 days measured at 48 equally spaced time points. Producing privacy-enhanced versions of summaries for such type of energy data is meaningful for protecting public energy institutions such as the power grid from hackers’ attacks. To measure the performance of each mechanism, since the true mean function is not available under such settings, we use the expected distance between the release summary, ˜µ, and the non-private sample means, ˆµ, i.e. E ∥˜µ − ˆµ∥2 L2. We consider Mat´ern kernel with α = 1.5 and 2.5 and ρ = 0.1. The expectation is approximated by Monte-Carlo with 1000 generated ˜µ. The results are reported in Table 2 and each data point is an average of 100 replicate experiments. We also visualized private mean estimates for each mechanism in Figure 5. From Figure 2, it can be observed that the expected distance decreases in a similar pattern as the privacy budget increases for both data sets. We can see that the expected distance of the IID Laplace soon stops changing, which shows that most of the errors of the IID Laplace are concentrated on statistical errors. This shows that in order to avoid adding too much noise to the later components, the IID Laplace has to compromise on using fewer leading components, which leads to higher statistical errors. This can also be seen in Figure 5, where the IID Laplace (green one) can only estimate an approximate shape, but fail to get a better shape estimate locally. ℓ1 regularization and Bernstein mechanism also have similar performance patterns and have much worse results for smaller budgets. Finally, the RKHS regularization performs the best among all approaches as its released summaries (blue ones) can estimate the shapes precisely and also have much smaller expected distances than the non-private mean. Eletricity (C1.5) IID Laplace Bernstein l1 RKHS non−private Eletricity (C2.5) IID Laplace Bernstein l1 RKHS non−private DTI (C1.5) IID Laplace Bernstein l1 RKHS non−private DTI (C2.5) IID Laplace Bernstein l1 RKHS non−private Figure 5: Non-private sample mean and private means for different mechanisms with Mat´ern kernel C 3 2 and C 5 2 with ϵ = 1. The curves in light grey indicate the original samples. 19Eletricity Demand Kernel ϵ I.I.D. Laplace Bernstein ℓ1 RKHS C 3 2 1/8 0.28280.1981 2.21101.5374 3.91102.9464 1.37261.2935 1/4 0.17310.0419 0.61400.3859 1.03910.8066 0.34590.2783 1/2 0.14530.0098 0.21170.1276 0.34150.1897 0.08950.0686 1 0.13830.0025 0.11060.0352 0.16490.0728 0.02410.0171 2 0.13660.0007 0.08580.0151 0.12040.0238 0.00800.0044 4 0.13620.0002 0.07930.0074 0.10890.0105 0.00390.0011 C 5 2 1/8 0.28050.1645 2.22791.5880 4.16143.5764 1.42611.3547 1/4 0.17030.0376 0.60530.4227 1.11680.8931 0.34680.2963 1/2 0.14270.0094 0.20920.0954 0.34970.2276 0.09200.0692 1 0.13600.0026 0.11090.0374 0.16390.0655 0.02510.0168 2 0.13420.0006 0.08570.0167 0.11640.0233 0.00890.0049 4 0.13380.0002 0.07940.0072 0.10440.0090 0.00480.0012 DTI(cca) Kernel ϵ I.I.D. Laplace Bernstein ℓ1 RKHS C 3 2 1/8 0.63050.2878 6.24734.2907 4.42453.9617 3.19962.6794 1/4 0.44180.0663 1.61221.0886 1.26911.1059 0.80390.6994 1/2 0.39520.0165 0.49290.3046 0.47010.2522 0.20400.1627 1 0.38360.0045 0.20800.0824 0.27470.0756 0.05550.0369 2 0.38090.0010 0.13610.0285 0.22650.0265 0.01860.0106 4 0.38010.0003 0.11870.0133 0.21430.0133 0.00940.0028 C 5 2 1/8 0.63030.2867 6.21554.8556 4.74534.0892 3.08412.7095 1/4 0.44370.0762 1.62751.1514 1.30490.9318 0.78200.7225 1/2 0.39740.0158 0.49070.3274 0.47930.2431 0.20280.1790 1 0.38590.0040 0.20730.0890 0.27480.0804 0.05810.0395 2 0.38300.0010 0.13660.0287 0.22310.0325 0.02160.0107 4 0.38230.0003 0.11870.0137 0.21020.0126 0.01260.0029 Table 2: Expected distance between release mean function estimate and sample mean for Electricity demand and DTI(cca) data sets. The numbers in the subscript indicate the standard error (×10−3). 7.2 Application to Human Mortality Data Publishing the entire age-at-death distribution in a given country/region usually provides more comprehensive information about human lifespan and health status than publishing crude mortality rates, and a privacy-enhanced version of this distributional type summary ensures that an attacker cannot infer information about individuals/groups in a particular age range. Following, we study how to release privacy-safe mortality distributions across different regions. The mortality data for each region are collected from the United Nation World Population Prospects 2019 Databases (https://population.un.org/wpp/Download), and the data table record the number of deaths for each region and age. We estimate the probability density function for each region and private the estimates via the proposed method and its competitors. The privacy budget is set to be 1. Again, 20we measure the performance between via E ∥ ˜f − ˆf∥2 L2 where ˜f is the private KDE and ˆf is non-private one, and the expectation is approximated via Monte Carlo by 200 private KDEs. The results are reported in Table 3. Again, we visualized the private KDEs for each region and mechanism in Appendix B. From the table, we see the ICLP mechanism (RKHS approach) with η = 1.01 has smaller errors in developing regions while IID Laplace is better in developed regions. This is reasonable as developed regions usually have better medical conditions so that the mortality age concentrates between 70 and 80 and their densities are unimodal, and thus a few leading components/basis are sufficient to represent the density function in these regions. Under this scenario, it is understandable that the IID Laplace is better since the noise needed are added to a few components. On the other hand, the situations are opposite in developing regions, where the infant mortality rates are higher. Therefore, these density functions show a multi-modal pattern and require more components/basis to get a close approximation, and the ICLP mechanism assigns less randomness to the summaries thanks to its heterogeneous variance noise injection procedure. Mechanisms Region RKHS (η = 1.01) RKHS (η = 1.05) I.I.D. Laplace Bernstein (K = 10) Bernstein (K = 20) Eastern Africa 1.68510.507 5.2497.549 5.2527.881 3.82312.243 2.40011.896 Middle Africa 1.1220.451 5.2810.410 4.8702.152 3.9690.313 2.0900.327 Northern Africa 2.3980.680 8.8730.656 3.9911.807 11.3150.546 5.7100.646 Southern Africa 2.4871.491 6.4270.919 2.9901.809 3.9640.666 2.3950.763 Western Africa 1.58813.699 5.7759.216 5.75211.482 4.34715.649 2.60915.493 Central Asia 5.8042.715 11.8291.547 7.3193.739 13.1631.346 8.1161.808 Eastern Asia 3.5514.414 13.4466.872 3.9006.958 15.87814.110 8.1629.175 Southern Asia 2.0402.267 8.6193.737 2.9682.962 10.0448.099 4.9624.888 South-Eastern Asia 2.2591.871 9.4983.565 2.5662.323 10.4288.328 5.1574.778 Western Asia 2.1950.585 8.6890.624 3.7682.005 9.3800.510 4.6400.578 Eastern Europe 3.9903.658 13.5014.924 4.8175.762 15.5429.356 8.4386.414 Northern Europe 7.3181.626 19.0871.187 7.1741.493 22.8461.218 13.3391.304 Southern Europe 7.4570.977 21.0760.931 7.1371.083 27.5000.852 15.8730.927 Western Europe 6.8192.184 19.9592.782 6.4453.417 25.6065.375 14.7603.965 Caribbean 6.4532.890 11.2472.022 5.9813.634 10.7141.580 7.2312.296 Central America 1.6360.548 7.7770.685 1.1460.537 5.6370.480 2.6160.448 South America 2.2442.498 9.4614.096 2.8863.276 9.2946.744 4.7004.333 Northern America 2.2051.734 10.6052.792 1.5372.297 10.0285.546 4.7463.153 Australia/New Zealand 23.52511.299 29.4955.040 22.7828.189 28.3395.053 23.9837.582 Table 3: Expected L2 distance between release KDEs and non-private KDEs for each region with ϵ = 1. The numbers in the subscript indicate the standard error (×10−3). 8. Conclusion In this paper, we proposed a new mechanism, the ICLP mechanism, to achieve ϵ-DP for infinite-dimensional objects. It provides a wide range of output privacy protections with more flexible data assumptions and a more robust noise injection process than current mechanisms. Theorem 6 and 8 establish its feasibility in separable Hilbert spaces and 21spaces of continuous functions. Several approaches are proposed to construct qualified summaries compatible with the ICLP mechanism., along with plug-in parameters selection to guarantee end-to-end protection. In the example of mean function privacy protection, we also show that slightly over-smoothing the summary promotes the trade-off between utility and privacy, and can match the known optimal rate for mean function estimation. There are some limitations of the proposed mechanism and interesting future works. As we show in Section 5, the implementation of the ICLP mechanism relies on the Karhunen- Lo´eve expansion and thus will be computationally expensive. Therefore, a computation approach that doesn’t rely on the Karhunen-Lo´eve expansion is an important future di- rection. Additionally, even though various experiment results show that by appropriately processing the sample trajectories and the ICLP covariance kernel, omitting the constant for plug-in values has led to satisfactory performance, we believe a more careful investigation of the constant can further enhance performance. Acknowledgments and Disclosure of Funding This work was partially supported by the National Science Foundation, NSF SES-1853209. A. Appendix: Main Proofs A.1 Proof of Lemma 5 Proof To show the explicit form of Randon-Nikodym derivative of PD w.r.t. P0, we define an isometry between H and l2 to avoid considering probability measures over H. Given an orthonormal basis {ϕj}∞ j=1 of H, one can define a mapping T : H → l2 by T (f) = {⟨f, ϕj⟩}∞ j=1, and its inverse is T −1({⟨f, ϕj⟩}) = �∞ j=1⟨f, ϕj⟩ϕj. This mapping is an isometry between H and l2 and we can consider the probability measure over l2 rather than over H. For a Laplace r.v.s. X ∼ Lap(µ, b) over R, it induces a probability measure over (R, B), where B is the borel set over R, as γµ,b(dx) = 1 2bexp � −|x − µ| b � dx. Let {(λj, ϕj)}j∈N be the eigencomponents (eigenfunctions and eigenvalues) of C and let λj = 2b2 j. By Existence of Product Measures Theorem (Tao, 2011) and isomorphism mapping T , PD◦T is a unique probability measure defined as γ(fD, C) := �∞ j=1 γfDj,bj over (R∞, B∞) := (�∞ j=1 Rj, �∞ j=1 Bj). We further restrict γ(fD, C) on (l2, σ(l2)) and keep denoting it by γ(fD, C). We now start to prove Lemma 1, by showing the form of Radom-Nikodym derivative of γ(h, C) w.r.t. γ(0, C) is the same as derivative of {PD : D ∈ D} to P0 and takes the form of dPh dP0 (z) = exp � − 1 σ � ∥z − h∥1,C − ∥z∥1,C �� , (12) 22First, we need to show the r.h.s. of the above Equation is well-defined when h ∈ HC. Define HM(z) = M � j=1 |zj − hj| − |zj| bj and H(z) = lim M→∞ HM(z). We need to show there exists a set A with P0(A) = 1 such that H(z) exists and is finite on A. Suppose zj ∼ Lap(0, bj), with some calculation, one has Var (HM(z)) = M � j=1 Var �|zj − hj| − |zj| bj � = M � j=1 � 1 − exp � −hj bj �� = M � j=1 h2 j b2 j , where the last equality is by Taylor expansion. By Fatou’s Lemma and condition h ∈ HC, Var(H(z)) < ∞. The set A is Ω and therefore with P0-measure 1. Therefore, once h ∈ HC, then r.h.s. of Equation exists and is well-defined. Next, we aim to prove that dPh dP0 (z) is the Radon-Nikodym derivative of h + σZ w.r.t. σZ. Let g(x) = exp � − √ σ2 � ∥x − h∥1,C − ∥x∥1,C �� and dP ∗ h(x) = g(x)dP0(x). Therefore, we only need to show that Ph and P ∗ h are the same measure. We finish this proof by showing they have the same moment generating function. MGFPh(t) = EPh exp {⟨X, t⟩H} = ∞ � j=1 � R exp {xjtj} dγhj,bj(xj) = ∞ � j=1 exp {hjtj} 1 − (bjtj)2 . where the second inequality comes from the result that Ph is product measure of γhj,bj. For the moment generate function of P ∗ h, MGFP ∗ h (t) = EP ∗ h exp {⟨X, t⟩H} = EQ g(X)exp {⟨X, t⟩H} = ∞ � j=1 � R exp � − 1 σ �|xj − hj| − |xj| bj � + xjtj � dγ0,bj(xj) = ∞ � j=1 � R exp {xjtj} dγhj,bj(xj) = ∞ � j=1 exp {hjtj} 1 − (bjtj)2 = MGFPh(h). 23Therefore, Ph and P ∗ h are the same measures. A.2 Proof of Theorem 3 Proof To prove the existence of the ICLP in H, we only need to prove E⟨X, X⟩H < ∞, then the proof can be done by Fubini’s theorem. Notice E⟨X, X⟩H = E � j≥1 � k≥1 � λj � λkZjZk⟨ϕj, ϕk⟩H = E ∞ � j=1 λjZ2 j Since �∞ j=1 E λjZ2 j = �∞ j=1 λj < ∞, by Fubini’s theorem, E⟨X, X⟩H < ∞, which proves the existence of X. A.3 Proof of Theorem 4 Proof By the fact that T is an isomorphism mapping between H and l2, to prove PD and PD′ are equivalent, it’s sufficient to prove γ(fD, C) and γ(fD′, C) are equivalent. We now formally prove γ(h, C) and γ(0, C) iff h ∈ HC. For “if” part, for two infinite product measures, we can apply Kakutani’s theorem (Kakutani, 1948). Then the two measure are equivalent if ∞ � j=1 log � R � γj(hj, bj) γj(0, bj) γj(0, bj)(dx). A few calculation leads to the target space is H∗ C =   h ∈ H : � j≥1 �|hj| 2bj − log(1 + |hj| 2bj ) � < ∞    , and we now prove that, H∗ C = HC. We only need to prove that for a non-negative sequence {aj}j≥1, the series � j≥1[an − log(1 + an)] converges if and only if � j≥1 a2 n converges. Let f(x) be the Taylor expansion of x − log(1 + x) and g(x) = x2. Besides, note that limn→∞ f(an) = 0 if and only if limn→∞ an = 0. Thus, • If � j≥1[an − log(1 + an)] < ∞, then limn→∞ f(an) = 0 and so limn→∞ an = 0. Then, lim n→∞ f(an) g(an) = lim n→∞ an − log(1 + an) a2n = 1 2 therefore by limit comparison test � j≥1 a2 n converges too. • If � j≥1 a2 n < ∞, by the same statement as above also holds and therefore � j≥1[an − log(1 + an)] < ∞. For the “only if” part, the argument is the same as proof of Theorem 2 in Reimherr and Awan (2019). 24A.4 Proof of Theorem 6 Proof We prove the theorem via contradiction. Assume if fD ∈ HC\H1,C, for any given fixed ϵ, ∃ σ ∈ R such that mechanism fD + σZ still satisfy ϵ-DP, then by post-process property of differential privacy, we know that for any transformation G : H → H, G(fD) is also ϵ-DP. Now, ∀ J ∈ N, consider G to be a projection mapping into first J components, i.e. GJ(fD) = �J j=1⟨fD, ϕj⟩ϕj. Therefore, by assumption, ∀ J ∈ N, GJ(fD) is ϵ-DP, i.e. exp   − √ 2 σ J � j=1 � |⟨fD − z, ϕj⟩| √ 2bj − |⟨z, ϕj⟩| √ 2bj �   ≤ exp {ϵ} , except for z ∈ A where A is zero-measure set. Define Bj = {zj : |zj| > |fj|} and SJ = � z ∈ l2 : zj ∈ Bj, ∀1 ≤ j ≤ J and zj ∈ R, ∀j > J � . Then ∀ z ∈ SJ, exp   − √ 2 σ J � j=1 � |⟨fD − z, ϕj⟩| √ 2bj − |⟨z, ϕj⟩| √ 2bj �   = exp    √ 2 σ J � j=1 � |⟨fD, ϕj⟩| √ 2bj �   ≤ exp {ϵ} . However, since h ∈ HK\H1,C, one can always find an J s.t. exp � 1 σ �J j=1 |⟨fD,ϕj⟩| bj � > exp {ϵ} and therefore contradiction holds and no such σ ∈ R+ exists. The remaining thing will be to prove SJ is not a zero-measure set. By Existence of Product Measure in Tao (2011), γ0,C(SJ) = J � j=1 γ0, √ 2bj(Bj) where r.h.s. greater than 0 by definition of Bj. A.5 Proof of Theorem 7 Proof By Lemma 1, the density of ˜fD w.r.t. to σZ is dPD dP0 (z) = exp � − 1 σ � ∥z − fD∥1,C − ∥z∥1,C �� . We aim to show that for any measurable subset A ⊆ H, one has PD(A) ≤ eϵPD′(A), which is equivalent to show PD(A) = � A dPD(x) = � A dPD dPD′ (x)dPD′(x) ≤ eϵ � A dPD′(x). 25Then dPD dPD′ (x) = dPD dP0 (x)/dPD′ dP0 (x) = exp � − 1 σ � ∥x − fD∥1,C − ∥x − fD′∥1,C �� ≤ exp � 1 σ ∥fD′ − fD∥1,C � . Recall the global sensitivity for the ICLP mechanism is ∆ = sup D∼D′ ∥fD − fD′∥1,C, therefore, ∀x ∈ H, dPD dPD′ (x) ≤ eϵ and PD(A) = � A dPD dPD′ (x)dPD′(x) ≤ eϵ � A dPD′(x) holds. A.6 Proof of Theorem 8 Proof Z(t) − Z(s) = � λ1/2 j Zj(ϕj(t) − ϕj(s)) So E[exp{t(Z(t) − Z(s))}] = ∞ � j=1 1 1 − t2λj(ϕj(t) − ϕj(s))2 = exp   − ∞ � j=1 log � 1 − t2λj 2 (ϕj(t) − ϕj(s))2 �   with t satisfies 0 ≤ t2λj(ϕj(t) − ϕj(s))2 < 1, ∀ j. Notice λj(ϕj(t) − ϕj(s))2 = λj⟨Ct − Cs, ϕj⟩2 C ≤ ⟨Ct − Cs, Ct − Cs⟩C = C(t, t) − 2C(t, s) + C(s, s) ≤ 2MC|t − s|α, MC be the H¨older-continuous constant, and this leads to 0 ≤ t ≤ ( 1 MC ) 1 2 |t − s|− α 2 . Now notice that for 0 ≤ x < 1 and f(x) = − log(1 − x) we have f′(x) = 1/(1 − x) and so − log(1 − x) = f(x) = f(0) + xf′(ζ) = x 1 − ζ ≤ x 1 − x, 26for some ζ ∈ (0, x). So we have then − log(1 − t2λj 2 (ϕj(t) − ϕj(s))2) ≤ t2λj 2 (ϕj(t) − ϕj(s))2 1 − t2λj 2 (ϕj(t) − ϕj(s))2 ≤ t2λj 2 (ϕj(t) − ϕj(s))2 max k � (1 − t2λk 2 (ϕk(t) − ϕk(s))2)−1 � . Again, choose t such that (1 − t2λk 2 (ϕk(t) − ϕk(s))2)−1 ≤ M0 < ∞ so that we get E[exp{t(Z(t) − Z(s))}] ≤ exp � M0 t2 2 � λj(ϕj(t) − ϕj(s))2 � = exp �M0t2 2 (C(t, t) − 2C(t, s) + C(s, s)) � ≤ exp � M0MCt2|t − s|α� . Therefore, by Chernoff bound, we got P (|X(s) − X(t)| ≥ a) ≤ E[exp{t(Z(t) − Z(s))}] exp{ta} ≤ exp � M0MCt2|t − s|α − ta � . The minimizer of r.h.s. with respect to t is t0 = a 2M0MC|t−s|α , with restriction of t, we get a ≤ 2M0|t − s| 1 2 α. We consider the following two cases: Case 1 : Suppose a ≤ 2M0|t − s| 1 2 α, then the minimizer is t0 = a 2M0MC|t−s|α , then P (|X(s) − X(t)| ≥ a) ≤ exp � − ˜ M1a2|t − s|−α� , for some generic constant ˜ M1 taking a(x) = C|x|β with β ≥ 1 2α, then ∞ � n=1 a(2−n) = ∞ � n=1 2−nβ and ∞ � n=1 2n exp{− ˜ M2|2n|α−2β}, The first series converges if β < 1. However, to make the second one converges, we need α > 2β which leads to α > α, contradiction. Case 2 : Suppose a > 2M0|t − s| 1 2 α, then the minimizer is t0 = ( 1 MC ) 1 2 |t − s|− 1 2 α, then r.h.s. = exp � M0 − � 1 MC � 1 2 |t − s|− 1 2 αa � . Therefore, for we pick function a(x) = 2M0|x|β > 2M0|x| 1 2 α, with β ∈ (0, 1 2α), then �∞ j=1 a(2−n) < ∞ and ∞ � j=1 2nb(2−n) = ∞ � j=1 2n exp � M0 − � 1 MC � 1 2 |2−n|− 1 2 αa(2−n) � = ∞ � j=1 2n exp � M0 − � 1 MC � 1 2 |2n| 1 2 α−β � 27by the range of β, we have �∞ j=1 2nb(2−n) < ∞. Then the proof is completed by Kolo- mogorov theorem. A.7 Proof of Theorem 9 Proof To obtain the close form of ℓ1-regularized estimator, we expand Xi − θ by the eigenfunctions ϕj, i.e. 1 n n � i=1 ∥Xi − θ∥2 + ψ∥θ∥1,Cηl = 1 n n � i=1 ������ ∞ � j=1 ⟨Xi − θ, ϕj⟩ϕj ������ 2 + ψ∥θ∥1,Cηl = 1 n n � i=1 ∞ � j=1 ⟨Xi − θ, ϕj⟩2 + ψ∥θ∥1,Cηl = ∞ � j=1 � 1 n n � i=1 (Xij − θj)2 + ψ |θj| ληl/2 j � (13) Solving the minimization problem with in the bracket, then for each j, ˆθj = sgn( ¯Xj − ψ 2ληl/2 j ) � ¯Xj − ψ 2ληl/2 j �+ . Then ˆµl D = ∞ � j=1 ˆθj = ∞ � j=1 sψ,2λ ηl/2 j �� ¯X, ϕj �� ϕj.ϕj For the global sensitivity, sup D∼D′ ∥ˆµl D − ˆµl D′∥1,C = sup D∼D′ Jτ � j=1 |sψ,λ ηl/2 j �� ¯XD, ϕj �� − sψ,λ ηl/2 j �� ¯XD′, ϕj �� | λ ηl 2 j ≤ sup D∼D′ Jτ � j=1 | � ¯XD − ¯XD′, ϕj � | λ ηl j2 ≤ 2τ n Jτ � j=1 1 λ ηl j2 , where the first inequality is based on the fact that, in the worst case, the j-th coefficients based on D and D′ will not be shrunk to 0 simultaneously and thus should have the same sensitivity without soft-threshold function. A.8 Proof of Theorem 11 Proof Recall the object function F(θ) = 1 n n � i=1 ∥Xi − θ∥2 + ψ∥θ∥2 Cηr . 28and after dropping everything not involving θ, we have F(θ) = −2⟨ ¯X, θ⟩H + ⟨θ, θ⟩H + ψ⟨θ, θ⟩Cηr = −2⟨ ¯X, Cηrθ⟩Cηr + ⟨θ, Cηrθ⟩Cηr + ψ⟨θ, θ⟩Cηr . The second equality is based on Hilbert space’s own dual, i.e ⟨·, ·⟩H = ⟨·, C(·)⟩HC. Thus the minimizer of the F(θ) is ˆµr D = (Cηr + ψI)−1 Cηr( ¯X) = ∞ � j=1 ληr j ληr j + ψ � ¯X, ϕj � H ϕj where the second equality follow by expansion ˆµr D under the eigenfunction ϕj. For the global sensitivity, the upper bound for supD∼D′ ∥ˆµr D − ˆµr D′∥1,C is sup D∼D′ ∥ˆµD − ˆµD′∥1,C = sup D∼D′ ∞ � j=1 λ ηr− 1 2 j ληr j + ψ ��� ¯X − ¯X′, ϕj ��� ≤ sup D∼D′ sup j    λ ηr− 1 2 j ληr j + ψ    ∞ � j=1 ��� ¯X − ¯X′, ϕj ��� ≤ sup D∼D′    1 n sup j    λ ηr− 1 2 j ληr j + ψ    ∞ � j=1 ��� � Xn − X ′ n, ϕj ����    ≤ 2τ n sup j    λ ηr− 1 2 j ληr j + ψ    . A.9 Proof of Theorem 13 Proof For RKHS regularization: Recall that the form of ˆµr D and its global sensitivity, for privacy cost: E ∥˜µ − ˆµ∥2 L2 = 2∆2 ϵ2 ∞ � j=1 λj ≲  sup j λ ηr− 1 2 j ληr j + ψ   2 1 (nϵ)2 . Consider f(x) = xηr−0.5 xηr+ψ , and observe that f(x) ≲ ψ− 1 2ηr , then, E ∥˜µ − ˆµ∥2 L2 ≲ n−2ψ− 1 η2 . 29For Statistical Error: The n−1 part comes from variance while for bias, ∥ E ˆµ − µ0∥2 L2 = ∞ � j=1 � ψ ληr j + ψ �2 ⟨µ0, ϕj⟩2 ≤ ψ∥µ0∥2 Cηr ≲ ψ, where the last inequality is by assuming ∥µ0∥Cηr < ∞. Combining privacy cost and statis- tical error, one get the desired results. For ℓ1 regularization: Consider privacy cost, let Jτ := max{j ≥ 1 : τ ≤ ψ/2ληl/2 j }, then E ∥˜µ − ˆµ∥2 L2 = 4τ 2 n2ϵ2 ∆2 Jτ � j=1 λj ≲ (ϵn)−2   Jτ � j=1 jν   2 Jτ � j=1 j−2ν ≲ (ϵn)−2 Jmax{3,2(ν+1)} τ . As we assume the noise kernel with finite trace, then ν > 1 2 and E ∥˜µ − ˆµ∥2 L2 ≲ (ϵn)−2 J2(ν+1) τ . Next, we turn to Statistical Error. Define µ0,ψ = �∞ j=1 f ψ,2λ ηl2 j (⟨µ0, ϕj⟩) ϕj, by triangular inequality E ∥ˆµ − µ0∥2 L2 ≤ E ∥ˆµ − µ0,ψ∥2 L2 + ∥µ0,ψ − µ0∥2 L2 . For the bias term, let A = � j : |µj| ≥ ψ 2λ ηl/2 j � , then ∥µ0,ψ − µ0∥2 L2 = � A � µ0j − f ψ,2λ ηl2 j (µ0j) �2 + � Ac � µ0j − f ψ,2λ ηl2 j (µ0j) �2 . Starting with the summation over A, since λ − ηl2 j 2 < |µ0j| ψ we have � A � µ0j − f ψ,2λ ηl2 j (µ0j) �2 = � A ψ2 4ληl j ≤ ψ � A |µ0j| 2λ ηl j2 ≤ ψ 2 ∥µ0∥1,Cηl . Turning to summation over Ac, since |µ0j| ≤ ψ 2λ ηl2 j , � Ac � µ0j − f ψ,2λ ηl2 j (µ0j) �2 = � Ac µ2 0j ≤ ψ � Ac |µ0j| 2λ ηl j2 ≤ ψ 2 ∥µ0∥1,Cηl . Therefore, the overall bias is bounded by ∥µ0,ψ − µ0∥2 L2 ≤ ψ 2 ∥µ0∥1,Cηl . 30Now consider variance term E ∥ˆµ − µ0,ψ∥2 L2, ∥ˆµ − µ0∥2 L2 = ∞ � j=1 � f ψ,2λ ηl2 j � ¯Xj � − f ψ,2λ ηl2 j (µ0j) �2 . Similar to bias part, the summation can be decomposed to sum of four disjoint pieces A0,0 = {| ¯Xj| ≤ ψ/2λ ηl 2 j , |µj| ≤ ψ/2λ ηl 2 j }, A0,1 = {| ¯Xj| ≤ ψ/2λ ηl 2 j , |µj| > ψ/2λ ηl 2 j }, A1,0 = {| ¯Xj| > ψ/2λ ηl 2 j , |µj| ≤ ψ/2λ ηl 2 j }, A1,1 = {| ¯Xj| > ψ/2λ ηl 2 j , |µj| > ψ/2λ ηl 2 j }. When j ∈ A0,0, the summation is zero. Consider j ∈ A0,1, since | ¯Xj| ≤ ψ 2λ ηl2 j , � f ψ,2λ ηl2 j � ¯Xj � − f ψ,2λ ηl2 j (µ0j) �2 = � f ψ,2λ ηl2 j (µ0j) �2 =  µ0j − sgn(µ0j) ψ 2λ ηl 2 j   2 ≤ � µ0j − ¯Xj �2 . By symmetry, we get the same bound over A1,0. So lastly we consider summation over A1,1 For j ∈ A1,1 we have � f ψ,2λ ηl2 j � ¯Xj � − f ψ,2λ ηl2 j (µ0j) �2 =  µ0j − ¯Xj − � sgn(µ0j) − sgn( ¯Xj) � ψ 2λ ηl 2 j   2 . If both µ0j and ¯Xj have the same sign, then this is just (µ0j − ¯Xj)2. If they have opposite signs, then we have ������ � sgn(µ0j) − sgn( ¯Xj) � ψ 2λ ηl 2 j ������ ≤ ��µ0j − ¯Xj �� . Therefore, � f ψ,2λ ηl2 j � ¯Xj � − f ψ,2λ ηl2 j (µ0j) �2 ≤ 4 � µ0j − ¯Xj �2 , for j ∈ A1,1. Finally, the overall variance term is bounded by E ∥ˆµ − µ0∥2 L2 ≤ 4 E �� ¯X − µ0 ��2 L2 ≤ 4 n E ∥X1∥2 L2 ≲ n−1. 31A.10 Proof of Theorem 14 Proof Recall that the exact form of the kernel density estimator is ˆKD(x) = 1 n � det(H) n � i=1 Kη � H− 1 2 (x − xi) � . Then by the definition of global sensitivity, ∆ = sup D∼D′ ��� ˆKD − ˆKD′ ��� 1,K ≤ 1 n � det(H) ���Kη(H− 1 2 xn) − Kη(H− 1 2 x ′ n) ��� Kη � tr(Kη−1) ≤ 1 n � det(H) � tr(Kη−1) � 2 � Kη(0) − Kη(H− 1 2 (xn − x ′ n)) � ≤ 2MK n � det(H) � tr(Kη−1). The first inequality is based on the Cauchy–Schwarz inequality, which is also used in deriving the RKHS regularization approach. The last inequality holds by the assumption that Kη(·, ·) is pointwise bounded. Turning to the utility, taking H to be a diagonal matrix with same entry, then assump- tions stated in the Theorem 14 and by the Theorem 6.28 in Wasserman (2006), the risk R satisfies R = E � T � ˜fD(x) − f0(x) �2 dx ≤ 2 ∗ � E � T � ˜fD(x) − ˆfD(x) �2 dx + E � T � ˆfD(x) − f0(x) �2 dx � ≤ O � c1 n2h2d + h4 + c2 nhd � . for some constants c1 and c2. A.11 Proof of Theorem 16 Proof Recall while deriving the RKHS regularization approach, for a given η > 1 s.t. tr(Cη−1) is finite, we have ∥h∥1,C ≤ ∥h∥Cη � trace(Cη−1), substituting h by ˆfD − ˆfD′ leads to ∥ ˆfD − ˆfD′∥1,C ≤ ��� ˆfD − ˆfD′ ��� Cη � trace(Cη−1), meaning that we need to found the upper bound for ∥ ˆfD − ˆfD′∥Cη. First, let t ∈ [0, 1], δD′,D = ˆfD′ − ˆfD and LD(f) = 1 n �n i=1 Ldi,f. Notice that ˆfD′ and ˆfD are the minimizers of (7), we have LD � ˆfD � + ψ ��� ˆfD ��� 2 Cη ≤ LD � ˆfD + tδD′,D � + ψ ��� ˆfD + tδD′,D ��� 2 Cη , 32and LD � ˆfD′ � + ψ ��� ˆfD′ ��� 2 Cη ≤ LD � ˆfD′ − tδD′,D � + ψ ��� ˆfD′ − tδD′,D ��� 2 Cη . Combining the two inequalities above, LD � ˆfD � −LD � ˆfD + tδD′,D � + LD � ˆfD′ � − LD � ˆfD′ − tδD′,D � ≤ ψ ���� ˆfD + tδD′,D ��� 2 Cη − ��� ˆfD ��� 2 Cη + ��� ˆfD′ − tδD′,D ��� 2 Cη − ��� ˆfD′ ��� 2 Cη � Then using the same proof techniques in Section 4.3 of Hall et al. (2013), we have ∥ ˆfD − ˆfD′∥Cη ≤ M ψn � sup x Cη(x, x), which completes the proof. B. Appendix: Additional Results for Numerical Experiments B.1 Results for Different ICLP Covariance Kernel We also conduct the simulations under different ICLP covariance kernels. We set α = 5/2, such that the corresponding RKHS of this kernel is tied to W 3 2 ([0, 1]) and thus λj ≍ j−6, i.e. ν = 3. We repeat the comparison between plug-in and PCV and the experiments that compare different mechanisms under different n. The results are reported in Figure 6. From the figure, it can be observed that the results based on C 5 2 are almost the same as the results based on C 3 2 . B.2 Visualization of Age-at-Death KDE We present the visualization of the comparison between non-private KDE and private KDEs for different mechanisms in Figure 7. 330 2000 4000 Sample Size n 10 3 10 2 10 1 100 MSE S - 1 0 2000 4000 Sample Size n 10 3 10 2 10 1 100 S - 2 0 2000 4000 Sample Size n 10 3 10 2 10 1 100 S - 3 0 2000 4000 Sample Size n 0.3 1.0 3.0 S - 4 IID Laplace ICLP- 1 ICLP-RKHS pcv plug pcv plug 100 300 1000 3000 Sample Size n 10 3 10 2 10 1 100 101 MSE S - 1 100 300 1000 3000 Sample Size n 10 3 10 2 10 1 100 101 S - 2 100 300 1000 3000 Sample Size n 10 3 10 2 10 1 100 101 S - 3 IID Laplace Bernstein ICLP- 1 ICLP-RKHS 100 300 1000 3000 Sample Size n 0.01 0.10 1.00 S - 4 Figure 6: Plugin and PCV comparison (Top) and MSE for different mechanism (Bottom) for Mat´ern Kernel with α = 5 2 and ρ = 0.1. References F. Alda and B. I. Rubinstein. The bernstein mechanism: Function release under differential privacy. In Thirty-First AAAI Conference on Artificial Intelligence, 2017. J. Awan and A. Slavkovi´c. Structure and sensitivity in differential privacy: Comparing k-norm mechanisms. Journal of the American Statistical Association, 116(534):935–954, 2021. J. Awan, A. Kenney, M. Reimherr, and A. Slavkovi´c. Benefits and pitfalls of the exponen- tial mechanism with applications to hilbert spaces and functional pca. In International Conference on Machine Learning, pages 374–384. PMLR, 2019. V. I. Bogachev. Gaussian measures. Number 62. American Mathematical Soc., 1998. D. Bosq. Linear processes in function spaces: theory and applications, volume 149. Springer Science & Business Media, 2000. O. Bousquet and A. Elisseeff. Stability and generalization. The Journal of Machine Learning Research, 2:499–526, 2002. K. Chandrasekaran, J. Thaler, J. Ullman, and A. Wan. Faster private release of marginals on small databases. In Proceedings of the 5th conference on Innovations in theoretical computer science, pages 387–402, 2014. 340 20 40 60 80 100 0.000 0.010 Eastern Africa 0 20 40 60 80 100 0.000 0.010 0.020 Middle Africa 0 20 40 60 80 100 0.000 0.010 0.020 Northern Africa 0 20 40 60 80 100 0.000 0.010 Southern Africa 0 20 40 60 80 100 0.000 0.010 0.020 Western Africa 0 20 40 60 80 100 0.000 0.010 0.020 Central Asia 0 20 40 60 80 100 0.000 0.015 Eastern Asia 0 20 40 60 80 100 0.005 0.015 Southern Asia 0 20 40 60 80 100 0.005 0.015 South−Eastern Asia 0 20 40 60 80 100 0.000 0.010 0.020 Western Asia 0 20 40 60 80 100 0.000 0.015 0.030 Eastern Europe 0 20 40 60 80 100 0.000 0.015 0.030 Northern Europe 0 20 40 60 80 100 0.00 0.02 Southern Europe 0 20 40 60 80 100 0.00 0.02 Western Europe 0 20 40 60 80 100 0.000 0.010 0.020 Caribbean 0 20 40 60 80 100 0.000 0.010 0.020 Central America 0 20 40 60 80 100 0.000 0.010 0.020 South America 0 20 40 60 80 100 0.000 0.015 Northen America non−private KDE RKHS(1.01) RKHS(1.05) IID Laplace Bernstein Figure 7: Non-private and private kernel density estimates of age-at-death density in dif- ferent regions under different mechanisms with ϵ = 1. 35K. Chaudhuri and S. A. Vinterbo. A stability-based validation procedure for differentially private machine learning. Advances in Neural Information Processing Systems, 26:2652– 2660, 2013. K. Chaudhuri, C. Monteleoni, and A. D. Sarwate. Differentially private empirical risk minimization. Journal of Machine Learning Research, 12(3), 2011. N. Cressie and H.-C. Huang. Classes of nonseparable, spatio-temporal stationary covariance functions. Journal of the American Statistical association, 94(448):1330–1339, 1999. C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis. In Theory of cryptography conference, pages 265–284. Springer, 2006. C. Dwork, A. Roth, et al. The algorithmic foundations of differential privacy. Found. Trends Theor. Comput. Sci., 9(3-4):211–407, 2014. F. Ferraty and Y. Romain. The Oxford handbook of functional data analaysis. Oxford University Press, 2011. R. Hall, A. Rinaldo, and L. Wasserman. Differential privacy for functions and functional data. The Journal of Machine Learning Research, 14(1):703–727, 2013. M. Hardt and K. Talwar. On the geometry of differential privacy. In Proceedings of the forty-second ACM symposium on Theory of computing, pages 705–714, 2010. S. Kakutani. On equivalence of infinite product measures. Annals of Mathematics, pages 214–224, 1948. G. Kimeldorf and G. Wahba. Some results on tchebycheffian spline functions. Journal of mathematical analysis and applications, 33(1):82–95, 1971. P. Kokoszka and M. Reimherr. Introduction to functional data analysis. Chapman and Hall/CRC, 2017. D. Kosambi. Statistics in function space. In DD Kosambi, pages 115–123. Springer, 2016. F. McSherry and K. Talwar. Mechanism design via differential privacy. In 48th Annual IEEE Symposium on Foundations of Computer Science (FOCS’07), pages 94–103. IEEE, 2007. C. A. Micchelli and G. Wahba. Design problems for optimal surface interpolation. Technical report, WISCONSIN UNIV-MADISON DEPT OF STATISTICS, 1979. A. Mirshani, M. Reimherr, and A. Slavkovi´c. Formal privacy for functional data with gaussian perturbations. In International Conference on Machine Learning, pages 4595– 4604. PMLR, 2019. N. Phan, M. Vu, Y. Liu, R. Jin, D. Dou, X. Wu, and M. T. Thai. Heterogeneous gaussian mechanism: Preserving differential privacy in deep learning with provable robustness. arXiv preprint arXiv:1906.01444, 2019. 36J. Ramsay, J. Ramsay, B. Silverman, et al. Functional Data Analysis. Springer Science & Business Media, 2005. J. Rao and A. Scott. A simple method for the analysis of clustered binary data. Biometrics, pages 577–585, 1992. M. Reimherr and J. Awan. Elliptical perturbations for differential privacy. arXiv preprint arXiv:1905.09420, 2019. J. Sacks and D. Ylvisaker. Designs for regression problems with correlated errors. The Annals of Mathematical Statistics, 37(1):66–89, 1966. J. Sacks and D. Ylvisaker. Designs for regression problems with correlated errors: many parameters. The Annals of Mathematical Statistics, 39(1):49–69, 1968. J. Sacks and D. Ylvisaker. Designs for regression problems with correlated errors iii. The Annals of Mathematical Statistics, 41(6):2057–2074, 1970. T. Tao. An introduction to measure theory, volume 126. American Mathematical Society Providence, RI, 2011. Z. Wang, K. Fan, J. Zhang, and L. Wang. Efficient algorithm for privately releasing smooth queries. In NIPS, pages 782–790. Citeseer, 2013. L. Wasserman. All of nonparametric statistics. Springer Science & Business Media, 2006. L. Wasserman and S. Zhou. A statistical framework for differential privacy. Journal of the American Statistical Association, 105(489):375–389, 2010. M. Yuan and T. T. Cai. A reproducing kernel hilbert space approach to functional linear regression. The Annals of Statistics, 38(6):3412–3444, 2010. J. Zhang, Z. Zhang, X. Xiao, Y. Yang, and M. Winslett. Functional mechanism: regression analysis under differential privacy. arXiv preprint arXiv:1208.0219, 2012. 37
Astronomy & Astrophysics manuscript no. main ©ESO 2023 September 4, 2023 Bayesian deep learning for cosmic volumes with modified gravity J.E. García-Farieta,1, 2 ⋆, Héctor J Hortúa3, 4 ⋆⋆ and Francisco-Shu Kitaura1, 2 1 Instituto de Astrofísica de Canarias, s/n, E-38205, La Laguna, Tenerife, Spain e-mail: jorge.farieta@iac.es 2 Departamento de Astrofísica, Universidad de La Laguna, E-38206, La Laguna, Tenerife, Spain 3 Grupo Signos, Departamento de Matemáticas, Universidad El Bosque, Bogotá, Colombia 4 Maestría en Ciencia de Datos, Universidad Escuela Colombiana de Ingeniería Julio Garavito, Bogotá, Colombia Received September 15, 1996; accepted March 16, 1997 ABSTRACT Context. The new generation of galaxy surveys will provide unprecedented data allowing us to test gravity deviations at cosmological scales at a much higher precision than achievable previously. A robust cosmological analysis of the large-scale structure demands exploiting the nonlinear information encoded in the cosmic web. Machine Learning techniques provide such tools, however, do not provide a priori assessment of uncertainties. Aims. This study aims at extracting cosmological parameters from modified gravity (MG) simulations through deep neural networks endowed with uncertainty estimations. Methods. We implement Bayesian neural networks (BNNs) with an enriched approximate posterior distribution considering two cases: one with a single Bayesian last layer (BLL), and another one with Bayesian layers at all levels (FullB). We train both BNNs with real-space density fields and power-spectra from a suite of 2000 dark matter only particle mesh N-body simulations including modified gravity models relying on MG-PICOLA covering 256 h−1 Mpc side cubical volumes with 1283 particles. Results. BNNs excel in accurately predicting parameters for Ωm and σ8 and their respective correlation with the MG parameter. Furthermore, we find out that BNNs yield well-calibrated uncertainty estimates overcoming the over- and under-estimation issues in traditional neural networks. We observe that the presence of MG parameter leads to a significant degeneracy with σ8 being one of the possible explanations of the poor MG predictions. Ignoring MG, we obtain a deviation of the relative errors in Ωm and σ8 by at least 30%. Moreover, we report consistent results from the density field and power spectra analysis, and comparable results between BLL and FullB experiments which permits us to save computing time by a factor of two. This work contributes in setting the path to extract cosmological parameters from complete small cosmic volumes towards the highly nonlinear regime. Key words. cosmology: – large-scale structure of Universe - cosmological parameters; methods: data analysis - statistical - numerical 1. Introduction Cosmic acceleration is one of the most critical concerns in modern cosmology. In the context of the concordance model ΛCDM (Λ-Cold Dark Matter), this acceleration is attributed to the existence of a fluid with negative pressure that is represented by the cosmological constant, Λ, in general relativity (GR) equations. However, the existence of such fluid introduces some conceptual and theoretical issues that have not been fully ad- dressed, either observational or theoretical. Alternative theories, such as modified gravity models, have attracted attention as a natural explanation for cosmic acceleration without invoking a cosmological constant (see, e.g., Nojiri et al. 2017, for a recent review). Among the plethora of alternative models, some parametrizations of f(R) gravity have gained popularity due to their ability to reproduce the standard model’s predictions accu- rately. Indeed, both cosmological scenarios, ΛCDM and f(R), are highly successful in providing an accurate description of the Universe on large scales, from cosmic microwave background (CMB) observations to the data of galaxy clustering (Berti et al. 2015). Unlike the standard scenario, the f(R) models do not require a cosmological constant but instead modify the behavior of ⋆ jorge.farieta@iac.es ⋆⋆ hhortuao@unbosque.edu.co gravity itself. The modification of Einstein’s general relativity involves the addition of a scalar field that emulates cosmic acceleration. This feature of f(R) models has made them perfect templates for tracking departures from standard gravity. Consequently, a crucial task within the scope of precision cosmology is to quantify the potential variations of gravity using appropriate techniques that are sensitive to modified gravity effects. Some of the approaches to achieve this aim include utilizing clustering anisotropies (Jennings et al. 2012; García-Farieta et al. 2019; Hernández-Aguayo et al. 2019; García-Farieta et al. 2020), tracer bias and sample selection (García-Farieta et al. 2021), cosmic voids (Voivodic et al. 2017; Perico et al. 2019; Contarini et al. 2021), halo mass functions (Hagstotz et al. 2019; Gupta et al. 2022) and peculiar velocities (Johnson et al. 2016; Ivarsen et al. 2016; Lyall et al. 2023). Matter distribution is a rich source of cosmological in- formation that has been exploited for many years through various techniques. One of the most used techniques to extract information from the large-scale structure (LSS) data, relies on the two-point statistics as described by the two-point cor- relation function or its equivalent in Fourier space, the matter power spectrum. Despite its success in capturing all possible cosmological information contained in a density field, it fails to capture features affected by the non-Gaussian nature of density perturbations, and its accuracy and precision cannot be relied Article number, page 1 of 13 arXiv:2309.00612v1 [astro-ph.CO] 1 Sep 2023A&A proofs: manuscript no. main upon for probing small angular scales. Since the estimators up to second order do not contain all cosmological information, other techniques beyond the two-point statistics have been studied to extract the additional information such are N-point correlation functions (Peebles 2001; Takada & Jain 2003; Zhang et al. 2022; Brown et al. 2022; Veropalumbo et al. 2022; Philcox et al. 2022), Minkowski functionals (Kratochvil et al. 2012; Hikage et al. 2003; Fang et al. 2017), peak count statistics (Kacprzak et al. 2016; Peel et al. 2017; Fluri et al. 2018; Harnois-Déraps et al. 2021), density split statistics (Paillas et al. 2021), cosmic shear (Kilbinger 2015; Van Waerbeke et al. 2001), cosmic voids Cai et al. (2015); Bos et al. (2012); Hamaus et al. (2016); Lavaux & Wandelt (2010), and tomo- graphic analysis based on the Alcock-Paczynski test (Park & Kim 2010; Zhang et al. 2019; Li et al. 2015; Luo et al. 2019; Dong et al. 2023). For a overview of contemporary cosmological probes, refer to Weinberg et al. (2013) and Moresco et al. (2022). Recently, Deep Neural Networks (DNNs) have been pro- posed as a new alternative for not only recollecting the three- dimensional (3D) density field information without specifying beforehand the summary statistic such as the power spectrum but also, for managing the demanding computational needs in astrophysics (Dvorkin et al. 2022). The CNN algorithms have been explored as a valuable tool in modified gravity scenar- ios, mainly with applications in weak lensing maps as: emu- lator building (Tamosiunas et al. 2021) as well as to investi- gate observational degeneracies between modified gravity mod- els and massive neutrinos (Merten et al. 2019; Peel et al. 2019), CMB patch maps analysis (Hortúa et al. 2020b), N-body sim- ulations (Lazanu 2021; de Oliveira et al. 2020; Kodi Ramanah et al. 2020), but also Bayesian Neural Networks (BNNs) have been employed to identify and classify power spectra that devi- ate from ΛCDM such modified gravity models (Mancarella et al. 2022). Despite their capability for extracting information from complex data, standard DNNs still suffer from overfit- ting/memorizing noisy labels during the training phase, and their point estimations are not always reliable. Bayesian Neural Net- works (BNNs) are extensions from those DNNs that provide probabilistic properties on their outcomes and yield predictive uncertainties. These BNNs employ Variational Inference (VI) to infer posterior distributions for the weights suitable to cap- ture uncertainties related to the network outputs (Graves 2011; Gunapati et al. 2022). Although VI speeds the computation of the posterior distribution when analytic approaches are consid- ered, these assumptions can also introduce a bias (Charnock et al. 2022) that yields overconfident uncertainty predictions and significant deviations from the true posterior. In Hortúa et al. (2020a) and Hortua (2021), the authors added normaliz- ing flows on top of BNNs to give the joint parameter distribu- tion more flexibility. However, that approach is not implemented into the Bayesian framework, still preserving the bias. In a re- cent work (Hortúa et al. 2023), the authors improved the previ- ous methodology by applying multiplicative normalizing flows, resulting in accurate uncertainty estimates. In this paper, we fol- low the same approach by building BNNs models adapted to both 3D-density field and its power spectra to constrain mod- ified gravity from cosmological simulations. We show that in the solely case of non-Gaussian signals it is possible to improve the posterior distributions and that when the additional informa- tion from the power spectrum is considered, they yield more sig- nificant performance improvements without underestimating the posterior distributions. This paper is organized as follows. Sec- tion 2 offers a summary of structure formation in modified grav- ity cosmologies and the reference simulations created for train- ing and testing the BNNs. In section 3 we briefly introduce the BNN concept and section 4 shows the architectures and config- uration used in the paper. The results are presented in section 5 and an extended discussion of the findings is presented in section 6. Conclusions are given in section 7. 2. Large-scale structure in modified gravity In this section, we present the gravity model which coincides with ΛCDM in the limiting case of a vanishing f(R) parameter introduced below. 2.1. Structure formation and background In f(R) cosmologies, the dynamics of matter is determined by the modified Einstein field equations. The most straightforward modification of GR that circumvents Λ emerge by including a function of the curvature scalar in the Einstein-Hilbert action. In this modification, the equations of motion are enriched with a term that depends on the curvature scalar and that creates the same effect as dark energy (for a review on different MG mod- els see e.g. Tsujikawa et al. 2008; De Felice & Tsujikawa 2010). For consistency across various cosmological scales, Hu & Saw- icki (2007, hereafter HS) proposed a viable f(R) function that is able to satisfy tight constraints at solar system scales, as well as accurately describe the dynamics of the ΛCDM background. In these models the modified Einstein-Hilbert action is given by S EH = � d4x √−g �R + f(R) 16πG � , (1) where g is the metric tensor, G the Newton’s gravitational con- stant, R the curvature scalar and f(R) a scalar function that con- strains the additional degree of freedom. In the HS model such function takes the form f(R) = −m2 c1 � −R/m2�n c2 �−R/m2�n + 1 , (2) where n, c1, c2 are model parameters, and m2 ≡ ΩmH2 0, with Ωm being the present fractional matter density and H0 the Hubble parameter at present time. For n = 1, which is the f(R) model we will consider in this paper the function can be written as follows f(R) ≈ −2Λ + | fR0|R2 0 R . (3) Here fR0 represents the dimensionless scalar field at present time, meaning the only additional degree of freedom that stems from the derivative of f(R) with respect to the curvature scalar, fR. The modified Einstein field equations and analogous Friedmann equations that describe the HS model’s background can be ob- tained from minimizing the action (for a thorough derivation see Song et al. 2007). To further understand the formation and evo- lution of large-scale structures in MG, it is crucial to describe the matter perturbations, δm, around the background (see Song et al. 2007). The MG effects are captured by the growth of den- sity perturbations in the matter dominated era when mildly non- linear regime is important (Laszlo & Bean 2008). In particular, when considering linear perturbations, the equations of the evo- lution of matter overdensities in Fourier space are as follows Article number, page 2 of 13Author: García-Farieta, Hortúa & Kitaura (Tsujikawa 2008): ¨δm + � 2H + ˙F 2F � ˙δm − ρm 2F δm = 1 2F �� −6H2 + k2 a2 � δF + 3Hδ ˙F + 3δ ¨F � , δ ¨F + 3Hδ ˙F + �k2 a2 + F 3FR − 4H2 − 2 ˙H � δF = 1 3δρm + ˙F˙δm , (4) with H being the Hubble parameter, k the comoving wavenum- ber of the perturbations, a the scale factor, ρm the matter den- sity field and F ≡ ∂ f/∂R. The solution to system of Eqs. (4) provides a detail description of δm, which includes most of the cosmological information, since it is a direct result of the grav- itational interaction of cosmic structures. In fact, to get insights into the underlying cosmic parameters, the density field is the primary source to be investigated using summary statistics. The Eqs. (4) make evident the connection between the density field and the scalaron of MG. Consequently, any departure from the GR would be measurable through the density field, either with the structure growth rate or its tracer distribution. A particular feature of the f(R) models is the so-called chameleon mecha- nism. This mechanism reconciles the departures of GR with the bounds imposed by local tests of gravity. It endows the mass of the scalar field with the ability to depend on the local matter density. More precisely, the signatures of MG can be detected in regions of lower matter density where the scalar field becomes lighter, leading to potential observable effects that deviate from standard gravity. The 3D matter power spectrum, denoted as P(k), is the pri- mary statistical tool employed to extract cosmological insights from the density field. It characterizes the overdensities as a function of scale and is estimated through the following average over Fourier space: (2π)3P(k)δ3 D �k − k′� = �δ(k)δ �k′�� , (5) where δ3 D is the three-dimensional Dirac-delta function. This function contains all information from the statistics of the den- sity field in the linear regime, and a decreasing fraction of the to- tal information on smaller scales, if initial density fields followed Gaussian statistics. In this work we use the Pylians31 library to estimate the overdensity field as well as the power spectrum. 2.2. Modified gravity simulations The simulations were created with the COmoving Lagrangian Acceleration (COLA) algorithm (Tassev et al. 2013; Koda et al. 2016), which is based on a Particle-Mesh code that solves the equations of motion following the Lagrangian Perturbation The- ory (LPT) trajectories of the particles. This algorithm speed up the computation of the gravitational force using a very few timesteps and still get correct results on the largest scales. In particular we used MG-PICOLA2 (Winther et al. 2017), a modified version of L-PICOLA (Howlett et al. 2015) that has been exten- sively tested against full N-body simulations and that extends the gravity solvers to MG models, including the HS parametrization. We run a set of 2500 MG simulations varying four cosmological 1 https://pylians3.readthedocs.io/en/master/index.html 2 The code can be found at https://github.com/HAWinther/ MG-PICOLA-PUBLIC Table 1. The summary of the set-up of the MG simulations. Left: cos- mology parameters. Right: set-up parameters used for MG-PICOLA code. Cosmologies Simulation setup Ωm [0.1, 0.5] Boxsize 256 h−1 Mpc h [0.5, 0.9] Np 1283 σ8 [0.6, 1.0] Grid force 1283 0.1 log10 | fR0| [0.4, 0.6] IC 2LPT zini = 49 Ωb 0.0489 Steps 100 ns 0.9665 kNy 1.58 Fig. 1. Diagram illustrating the multidimensional parameter space vari- ations. Each line represents a data point’s parameter values, with four parameters {Ωm, h, σ8, 0.1 log10 |fR0|} visualized along separate axes. Fig. 2. The projected overdensity field at redshift z = 0 derived from an arbitrary chosen simulation within the ensemble of 2500 MG simu- lations. The normalized density field was calculated using a CIC mass assignment scheme. parameters Θ = {Ωm, h, σ8, fR0}, where h is the reduced Hub- ble parameter, σ8 the r.m.s. density fluctuation within a top-hat sphere of 8 h−1 Mpc radius and fR0 the amplitude of the modi- fied gravity function in the HS model. The remaining cosmolog- ical parameters are set to Ωb = 0.048206 and ns = 0.96, which correspond to the values reported by Planck Collaboration et al. Article number, page 3 of 13A&A proofs: manuscript no. main Fig. 3. Projected density field of dark matter in a region of 256 × 256 × 20 (h−1Mpc)3 from 100 out of 2500 simulations of MG arbitrarily chosen. The snapshots are taken at z = 0 and the legend displays the set of cosmological parameters to be {Ωm, h, σ8, fR0}. The cuts in the density field highlight the broad coverage of the parameter space of the MG simulations. Different features can be observed by naked-eye, such as variations in the filamentary structure of the cosmic web. (2020). The parameter space is sampled with random numbers uniformly distributed within the specified ranges for each param- eter (see Table 1). Since the typical values of the modified gravity parameter goes as powers of ten, | fR0| ∼ 10n with n ∈ [−4, −6], we choose to sample a fraction of its logarithm in order to cover the range of powers equally, i.e., �fR0 = 0.1 log10 | fR0|. Figure 1 illustrates the parameter space variations of the 2500 MG cos- mologies, each one is represented by a gray line. The values of the cosmological parameters, Θ, are distributed along the differ- ent vertical axis. Each simulation follows the dynamics of 1283 particles in a small box of comoving side-length 256 h−1 Mpc using 100 timesteps from an initial redshift zi = 49 up to red- shift z = 0. This simulation resolution allows us to reasonably Article number, page 4 of 13Author: García-Farieta, Hortúa & Kitaura investigate the impacts of MG on large scales, in particular for the fR0 values considered in this work. However, it is not as ef- fective at very small scales, where higher resolution is required. In fact, MG solvers have undergone extensive testing using low- resolution simulations (see, for example, Puchwein et al. (2013); Li et al. (2012); Hernández-Aguayo et al. (2022)). These tests show the enhancement of the power spectrum in simulations of 256 h−1 Mpc, where MG effects begins to be appreciable. The setup of the MG simulations used in this work is summarized in Table 1. In a recent research, a similar setup was employed with light-weight deterministic CNN to estimate a subset of parame- ters of a flat ΛCDM cosmology (Pan et al. 2020), however in this work we choose a time-step larger by a factor of 2.5. The initial conditions for the MG simulations were created with 2LPTic (Crocce et al. 2006, 2012) based on a ΛCDM template at zi, moreover, a distinct random seed was assigned to each simula- tion to generate varying distributions of large-scale power. This approach allows our neural network to effectively capture the in- herent cosmic variance. We calculate the overdensity field, δm, for each snapshot at redshift z = 0, employing the cloud-in-cell (CIC) mass assig- ment scheme (Hockney & Eastwood 1981) on a regular grid con- sisting of N3 = 1283 voxels. The training set comprises 80% of the data, which corresponds to 2000 boxes containing the over- density fields, while the remaining 20% of the data was used for testing. Fig. 2 displays the 3D overdensity field plus the unity, δm + 1 = ρm/¯ρm, projected along each plane of the box. The displayed data corresponds to an arbitrarily chosen combination of parameters within the MG simulation suite at redshift z = 0. Similarly, Fig. 3 displays the 2D density field of dark matter in a region of 256×256×20 (h−1Mpc)3 from 100 out of 2500 simula- tions of MG arbitrarily chosen, with the cosmological parameter combination as indicated by the labels. The cuts in the density field provides a visual means to discern distinct features of the cosmic web, observable to the naked eye. These features include variations in the filamentary structure of the cosmic web, which become evident in the zones of under- and over-densities. Addi- tionally, we output the matter power spectrum of all realizations by directly computing the modulus of each Fourier mode from the particle distribution, |δm(k)|2. The Fig. 4 shows the different matter power spectrum for the entire MG simulation suit. The variations in the shape of the spectrum correspond to the joint effect of cosmological parameters that were varied as shown in the label. We consider the effective range of the power spectrum up to the Nyquist frequency, kNy, which in our simulations cor- responds to k ≈ 1.58 Mpc/h. The full datasets used in this pa- per, 3D overdensity fields as well as power spectra, are publicly available in Zenodo3. 3. Bayesian Neural Networks The primary goal of Bayesian Neural Networks (BNNs) is to estimate the posterior distribution p(w|D), which represents the probability distribution of the weights w of the network given the observed data D = (X, Y) (Abdar et al. 2021; Gal 2016; Graves 2011). The posterior distribution, denoted as p(w|D), can be de- rived using Bayes’ law: p(w|D) ∼ p(D|w)p(w). This expres- sion involves a likelihood function, p(D|w), which represents the probability of the observed data D given the weights w, as well as a prior distribution on the weights, denoted as p(w). After the computation of the posterior, the probability distribution of a 3 Data will be available upon publication. Please contact the authors for earlier access Fig. 4. The matter power spectrum at z = 0 of the MG simulation suit. The variations in the spectrum correspond to changes in each of the four parameters that were varied, {Ωm, h, σ8, |0.1 log fR0|}. The correspond- ing range of each of parameter is shown in the label. new test example x∗ can be determined by p(y∗|x∗, D) = � w p(y∗|x∗, w)p(w|D)dw , (6) being p(y∗|x∗, w) the predictive distribution corresponding to the set of weights. In the context of neural networks, it is important to note that the direct computation of the posterior is not fea- sible (Gal 2016). To circumvent this limitation, variational in- ference (VI) techniques approximating the posterior distribution have been introduced (Graves 2011). VI considers a family of simple distributions, denoted as q(w|θ), which is characterized by a parameter θ. The objective of VI is to identify a distribution q(w|θ∗) that minimizes the Kullback-Leibler divergence between q(w|θ) and p(w|D), where θ∗ represents the optimal parameter values, being KL[·∥·] the Kullback-Leibler divergence. This min- imization is equivalent to maximizing the evidence lower bound (ELBO) (Gal 2016), ELBO(θ) = Eq(w|θ) � log p(Y|X, w)� − KL �q(w|θ) ���p(w)� , (7) where Eq(w|θ)[log p(Y|X, w)] is the expected log-likelihood with respect to the variational posterior and KL[q(w|θ)||p(w)] is the divergence of the variational posterior from the prior. It can be observed from Eq. (7) that the Kullback-Leibler (KL) divergence serves as a regularizer, compelling the variational posterior to shift towards the modes of the prior. A frequently employed op- tion for the variational posterior entails utilizing a product of in- dependent Gaussian distributions, specifically mean-field Gaus- sian distributions, with each parameter w being associated with its own distribution (Abdar et al. 2021) q(w|θ) = � i j N(w; µi j, σ2 i j) , (8) where i and j are the indices of the neurons from the previous- and the current layers, respectively. Applying the reparametriza- tion trick we obtain wi j = µi j + σi j ∗ ϵi j, where ϵi j is drawn from the normal distribution. Moreover, if the prior is a composition of independent Gaussian distributions, the KL-divergence between Article number, page 5 of 13A&A proofs: manuscript no. main the prior and the variational posterior can be calculated analyt- ically. This characteristic enhances the computing efficiency of this approach. 3.1. Multiplicative normalizing flows Gaussian mean-field distributions described in Eq. (8) are the most commonly utilized family for the variational posterior in BNNs. Unfortunately, this distribution lacks the capacity to ade- quately represent the intricate nature of the true posterior. Hence, it is anticipated that enhancing the complexity of the variational posterior will yield substantial improvements in performance. This is attributed to the capability of sampling from a more re- liable distribution, which closely approximates the true poste- rior distribution. Indeed, the process of improving the variational posterior demands efficient computational methods while ensur- ing its numerical feasibility. Multiplicative Normalizing Flows (MNFs) have been proposed to efficiently adapt the posterior dis- tributions through the utilization of auxiliary random variables and the normalizing flows (Louizos & Welling 2017). Mixture normalizing flows (MNFs) suggest that the variational posterior can be mathematically represented as an infinite mixture of dis- tributions q(w|θ) = � q(w|z, θ)q(z|θ)dz , (9) with θ the learnable posterior parameter, and z ∼ q(z|θ) ≡ q(z)4 the vector with the same dimension of the input layer, which plays the role of an auxiliary latent variable. Also, allowing for local reparametrizations, the variational posterior for fully con- nected layers becomes w ∼ q(w|z) = � ij N(w; ziµij, σ2 ij) . (10) Where we can increase the flexibility of the variational pos- terior by enhancing the complexity of q(z). This can be done using normalizing flows since the dimensionality of z is much lower compared to the weights. Starting from samples z0 ∼ q(z0) from fully factorized Gaussians (see Eq. (8)), a rich distribu- tion q(zK) can be obtained by applying successively invertible fk-transformations zK = NF(z0) = fK ◦ · · · ◦ f1(z0) , (11) log q(zK) = log q(z0) − K � k=1 log �����det ∂fk ∂zk−1 ����� . (12) To handle the intractability of the posterior, Louizos & Welling (2017) suggest to use again Bayes law q(zK)q(w|zK) = q(w)q(zK|w) and introduce a new auxiliary distribution r(zK|w, ϕ) parameterized by ϕ, with the purpose of approximating the pos- terior distribution of the original variational parameters q(zK|w) to further lower the bound of the KL divergence term. Accord- ingly, the KL-divergence term can be rewritten as follows − KL �q(w) ���p(w)� ≥ Eq(w,zK) � − KL �q(w|zK) ���p(w)� + log q(zK) + log r(zK|w, ϕ) � . (13) 4 For the sake of clarity in notation, the parameter θ will no longer be considered in the subsequent discussion. The first term can be analytically computed since it will be the KL-divergence between two Gaussian distributions, while the second term is computed via the normalizing flow generated by fK (see Eq. (12)). Furthermore, the auxiliary posterior term is parameterized by inverse normalizing flows as follows (Touati et al. 2018) z0 = NF−1(zK) = g−1 1 ◦ · · · ◦ g−1 K (zK) , (14) and log r(zK|w, ϕ) = log r(z0|w, ϕ) + K � k=1 log ������det ∂g−1 k ∂zk ������ , (15) where one can parameterize g−1 K as another normalizing flow. A flexible parametrization of the auxiliary posterior can be given by z0 ∼ r(zK|w, ϕ) = � i N(z0; ˜µi(w, ϕ), ˜σ2 i (w, ϕ)) , (16) where the parameterization of the mean ˜µ, and the variance ˜σ2 is carried out by the masked RealNVP (Dinh et al. 2017) as the choice of normalizing flows. 3.2. Multiplicative normalizing flows in voxel-grid representation In this section we present our first result, where we have gener- alized Eq. (10) to 3D convolutional layers where cosmological simulated data are structured. To this end, we started with the extension of sampling from the variational posterior as w ∼ q(w|z) = Dd � i Dh � j Dw � k Df � l N(w; zlµi jkl, σ2 i jkl) , (17) where Dh, Dw, Dd are three spatial dimensions of the boxes, Algorithm 1 Forward propagation for each Convolutional 3D layer. Mw, Σw are the means and variances of each layer, H is the input layer, and NF(·) is the masked RealNVP normalizing flow. ⊙ corresponds to element-wise multiplication. H ← Input conv3D-layer (minibatch) z0 ∼ q(z0) zT f = NF(z0) Mh = H ∗ (Mw ⊙ reshape(zT f , [1, 1, 1, Df ])) Vh = H2 ∗Σw E ∼ N(0, 1) return Mh + √Vh ⊙ E and Df is the number of filters for each kernel. The objective is to address the challenge of enhancing the adaptability of the approximate posterior distribution for the weight coming from a 3D-convolutional layer. Algorithm 1 outlines the procedure to forward propagation for each 3D-convolutional layer. Similar to the fully connected case, the auxiliary parameter affects only the mean with the purpose of avoiding large variance and we kept a linear mapping to parametrize the inverse normalizing flows instead of applying tanh feature transformations. Article number, page 6 of 13Author: García-Farieta, Hortúa & Kitaura Table 2. Configuration of the (Se)-ResNet backbone used for all exper- iments presented in this paper. (Se)-ResNet-18 backbone Layer Name Input Shape Output Shape Batch Norm (Nbatch, 128,128,128,1) (Nbatch, 128,28,128,1) 3D Convolutional (Nbatch, 128,128,128,1) (Nbatch, 64,64,64,16) Batch Norm+ReLU (Nbatch, 64,64,64,16) (Nbatch, 64,64,64,16) Max Pooling 3D (Nbatch, 64,64,64,16) (Nbatch, 32,32,32,16) Batch Norm+ReLU (Nbatch, 32,32,32,16 ) (Nbatch, 32,32,32,16 ) Resblock 1 � (Nbatch, 32, 32, 32, 16) (Nbatch, 16, 16, 16, 32) � (Nbatch, 16,16,16,32) Batch Norm+ReLU (Nbatch, 16,16,16,32) (Nbatch, 16,16,16,32 ) Resblock 2 � (Nbatch, 16, 16, 16, 32) (Nbatch, 8, 8, 8, 64) � (Nbatch, 8,8,8,64) Batch Norm+ReLU (Nbatch, 8,8,8,64) (Nbatch, 8,8,8,64 ) Resblock 3 � (Nbatch, 8, 8, 8, 64) (Nbatch, 4, 4, 4, 128) � (Nbatch, 4,4,4,128) Batch Norm+ReLU (Nbatch, 4,4,4,128) (Nbatch, 4,4,4,128) Resblock 4 � (Nbatch, 4, 4, 4, 128) (Nbatch, 2, 2, 2, 256) � (Nbatch, 2,2,2,256) Batch Norm+ReLU (Nbatch, 2,2,2,256 ) (Nbatch, 2,2,2,256) Global Avg Pooling (Nbatch, 2,2,2,256) (Nbatch, 256) 4. The Bayesian architecture set-up We will examine four distinct architectures of Bayesian Neu- ral Networks (BNNs) as outlined in Section 3. Two of these architectures include Bayesian layers located only on the top of the network, the so-called Bayesian Last Layer (denoted as BLL), while the remaining have Bayesian layers at all their levels (FullB). The pipelines utilized in our study were de- veloped using TensorFlow v:2.12 and TensorFlow-probability v:0.19 (Abadi et al. 2015). The architecture used for all net- works has ResNet-18 as the backbone, which is depicted in a schematic manner in Table 2. The Resblock nature depends on whether we build either ResNet or SeResNet topology. The lat- ter is a variant of ResNet that employs squeeze-and-excitation blocks that adaptively recalibrate channel-wise feature responses by explicitly modeling interdependencies between channels (Hu et al. 2019). Fig. 5 depicts each Resblock and how the skip con- nections are defined. These architectures were designed using the GIT repository classification-models-3D. ResNet18 contains 2510149 trainable parameters while SeResNet has 3069270 but, these numbers are duplicates when we move towards a fully Bayesian scheme because two parameters need to be optimized (the mean and standard deviation) for each network parameter. In this study, 50 layers were employed for the masked RealNVP normalizing flow. The development of these convolutional lay- ers was made using also the repositories TF-MNF-VBNN and MNF-VBNN (Louizos & Welling 2017). Finally, all networks end with a multivariate Gaussian distribution layer, consisting of fourteen trainable parameters. These parameters include four means, denoted as µ, which correspond to the cosmological pa- rameters, as well as ten elements representing the covariance ma- trix Σ. The loss function to be optimized is given by the ELBO, Eq. (7), where the second term is associated with the negative SeResNet18 ResNet18 Fig. 5. Resblock schema depending on the architecture used. Top: Res- block when SeResNet18 is employed. The orange dashed rectangle shows the skip SE-connection schema used in the SeResNet18 resblock. Bottom: Resblock when ResNet is employed. log-likelihood (NLL) −NLL ∼ 1 2 log |Σ| + 1 2(y − µ)⊤ (Σ)−1 (y − µ) , (18) averaged over the mini-batch. The optimizer used was Adam with first and second moments exponential decay rates of 0.9 and 0.999, respectively (Kingma & Ba 2014). The learning rate starts from 5×10−4 and it is reduced by a factor of 0.9 in case any improvement has not been observed after 8 epochs. Furthermore, we have applied a warm-up period for which the model turns on progressively the KL term in Eq. (7). This is achieved by intro- ducing a β variable in the ELBO, i.e., β · KL �q(w|θ) ���p(w)�, so, this parameter starts being equal to 0 and grows linearly to 1 dur- ing 12 epochs (Sønderby et al. 2016). BNNs were trained with 8 batches and early stopping callback was presented to avoid over- fitting. The infrastructure put in place by Google Cloud Platform (GCP) uses a nvidia-tesla-t4 of 16 GB GDDR6 in a N1 machine series shared-core. 4.1. Quantifying the performance The metrics employed for determining the network performance were the Mean Square Error (MSE), ELBO, and the coefficient of determination r2. Moreover, we quantify the quality of the un- certainty estimates through reliability metrics. Following Laves et al. (2020) and Guo et al. (2017), we can define a perfect cali- bration of regression uncertainty as E ˆσ2 � abs��||y − µ||2 ��� ˆσ2 = α2� − α2�� ∀ � α2 ∈ R ��� α2 ≥ 0 � , (19) being abs[.] the absolute value function. So, the predicted uncer- tainty ˆσ2 is partitioned into K bins with equal width, to variance Article number, page 7 of 13A&A proofs: manuscript no. main per bin is defined as var(Bk) := 1 ���Bk ��� � i∈Bm 1 N N � n=1 �µi,n − yi �2 , (20) with N stochastic forward passes. In addition, the uncertainty per bin is defined as uncert(Bk) := 1 |Bk| � i∈Bk ˆσ2 i , (21) allow us to compute the expected uncertainty calibration error (UCE) in order to quantify the miscalibration UCE := K � k=1 |Bk| m ���var(Bk) − uncert(Bk) ��� , (22) with the number of inputs m and set of indices Bk of inputs, for which the uncertainty falls into the bin k. 5. Results In this section, we present the results of several experiments de- veloped to quantify the performance of Bayesian deep learning neural networks for constraining the cosmological parameters in modified gravity scenarios. 5.1. Parameter estimation from the overdensity field under voxel-grid representation Using the configuration described in Sec. 4, we designed four experiments inspired by two successful deep learning architec- tures, ResNet18 and Se-ResNet18. The former is a residual net- work commonly known due to its efficiency in several computer vision tasks, while the latter has been chosen because of its abil- ity to improve the interdependencies between the channels of convolutional feature layers (Hu et al. 2019). Furthermore, the modification of the models was also based on the insertion of a set of Bayesian layers at either the top of the model (BLL), or in the entire architecture (FullB). The motivation for exploring both possibilities comes from the fact that intuitively, adding a Bayesian layer at the end of the network (BLL), can be viewed as Bayesian linear regression with a learnable projected feature space, allowing for a successful balance between scalability and the degree of model-agnosticism (Fiedler & Lucia 2023; Watson et al. 2021). On the contrary, although fully Bayesian networks (FullB) would demand high computational resources, it has been reported that their Bayesian hidden layers are susceptible to out- of-distribution (OOD) examples that might improve predictive uncertainty estimates (Henning et al. 2021). The results of the experiments performed in this work are summarized in Table 3. Here we can observe the performance of each architecture on the test set. In the top part of the table, the results of SeRes- Net18 topology are shown, while in the bottom part, the results of ResNet18 are presented. The left columns of the table corre- spond to the FullB scheme, while the left one to the Bayesian Last Layer, BLL. Comparing all approaches, we observe that FullB-SeResNet18 slightly outperforms the rest of the models in terms of accuracy (described by r2) and uncertainty quality pro- vided by UCE. However, significant differences were not found in the reported metrics for ResNet and its SeResNet counterpart, except for the inference time where the BLL models clearly out- perform the FullB ones. This brings the idea that FullBs yield small improvements in the computation of uncertainties at the expense of duplicating the inference time. In addition, both ar- chitectures estimate σ8 more efficiently than for any other pa- rameter, especially in contrast to h or 0.1 log10 | fR0|, although the FullBs respond slightly better to MG effects. Fig. 6 displays the scatter relationship between the predicted and ground truth val- ues of each cosmological parameter using FullB-SeResNet18. It also shows the degeneracy directions that arise from obser- vations defined as Ωmh2 and σ8Ω0.25 m . The diagonal gray lines correspond to the ideal case of a perfect parameter prediction. Each data point represents the mean value of the predicted dis- tributions, and the error bars stand for the heteroscedastic un- certainty associated with epistemic plus aleatoric uncertainty at 1-σ confidence level. As we observe, BNNs learn how to accu- rately predict the value for Ωm and σ8, but they fail in capturing information related to the MG effects and the Hubble parame- ter. Even though parameter estimation derives from all features of the fully nonlinear 3D overdensity field, the horizontal scatter pattern that exhibits the Hubble and MG parameters implies that essential underlying connections are not effectively captured. A similar result for the Hubble parameter using DCNNs in ΛCDM can be found in Villaescusa-Navarro et al. (2020). 5.2. Parameter estimation coming from the matter power spectrum In this section, we show the results of using the power spec- trum to extract the cosmological parameters in MG scenarios. Following the same methodology as described in the voxel- grid representation, we implement two BNN models that provide distributed predictions for the cosmological parameters. Table 4 schematically presents the architecture used for this purpose. This represents a Fully Connected Network (FCN) with 60000 trainable parameters, and it was derived from KerasTuner5 as a framework to make scalable hyperparameter optimization. We work with a Bayesian Last Layer model (BLL-FCN) along with a Full Bayesian topology where all dense layers are probabilis- tic (FullB-FCN). Here, the power spectrum computed from the N-body simulations is kept until k ≈ 1.58h−1 Mpc, obtaining ar- rays of 85 dimensions. The results of this approach are shown in Table 5. In contrast to the voxel-grid representation where the Full Bayesian approach outperforms most of the models, here we clearly observe that BLL approach works better than the fully Bayesian one. These results show a similar performance compared to the 3D overdensity field. We expected this behavior since most of the voxel-grid information should be encoded into the two-point correlator. Notice also that some parameters such as σ8 or the derived parameters provide higher accuracy when they are predicted with the voxel-grid approach supporting the fact that 3D-convolutional layer extracts further information be- yond the linear part. The interplay between the fR0 parameter and the shape of the power spectrum is essential for testing and constraining gravity theories. The immediate effect of fR0 on the power spectrum is to modulate its amplitude, most notably at small scales. Furthermore, this parameter of the HS model ex- hibits a substantial degeneracy with σ8, which produces a sim- ilar effect on the power amplitude, but not in a scale-dependent manner as MG does. The strongest deviations of the power spec- trum from the ΛCDM model are observed for high values of fR0, in our case ∼ 10−4 (see Fig. 4). Because of this degeneracy, it is probable that some of the MG information is being encoded in the σ8 parameter rather than the fR0 parameter. This hypothesis, 5 https://keras.io/keras_tuner/ Article number, page 8 of 13Author: García-Farieta, Hortúa & Kitaura Table 3. Metrics for the test set for all BNNs architectures. Top: SeResNet18, bottom: ResNet18. High UCE values indicate miscalibration. Bold text is the minimum (maximum) value, ↓ (↑) as indicated in the metric name, among the different parameters. Metrics FullB-SeResNet18 BLL-SeResNet18 Ωm h σ8 0.1 log10 |fR0| Ωmh2 σ8Ω0.25 m Ωm h σ8 0.1 log10 | fR0| Ωmh2 σ8Ω0.25 m MSE ↓ 0.001 0.01 0.0007 0.003 0.0009 0.0008 0.003 0.013 0.0012 0.0035 0.0009 0.0013 r2 ↑ 0.86 0.15 0.94 0.04 0.85 0.93 0.80 0.03 0.90 0.008 0.85 0.89 UCE ↓ 0.07 0.07 0.08 0.02 0.08 0.12 0.03 0.3 0.08 0.05 0.022 0.15 AV-MSE ↓ 0.0043 0.0051 NLL ↓ -99.21 -3.38 Inf.Time [ms] 397 290 Metrics FullB-ResNet18 BLL-ResNet18 Ωm h σ8 0.1 log10 |fR0| Ωmh2 σ8Ω0.25 m Ωm h σ8 0.1 log10 | fR0| Ωmh2 σ8Ω0.25 m MSE ↓ 0.001 0.01 0.0007 0.003 0.001 0.0008 0.0025 0.012 0.0015 0.003 0.0015 0.001 r2 ↑ 0.86 0.15 0.95 0.04 0.83 0.93 0.82 0.10 0.89 0.05 0.75 0.92 UCE ↓ 0.07 0.09 0.09 0.01 0.08 0.20 0.014 0.078 0.09 0.07 0.024 0.14 AV-MSE ↓ 0.0043 0.0048 NLL ↓ -95.12 -3.34 Inf.Time [ms] 345 262 Fig. 6. True vs Predicted values provided by the FullB model, for Ωm, σ8, and some derivative parameters. Points are the mean of the predicted distributions, and error bars stand for the heteroscedastic uncertainty associated with epistemic and aleatoric uncertainty at 1σ. Table 4. Configuration of the fully connected neural network used for constraining parameters from the power spectrum. Fully connected neural network Layer Name Input Shape Output Shape Dense Layer (Nbatch, 85) (Nbatch, 64) ReLU (Nbatch, 64) (Nbatch, 64) Dense Layer (Nbatch, 64) (Nbatch, 64) ReLU+Batch Norm (Nbatch, 64) (Nbatch, 64) Dense Layer (Nbatch, 64 ) (Nbatch, 64) ReLU (Nbatch, 64) (Nbatch, 64) Dense Layer (Nbatch, 64 ) (Nbatch, 14) Multivariate normal (Nbatch, 14) (Nbatch, 4 ) however, would require additional tests of the BNN with a re- duced parameter space in addition to isolating the impact of the sole case of a zero fR0, which we leave for future work. 5.3. Comparison among approaches based on marginalized parameter constraints Finally, we choose one example from the test set to compare the constrain contours predicted by the best models presented in the paper so far. Fig. 7 compares the parameter constraints at 68% and 95% confidence levels predicted for the FullB-SeResNet18 and FullB-FCN models. The true values of the example are re- ported in Table 6 as well as represented by dashed lines in the triangular plot. Notice that both models yield decent predictions for the marginal distribution, but they differ in the correlation among cosmological parameters, as σ8 and fR0 where this be- Article number, page 9 of 13A&A proofs: manuscript no. main Table 5. Metrics for the power spectra test set with Fully-Connected Networks (FCN). High UCE values indicate miscalibration. MSE and NLL are computed only over the cosmological parameters. Bold text is the minimum (maximum) value, ↓ (↑) as indicated in the metric name, among the different parameters. Metrics FullB-FCN BLL-FCN Ωm h σ8 0.1 log10 | fR0| Ωmh2 σ8Ω0.25 m Ωm h σ8 0.1 log10 | fR0| Ωmh2 σ8Ω0.25 m MSE ↓ 0.0023 0.012 0.0007 0.003 0.0013 0.0011 0.0023 0.011 0.00078 0.0030 0.0012 0.0012 r2 ↑ 0.83 0.11 0.94 0.06 0.77 0.90 0.83 0.16 0.94 0.073 0.80 0.89 UCE ↓ 0.026 0.12 0.022 0.022 0.026 0.092 0.023 0.15 0.017 0.023 0.016 0.10 AV-MSE ↓ 0.0045 0.0043 NLL ↓ 64.86 1.80 Inf.Time [ms] 3.01 2.21 Table 6. Parameters in the 95% intervals taken from the parameter con- straint contours from one example of MG simulations test set predicted by the FullB-SeResnet18 and FullB-FCN. Parameter SeResNet18 FCN Target Ωm 0.36+0.13 −0.13 0.37+0.12 −0.12 0.3865 h 0.69+0.22 −0.21 0.72+0.23 −0.23 0.6274 σ8 0.664+0.081 −0.082 0.667+0.060 −0.060 0.6822 0.1 log10 |fR0| 0.51+0.11 −0.11 0.51+0.13 −0.14 0.5557 σ8Ω0.25 m 0.512+0.081 −0.082 0.519+0.059 −0.059 0.5379 Ωmh2 0.167+0.085 −0.079 0.190+0.096 −0.091 0.1521 Table 7. Relative error comparison among different CNN approaches for MG and ΛCDM simulations. The relative error has been defined as δy ≡ ∆y/y, where y stands for Ωm, σ8 and ∆y is the uncertainty. Method δΩm δσ8 Reference CNN 0.0048 0.0053 Pan et al. (2020) CNN 0.0280 0.0120 Ravanbakhsh et al. (2017) VBNNs 0.2128 0.0545 Hortúa et al. (2023) FlipoutBNN 0.2444 0.0844 Hortúa et al. (2023) SeResNet 0.3611 0.1220 This work FCN 0.3243 0.0900 This work havior is more notorious. It implies clearly that 3D-convolutions extract further information beyond the linear regime that allows to constrain more tightly the parameter estimation. 6. Summary and discussion We consider a wide range of MG simulations, varying their cos- mological parameters, encompassing cosmologies with large de- viations from the standard GR to parameters closest to those that mimic the dynamics of a Universe based on GR. The overdensity field of each snapshot was computed using the CIC mass assign- ment and subsequently, we obtained its power spectrum. To con- strain the main set of cosmological parameters, we introduced a novel architecture of a BNN and designed several experiments to test its ability to predict MG cosmologies. The experiments consist of building two Bayesian networks based on stochastic layers located at either the top or at all levels of the architecture. This approach is motivated by the question of whether BNNs provide better accuracy and robustness performance when we work with full or partial network configurations. Starting from the 3D overdensity field, we found that although the FullB pre- dicts slightly better the cosmological parameters than the BLL, the latter is accurate enough to retrieve cosmological informa- tion from the density field, especially for Ωm and σ8. Similarly, we tested BNNs using the two-point statistics described by the power spectrum for reasonable scales limited by the Nyquist fre- quency. The results of this experiment show that the information learned by the networks can predict the parameters with sim- ilar accuracy to the 3D field. Both configurations of the BNN architectures fall short of capturing both the Hubble parameter and the MG effects. This underscores the necessity of improving the training dataset in terms of resolution and scale for the 3D density setup. Despite the slight constraints for some cosmolog- ical parameters, the methodology can be relevant in applications where it is combined with classical inference methods (Hortúa et al. 2020b). The multiplicative normalizing flows technique in BNNs employed in this paper has proved to bring well predic- tions and accurate uncertainty estimates thanks to the ability to transform the approximate posterior into a more expressive dis- tribution consistent with the data complexity. This is a significant improvement compared to standard VI where the posterior is re- stricted only to a Gaussian configuration. Nevertheless, the effect of assuming a Gaussian prior distribution of the weights under this approach is still unknown (Fortuin et al. 2022). In future work, we will explore Multiplicative normalizing flows with dif- ferent prior distributions over the weights and analyze how the prior influences the uncertainty calibration and performance. The finding that the MG parameter is poorly predicted when using the information provided by the density field demonstrates, on the one hand, the effectiveness of the chameleon screening mechanism in mimicking the ΛCDM model, as well as the need for further analysis with other datasets more sensitive to the ef- fects of MG. It should be noted that in our study we have con- sidered parameters that produce the same effect, i.e., that are de- generate. Therefore, it is not straightforward to attribute a single characteristic of the overdensity field exclusively to a single pa- rameter, as in the case of fR0 and σ8. The proposed architectures are sufficiently general from a statistical standpoint to estimate posterior distributions. However, this study has revealed that the available information is inadequate to predict all parameters solely from a single source. This underscores the significance of resolving degeneracies between cosmological parameters by in- corporating supplementary data or diverse features present in the cosmological simulations. Such an approach enables the BNNs to gain a richer learning phase and parse out the signals of each cosmology. This task will be the focus of a forthcoming paper, where we plan to evaluate the BNNs robustness using simula- tions of higher resolution and more intricate datasets in redshift space, incorporating velocity information alongside particle po- sitions. In Table 7, we also present a comparison of the relative errors for the two best-estimated parameters using CNN and N-body simulations from the literature. We observed significant discrep- ancies in the relative errors of σ8 and Ωm, approximately 90% when Bayesian inference is not employed (see Ravanbakhsh et al. 2017; Pan et al. 2020). This outcome arises from using solely ΛCDM simulations in both training and test datasets, in Article number, page 10 of 13Author: García-Farieta, Hortúa & Kitaura 0.2 0.3 0.4 0.5 0.6 m 0.1 0.2 0.3 mh2 0.4 0.5 0.6 8 0.25 m 0.3 0.4 0.5 0.6 0.7 0.1log10|fR0| 0.6 0.7 0.8 8 0.4 0.6 0.8 1.0 h 0.4 0.6 0.8 1.0 h 0.6 0.7 0.8 8 0.3 0.4 0.5 0.6 0.7 0.1log10|fR0| 0.4 0.5 0.6 8 0.25 m 0.1 0.2 0.3 mh2 FullB-SeResNet18 FullB-FCN Fig. 7. 68% and 95% parameter constraint contours from one example of the test dataset using FullB-SeResNet and Full-FCN. The diagonal plots are the marginalized parameter constraints, and dashed lines stand for the true values reported in Table 6. We derive these posterior distributions using GetDist (Lewis 2019). contrast to our estimates that encompass an additional parameter accounting for MG and include a calibration procedure of the uncertainties. Furthermore, when contrasting the performance of BLL architectures on MG and ΛCDM simulations, such as QUIJOTE (see e.g. Hortúa et al. 2023), we find a deviation of the relative errors close to 30% when modified gravity effects are not considered. This result clarifies that when utilizing FullB- SeResNet18, the errorbars for Ωm are 1.3 times larger and for σ8 are 2.1 times larger in comparison to FlipoutBNN. In the con- text of BNNs, when separately considering the two cosmologi- cal models — MG and ΛCDM — we assess the performance in terms of the MSE metric, comparing it to the results presented by Hortúa et al. (2023), who employed a similar architecture. Specifically, using FullBs in both cosmological models, we ob- serve an improvement by a factor 13 in the MSE of the MG pre- dictions over the ΛCDM ones. The r2 metric was used to com- pare the confidence range of the individual parameters. In terms of this metric, we report that σ8 has a larger deviation (r2 = 0.95 in MG and r2 =0.99 in ΛCDM), which accounts for 4.2% of the expected uncertainty. The marginal difference in the coefficient Article number, page 11 of 13A&A proofs: manuscript no. main of determination for predicting Ωm is only 0.01 when comparing the results of the model trained with MG against the one trained with ΛCDM. In both cases, it is noteworthy that a high r2 value does not necessarily confer complete certainty regarding indi- vidual parameter estimates, particularly when parameter degen- eracy is taken into account. Furthermore, one interesting possi- bility to refine the constraints on f(R) gravity is given by training a specialized network that distinguishes entirely between ΛCDM and f(R), offering the potential to detect a non-zero fR0. Further investigations, including high-resolution simulations as well as extensions beyond ΛCDM, promise to further enhance the ca- pabilities of the BNNs approach. The techniques explored in this work still need further developments to be applied to ob- servational data. The bias and peculiar motions of the tracers and the systematics coming from galaxy surveys still need to be taken into account. They could be nonetheless, potentially, im- plemented in Bayesian inference algorithms of the large-scale structure (such as, e.g., Kitaura et al. 2021). Alongside this pa- per, we make available the scripts which can be accessed at the github repository https://github.com/JavierOrjuela/ Bayesian-Neural-Net-with-MNFs-for-f-R-. 7. Conclusions One of the intriguing possibilities for explaining the observed accelerated expansion of the Universe is the modification of gen- eral relativity on large scales. Matter distribution analysis via N-body simulations offers a perfect scenario for tracking de- partures from standard gravity. Among different parametriza- tions, f(R) has emerged as an interesting model due to its abil- ity to reproduce the standard model’s predictions accurately. In this manuscript, we analyzed the possibility of using Bayesian Deep Learning methods for constraining cosmological param- eters from modified gravity simulations. Below, we summarize the main take-aways from this study: 1. BNNs can predict with higher accuracy cosmological param- eters, especially for Ωm and σ8 from the overdensity field. However, based on the assumption of simulating boxes with 256 h−1 Mpc to acquire MG effects on large scales, BNNs were unable to effectively extract MG patterns from the overdensity field to yield accurate f(R)-parameter estima- tion. However when comparing parameter estimation with ΛCDM-only simulations, we find that there is a consider- able underprediction of the uncertainties of σ8 when possible modified gravity effects are not taken into account. In addi- tion, special attention should be paid to parameter degenera- cies that may be present not only in two-point statistics but in more features of the density field. We conclude that higher resolution and further intricate datasets in redshift space, in- corporating velocity information alongside particle positions can be approaches that should be addressed to improve the network predictions. 2. It is observed that cosmological parameters can be recov- ered directly from the simulations using convolutional-based models with the potential of extracting patterns without spec- ifying any N-point statistics beforehand. This is supported by the fact that networks trained with overdensity fields and power spectra predicted decent predictions but with distinc- tive correlations among the parameters. 3D-convolutions ex- tracted supplementary information beyond the linear regime that allowed them to constrain tightly the parameter estima- tion. 3. We generalized the Multiplicative normalizing flows for BNNs to the 3D convolutional level, allowing us to work with fully transformed stochastic neural networks. As a proof of concept, we ran several experiments in order to verify that this approach not only achieved the performance reached by the deterministic models but also yielded well-calibrated un- certainty estimates. 4. We probed the impact of the parameter estimation based on the Bayesian Last Layer (BLL) and fully Bayesian ap- proaches. The results showed that fullBs provide slightly higher quality predictions along with accurate uncertainty estimates. Nevertheless, this improvement is not significant enough to prefer this approach with respect to the BLL, where the latter has the advantage of being relatively model- agnostic, easily scalable, and 2×inference time faster. Acknowledgements. This paper is based on work supported by the Google Cloud Research Credits program with the award GCP19980904. HH acknowledges support from créditos educación de doctorados nacionales y en el exterior- Colciencias and the grant provided by the Google Cloud Research Credits pro- gram. JEGF is supported by the Spanish Ministry of Universities, through a María Zam- brano grant (program 2021-2023) at Universidad de La Laguna with reference UP2021-022, funded within the European Union-Next Generation EU. FSK and JEGF acknowledge the IAC facilities and the Spanish Ministry of Science and Innovation (MICINMINECO) under project PID2020-120612GB-I00. We also thank the personnel of the Servicios Informáticos Comunes (SIC) of the IAC. References Abadi, M., Agarwal, A., Barham, P., et al. 2015, TensorFlow: Large-Scale Ma- chine Learning on Heterogeneous Systems, software available from tensor- flow.org Abdar, M., Pourpanah, F., Hussain, S., et al. 2021, Information Fusion, 76, 243 Berti, E., Barausse, E., Cardoso, V., et al. 2015, Classical and Quantum Gravity, 32, 243001 Bos, E. G. P., van de Weygaert, R., Dolag, K., & Pettorino, V. 2012, MNRAS, 426, 440 Brown, Z., Mishtaku, G., & Demina, R. 2022, A&A, 667, A129 Cai, Y.-C., Padilla, N., & Li, B. 2015, MNRAS, 451, 1036 Charnock, T., Perreault-Levasseur, L., & Lanusse, F. 2022, Bayesian Neural Net- works (WORLD SCIENTIFIC), 663–713 Contarini, S., Marulli, F., Moscardini, L., et al. 2021, MNRAS, 504, 5021 Crocce, M., Pueblas, S., & Scoccimarro, R. 2006, MNRAS, 373, 369 Crocce, M., Pueblas, S., & Scoccimarro, R. 2012, 2LPTIC: 2nd-order La- grangian Perturbation Theory Initial Conditions, Astrophysics Source Code Library, record ascl:1201.005 De Felice, A. & Tsujikawa, S. 2010, Living Reviews in Relativity, 13, 3 de Oliveira, R. A., Li, Y., Villaescusa-Navarro, F., Ho, S., & Spergel, D. N. 2020, Fast and Accurate Non-Linear Predictions of Universes with Deep Learning Dinh, L., Sohl-Dickstein, J., & Bengio, S. 2017, in International Conference on Learning Representations Dong, F., Park, C., Hong, S. E., et al. 2023, ApJ, 953, 98 Dvorkin, C., Mishra-Sharma, S., Nord, B., et al. 2022, arXiv e-prints, arXiv:2203.08056 Fang, W., Li, B., & Zhao, G.-B. 2017, Phys. Rev. Lett., 118, 181301 Fiedler, F. & Lucia, S. 2023, Improved uncertainty quantification for neural net- works with Bayesian last layer Fluri, J., Kacprzak, T., Sgier, R., Refregier, A., & Amara, A. 2018, J. Cosmology Astropart. Phys., 2018, 051 Fortuin, V., Garriga-Alonso, A., Ober, S. W., et al. 2022, in International Confer- ence on Learning Representations Gal, Y. 2016, PhD thesis, University of Cambridge García-Farieta, J. E., Hellwing, W. A., Gupta, S., & Bilicki, M. 2021, Phys. Rev. D, 103, 103524 García-Farieta, J. E., Marulli, F., Moscardini, L., Veropalumbo, A., & Casas- Miranda, R. A. 2020, MNRAS, 494, 1658 García-Farieta, J. E., Marulli, F., Veropalumbo, A., et al. 2019, MNRAS, 488, 1987 Graves, A., ed. 2011, Practical Variational Inference for Neural Networks, ed. A. Graves, Vol. 24 (Curran Associates, Inc.) Gunapati, G., Jain, A., Srijith, P. K., & Desai, S. 2022, Publications of the Astro- nomical Society of Australia, 39, e001 Guo, C., Pleiss, G., Sun, Y., & Weinberger, K. Q. 2017, in Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML 17 (JMLR.org), 1321–1330 Article number, page 12 of 13Author: García-Farieta, Hortúa & Kitaura Gupta, S., Hellwing, W. A., Bilicki, M., & García-Farieta, J. E. 2022, Phys. Rev. D, 105, 043538 Hagstotz, S., Costanzi, M., Baldi, M., & Weller, J. 2019, MNRAS, 486, 3927 Hamaus, N., Pisani, A., Sutter, P. M., et al. 2016, Phys. Rev. Lett., 117, 091302 Harnois-Déraps, J., Martinet, N., Castro, T., et al. 2021, MNRAS, 506, 1623 Henning, C., D’Angelo, F., & Grewe, B. F. 2021, Are Bayesian neural networks intrinsically good at out-of-distribution detection? Hernández-Aguayo, C., Hou, J., Li, B., Baugh, C. M., & Sánchez, A. G. 2019, MNRAS, 485, 2194 Hernández-Aguayo, C., Ruan, C.-Z., Li, B., et al. 2022, J. Cosmology Astropart. Phys., 2022, 048 Hikage, C., Schmalzing, J., Buchert, T., et al. 2003, PASJ, 55, 911 Hockney, R. W. & Eastwood, J. W. 1981, Computer Simulation Using Particles (crc Press) Hortua, H. J. 2021, arXiv e-prints, arXiv:2112.11865 Hortúa, H. J., García, L., & Castaneda, L. 2023, Front. Astron. Space Sci., 10 Hortúa, H. J., Malagò, L., & Volpi, R. 2020a, Machine Learning: Science and Technology, 1, 035014 Hortúa, H. J., Volpi, R., Marinelli, D., & Malagò, L. 2020b, Physical Review D, 102 Howlett, C., Manera, M., & Percival, W. J. 2015, Astronomy and Computing, 12, 109 Hu, J., Shen, L., Albanie, S., Sun, G., & Wu, E. 2019, Squeeze-and-Excitation Networks Hu, W. & Sawicki, I. 2007, Phys. Rev. D, 76, 064004 Ivarsen, M. F., Bull, P., Llinares, C., & Mota, D. 2016, A&A, 595, A40 Jennings, E., Baugh, C. M., Li, B., Zhao, G.-B., & Koyama, K. 2012, MNRAS, 425, 2128 Johnson, A., Blake, C., Dossett, J., et al. 2016, MNRAS, 458, 2725 Kacprzak, T., Kirk, D., Friedrich, O., et al. 2016, MNRAS, 463, 3653 Kilbinger, M. 2015, Reports on Progress in Physics, 78, 086901 Kingma, D. P. & Ba, J. 2014, arXiv e-prints, arXiv:1412.6980 Kitaura, F.-S., Ata, M., Rodríguez-Torres, S. A., et al. 2021, MNRAS, 502, 3456 Koda, J., Blake, C., Beutler, F., Kazin, E., & Marin, F. 2016, MNRAS, 459, 2118 Kodi Ramanah, D., Charnock, T., Villaescusa-Navarro, F., & Wandelt, B. D. 2020, Monthly Notices of the Royal Astronomical Society, 495, 4227–4236 Kratochvil, J. M., Lim, E. A., Wang, S., et al. 2012, Phys. Rev. D, 85, 103513 Laszlo, I. & Bean, R. 2008, Phys. Rev. D, 77, 024048 Lavaux, G. & Wandelt, B. D. 2010, MNRAS, 403, 1392 Laves, M.-H., Ihler, S., Fast, J. F., Kahrs, L. A., & Ortmaier, T. 2020, in Medical Imaging with Deep Learning Lazanu, A. 2021, Journal of Cosmology and Astroparticle Physics, 2021, 039 Lewis, A. 2019, arXiv e-prints, arXiv:1910.13970 Li, B., Zhao, G.-B., Teyssier, R., & Koyama, K. 2012, J. Cosmology Astropart. Phys., 2012, 051 Li, X.-D., Park, C., Sabiu, C. G., & Kim, J. 2015, MNRAS, 450, 807 Louizos, C. & Welling, M. 2017, in Proceedings of the 34th International Confer- ence on Machine Learning - Volume 70, ICML’17 (JMLR.org), 2218–2227 Luo, X., Wu, Z., Li, M., et al. 2019, ApJ, 887, 125 Lyall, S., Blake, C., Turner, R., Ruggeri, R., & Winther, H. 2023, MNRAS, 518, 5929 Mancarella, M., Kennedy, J., Bose, B., & Lombriser, L. 2022, Phys. Rev. D, 105, 023531 Merten, J., Giocoli, C., Baldi, M., et al. 2019, MNRAS, 487, 104 Moresco, M., Amati, L., Amendola, L., et al. 2022, Living Reviews in Relativity, 25, 6 Nojiri, S., Odintsov, S. D., & Oikonomou, V. K. 2017, Phys. Rep., 692, 1 Paillas, E., Cai, Y.-C., Padilla, N., & Sánchez, A. G. 2021, MNRAS, 505, 5731 Pan, S., Liu, M., Forero-Romero, J., et al. 2020, Science China Physics, Mechan- ics, and Astronomy, 63, 110412 Park, C. & Kim, Y.-R. 2010, ApJ, 715, L185 Peebles, P. J. E. 2001, in Astronomical Society of the Pacific Conference Series, Vol. 252, Historical Development of Modern Cosmology, ed. V. J. Martínez, V. Trimble, & M. J. Pons-Bordería, 201 Peel, A., Lalande, F., Starck, J.-L., et al. 2019, Phys. Rev. D, 100, 023508 Peel, A., Lin, C.-A., Lanusse, F., et al. 2017, A&A, 599, A79 Perico, E. L. D., Voivodic, R., Lima, M., & Mota, D. F. 2019, A&A, 632, A52 Philcox, O. H. E., Slepian, Z., Hou, J., et al. 2022, MNRAS, 509, 2457 Planck Collaboration, Aghanim, N., Akrami, Y., et al. 2020, A&A, 641, A6 Puchwein, E., Baldi, M., & Springel, V. 2013, MNRAS, 436, 348 Ravanbakhsh, S., Oliva, J., Fromenteau, S., et al. 2017, arXiv e-prints, arXiv:1711.02033 Sønderby, C. K., Raiko, T., Maaløe, L., Sønderby, S. K., & Winther, O. 2016, in Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS’16 (Red Hook, NY, USA: Curran Associates Inc.), 3745–3753 Song, Y.-S., Hu, W., & Sawicki, I. 2007, Phys. Rev. D, 75, 044004 Takada, M. & Jain, B. 2003, MNRAS, 340, 580 Tamosiunas, A., Winther, H. A., Koyama, K., et al. 2021, MNRAS, 506, 3049 Tassev, S., Zaldarriaga, M., & Eisenstein, D. J. 2013, J. Cosmology Astropart. Phys., 2013, 036 Touati, A., Satija, H., Romoff, J., Pineau, J., & Vincent, P. 2018, arXiv e-prints, arXiv:1806.02315 Tsujikawa, S. 2008, Phys. Rev. D, 77, 023507 Tsujikawa, S., Uddin, K., Mizuno, S., Tavakol, R., & Yokoyama, J. 2008, Phys. Rev. D, 77, 103009 Van Waerbeke, L., Mellier, Y., Radovich, M., et al. 2001, A&A, 374, 757 Veropalumbo, A., Binetti, A., Branchini, E., et al. 2022, J. Cosmology Astropart. Phys., 2022, 033 Villaescusa-Navarro, F., Hahn, C., Massara, E., et al. 2020, ApJS, 250, 2 Voivodic, R., Lima, M., Llinares, C., & Mota, D. F. 2017, Phys. Rev. D, 95, 024018 Watson, J., Andreas Lin, J., Klink, P., Pajarinen, J., & Peters, J. 2021, in Proceed- ings of Machine Learning Research, Vol. 130, Proceedings of The 24th Inter- national Conference on Artificial Intelligence and Statistics, ed. A. Banerjee & K. Fukumizu (PMLR), 1198–1206 Weinberg, D. H., Mortonson, M. J., Eisenstein, D. J., et al. 2013, Phys. Rep., 530, 87 Winther, H. A., Koyama, K., Manera, M., Wright, B. S., & Zhao, G.-B. 2017, J. Cosmology Astropart. Phys., 2017, 006 Zhang, H., Samushia, L., Brooks, D., et al. 2022, MNRAS, 515, 6133 Zhang, Z., Gu, G., Wang, X., et al. 2019, ApJ, 878, 137 Article number, page 13 of 13
Fast and Regret Optimal Best Arm Identification: Fundamental Limits and Low-Complexity Algorithms Qining Zhang University of Michigan, Ann Arbor qiningz@umich.edu Lei Ying University of Michigan, Ann Arbor leiying@umich.edu Abstract This paper considers a stochastic multi-armed bandit (MAB) problem with dual objectives: (i) quick identification and commitment to the optimal arm, and (ii) reward maximization throughout a sequence of T consecutive rounds. Though each objective has been individually well-studied, i.e., best arm identification for (i) and regret minimization for (ii), the simultaneous realization of both objectives remains an open problem, despite its practical importance. This paper introduces Regret Optimal Best Arm Identification (ROBAI) which aims to achieve these dual objectives. To solve ROBAI with both pre-determined stopping time and adaptive stopping time requirements, we present the EOCP algorithm and its variants respectively, which not only achieve asymptotic optimal regret in both Gaussian and general bandits, but also commit to the optimal arm in Oplog Tq rounds with pre-determined stopping time and Oplog2 Tq rounds with adaptive stopping time. We further characterize lower bounds on the commitment time (equivalent to sample complexity) of ROBAI, showing that EOCP and its variants are sample optimal with pre-determined stopping time, and almost sample optimal with adaptive stopping time. Numerical results confirm our theoretical analysis and reveal an interesting “over-exploration” phenomenon carried by classic UCB algorithms, such that EOCP has smaller regret even though it stops exploration much earlier than UCB (Oplog Tq versus OpTq), which suggests over-exploration is unnecessary and potentially harmful to system performance. 1 Introduction The stochastic multi-armed bandit problem (MAB) (Thompson, 1933), which models a wide range of applications including online recommendations (Yang et al., 2022a; Kohli et al., 2013; Zeng et al., 2016), job assignments (Liu et al., 2020; Huang et al., 2023; Yang et al., 2022b), clinical trials (Villar et al., 2015; Aziz et al., 2021), and etc, is a sequential decision-making process between an agent and an environment which consists of a number of actions. Most existing studies, say Auer et al. (2002); Agrawal and Goyal (2012); Garivier and Cappé (2011); Lai et al. (1985); Katehakis and Robbins (1995); Kaufmann et al. (2012b), formulate MAB as a regret minimization problem, where the agent aims to maximize the cumulative reward through interacting with the environment for a consecutive of T rounds. The UCB algorithm Auer et al. (2002) and its variants are among the most popular MAB algorithms for regret minimization, which exhibit outstanding performances both theoretically and empirically, not only in bandits but also have been used in reinforcement learning. However, the UCB algorithms do not commit to a single action and continue to change the action based on reward signals received. Since one of the ultimate goals of MAB is to find the optimal arm, it would be ideal if the algorithm can commit to an arm quickly without sacrificing the regret. In fact, in practice, a number of applications such as occupational decisions Phillips et al. (1984), medicine release and pandemic control Higuchi (1963); Zhang et al. (2022), and long-term investments Siegel (2021), require or prefer quick commitment to 1 arXiv:2309.00591v1 [cs.LG] 1 Sep 2023Algorithm Regret Setting Optimality Commitment Time Confidence UCB 2`op1q ∆ log T Gaussian Optimal T N/A KL-UCB ∆`op1q KLpµ2,µ1q log T General Optimal T N/A TS ∆`op1q KLpµ2,µ1q log T General Optimal T N/A BAI-ETC 4`op1q ∆ log T Gaussian Not Optimal Oplog Tq ˜OpT ´1q DETC 2`op1q ∆ log T Gaussian Optimal Ωplog2 Tq OpT ´1q UCBα 2α2`op1q ∆ log T Gaussian Not Optimal Oplog Tq ˜OpT ´1q EOCP (Ours) 2`op1q ∆ log T Gaussian Optimal Oplog Tq O ` T ´1˘ EOCP-UG (Ours) 2`op1q ∆ log T Gaussian Optimal Oplog2 Tq O ` T ´1˘ KL-EOCP (Ours) ∆`op1q KLpµ2,µ1q log T General Optimal Oplog Tq O ` T ´1˘ LB (pre-determined) 2`op1q ∆ log T Gaussian Oplogc Tq Oplog Tq OpT ´1q LB (adaptive) 2`op1q ∆ log T Gaussian Oplogc Tq Oplog2´c Tq OpT ´1q Table 1: Caparison under 2-armed bandits with algorithms in the literature: UCB (Garivier et al., 2016), KL-UCB (Garivier and Cappé, 2011), TS (Kaufmann et al., 2012b), BAI-ETC (Garivier et al., 2016), DETC (Jin et al., 2021a), UCBα (Degenne et al., 2019). ∆ “ |µ1 ´ µ2| is the expected reward difference, KLpµ2, µ1q is the Kullback-Leibler divergence between reward distributions, and α ą 1. EOCP and KL-EOCP use a pre-determined stopping time and require the knowledge of ∆, while EOCP-UG uses adaptive stopping time. Both LBs represent the commitment time lower bound under Gaussian bandits for regret optimal algorithms with Oplogc Tq finite-time regret violation and OpT ´1q confidence. an action instead of continuous exploration. This motivated us to consider a MAB problem with dual objectives: (i) quick identification and commitment to the optimal arm, and (ii) minimization of the cumulative regret throughout a sequence of T consecutive rounds. Regret Optimal Best Arm Identification: The lack of commitment in traditional regret mini- mization formulation motivates us to propose a new viewpoint towards online decision-making called regret optimal best arm Identification (ROBAI), which intends to manage two goals at the same time: minimizing regret while committing to an action quickly. Specifically, it is ideal for the agent to quickly commit to the optimal action which has the highest expected reward while minimizing the exploration regret. To solve ROBAI, we need to answer three fundamental questions: (1) how should the learner explore actions while maintaining low regret performance (exploration strategy)? (2) when should the learner stops exploration and commit to an action (stopping time)? and (3) which action to commit to when the exploration ends (action selection strategy)? All three components need to be designed together to make the algorithm most efficient. The fundamental question this paper addresses is: Can we design an efficient algorithm that is both regret optimal and identifies the optimal action quickly, and what are the fundamental limits of such algorithms? Connection to Best Arm Identification: One approach people may take to solve ROBAI is the best arm identification (BAI) algorithm, which focuses on identifying the best arm with a minimum number of samples (rounds) and then commit to the selected action (arm). However, since BAI focuses on sample complexity (or commitment time), the algorithms for BAI often explore sub-optimal actions too aggressively and too often, leading to large regret. As shown in Garivier et al. (2016), the regret is at least twice as large as the regret under UCB asymptotically. Modifications shown in Jin et al. (2021a) may lead to better regret performance, but they also make the algorithm too complicated to find the optimal action quickly. Moreover, these algorithms adapted from BAI often exhibit poor empirical regret performances as shown in our numerical experiments of Fig. 1. 2Our Contributions: We propose an algorithm called Explore Optimistically then Commit Pes- simistically (EOCP) to solve the ROBAI problem. It first uses an optimistic modified UCB algorithm to explore actions with a slightly larger exploration function , and then commits to actions according to a pessimistic LCB algorithm when the exploration ends. The exploration and action identification strategies are respectively motivated by the inflated bonus trick Degenne et al. (2019) and the principle of pessimism from the literature of offline bandits (Rashidinejad et al., 2021; Li et al., 2022a; Xiao et al., 2021) and offline reinforcement learning (Li et al., 2022b; Shi et al., 2022; Jin et al., 2021b). Our greatest contributions include designing new stopping rules with both pre-determined stopping time (vanilla EOCP) and adaptive stopping time (the EOCP-UG variant), which provably balance the trade-off between regret minimization and optimal action identification. We theoretically show that both algorithms are asymptotically regret optimal in Gaussian bandits. Moreover, EOCP and EOCP-UG algorithms commit to the optimal action in Oplog Tq and Oplog2 Tq number of rounds respectively, both with OpT ´1q confidence. We further characterize the fundamental commitment time (sample complexity until commitment) limits of optimal action identification for regret optimal algorithms, which shows that Oplog Tq number of samples is always required with pre-determined stopping time, and Oplog2´c Tq number of samples is required with adaptive stopping time if the finite-time regret of an algorithm does not exceed its asymptotic regret by Oplogc Tq. This shows that EOCP is sample optimal and EOCP-UG is nearly sample optimal. We also proposed an improved algorithm called KL-EOCP to achieve regret optimality in general bandits. To the best of our knowledge, KL-EOCP is the first algorithm that not only achieves asymptotic regret optimality in general bandits but also commits to the optimal action in Oplog Tq rounds, which also matches the commitment time lower bound. The more detailed comparison between existing algorithms and our proposed algorithms with lower bounds are summarized in Tab. 1. Numerical experiments confirm the superiority of our proposed algorithm and show an interesting “over-exploration” phenomenon carried by UCB algorithms. As shown in Fig. 1, our EOCP algorithm reduces more than 20% of regret compared to the vanilla UCB algorithm by finding and committing to the optimal action early. 2 Preliminaries Stochastic Multi-armed Bandits: A stochastic multi-armed bandit problem is an online decision- making process between an agent and an environment for a consecutive of T number of rounds. At each round t P t1, 2, ¨ ¨ ¨ , Tu, the agent can choose an action At among a set of A actions (arms) denoted by A “ t1, 2, ¨ ¨ ¨ , Au to interact with the environment. Each action a is associated with a probability distribution νa and we denote the set of distributions as ν “ tν1, ¨ ¨ ¨ , νAu with respective expectations µ “ tµ1, ¨ ¨ ¨ , µAu which is unknown to the agent a priori. The expectations are assumed to be bounded so without loss of generality, we have µa P r0, 1s for any action a. After the agent chooses an action, say action At at round t, it will observe an independent reward rt which is sampled from the distribution νAt associated with the action At that it chooses. We define the optimal action a˚ to be the action which has the highest expected reward, i.e., µa˚ “ arg maxaPA µa, and it is unique. For simplicity, let ∆a “ |µa˚ ´ µa| P p0, 1s to be the gap in terms of expected reward between the optimal action and a sub-optimal action a, and we use ∆min “ mina:∆aą0 |µa˚ ´µa| to denote the minimum reward gap among sub-optimal actions. Regret: The goal of the agent is to maximize the expected cumulative reward from the total T rounds of interactions with the environment, i.e., to maximize Err1 `r2 `¨ ¨ ¨`rT s, where the expectation is taken over all randomness. The performance of any MAB algorithm Alg chosen by the agent is usually 3measured by the cumulative Regret up to round T defined as follows: RegAlg µ pTq “ Tµa˚ ´ Eµ « Tÿ t“1 rt ff , where the subscript µ denotes the bandit instance represented by the reward expectations. Maximizing reward is equivalent to minimizing the cumulative regret. For most of the algorithms to achieve this goal, the agent will make action-choosing decisions based on two statistics maintained and updated at each round for every action: the empirical mean ¯rtpaq and the number of pulls Ntpaq in previous rounds. They are defined as Ntpaq “ řt k“1 1At“a, ¯rtpaq “ 1 Ntpaq řt k“1 rt1At“a. The theoretical regret limit of any algorithm Alg is studied and characterized in (Lai et al., 1985), which shows: lim inf TÑ8 RegAlg µ pTq log T ě ÿ a:∆aą0 ∆a KLpνa, νa˚q, (1) where KLp¨, ¨q denotes the Kullback–Leibler divergence between two distributions. We call an algorithm Alg regret optimal (asymptotically) if the asymptotic regret performance of Alg achieves this lower bound. Therefore, whether an algorithm is regret asymptotic optimal will depend on the distributions ν of the rewards. Specifically, for Gaussian bandits, KL divergence between distributions Npµa, 1q and Npµa1, 1q is simply pµa ´ µa1q2{2. So the asymptotic regret rate lower bound in the RHS for Gaussian bandits would be 2 ř a:∆aą0 ∆´1 a . Commitment: In ROBAI, commitment to a single action ˆa (ideally, the optimal action) is required. After a stopping time Tc, the agent will not be allowed to switch actions and will commit to the same action until the end. We consider two categories of commitment: the pre-determined stopping-time setting and the adaptive stopping time setting. The pre-determined stopping time requires Tc to be pre-specified before the first round of interaction, while the adaptive stopping criterion requires Tc to be a measurable stopping time. How quickly the agent commits is measure by the Sample Complexity until Commitment (also called commitment time) which is the expected number of rounds until commitment, i.e., SCCAlg µ pTq “ EµrTcs. We also care about the confidence of action identification, which is the probability that the agent commits to a sub-optimal action, i.e., Pµpˆa ‰ a˚q. ROBAI Problem Formulation: We use ΠRO to denote the class of regret optimal algorithms which commits to the optimal action with confidence lower than OpT ´1q, i.e., ΠRO “ # Alg ˇˇˇˇˇ lim sup TÑ8 RegAlg µ pTq log T ď ÿ a:∆aą0 ∆a KLpνa, νa˚q and Pµpˆa ‰ a˚q “ O ` T ´1˘ + . ROBAI aims to design a regret optimal algorithm Alg P ΠRO to minimize the commmitment time: min SCCAlg µ pTq “ EµrTcs, s.t., Alg P ΠRO. 3 Low-Complexity Algorithms for ROBAI In this section, we propose a low-complexity algorithms called EOCP with pre-determined stopping time to solve ROBAI. Then, we propose its variant called EOCP-UG with adaptive stopping time. 3.1 The Pre-determined Stopping Time Setting The pre-determined stopping time setting is motivated by real-world applications such as A/B testings in medical experiments with budget limits (Siroker and Koomen, 2015). In these applications, the number 4of testers is usually pre-determined before the trial starts. Therefore, we require the agent to pre-specify the stopping time Tc before the first round and assume it knows the strictly positive reward gap ∆min between the optimal action and sub-optimal actions1. We then propose EOCP in Algorithm. 1. Algorithm 1 EOCP with Pre-determined Stopping Time Require: Exploration function l. 1: Let Tc “ 8Al ∆2 min ` A be the pre-determined stopping time. 2: Initialize by pulling each arm a once. 3: for t “ A ` 1 : Tc do 4: Set uncertainty bonus bt´1paq “ b 2l Nt´1paq, and UCBt´1paq “ ¯rt´1paq ` bt´1paq. 5: Take action At “ arg maxa UCBt´1paq. // UCB Exploration 6: end for 7: Set bonus bTcpaq “ b 2l NTcpaq, and LCBTcpaq “ ¯rTcpaq ´ bTcpaq. 8: For t P rTc ` 1, Ts, commit to action ˆa “ arg maxa LCBTcpaq. // LCB Commitment In EOCP, the agent will spend the first A rounds in exploring each arm once as a start. Our choice of exploration strategy is a modified version of UCB algorithm where the agent will choose the action with the largest upper confidence bound in terms of empirical reward. The exploration function l controls the intensity of exploration to achieve the optimal trade-off between reducing uncertainty for action identification and minimizing regret. After the pre-determined stopping time Tc, the agent will commit to an action which has the largest lower confidence bound of empirical reward. This LCB commitment strategy is inspired by the principle of pessimism from the literature of offline learning (Rashidinejad et al., 2021; Li et al., 2022a; Xiao et al., 2021; Li et al., 2022b; Shi et al., 2022; Jin et al., 2021b), where the empirical reward of each action is penalized by the amount of uncertainty to combat the imbalanced data coverage of actions in the offline dataset. It is also shown that the pessimistic principle works well when the data coverage are concentrated on the optimal action, i.e., the optimal action has the largest number of pulls. This trait of the LCB algorithm matches the trait of UCB exploration, which the optimal action will be chosen much more often than sub-optimal actions in exploration. So by designing such a proper Tc, we will be able to achieve the best of both worlds: a low-regret exploration of UCB algorithm, and a fast best arm identification through the choice of LCB algorithm. 3.2 The Adaptive Stopping Time Setting In this setting, the minimum reward gap ∆min is unknown to the agent a priori, so there is no hope to design a pre-determined stopping time. Instead, we design our stopping criterion based on the samples collected from the explorations, which lead to the fact that Tc is a measurable stopping time. We propose our EOCP-UG algorithm in Algorithm 2 corresponding to unknown gap. In EOCP-UG, we use the same UCB exploration and LCB best action identification strategies as in the pre-determined stopping time setting. The only difference compared to EOCP comes from the new stopping rule based on the number of pulls Ntpaq for each action. Specifically, the exploration ends if there is an imbalanced fraction of Ntpaq among all actions, that is, one action has l times more pulls than other actions in previous rounds. Here, l is the exploration function and we will select l to be slightly larger than logpTq in later sections. The intuition of such stopping criterion comes from the characteristics of UCB exploration, i.e., as round t increases, the algorithm will slowly adapt to choosing the optimal action more often. When the fraction between the number of pulls for the optimal action and any sub-optimal action is large enough, the optimal action will be identifiable. Note that in action 1The results can be generalized to knowing a lower bound of ∆min. 5Algorithm 2 EOCP-UG with Adaptive Stopping Time Require: Exploration function l. 1: Initialize by pulling each arm a once. 2: while maxa mina1 Nt´1paq ´ lNt´1pa1q ď 1 do 3: Set uncertainty bonus bt´1paq “ b 2l Nt´1paq, UCBt´1paq “ ¯rt´1paq ` bt´1paq. 4: Take action At “ arg maxa UCBt´1paq. // UCB Exploration 5: end while 6: Let Tc Ð t ´ 1, bonus bTcpaq “ b 2l NTcpaq, and LCBTcpaq “ ¯rTcpaq ´ bTcpaq. 7: For t P rTc ` 1, Ts, commit to the action ˆa “ arg maxa LCBTcpaq. // LCB Commitment identification, we can simply choose the action which has the largest number of pulls NTcpaq when we stop, and obtain exactly the same performance guarantees. However, to keep it consistent with the pre-determined stopping time setting, we use the LCB commitment. 4 Main Results In this section, we assume the distributions tν1, ¨ ¨ ¨ , νAu come from a Gaussian family2, which means νa associated with action a follows a Gaussian distribution with mean µa and unit variance, i.e., νa „ Npµa, 1q. Inspired by the inflated bonus from (Degenne et al., 2019), we choose the exploration function l to be logpTq ` 4 a 2 logpTq which is slightly larger than logpTq used in vanilla UCB algorithms. This slight inflation will help us identify the optimal action quickly while controlling regret. 4.1 Regret Optimality for Gaussian Bandits with Pre-Determined Stopping Time In Theorem. 1, we present the theoretical regret performance guarantee of the EOCP algorithm: Theorem 1 If we choose l “ logpTq ` 4 a 2 logpTq, the expected regret of the EOCP algorithm in Algorithm. 1 with pre-determined stopping time is asymptotically upper bounded by: lim sup TÑ8 RegEOCP µ pTq log T ď ÿ a:∆aą0 2 ∆a . It is clear that under Gaussian bandit setting, EOCP is asymptotically regret optimal. The commitment time and confidence level guarantees can be extracted from the setup of Algorithm. 1 itself and the proof of Theorem. 1. Recall that in Algorithm. 1, we pre-determined the length of exploration Tc to be Op∆´2 minlq. The follow corollary characterizes these parts of theoretical performance: Corollary 1 If we choose l “ logpTq ` 4 a 2 logpTq, the expected commitment time for EOCP in Algo- rithm. 1 is given by SCCEOCP µ pTq “ Op∆´2 min log Tq, and the confidence level is OpT ´1q. The complete proofs of Theorem. 1 and Corollary. 1 are provided in the supplementary material. In order to upper bound the cumulative regret of T rounds, we divide the total regret into the regret accumulated in exploration and the regret accumulated in commitment. Bounding Regret from Exploration: To bound the regret accumulated in exploration and since we use a variant of UCB exploration, we follow the standard procedure of proofs for UCB algorithms, e.g., proof of Theorem 8 from. (Garivier et al., 2016). Then, this procedure results in a order Oplq dominating 2The results can be easily generalized to sub-Gaussian distributions. 6regret term, which is OplogpTqq by the choice of our exploration function. Through carefully applying any-time concentration inequalities, we are able to show that the constant in front of this dominating regret term is exactly the constant we obtained in Theorem. 1. Bounding Regret from Committing to the Wrong Action: As for the regret accumulated from commitment, the key is to prove the OpT ´1q confidence level upper bound presented in Corollary. 1. We follow a procedure similar to the proof of Theorem 1 in (Yang et al., 2022a) to utilize the adaptivity of UCB exploration and the pessimistic LCB commitment. We first show that with high probability, the number of pulls Ntpaq for any sub-optimal actions in the exploration phase is upper bounded. This is because after certain number of pulls, the uncertainty bonus bt´1paq for any sub-optimal action will be so small that the upper confidence bound UCBt´1paq cannot be larger than µa˚, thus less than the upper confidence bound of the optimal action. Therefore, sub-optimal actions will not be chosen in future rounds. After our carefully designed Tc, we make sure that the optimal action has the largest number of pulls NTcpaq among all actions, thus its bonus is so small so that its lower confidence bound LCBTcpa˚q is larger than the expectations µa of any sub-optimal action a, and thus larger than the lower confidence bound of other actions. So with high probability, we will commit to the optimal action. Then, the OpT ´1q confidence level will provide us with a constant regret in commitment,and the overall dominating regret comes from exploration. Combining both bounds, we are able to show the regret performance upper bound in Theorem. 1. 4.2 Regret Optimality for Gaussian Bandits with Adaptive Stopping Time We present the regret performance of EOCP-UG in Algorithm. 2 in Theorem. 2. Compared to Theorem. 1, EOCP-UG has the same performance guarantee as EOCP, which implies the knowledge of ∆min does not affect regret. So, EOCP-UG is also regret optimal in Gaussian bandit settings. Theorem 2 If we choose l “ logpTq ` 4 a 2 logpTq, the expected regret of the EOCP-UG algorithm in Algorithm. 2 with adaptive stopping time is asymptotically upper bounded by: lim sup TÑ8 RegEOCP-UG µ pTq log T ď ÿ a:∆aą0 2 ∆a . With adaptive stopping, the sample complexity until commitment is not a pre-determined value. However, we can still extract similar guarantee along with the confidence level from the proof of Theorem. 2. We present these results in the following corollary: Corollary 2 If we choose l “ logpTq ` 4 a 2 logpTq, the sample complexity until commitment for EOCP- UG algorithm in Algorithm. 2 is asymptotically upper bounded by: lim sup TÑ8 SCCEOCP-UG µ pTq log2 T ď ÿ a:∆aą0 8 ∆2a , and the confidence level is upper bounded by OpT ´1q. Proof Roadmap: The complete proofs of Theorem. 2 and Corollary 2 are provided in the supplementary material. Compared to the proof of Theorem. 1, the major difference lies in bounding the regret of commitment. Similarly, we require to bound the probability of committing to a sub-optimal action. We first show that when the agent stops exploration according to Line 2 of Algorithm. 2, the action which has the maximum number of pulls is the optimal action with high probability. Then, we show that under this event we will to commit to the optimal action if we use LCB commitment. 7Magic Choice of Exploration Function: Bounding the regret in both exploration and commitment requires a delicate analysis with any-time concentration inequalities. Our choice of exploration function l plays an important role which manages the trade-off between low-regret exploration and high-probability optimal action commitment. If the exploration function is too large, i.e., if l “ 2 log T, the regret in exploration will not be optimal. If the exploration function is too small, i.e., if l “ log T, the probability of committing to the wrong action can not be bounded by OpT ´1q. Our magic choice of exploration function l achieves the best of both worlds. Loss of SCC from Unknown Gap: Even though we have the same regret performance, the guarantee for sample complexity until commitment is now Oplog2pTqq which is worse than OplogpTqq in the pre-determined stopping time setting. So, is this order fundamental with adaptive stopping time? In the next section, we provide the answer by investigating the theoretical limits of commitment time for asymptotic regret optimal algorithms. 4.3 Fundamental Limits of Sample Complexity until Commitment In order to answer the question regarding the fundamental sample complexity until commitment for regret optimal algorithms with both pre-determined and adaptive stopping times, we provide the following theorem in a 2-armed Gaussian bandit model, where the reward gap between the two actions is ∆, and the optimal regret is 2∆´1 logpTq asymptotically: Theorem 3 (Information-Theoretic Limits of SCC) Consider a 2-armed Gaussian bandits, for any asymptotically regret optimal algorithm Alg which has c-logarithm regret violation, i.e., there exists c P p0, 1q such that when T is large enough, the following relationship always holds: ˇˇˇˇRegAlg µ pTq ´ 2 logpTq ∆ ˇˇˇˇ “ Oplogc Tq. In order to guarantee OpT ´1q confidence level, the sample complexity until commitment with pre- determined stopping time is lower bounded by Tc “ Ωp∆´2 logpTqq, and with adaptive stopping time, the sample complexity until commitment is lower bounded by: EµrTcs “ Ωp∆´2 log2´cpTqq. Proof Roadmap: The proof is provided in the supplementary material. The proof idea relies on the well-known “transportation” lemma (Kaufmann et al., 2016, Lemma. 1) originally derived to prove the theoretical limits of best arm identification algorithms. This lemma characterizes the expected number of pulls for each action by hypothesis testing between the original bandit problem and another bandit instance with a different optimal action. Then by finding a proper bandit instance and combining the lemma with regret optimal algorithms, we will be able to prove Theorem. 3 for both settings. Sample Optimality: It is shown by Corollary. 1 and Theorem. 3 together that our proposed EOCP algorithm from Algorithm. 1 achieves the optimal commitment time with Oplog Tq with pre-determined stopping time. However, with adaptive stopping time, our EOCP-UG algorithm has 1{2-logarithm regret violation and Oplog2 Tq commitment time indicated by the proof of Theorem. 2. However, Theorem. 3 implies Oplog1.5 Tq commitment time lower bound for such algorithms. Even though EOCP-UG is not exactly sample optimal, we conjure that this gap comes from our analysis techniques which make one of the bounds (maybe both) not tight. 5 Regret Optimality for General Bandits Even though EOCP is applicable in sub-Gaussian bandits, it is not regret optimal beyond Gaussian bandits. Comparing Theorem. 1 and the fundamental regret limit (1) with Pinsker’s inequality, it is clear 8that the lower bound is smaller than our upper bound even asymptotically. To close this gap, we propose an improved algorithm called KL-EOCP which is provably regret optimal in general bandits. Natural Exponential Family: We assume the reward distributions of each action belong to a natural exponential family, i.e., P “ tpνθqθPΘ : dνθ{dξ “ exppθx ´ bpθqqhpxqu, where Θ Ă R is the set of all parameters θ such that the expectation µ is positive and bounded, i.e., µ P r0, 1s. ξ is some reference measure on R and b : Θ Ñ R is a convex twice differentiable function. This distribution νθ can also be parameterized by its expectation µ “ b1pθq, the derivative of bp¨q, and for every µ we denote by νµ the unique distribution in P with expectation µ and by θµ its corresponding parameter. Gaussian distribution with unit variance is an example of this family. Moreover, the Kullback-Leibler divergence from νθ1 to νθ2 (with a little abuse of notation) can be expressed as (Garivier and Cappé, 2011): KLpµ1, µ2q “ KLpνθ1, νθ2q “ bpθ2q ´ bpθ1q ´ b1pθ1qpθ2 ´ θ1q. The set of exponential family bandit models ν “ pνθ1, ¨ ¨ ¨ , νθAq can be characterized by the expectations of the actions µ “ pµ1, ¨ ¨ ¨ , µAq. We assume for all λ P R, and θ P Θ the moment generating function Mνθpλq “ EνθrexppλWqs for the distribution νθ is well-defined and is finite. Algorithm: Analog to ∆min in Algorithm. 1, the KL-EOCP algorithm requires the knowledge of a strictly positive “minimum KL divergence”, denoted as KLmin, which captures the minimum reward distri- bution gap (distance) between the optimal action and any sub-optimal action. Considering the asymmetric nature of the KL divergence, we define KLmin as follows: KLmin “ mina‰a˚ min tKLpµa, µ1q, 4KLpµ1 a, µaqu , where µ1 a P pµa, µ1q and 4KLpµ1 a, µ1q “ KLpµa, µ1q. The term 4KLpµ1 a, µaq reflects the skew of KL diver- gence when the two distributions are switched. A lower bound to KLmin can be computed given the knowledge of minimum reward gap ∆min with the exponential family, and we only require the knowledge of such lower bound. The KL-EOCP algorithm is summarized in Algorithm. 3. It designs the UCB and LCB bonuses based on the KL divergence of the reward distributions. In general, these designs would lead to smaller confidence intervals with the same OpT ´1q probability guarantee as first shown in (Garivier and Cappé, 2011). If KLmin is not known, we can combine Algorithm. 3 with the adaptive stopping time of EOCP-UG to deal with this setting. Algorithm 3 KL-EOCP with Pre-Determined Stopping Time Require: Exploration function l. 1: Let Tc “ 4Al KL2 min ` A be the length of the exploration phase. 2: Initialize by pulling each arm a once. 3: for t “ A ` 1 : Tc do 4: Set upper confidence bound: UCBt´1paq “ arg max µě¯rt´1paq tNt´1paqKLp¯rt´1paq, µq ď lu . 5: Take action At “ arg maxa UCBt´1paq. // UCB Exploration 6: end for 7: Set lower confidence bound: LCBTcpaq “ arg min µď¯rTcpaq tNTcpaqKLp¯rTcpaq, µq ď lu . 8: For t P rTc ` 1, Ts, commit to action ˆa “ arg maxa LCBTcpaq. // LCB Commitment Regret Optimality in General Bandits: The theoretical regret performance of the KL-EOCP 9100 101 102 103 104 105 106 Number of Rounds 0 10 20 30 40 50 60 70 80 Regret EOCP EOCP-UG UCB BAI-ETC DETC (a) Gaussian Bandits 100 101 102 103 104 105 Number of Rounds 0 10 20 30 40 50 60 Regret EOCP EOCP-UG KL-EOCP UCB KL-UCB BAI-ETC DETC (b) Bernoulli Bandits Figure 1: Comparison of regret performance of EOCP with variants and existing algorithms in the literature. The gap ∆ between the two arms is 0.5 and results are averaged over 105 iterations. is sumarized in the following Theorem. To the best of our knowledge, it is the first result achieving asymptotic regret optimality in general bandit problems with commitment. Theorem 4 If we choose l “ logpTq`4 a 2 logpTq, when the reward distributions ν of each action belong to the same natural exponential family, the expected regret of the KL-EOCP algorithm in Algorithm. 3 is asymptotically upper bounded by: lim sup TÑ8 RegµpTq log T ď ÿ a:∆aą0 ∆a KLpµa, µ1q. The complete proof of Theorem. 4 is provided in the supplementary materials. Even though the proof roadmap is similar to the proof of Theorem. 1, the major difference comes from the use of a tighter concentration lemma modified from (Garivier and Cappé, 2011, Theorem. 11) which captures the low probability event when the KL divergence of the empirical mean is far away from its expectation. The sample complexity until commitment and confidence level guarantees can also be extracted from the proof. Specifically, SCCKL-EOCP µ pTq “ OpKL´1 min log Tq and the confidence level is upper bounded by OpT ´1q. 6 Numerical Experiments In this section, we study the empirical performance of our proposed EOCP algorithm with variants compared to existing algorithms in the literature, including BAI-ETC Garivier et al. (2016), UCB Auer et al. (2002), KL-UCB Garivier and Cappé (2011), and DETC Jin et al. (2021a) in both Gaussian and Bernoulli bandit settings. In the Gaussian setting, we test all the algorithms on a two-armed bandit problem with distributions Npµi, 1q for arm i “ 1, 2 with a total of 106 rounds, and in the Bernoulli bandit setting, we test the algorithms with distribution Berpµiq for arm i “ 1, 2 with a total of 105 rounds. We set µ1 “ 0.7 and µ2 “ 0.2, so the gap between the arms is ∆ “ 0.5. The results are averaged over 106 iterations and shown in Fig. 1. In the Gaussian bandit setting, it shows that both BAI-ETC and DETC algorithms exhibit unsatisfac- torily high regret. On the contrary, our proposed algorithms EOCP and EOCP-UG have lower final regret 10even compared to the popular UCB algorithm, surprisingly. Even though our algorithms use a larger exploration function than the vanilla UCB algorithm, and accumulate regret more quickly in exploration, i.e., approximately the first 1000 rounds, it allows us to commit to the optimal action. Our algorithms almost find the optimal action in all simulated traces which give rise to the very slowly-increasing behavior of regret in the commitment phase. This phenomenon coincides with our theoretical analysis which shows that the regret in commitment is Op1q. However, the UCB algorithm continues to explore sub-optimal actions, so its regret continues to grow when the EOCP algorithm has already committed to the optimal action. Numerically, the EOCP algorithm reduces 20% of the final regret compared to UCB through early commitment, and we conjure that preventing the “over-exploration” phenomenon of UCB algorithm is behind the reason for such empirical regret reduction. Comparing the commitment time of EOCP-UG and EOCP, we can see that both algorithms stop exploration at approximately 1000 rounds. This means that not knowing the gap information won’t harm the empirical sample complexity until commitment too much, which in turn may imply that our theoretical analysis of commitment time upper bound in Corollary. 2 is not tight enough. The same trend can be witnessed from results of Bernoulli bandits. However, in Bernoulli bandits, KL-UCB and KL-EOCP algorithms have a much better regret performance than other algorithms, which shows the knowledge of the reward distribution family improves the performance significantly. 7 Conclusion We studied ROBAI which intends to both minimize regret and commit to the optimal action. We proposed EOCP with variants, which combine UCB exploration and LCB commitment with novel stopping criterion in both pre-determined and adaptive settings. We showed that both EOCP and EOCP-UG are regret asymptotic optimal in Gaussian bandits with Oplog Tq and Oplog2 Tq commitment time respectively, almost matching the theoretical limits we derived. For general bandits, we proposed KL-EOCP which is provably regret optimal. Numerical experiments confirmed the superiority of our algorithms and revealed the “over-exploration” phenomenon of UCB algorithms. References Agrawal, S. and Goyal, N. (2012). Analysis of thompson sampling for the multi-armed bandit problem. In Conference on learning theory, pages 39–1. JMLR Workshop and Conference Proceedings. Auer, P., Cesa-Bianchi, N., and Fischer, P. (2002). Finite-time analysis of the multiarmed bandit problem. Machine learning, 47:235–256. Aziz, M., Kaufmann, E., and Riviere, M.-K. (2021). On multi-armed bandit designs for dose-finding clinical trials. The Journal of Machine Learning Research, 22(1):686–723. Degenne, R., Nedelec, T., Calauzènes, C., and Perchet, V. (2019). Bridging the gap between regret minimization and best arm identification, with application to a/b tests. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1988–1996. PMLR. Garivier, A. and Cappé, O. (2011). The kl-ucb algorithm for bounded stochastic bandits and beyond. In Proceedings of the 24th annual conference on learning theory, pages 359–376. JMLR Workshop and Conference Proceedings. Garivier, A. and Kaufmann, E. (2016). Optimal best arm identification with fixed confidence. In Conference on Learning Theory, pages 998–1027. PMLR. 11Garivier, A., Lattimore, T., and Kaufmann, E. (2016). On explore-then-commit strategies. Advances in Neural Information Processing Systems, 29. Higuchi, T. (1963). Mechanism of sustained-action medication. theoretical analysis of rate of release of solid drugs dispersed in solid matrices. Journal of pharmaceutical sciences, 52(12):1145–1149. Huang, J., Golubchik, L., and Huang, L. (2023). Queue scheduling with adversarial bandit learning. arXiv preprint arXiv:2303.01745. Jin, T., Xu, P., Xiao, X., and Gu, Q. (2021a). Double explore-then-commit: Asymptotic optimality and beyond. In Conference on Learning Theory, pages 2584–2633. PMLR. Jin, Y., Yang, Z., and Wang, Z. (2021b). Is pessimism provably efficient for offline rl? In International Conference on Machine Learning, pages 5084–5096. PMLR. Katehakis, M. N. and Robbins, H. (1995). Sequential choice from several populations. Proceedings of the National Academy of Sciences, 92(19):8584–8585. Kaufmann, E., Cappé, O., and Garivier, A. (2012a). On bayesian upper confidence bounds for bandit problems. In Artificial intelligence and statistics, pages 592–600. PMLR. Kaufmann, E., Cappé, O., and Garivier, A. (2016). On the complexity of best arm identification in multi-armed bandit models. Journal of Machine Learning Research, 17:1–42. Kaufmann, E., Korda, N., and Munos, R. (2012b). Thompson sampling: An asymptotically optimal finite-time analysis. In Algorithmic Learning Theory: 23rd International Conference, ALT 2012, Lyon, France, October 29-31, 2012. Proceedings 23, pages 199–213. Springer. Kohli, P., Salek, M., and Stoddard, G. (2013). A fast bandit algorithm for recommendation to users with heterogenous tastes. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 27, pages 1135–1141. Lai, T. L., Robbins, H., et al. (1985). Asymptotically efficient adaptive allocation rules. Advances in applied mathematics, 6(1):4–22. Lattimore, T. and Szepesvári, C. (2020). Bandit algorithms. Cambridge University Press. Li, G., Ma, C., and Srebro, N. (2022a). Pessimism for offline linear contextual bandits using lp confidence sets. arXiv preprint arXiv:2205.10671. Li, G., Shi, L., Chen, Y., Chi, Y., and Wei, Y. (2022b). Settling the sample complexity of model-based offline reinforcement learning. arXiv preprint arXiv:2204.05275. Liu, X., Li, B., Shi, P., and Ying, L. (2020). Pond: Pessimistic-optimistic online dispatching. arXiv preprint arXiv:2010.09995. Nie, G., Agarwal, M., Umrawal, A. K., Aggarwal, V., and Quinn, C. J. (2022). An explore-then-commit algorithm for submodular maximization under full-bandit feedback. In Uncertainty in Artificial Intelligence, pages 1541–1551. PMLR. Phillips, S. D., Pazienza, N. J., and Walsh, D. J. (1984). Decision making styles and progress in occupational decision making. Journal of Vocational Behavior, 25(1):96–105. 12Rashidinejad, P., Zhu, B., Ma, C., Jiao, J., and Russell, S. (2021). Bridging offline reinforcement learning and imitation learning: A tale of pessimism. Advances in Neural Information Processing Systems, 34:11702–11716. Russo, D. and Van Roy, B. (2016). An information-theoretic analysis of thompson sampling. The Journal of Machine Learning Research, 17(1):2442–2471. Russo, D. J., Van Roy, B., Kazerouni, A., Osband, I., Wen, Z., et al. (2018). A tutorial on thompson sampling. Foundations and Trends® in Machine Learning, 11(1):1–96. Shi, L., Li, G., Wei, Y., Chen, Y., and Chi, Y. (2022). Pessimistic q-learning for offline reinforcement learning: Towards optimal sample complexity. arXiv preprint arXiv:2202.13890. Siegel, J. J. (2021). Stocks for the long run: The definitive guide to financial market returns & long-term investment strategies. McGraw-Hill Education. Siroker, D. and Koomen, P. (2015). A/B testing: The most powerful way to turn clicks into customers. John Wiley & Sons. Srikant, R. and Ying, L. (2013). Communication networks: an optimization, control, and stochastic networks perspective. Cambridge University Press. Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3-4):285–294. Villar, S. S., Bowden, J., and Wason, J. (2015). Multi-armed bandit models for the optimal design of clinical trials: benefits and challenges. Statistical science: a review journal of the Institute of Mathematical Statistics, 30(2):199. Xiao, C., Wu, Y., Mei, J., Dai, B., Lattimore, T., Li, L., Szepesvari, C., and Schuurmans, D. (2021). On the optimality of batch policy optimization algorithms. In International Conference on Machine Learning, pages 11362–11371. PMLR. Yang, Z., Liu, X., and Ying, L. (2022a). Exploration. exploitation, and engagement in multi-armed bandits with abandonment. In 2022 58th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 1–2. Yang, Z., Srikant, R., and Ying, L. (2022b). Maxweight with discounted ucb: A provably stable scheduling policy for nonstationary multi-server systems with unknown statistics. arXiv preprint arXiv:2209.01126. Yekkehkhany, A., Arian, E., Hajiesmaili, M., and Nagi, R. (2019). Risk-averse explore-then-commit algorithms for finite-time bandits. In 2019 IEEE 58th Conference on Decision and Control (CDC), pages 8441–8446. IEEE. Yekkehkhany, A., Arian, E., Nagi, R., and Shomorony, I. (2021). A cost–based analysis for risk–averse explore–then–commit finite–time bandits. IISE Transactions, 53(10):1094–1108. Zeng, C., Wang, Q., Mokhtari, S., and Li, T. (2016). Online context-aware recommendation with time varying multi-armed bandit. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pages 2025–2034. Zhang, Q., Wei, H., Wang, W., and Ying, L. (2022). On low-complexity quickest intervention of mutated diffusion processes through local approximation. In Proceedings of the Twenty-Third International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing, pages 141–150. 13A Related Works In this section, we provide a more detailed review of related works. We first review two classic problem formulations of the multi-armed bandit model: the regret minimization problem and the best arm identification problem. Then we review previous works on explore-then-commit algorithms which takes commitment into account. Finally, we review the offline stochastic bandit literature which motivated our choice of pessimistic principle in action identification. A.1 Regret Minimization The theoretical limits of regret minimization have been revealed by Lai et al. (1985); Katehakis and Robbins (1995), which shows that the expected regret of any algorithm is lower bounded when horizon T approaches infinity: lim inf TÑ8 RegAlg µ pTq log T ě ÿ a:∆aą0 ∆a KLpνa, νa˚q, where KLp¨, ¨q denotes the Kullback–Leibler divergence between two distributions. Based on the asymptotic lower bound, we say an algorithm is asymptotically regret optimal if its regret performance achieves the regret lower bound asymptotically. Two sets of algorithms prevail in the regret minimization literature. One is the family of UCB algorithms Auer et al. (2002); Garivier and Cappé (2011); Kaufmann et al. (2012a), which reflects the principal of optimism in action selection to encourage exploration. To be specific, the UCB algorithms will select the action which has the largest upper confidence bound of reward estimation. This upper confidence bound represents the highest possible expected reward given the samples collected from previous rounds under a high probability event. Usually the additional bonus from the empirical mean to the upper confidence bound for a specific action decreases as the number of pulls increases. Therefore, actions which have not been tried frequently in previous rounds will have larger bonus. This trait encourages exploration. By designing the bonus carefully, one can find the optimal trade-off between exploration and exploitation. The other set of prevailing algorithms is the family of Thompson Sampling Algorithms Thompson (1933); Russo et al. (2018); Agrawal and Goyal (2012); Kaufmann et al. (2012b); Russo and Van Roy (2016). The TS algorithm assumes each action is associated with a posterior distribution given the reward feedback from previous rounds. Then the agent will collect one virtual sample from each posterior distribution and choose the action which has the largest virtual sample. As the number of pulls grows, the posterior distribution will be more and more concentrated around the expectation so the virtual sample will also be closer to its expectation. On the other hand, the remaining randomness encourages exploration of other under-explored actions. However, both algorithms do not commit to a single action because their action selection policies have to be re-evaluated at each round based on new observations. A.2 Best Arm Identification The best arm identification algorithms consist of three components: an action sampling rule deciding which action to choose at each round, a stopping rule deciding a time τ to stop collecting new samples, and a decision rule which outputs a best action candidate ˆa. We call an algorithm δ-PAC if the probability of outputting a sub-optimal action is less than δ, i.e., Pµpˆa ‰ 1q ď δ. Here, δ is also called the confidence level. The performance of best arm identification algorithms is measured by both the sample complexity until identifying the optimal action and the confidence level. The theoretical limits for best arm identification algorithms are also well studied in Garivier and Kaufmann (2016); Kaufmann et al. (2016), which shows that any δ-PAC algorithm would incur at least Oplog δ´1q sample complexity 14asymptotically. The constant in front of the logarithmic term depends on the bandit instance, i.e., the reward expectation and distribution of every action. Based on the this lower bound, the authors of Garivier and Kaufmann (2016) proposed the Track and Stop (TAS) algorithm which proved to be asymptotic optimal if the distribution associated with each action is from a single parameter natural exponential family. The TAS algorithm estimates the expectation of each action and at the same time calculates the optimal proportion of pulls for each action so that the optimal action is identifiable from the sub-optimal ones. Then, it designs a feedback control dynamic to track this proportion, similar to the Max-weight dynamic in queueing systems Srikant and Ying (2013). The stopping criterion of TAS is based on a generalized likelihood test. When the optimal action is identifiable from sub-optimal ones, the optimal action candidate ˆa is chosen to be the action which has the largest empirical reward mean. A.3 Explore-Then-Commit Algorithms A natural bridge between regret minimization and best arm identification problems, which also takes into account action commitment, is the explore-then-commit algorithms (Lattimore and Szepesvári, 2020; Garivier et al., 2016; Jin et al., 2021a; Nie et al., 2022; Yekkehkhany et al., 2019, 2021).Researchers have long been hoping that such algorithms will achieve the best of both worlds: maintaining low regret and identifying the optimal action quickly. In these algorithms, a clear separation of exploration phase and exploitation phase exists. At each round, the agent will only make action selection decisions based on the samples collected from the exploration phase, and commitment to a single action during the exploitation phase is required. No samples from the exploitation phase can be utilized although the agent may determine the length of both phases. It is clear that explore-then-commit algorithms can be easily designed from best arm identification algorithms, i.e., one would first run the best arm identification algorithm in the exploration phase, and then commit to the action ˆa found by the algorithm. However, it is shown in (Garivier et al., 2016) that these type of BAI-ETC algorithms is essentially regret sub-optimal. To be specific, the regret lower bound of such algorithms is asymptotically twice as large as the upper bound of optimal regret minimization algorithms such as UCB and Thompson Sampling even with careful tuning. A recent work (Jin et al., 2021a) provides a double explore-then-commit algorithm called DETC which is shown to be asymptotically regret optimal. But the algorithm itself is very complex and requires multiple stages of exploration and exploitation. This trait makes its sample complexity to identify the optimal action large, i.e., Ωplog2 Tq. Moreover, in empirical studies, the DETC algorithm incurs very large regret which is no match for the vanilla UCB algorithm. Another work (Degenne et al., 2019) proposes an exploration algorithm with inflated UCB, which can be naturally adapted to an ETC algorithm. Even though the sample complexity is order optimal, i.e., Oplog Tq, the regret performance is essentially sub-optimal due to the inflation of UCB bonus. A.4 Offline Stochastic Bandits and Reinforcement Learning Our action identification policy is inspired by the principle of pessimism widely adopted in offline stochastic bandit problems Li et al. (2022a); Xiao et al. (2021); Rashidinejad et al. (2021) and reinforcement learning problems Li et al. (2022b); Jin et al. (2021b); Shi et al. (2022). In offline bandit problems, the agent is not allowed to interact with the environment at will. Instead, it is provided with a training dataset which contains action and reward pairs collected from the same bandit problem. Based on this dataset, the agent is asked to choose the optimal action. It is shown in Rashidinejad et al. (2021) that greedily selecting the action with largest empirical mean would fail to produce the optimal action in some bandit problems. This failure results from the randomness of samples in the dataset and the imbalanced number of samples for each action. Instead, choosing the action with largest lower confidence bound to combat the imbalanced uncertainty between estimations of different actions leads to better performance as justified 15in Xiao et al. (2021); Rashidinejad et al. (2021). Similar to the UCB algorithm, the penalty term between the empirical mean and the lower confidence bound for each action decreases as the number of pulls increases, so the action which has not been tried frequently will suffer large penalty. In this way, the agent will avoid selecting an action which has large uncertainty to enhance stability. Our proposed algorithm incorporates the idea of pessimism in action identification. B Proofs of Main Results for EOCP and EOCP-UG In this section, we provide the proofs of main results presented in Section. 4. Throughout the proof section, we let Wa,i be the i-th sample from pulling arm a the i-th time. Let ¯ra,s be the empirical mean of arm a after it has been pulled s times. Before proving the theorems, we provide several lemmas which gathers specific large deviation results useful in our analyses. Then we prove the regret performance based on the concentration lemmas. B.1 Concentration Inequalities We first present two concentration lemmas which characterize the sum and mean of empirical realizations of independent sub-Gaussian random variables as follows: Lemma 1 (Theorem 9.2 in (Lattimore and Szepesvári, 2020)) Let W1, W2, ¨ ¨ ¨ , WT be a sequence of independent σ-subgaussian random variables with ErW1s “ 0. Then, for any δ ą 0, we have: P ˜ Ds ď T, sÿ i“1 Wi ě δ ¸ ď exp ˆ ´ δ2 2Tσ2 ˙ . (2) Lemma 2 (Lemma C.3 in (Jin et al., 2021a)) Let T1 ď T2 ď T be two real numbers in R`. Let W1, W2, ¨ ¨ ¨ , WT be a sequence of identically and independently distributed random variable according to a σ-subgaussian distribution with ErW1s “ 0. Then, for any δ ą 0, we have: P ˆ DT1 ď s ď T2, řs i“1 Wi s ě δ ˙ ď exp ˆ ´T1δ2 2σ2 ˙ (3) The proofs of aforementioned concentration lemmas can be found in the references respectively. Now we present the concentration results for our design of confidence bonuses as follows: Lemma 3 Let W1, W2, ¨ ¨ ¨ , WT be identically and independently distributed 1-sub-Gaussian random variables with ErW1s “ 0. Let T1 ď T2 ď T, then the following holds: (a) if l ě 2, P ˜ Ds P pT1, T2s, řs i“1 Wi s ` c 2l s ď 0 ¸ ď min tT2 ´ T1, el plog T2 ´ log T1q ` eu expplq ; (4a) (b) if δ P p0, ? 3s, P ˜ Ds P rT1, T2s, řs i“1 Wi s ` c 2l s ` δ ď 0 ¸ ď 4 δ2 exp ˜ˆ? l ` δ b T1 2 ˙2¸; (4b) (c) if l ě T1δ2 2 , T2 ÿ s“T1`1 P ˜řs i“1 Wi s ` c 2l s ě δ ¸ ď 2l ` ? 4πl ` 2 δ2 ` 1 ´ T1. (4c) The proof of the Lemma. 3 will be delayed to Sec. D. 16B.2 Proof of Regret Optimality for EOCP with Pre-Determined Stopping Time We prove the following Theorem which characterizes the finite-time performance of Algorithm. 1. Theorem. 1 can be directly derived from Theorem. 1 by letting T approaches infinity. Theorem 5 If l “ logpTq ` 4 a 2 logpTq and T ě 16, the expected regret of the EOCP algorithm in Algorithm. 1 with pre-determined stopping time is upper bounded by: RegEOCP µ pTq ď ÿ a:∆aą0 ˆ2 log T ∆a ` p8 ` ? 20πq?log T ∆a ` 2 ∆a ` ∆a ˙ ` op1q. Remark: The asymptotic result in Theorem. 1 is clear from Theorem.5 by letting T increases to infinity, i.e., lim sup TÑ8 RegEOCP µ pTq log T ď lim sup TÑ8 ÿ a:∆aą0 ˆ 2 ∆a ` p8 ` ? 20πq ∆a ?log T ` 2 ∆a log T ` ∆a log T ˙ ` op1q “ ÿ a:∆aą0 2 ∆a . Proof. Without loss of generality, assume action 1 is the unique optimal action. From the regret decomposition lemma (Lattimore and Szepesvári, 2020, Lemma 4.5), we can decompose the regret of Algorithm. 1 to the number of pulls for each sub-optimal arm as follows: RegEOCP µ pTq “ ÿ a:∆aą0 ∆aErNT paqs. Then, the key to bound the total regret is to bound the number of pulls for each sub-optimal arms. Since our EOCP algorithm has a clear separation of exploration and exploitation phases, so for any sub-optimal action a, we can bound its pulls in different phases as follows: ErNapTqs ď E rN2pTcqs looooomooooon I1 `pT ´ Tcq Ppˆa “ aq looomooon I2 , where Tc is the end of exploration phase and ˆa is the action that the algorithm commits to. Notice that the bound on I2 gives the confidence level result provided in Corollary. 1. Bounding I1: We first bound I1, which is similar to the proof of bounding the regret for the traditional UCB algorithm (Garivier and Cappé, 2011; Auer et al., 2002; Garivier et al., 2016). We have: I1 “ E « Tc ÿ t“1 1At“a ff “ 1 ` E « Tc ÿ t“A`1 1At“a1UCBt´1paqěµ1 ff looooooooooooooooooomooooooooooooooooooon A1 ` E « Tc ÿ t“A`1 1At“a1UCBt´1paqăµ1 ff looooooooooooooooooomooooooooooooooooooon A2 . Notice that At “ a indicates UCBt´1p1q ă UCBt´1paq due to the dynamic of UCB exploration, we can bound the last term as: A2 ď Tc ÿ t“A`1 E “ 1UCBt´1p1qăµ1 ‰ “ Tc ÿ t“A`1 P pUCBt´1p1q ă µ1q . 17Event tUCBt´1p1q ă µ1u means that at round t, the mean estimation ¯rt´1p1q of the optimal arm from previous pulls is lower than µ1 ´ bt´1p1q, which incurs a large deviation, so we can bound this event with a union over all rounds in the exploration phase: tUCBt´1p1q ă µ1u Ă ␣ Dt1 P rA, Tcq, UCBt1p1q ă µ1 ( , which doesn’t depend on round number t any more, so we have: Tc ÿ t“A`1 P pUCBt´1p1q ă µ1q ď Tc ÿ t“A`1 P ` Dt1 P rA, Tcq, UCBt1p1q ă µ1 ˘ “pTc ´ AqP ` Dt1 P rA, Tcq, UCBt1p1q ă µ1 ˘ “pTc ´ AqP ˜ Dt1 P rA, Tcq, ¯rt1p1q ` d 2l Nt1p1q ă µ1 ¸ It is worth-noting that if there is no pulls for arm 1 between a time interval, its empirical mean and number of pulls will remain the same. So we have: # Dt1 P rA, Tcq, ¯rt1p1q ` d 2l Nt1p1q ă µ1 + Ă # Ds P r1, NTcpa˚qq , ¯rs,1 ` c 2l s ă µ1 + Notice that NTcp1q ď Tc ´ A ` 1, so we can bound the probability as follows: P ˜ Dt1 P rA, Tcq, ¯rt1p1q ` d 2l Nt1p1q ă µ1 ¸ ďP ˜ Ds P r1, Tc ´ A ` 1q, ¯rs,1 ´ µ1 ` c 2l s ă 0 ¸ ďP ˜ Ds P r1, Tc ´ A ` 1q, řs i“1pW1,i ´ µ1q i ` c 2l s ă 0 ¸ ďTc ´ A ` 1 expplq , where the last inequality uses Eq. (4a) of Lemma 3 and the fact that W1,i ´ µ1 is 1-sub-Gaussian with zero mean. Notice that ?log T ě log log T when T is large and we have: expplq “ exp ´ log T ` 4 a 2 log T ¯ ě expplog T ` 4 log log Tq “ 1 T log4 T . So putting the above bound back to the bound of A2, we have: A2 ď pTc ´ A ` 1q2 T log2 T “ ˆ 8Aplog T`4?2 log Tq ∆2 min ` 1 ˙2 T log4 T “ o ˆ 1 T ˙ , where the last inequality is due to the fact that when T ě 3, ?log T ď log T. Next, we attempt to bound the middle term A1 as follows: A1 “ E « Tc ÿ t“A`1 1At“a1UCBt´1paqěµ1 ff “E « Tc ÿ t“A`1 1At“a1¯rt´1paq` b 2l Nt´1paq ěµ1 ff 18Notice that the number of pulls Ntpaq will only increase by 1 every time there is new pull, i.e., when At “ a. Otherwise, the term inside summation is 0. So instead of counting on the time step t, we can count over the number of pulls over arm a as follows: E « Tc ÿ t“A`1 1At“a1¯rt´1paq` b 2l Nt´1paq ěµ1 ff “E » – NTcpaq ÿ s“1 1¯rs,a` b 2l s ěµ1 fi fl ď Tc ÿ s“1 P ˜ ¯rs,a ´ µa ` c 2l s ě ∆a ¸ , where the last inequality is due to NTcpaq ď Tc. Then, by Eq. (4c) of Lemma. 3, we have: Tc ÿ s“1 P ˜ ¯rs,a ´ µa ` c 2l s ě ∆a ¸ “ Tc ÿ s“1 P ˜řs i“1pWa,i ´ µaq i ` c 2l s ě ∆a ¸ ď 2 ` log T ` 4?2 log T ˘ ` b 4π ` log T ` 4?2 log T ˘ ` 2 ∆2a ` 1 ď2 log T ∆2a ` p8 ` ? 20πq?log T ∆2a ` 2 ∆2a ` 1, where the first inequality is due to the fact that Wa,i ´ µa is 1-sub-Gaussian with zero mean, and the last inequality is due to ?2 log T ď log T when T ě 9. Therefore, combining the bounds on A1 and A2, we can bound I1 as follows: I1 ď 2 log T ∆2a ` p8 ` ? 20πq?log T ∆2a ` 2 ∆2a ` 1 ` o p1q . Bounding I2: The bound on I2 gives the confidence level result provided in Corollary. 1. After the exploration phase, recall that we select the arm with the largest lower confidence bound to commit to. The idea of bounding I2 is very similar to the regret bound in (Yang et al., 2022a) where since we use UCB to explore in the exploration phase, the optimal arm should be pulled very often such that its bonus becomes very small when exploration phase ends. Then selecting the arm with largest LCB will ensure we select the optimal arm with high probability. This is because the LCB of the optimal arm is larger than the true means of any sub-optimal arms, and the true means of sub-optimal arms are larger than their respective LCB, both with high probability. We can first bound the probability of selecting a sub-optimal arm a as: P pˆa “ aq ďP pLCBTcpaq ě LCBTcp1qq ď P ˆ LCBTcpaq ě LCBTcp1q, NTcp1q ą 8l ∆2 min ˙ loooooooooooooooooooooooooooomoooooooooooooooooooooooooooon B1 ` P ˆ NTcp1q ď 8l ∆2 min ˙ loooooooooooomoooooooooooon B2 We first bound the term B2 which states that during the first exploration phase with UCB exploration, the optimal arm is under-pulled, which means that one of the sub-optimal arms have been pulled with larger number of times. Therefore, we have: P ˆ NTcp1q ď 8l ∆2 min ˙ “ P ˜ ÿ a:∆aą0 NTcpaq ą Tc ´ 8l ∆2 min ¸ . Recall that Tc “ 8Al ∆2 min ` A, so we have: P ˆ NTcp1q ď 8l ∆2 min ˙ “P ˜ ÿ a:∆aą0 NTcpaq ą 8pA ´ 1ql ∆2 min ` A ¸ ď P ˆ Da ą 1, NTcpaq ą 8l ∆2 min ` 1 ˙ . 19Then by union bound, we have: P ˆ Da ą 1, NTcpaq ą 8l ∆2 min ` 1 ˙ ď ÿ a:∆aą0 P ˆ NTcpaq ą 8l ∆2 min ` 1 ˙ . Consider a fixed sub-optimal arm a, then for each probability inside the summation, we have: P ˆ NTcpaq ą 8l ∆2 min ` 1 ˙ ďP ˆ Dt P rA ` 1, Tcs, Nt´1paq “ R 8l ∆2 min V , At “ 2 ˙ ďP ˆ Dt P rA ` 1, Tcs, Nt´1paq “ R 8l ∆2 min V , UCBt´1p1q ď UCBt´1paq ˙ ď P ˆ Dt P rA ` 1, Tcs, Nt´1paq “ R 8l ∆2 min V , µ1 ď UCBt´1paq ˙ looooooooooooooooooooooooooooooooooooooomooooooooooooooooooooooooooooooooooooooon B3 ` P pDt P rA ` 1, Tcs, UCBt´1p1q ď µ1q looooooooooooooooooooooomooooooooooooooooooooooon B4 . Notice that B4 can be bounded from concentration lemma. First, we switch the count from time step t to the number of pulls for arm 1 since the empirical estimation ¯rt´1p1q and count for pulls Nt´1p1q won’t change unless there is a new pull. Then, we will apply Eq. (4a) to the probability as follows: B4 “P ˜ Dt P rA ` 1, Tcs, ¯rt´1p1q ´ µ1 ` d 2l Nt´1p1q ď 0 ¸ ďP ˜ Ds P r1, Tc ´ A ` 1q, ¯rs,1 ´ µ1 ` c 2l s ď 0 ¸ ďP ˜ Ds P r1, Tc ´ A ` 1q, řs i“1pW1,i ´ µ1q s ` c 2l s ď 0 ¸ ďTc ´ A exp l , where the last inequality is due to the fact that W1,i ´ µ1 is 1-sub-Gaussian with zero mean. Similar to the procedure of bounding I1, notice that expplq ě T log4 T, we have: B4 ď 8A ` log T ` 4?2 log T ˘ ∆2 min 1 T log4 T “ o ˆ 1 T ˙ . On the other hand, let γ “ Q 8l ∆2 min U , and we also change the count from time step t to the number of pulls for arm a. Then, B3 can be expressed as follows: B3 “P ˜ Dt P rA ` 1, Tc ´ 1s, Nt´1paq “ γ, µ1 ď ¯rt´1paq ` d 2l Nt´1paq ¸ “P ˜ ¯rγ,a ` d 2l γ ą µ1 ¸ Notice that γ ě 8l ∆2 min ě 8l ∆2a , so we have b 2l γ ď b 2l∆2a 8l “ ∆a 2 . Then B3 can be bounded as follows: B3 ď P ˆ ¯rγ,a ` ∆a 2 ą µ1 ˙ “ P ˆ ¯rγ,a ´ µa ą ∆a 2 ˙ ď exp ˆ ´γ∆2 a 8 ˙ ď 1 expplq ď 1 T log4 T , 20where the first inequality uses Hoeffding’s inequality and the second inequality uses the lower bound on γ mentioned above. The last inequality is due to the fact that expplq ě T log4 T. So combine B3 and B4 together, we can bound B2 as follows: B2 ď ÿ a:∆aą0 pB3 ` B4q “ o ˆ 1 T ˙ . Next, we attempt to bound B1. Notice that B1 can also be bounded as follows: B1 “P ˆ LCBTcpaq ě LCBTcp1q, NTcp1q ą 8l ∆2 min ˙ ď P ˆ µa ě LCBTcp1q, NTcp1q ą 8l ∆2 min ˙ looooooooooooooooooooooomooooooooooooooooooooooon B5 ` P pµa ď LCBTcpaqq loooooooooomoooooooooon B6 . The term B6 can be expressed by counting the number of pulls of arm a as follows: B6 “P ˜ µa ď ¯rTcpaq ´ d 2l NTcpaq ¸ ďP ˜ Ds P r1, Tc ´ A ` 1s, µa ´ ¯rs,a ` c 2l s ď 0 ¸ “P ˜ Ds P r1, Tc ´ A ` 1s, řs i“1pµa ´ Wa,iq s ` c 2l s ď 0 ¸ . Notice that pµa ´ Wa,iq is 1-sub-Gaussian with zero mean, so we can apply concentration lemma Eq. (4a) from Lemma. (3) as follows: B6 ď Tc ´ A ` 1 expplq “ 8A ` log T ` 4?2 log T ˘ ∆2 min expplq ` 1 expplq “ o ˆ 1 T ˙ , where the last inequality is due to log T ě ?2 log T and expplq ě T log4 T when T ě 9. Similarly, term B5 can be expressed as: B5 “ P ˜ µa ě ¯rTcp1q ´ d 2l NTcp1q, NTcp1q ą 8l ∆2 min ¸ . First notice that when NTcp1q ą 8l ∆2 min , we have the bonus term bTcp1q “ b 2l NTcp1q ď ∆min 2 ď ∆a 2 . Then we have: B5 ď P ˆ µa ě ¯rTcp1q ´ ∆a 2 , NTcp1q ą 8l ∆2 min ˙ “ P ˆ µ1 ´ ¯rTcp1q ě ∆a 2 , NTcp1q ą 8l ∆2 min ˙ Then it is equivalent to count over the number of pulls for arm 1: B5 ďP ˆ Ds P „ 8l ∆2 min , T ȷ , µ1 ´ ¯rs,1 ě ∆a 2 ˙ “P ˆ Ds P „ 8l ∆2 min , T ȷ , řs i“1pµ1 ´ W1,iq s ě ∆a 2 ˙ . 21Then, by the maximal concentration Eq. (3) from Lemma. 2, we can bound B5 as follows: P ˆ Ds P „ 8l ∆2 min , T ȷ , řs i“1pµ1 ´ W1,iq s ě ∆a 2 ˙ ď exp ¨ ˝´ 8l ∆2 min ` ∆a 2 ˘2 2 ˛ ‚“ 1 expplq ď 1 T log4 T . Therefore, collecting the bounds for B5 and B6, we have a bound for B1 as follows: B1 ď B5 ` B6 “ o ˆ 1 T ˙ . And therefore from the bounds of B1 and B2, we can bound term I2 as follows: I2 ď B1 ` B2 “ o ˆ 1 T ˙ . This means the the confidence of selecting the wrong action to commit to is opT ´1q as indicated in Corollary. 1. Finally, putting the bounds on I1 and I2 together, we have: ErNapTqs ďI1 ` TI2 ď 2 log T ∆2a ` p8 ` ? 20πq?log T ∆2a ` 2 ∆2a ` 1 ` o p1q . Therefore, by the regret decomposition lemma, we can bound the total regret as follows: RegµpTq ď ÿ a:∆aą0 ˆ2 log T ∆a ` p8 ` ? 20πq?log T ∆a ` 2 ∆a ` ∆a ` o p1q ˙ “ ÿ a:∆aą0 ˆ2 log T ∆a ` p8 ` ? 20πq?log T ∆a ` 2 ∆a ` ∆a ˙ ` op1q. B.3 Proof of Regret Optimality for EOCP-UG with Adaptive Stopping Time We prove the following theorem which characterizes the finite-time performance of Algorithm. 2. Theo- rem. 2 can be derived by letting T approaches infinity. Theorem 6 Under Gaussian bandits time horizon T ě 16, the Regret of EOCP algorithm in Algorithm. 2 with adaptive stopping time can be upper bounded as: RegEOCP-UG µ pTq ď ÿ a:∆aą0 ˆ2 log T ∆a ` p8 ` ? 20πq?log T ∆a ˙ ` Op1q. Remark: The asymptotic result in Theorem. 2 is clear from Theorem.6 by letting T increases to infinity, i.e., lim sup TÑ8 RegEOCP-UG µ pTq log T ď lim sup TÑ8 ÿ a:∆aą0 ˆ 2 ∆a ` p8 ` ? 20πq ∆a ?log T ˙ ` Oplog´1pTqq “ ÿ a:∆aą0 2 ∆a . Proof. Without loss of generality, let action 1 be the unique optimal action. The first step for proving the regret performance is regret decomposition lemma (Lattimore and Szepesvári, 2020, Lemma 4.5). We also decompose the regret of Algorithm. 2 into the number of pulls for each sub-optimal arm as follows: RegµpTq “ ÿ a:∆aą0 ∆aErNT paqs. 22Then, for a specific sub-optimal arm a, we bound the number of pulls. Since our EOCP-UG algorithm has a clear separation of exploration and exploitation phases, we can bound the pulls in the two phases respectively. However, the unique characteristic of unknown gap scenario is we don’t have a fixed end time of exploration phase. Recall that Tc ď T is the stopping time that the exploration phase ends and ˆa is the arm we choose for commitment, so we can decompose the number of pulls into two phases as: ErNapTqs “ ErNapTcqs ` ErpT ´ Tcq1ˆa“as ď ErNapTcqs loooomoooon I1 `T Ppˆa “ aq looomooon I2 . Notice that upper bound of I2 gives the confidence level result in Corollary. 2. We then bound the two terms I1 and I2 separately. In order to simplify the proof, we first prove a lemma which characterizes a high probability upper bound for the stopping time Tc. This lemma also proves the sample complexity to commitment result in Corollary. 2. The proof of Lemma. 4 will be delayed. Lemma 4 Under Gaussian bandits with time horizon T, our stopping time Tc for exploration of Algo- rithm. 2 is upper bounded with high probability: P ˜ Tc ě ÿ a:∆aą0 8pl ` 1q2 ∆2a ` Apl ` 2q ¸ ď 10eA T log2 T . Bounding I1: With the help of Lemma. 4, we have a high probability upper bound for the exploration phase. We let T u c “ ř a:∆aą0 8pl`1q2 ∆2a ` Apl ` 2q to be the high probability upper bound of our stopping time to end the exploration phase. Notice that NapTcq ď T, we have: I1 “ErNapTcq1TcěT u c s ` ErNapTcq1TcďT u c s ď TP pTc ě T u c q ` ErNapTcq1TcďT u c s ď ErNapTcq1TcďT u c s looooooooomooooooooon I3 ` 10eA log2 T . Then, bounding I3 is similar to bounding I1 for Theorem. 5. We decompose I3 as follows: I3 “E « 1TcďT u c Tc ÿ t“1 1At“a ff “1 ` E « 1TcďT u c Tc ÿ t“A`1 1At“a1UCBt´1paqěµ1 ff loooooooooooooooooooooooomoooooooooooooooooooooooon A1 ` E « 1TcďT u c Tc ÿ t“A`1 1At“a1UCBt´1paqăµ1 ff loooooooooooooooooooooooomoooooooooooooooooooooooon A2 , In order to bound A1 and A2, we assume there is a virtual process that after the end time Tc of exploration phase, it continues to select the arm with largest UCB and receive the corresponding reward until time T u c . This is only a virtual process used in our proof, while in reality our algorithm will stop exploration after stopping time Tc. We will use E1 and P1 to denote the expectation and probability over this virtual process. Then, we can bound A2 as follows: A2 ďE « 1TcďT u c Tc ÿ t“A`1 1UCBt´1p1qăµ1 ff ď E1 « T u c ÿ t“A`1 1UCBt´1p1qăµ1 ff “ T u c ÿ t“A`1 P1 pUCBt´1p1q ă µ1q . where the first inequality is because At “ a indicates UCBt´1p1q ă UCBt´1paq due to the dynamic of UCB exploration. The second inequality is because E and E1 are totally the same for the first Tc time 23steps. This step also allows us to bound A2 over the events on a different probability measure E1 and P1. Event tUCBt´1p1q ă µ1u means that at time step t, the mean estimation ¯rt´1p1q of the optimal arm from previous pulls is lower than µ1 ´ bt´1p1q, which incurs a large deviation, so we can bound this event with a union over all time steps: tUCBt´1p1q ă µ1u Ă ␣ Dt1 P rA, T u c q, UCBt1p1q ă µ1 ( , which doesn’t depend on time step t any more, so we have: T u c ÿ t“A`1 P1 pUCBt´1p1q ă µ1q ď T u c ÿ t“A`1 P1 ` Dt1 P rA, T u c q, UCBt1p1q ă µ1 ˘ “pT u c ´ AqP1 ` Dt1 P rA, T u c q, UCBt1p1q ă µ1 ˘ “pT u c ´ AqP1 ˜ Dt1 P rA, T u c q, ¯rt1p1q ` d 2l Nt1p1q ă µ1 ¸ . If there is no pulls for arm 1 in a time interval, the empirical mean and number of pulls will remain the same. So instead of counting on the time steps, we can count the number of pulls for arm 1: # Dt1 P rA, T u c q, ¯rt1p1q ` d 2l Nt1p1q ă µ1 + Ă # Ds P “ 1, NT u c paq ˘ , ¯rs,1 ` c 2l s ă µ1 + . Notice that NT u c paq ď T u c ´ A ` 1, so we can bound the probability as follows: P1 ˜ Dt1 P rA, T u c q, ¯rt1p1q ` d 2l Nt1p1q ă µ1 ¸ ďP1 ˜ Ds P r1, T u c ´ A ` 1q, ¯rs,1 ´ µ1 ` c 2l s ă 0 ¸ ďP1 ˜ Ds P r1, T u c ´ A ` 1q, řs i“1pW1,i ´ µ1q i ` c 2l s ă 0 ¸ ďT u c ´ A expplq , where the last inequality uses Eq. (4a) of Lemma 3 and the fact that W1,i ´ µ1 is 1-sub-Gaussian with zero mean. Notice that expplq ě T log4 T, so putting the above bound back to the bound of A2, we have: A2 ď ´ř a:∆aą0 8l2 ∆2a ` Al ´ A ¯2 T log4 T ď ´ř a:∆aą0 200 log2 T ∆2a ` 5A log T ¯2 T log4 T “ OpT ´1q. where the first inequality is due to the fact that when T ě 3, ?log T ď log T. Next, we attempt to bound A1 as follows: A1 “ E « 1TcďT u c T u c ÿ t“A`1 1At“a1UCBt´1paqěµ1 ff ďE1 « T u c ÿ t“A`1 1At“a1¯rt´1paq` b 2l Nt´1paq ěµ1 ff , where the inequality is also due to the fact that the virtual and real processes are identical before time Tc. Notice that the number of pulls Ntpaq will only increase by 1 every time there is new pull, i.e., when 24At “ a. Otherwise, the term inside summation is 0. So instead of counting on the time step t, we can count over the number of pulls over arm a as follows: E1 « T u c ÿ t“A`1 1At“a1¯rt´1paq` b 2l Nt´1paq ěµ1 ff “E1 » – NT u c paq ÿ s“1 1¯rs,a` b 2l s ěµ1 fi fl ď T u c ÿ s“1 P1 ˜ ¯rs,a ´ µa ` c 2l s ě ∆a ¸ , where the last inequality is due to NT u c paq ď T u c . Then, by Eq. (4c) of Lemma. 3, we have: T u c ÿ s“1 P1 ˜ ¯rs,a ´ µa ` c 2l s ě ∆a ¸ “ T u c ÿ s“1 P1 ˜řs i“1pWa,i ´ µaq i ` c 2l s ě ∆a ¸ ď 2 ` log T ` 4?2 log T ˘ ` b 4π ` log T ` 4?2 log T ˘ ` 2 ∆2a ` 1 ď2 log T ∆2a ` p8 ` ? 20πq?log T ∆2a ` 2 ∆2a ` 1, where the first inequality is due to the fact that Wa,i ´ µa is 1-sub-Gaussian with zero mean, and the last inequality is due to ?2 log T ď log T when T ě 9. Therefore, combining the bounds on A1 and A2, we can bound I3 as follows: I3 ď 2 log T ∆2a ` p8 ` ? 20πq?log T ∆2a ` 2 ∆2a ` 1 ` OpT ´1q. Therefore, a similar bound can be established on I1 as follows: I1 ď 2 log T ∆2a ` p8 ` ? 20πq?log T ∆2a ` 2 ∆2a ` 1 ` 10eA log2 T ` OpT ´1q. Bounding I2: Recall that our stopping criterion is when there exists an arm ˜a whose number of pulls is significantly larger than other arms, i.e., NTcp˜aq ě l maxa‰˜a NTcpaq. Therefore, its bonus bTcp˜aq should be very small compared to other arms. Also recall that we select the arm ˆa with highest LCB to commit to, so the proof follows two steps. First we will show that with high probability the arm ˜a with most number of pulls is the best arm. Then we will show that under this circumstance, the maximum LCB arm is also the best arm. Therefore, consider an arbitrary sub-optimal arm a: P p˜a “ aq ď P pNTcpaq ě rlNTcp1qs ` 1q . Notice that if NTcpaq ě rlNTcp1qs ` 1 at the end time Tc, we must have pulled arm a at time step Tc. This also means that the number of pulls NTc´1paq for arm a after time step Tc ´ 1 is exactly rlNTcp1qs and and NTc´1p1q “ NTcp1q. It also means that arm a has the largest UCB. So we have: P pNTcpaq ě rlNTcp1qs ` 1q ďP pNTc´1paq “ rlNTc´1p1qs , UCBTc´1paq ě UCBTc´1p1qq ďP ˜ Dt ď Tc, Ntpaq “ rlNtp1qs , ¯rtpaq ` d 2l Ntpaq ě ¯rtp1q ` d 2l Ntp1q ¸ 25Notice that we can separate the two random quantities with union bound as follows: P pNTcpaq ě rlNTcp1qs ` 1q ď P ˜ Dt ď Tc, ¯rtpaq ě µa ` d 2l Ntpaq ¸ looooooooooooooooooooooomooooooooooooooooooooooon B1 ` P ˜ Dt ď Tc, Ntpaq “ rlNtp1qs , µa ` 2 d 2l Ntpaq ě ¯rtp1q ` d 2l Ntp1q ¸ loooooooooooooooooooooooooooooooooooooooooooomoooooooooooooooooooooooooooooooooooooooooooon B2 We first bound B1. Notice that we can switch from counting of time step t to count the number of pulls for arm a. It is clear that Ntpaq ď Tc ď T when t ď Tc, so we have: B1 ďP ˜ Ds ď T, ¯rs,a ě µa ` c 2l s ¸ “ P ˜ Ds ď T, řs i“1pµa ´ Wa,iq s ` c 2l s ď 0 ¸ ďel log T ` e exp l , where the last inequality uses Eq. (4a) of Lemma. 3. Also notice that expplq ě T log4 T, so we have: B1 ď 10e log2 T ` e T log4 T “ o ˆ 1 T ˙ . Next, we bound the term B2, we can rearrange the terms inside B2 as follows: B2 “ P ˜ Dt ď Tc, Ntpaq “ rlNtp1qs , ¯rtp1q ´ µ1 ` ˆ 1 ´ 2 ? l ˙ d 2l Ntp1q ` ∆a ď 0 ¸ . Denote α “ 1 ´ 2 ? l, and switch the count from time step t to the number of pulls for arm 1, we can bound B2 as follows: B2 ďP ˜ Ds ď T, ¯rs,1 ´ µ1 ` c 2α2l s ` ∆a ď 0 ¸ “ P ˜ Ds ď T, řs i“1pW1,i ´ µ1q s ` c 2α2l s ` ∆a ď 0 ¸ ď 4 ∆2a exp pα2lq. When T is large enough, we have α2l “ l ´ 4 ? l ` 4 ě log T ` 4, so we have: B2 ď 4 ∆2aT . Therefore, combining B1 and B2 together, we can bound the probability as follows: P p˜a “ aq ď P pNTcpaq ě rlNTcp1qs ` 1q ď B1 ` B2 ď O ˆ 1 T ˙ . Recall that arm 1 is the optimal arm. Therefore, by a union bound, we can characterize the probability that of event t˜a ‰ 1u as follows: P p˜a ‰ 1q ď ÿ a:∆aą0 P p˜a “ aq ď O ˆ 1 T ˙ . 26Next, we investigate the probability of choosing the wrong arm ˆa to commit to. Recall that for the arm which we commit to, we choose the one with the largest LCB. Consider any sub-optimal arm a, if we wrongly choose the arm ˆa “ a, it means its LCB should be larger than the LCB of the optimal arm, which with high probability has the largest number of pulls. So, we have: P pˆa “ aq ď P pˆa “ a, ˜a “ 1q ` P p˜a ‰ 1q ď P pLCBTcpaq ě LCBTcp1q, ˜a “ 1q ` O ˆ 1 T ˙ . When ˜a “ 1, arm 1 has l times more pulls than arm a when the exploration phase stops, so we have: P pLCBTcpaq ě LCBTcp1q, ˜a “ 1q ďP ˜ ¯rTcpaq ´ d 2l NTcpaq ě ¯rTcp1q ´ d 2l NTcp1q, NTcp1q ě lNTcpaq ¸ . Similarly, we separate the two random variables as follows: P pLCBTcpaq ě LCBTcp1q, ˜a “ 1q ďP ˜ ¯rTcpaq ´ d 2l NTcpaq ě µ1 ´ 2 d 2l NTcp1q, NTcp1q ě lNTcpaq ¸ ` P ˜ ¯rTcp1q ď µ1 ´ d 2l NTcp1q ¸ ď P ˜ pµa ´ ¯rTcpaqq ` ˆ 1 ´ 2 ? l ˙ d 2l NTcpaq ` ∆a ď 0 ¸ looooooooooooooooooooooooooooooooooomooooooooooooooooooooooooooooooooooon B3 ` P ˜ ¯rTcp1q ď µ1 ´ d 2l NTcp1q ¸ loooooooooooooooooomoooooooooooooooooon B4 . For B4, notice that Tc is a random stopping time, so we bound the probability over all time step when the number of pulls is larger than l as follows: B4 ď P ˜ Ds P rl, Ts, ¯rs,1 ´ µ1 ` c 2l s ď 0 ¸ “ P ˜ Ds P rl, Ts, řs i“1pW1,i ´ µ1q s ` c 2l s ď 0 ¸ . Since W1,i ´ µ1 is 1-subgaussian, we can use Eq. (4a) from the concentration Lemma. 3 to bound B4 as follows: B4 ď P ˜ Ds P rl, Ts, řs i“1pW1,i ´ µ1q s ` c 2l s ď 0 ¸ ď el log T ` e exp l “ o ˆ 1 T ˙ , where the last inequality is due to ?2 log T ď log T when T ě 9 and expplq ě T log4 T. On the other hand, bounding B3 is similar to the proof of probability upper bound regarding ˜a. recall that α “ ´ 1 ´ 2 ? l ¯ , and notice that Tc is a random variable, so we use a union over all possible arm pulls to bound the event on time step Tc as follows: B3 ďP ˜ Ds ď T, pµa ´ ¯rs,aq ` c 2α2l s ` ∆a ď 0 ¸ ďP ˜ Ds ď T, řs i“1pµa ´ Wa,iq s ` c 2α2l s ` ∆a ď 0 ¸ ď 4 ∆2a exp pα2lq, 27where the last inequality uses the second concentration inequality Eq. (4b) from Lemma. 3 and the fact that µa ´ Wa,i is 1-subgaussian. Then when T is large enough, we have: B3 ď 4 ∆2aT . Combining B3 and B4, we can bound the probability that we select the wrong arm as: P pLCBTcpaq ě LCBTcp1q, ˜a “ 1q ď B3 ` B4 ď 4 ∆2aT ` o ˆ 1 T ˙ ď O ˆ 1 T ˙ . Therefore, we finally bound I2 as follows: I2 “ Ppˆa “ aq ď P pˆa “ a, ˜a “ 1q ` P p˜a ‰ 1q “ O ˆ 1 T ˙ . Here, the bound on I2 shows that the confidence level of our algorithm is OpT ´1q indicated in Corollary. 2. Combining I1 and I2, we can bound ErNapTqs in finite time as follows: ErNapTqs ď I1 ` TI2 ď 2 log T ∆2a ` p8 ` ? 20πq?log T ∆2a ` Op1q. So by the regret decomposition lemma, we can bound the regret performance as: RegEOCP-UG µ pTq ď ÿ a:∆aą0 ˆ2 log T ∆a ` p8 ` ? 20πq?log T ∆a ˙ ` Op1q. B.4 Proof of Lemma. 4 and Corollary. 2 Consider a specific sub-optimal arm a. We first show that it can only be pulled Oplog Tq with high probability during the exploration phase due to the dynamic of our exploration strategy, i.e., the UCB exploration strategy. To be specific, we intend to show the following inequality: P ˆ Dt ď Tc, Ntpaq ě 8l ∆2a ` 1 ˙ ď 10e T log2 T . Since during the exploration phase, we select the arm with the largest UCB to explore, if there exists a time t which the number of pulls for action a is larger than 8l ∆2a ` 1, which means that there exists a time t1 P rA, tq, where at time t1 ` 1 the number of pulls for previous rounds Nt1paq is exactly 8l ∆2a and arm a has the largest UCB. Therefore, we can bound the probability as: P ˆ Dt ď Tc, Ntpaq ě 8l ∆2a ` 1 ˙ ď P ˆ Dt1 ď t ď T, Nt1paq “ 8l ∆2a , a “ arg max a1 UCBt1pa1q ˙ . Therefore, the UCB of arm a should be larger than the UCB of the optimal arm 1, i.e., UCBt1paq ě UCBt1p1q. So we can further derive an upper bound as follows: P ˆ Dt1 ď t ď T, Nt1paq “ R 8l ∆2a V , a “ arg max a1 UCBt1pa1q ˙ ďP ˆ Dt1 ď T, Nt1paq “ R 8l ∆2a V , UCBt1paq ě UCBt1p1q ˙ “P ˜ Dt1 ď T, Nt1paq “ R 8l ∆2a V , ¯rt1paq ` d 2l Nt1paq ě ¯rt1p1q ` d 2l Nt1p1q ¸ ďP ˜ Dt1 ď T, ¯rR 8l ∆2a V ,a ` ∆a 2 ě ¯rt1p1q ` d 2l Nt1p1q ¸ . 28Then, we can separate the two empirical means ¯rR 8l ∆2a V ,a and ¯rt1p1q with a union bound as follows: P ˜ Dt1 ď T, ¯rR 8l ∆2a V ,a ` ∆a 2 ě ¯rt1p1q ` d 2l Nt1p1q ¸ ď P ˜ Dt1 ď T, ¯rt1p1q ` d 2l Nt1p1q ď µa ` ∆a ¸ looooooooooooooooooooooooooomooooooooooooooooooooooooooon A1 ` P ˜ ¯rR 8l ∆2a V ,a ě µa ` ∆a 2 ¸ looooooooooooooomooooooooooooooon A2 . The term A2 can be easily bounded through Hoeffding’s inequality as: A2 “ P ¨ ˚ ˚ ˝ ř R 8l ∆2a V i“1 pWa,i ´ µaq Q 8l ∆2a U ě ∆a 2 ˛ ‹‹‚ď exp ˆ ´ R 8l ∆2a V ∆2 a 8 ˙ ď 1 expplq, where the first inequlity is due to the fact that Wa,i ´ µa is 1-subgaussian. For term A1, we notice that the empirical estimation ¯rt1p1q and the counter Nt1p1q will remain the same if there is no pull for arm 1, so we can switch the count of time step t1 to the count of number of pulls for N1 tp1q. To be specific, A1 ď P ˜ Ds ď T, ¯rs,1 ` c 2l s ď µa ` ∆a ¸ “ P ˜ Ds ď T, řs i“1pW1,i ´ µ1q s ` c 2l s ď 0 ¸ . Since W1,i ´ µ1 is 1-subgaussian, we can use the concentration result Eq. (4a) of Lemma. 3 to bound A1 as follows: P ˜ Ds ď T, řs i“1pW1,i ´ µ1q s ` c 2l s ď 0 ¸ ď el log T ` e expplq . Therefore, we have can bound the probability of overpull for sub-optimal arm a as follows: P ˆ Dt ď Tc, Ntpaq ě 8l ∆2a ` 1 ˙ ď A1 ` A2 ď eplog T ` 4?2 log Tq log T ` 2e expplq ď 10e T log2 T , where the third inequality is due to log T ě ?2 log T when T ě 9 and the last inequality is due to expplq ě T log4 T. Recall that in Algorithm. 2, the exploration phase will stop if there exists an arm ˜a whose pulls NTcp˜aq at the stopping time have exceeds l times of all other arms, i.e., l max a‰˜a NTcpaq ` 2 ě NTcp˜aq ą l max a‰˜a NTcpaq ` 1. So if Tc is larger than ř a:∆aą0 8pl`1q2 ∆2a ` Apl ` 2q, it means that there exists at least one sub-optimal arm a whose pulls NTcpaq is larger than 8l ∆2a ` 1. So we have: P ˜ Tc ě ÿ a:∆aą0 8pl ` 1q2 ∆2a ` Apl ` 2q ¸ ďP ˆ Da : ∆a ą 0, NTcpaq ě 8l ∆2a ` 1 ˙ ď ÿ a:∆aą0 P ˆ Dt ď Tc, Ntpaq ě 8l ∆2a ` 1 ˙ ď 10eA T log2 T , 29where the second inequality, we use union bound over a. Then Corollary. 2 can be easily proved with: SCCEOCP-UG µ pTq “ErTcs “E « Tc1Tcěř a:∆aą0 8pl`1q2 ∆2a `Apl`2q ff ` E « Tc1Tcăř a:∆aą0 8pl`1q2 ∆2a `Apl`2q ff ďTP ˜ Tc ě ÿ a:∆aą0 8pl ` 1q2 ∆2a ` Apl ` 2q ¸ ` ÿ a:∆aą0 8pl ` 1q2 ∆2a ` Apl ` 2q ď ÿ a:∆aą0 8 log2 T ` 80 log 3 2 T ` 200 log T ∆2a ` 6A log T ` 10eA log2 T . Therefore, taking T to infinity, we have: lim sup TÑ8 SCCEOCP-UG µ pTq log2 T “ ÿ a:∆aą0 8 ∆2a . B.5 Proof of Theorem. 3 We first prove the fundamental limits of sample complexity until commitment in the pre-determined stopping time setting. Suppose the bandit problem (instance) is as follows: the reward expectation of action 1 is µ1 and the reward expectation of action 2 is µ2. We assume µ1 ą µ2 and ∆ “ µ1 ´ µ2. Consider another bandit instance with reward expectations λ1 and λ2 for the two actions respectively, and λ1 ` ∆ ď λ2. By the “transportation” lemma (Kaufmann et al., 2016, Lemma. 1) when T ě 10, we have for any stopping time τ that: EµrN1pτqsKLpµ1, λ1q ` EµrN2pτqsKLpµ2, λ2q ě log ˆ T 2.4 ˙ . Since under Gaussian bandit, the KL divergence is simply the squared norm, we have: EµrN1pτqspµ1 ´ λ1q2 2 ` EµrN2pτqspµ2 ´ λ2q2 2 ě log ˆ T 2.4 ˙ . Since the inequality holds for any λ1 and λ2 such that that λ1 ` ∆ ď λ2, we can minimize the LHS to obtain tighter bounds. So minimizing the LHS over λ1 and λ2 gives: λ1 “ µ1EµrN1pτqs ` pµ2 ´ ∆qEµrN2pτqs EµrN1pτqs ` EµrN2pτqs , λ2 “ pµ1 ` ∆qEµrN1pτqs ` µ2EµrN2pτqs EµrN1pτqs ` EµrN2pτqs . So plug the optimization result into the “transportation” lemma, we have: 2∆2EµrN2pτqs2 pEµrN1pτqs ` EµrN2pτqsq2 ` 2∆2EµrN1pτqs2 pEµrN1pτqs ` EµrN2pτqsq2 ě log ˆ T 2.4 ˙ . Therefore, rearranging the terms in the inequality, we can derive a lower bound on EµrN1pτqs as follows: EµrN1pτqs ě 2 log ` T 2.4 ˘ EµrN2pτqs 2∆2EµrN2pτqs ´ 2 log ` T 2.4 ˘. 30Let τ “ Tc be the fixed length of exploration. Thus, we can derive the following lower bound on the expectation of stopping time as follows: EµrTcs “ EµrN1pTcqs ` EµrN2pTcqs ě 2∆2 pEµrN2pTcqsq2 2∆2EµrN2pTcqs ´ 2 log ` T 2.4 ˘. According to our assumption, the algorithm has c-logarithm regret violation with OpT ´1q confidence, which means the the number of pulls for the sub-optimal action is upper and lower bounded when T is large enough. So we have: ˇˇˇˇEµ rN2pTqs ´ 2 logpTq ∆2 ˇˇˇˇ “ Oplogc Tq. Recall that ˆa is the action the algorithm chooses to commit to and action 1 is the optimal action in instance µ, so we have E rN2pTqs ď Eµ rN2pTcqs ` TPµ pˆa ‰ 1q. Since the algorithm has OpT ´1q confidence, so we have Pµ pˆa ‰ 1q “ OpT ´1q and Eµ rN2pTqs ď Eµ rN2pTcqs ` Op1q. So we can lower bound the numerator as follows: 2∆2 pEµrN2pTcqsq2 ě2∆2 pEµrN2pTqs ´ Op1qq2 ě2∆2 ˆ 4 ∆4 log2 T ´ Oplog1`c Tq ˙ “ 8 ∆2 log2 T ´ Oplog1`c Tq. Notice that EµrN2pTcqs ď EµrN2pTqs.For the denominator, we can derive an upper bound similarly as follows: 2∆2EµrN2pTcqs ´ 2 log ˆ T 2.4 ˙ ď2∆2EµrN2pTqs ´ 2 log ˆ T 2.4 ˙ ď2∆2 2 ∆2 log T ` Oplogc Tq ´ 2 log ˆ T 2.4 ˙ ď2 log T ` Oplogc Tq, where the second inequality uses the fact that the algorithm has c-logarithm regret violation. So combining both bounds in the numerator and the denominator, we have: EµrTcs ě 8 ∆2 log2 T ´ Oplog1`c Tq 2 log T ` Oplogc Tq “ Ω ˆlog T ∆2 ˙ , which concludes the proof for the pre-determined stopping time setting. In the adaptive stopping time setting, we can use the same procedure to prove the lower bound of sample complexity until commitment, so we also create another bandit instance with reward expectations λ1 and λ2. Unlike in the pre-determined stopping time setting, now λ1 and λ2 satisfies λ1 ď λ2. We also use the “transportation” lemma and optimize over λ1 and λ2 to get a lower bound for ErTcs as follows: EµrTcs “ EµrN1pTcqs ` EµrN2pTcqs ě ∆2 pEµrN2pTcqsq2 ∆2EµrN2pTcqs ´ 2 log ` T 2.4 ˘. Similar to the proof of pre-determined stopping time setting, we utilize the fact that Eµ rN2pTcqs ď Eµ rN2pTqs ď Eµ rN2pTcqs ` Op1q and the algorithm has c-logarithm regret violation to bound the 31numerator and denominators separately. For the numerator, we have: ∆2 pEµrN2pTcqsq2 ě∆2 pEµrN2pTqs ´ Op1qq2 ě∆2 ˆ 4 ∆4 log2 T ´ Oplog1`c Tq ˙ “ 4 ∆2 log2 T ´ Oplog1`c Tq. For the denominator, we use the fact that the algorithm has c-logarithm regret violation: ∆2EµrN2pTcqs ´ 2 log ˆ T 2.4 ˙ ď∆2 2 ∆2 log T ` Oplogc Tq ´ 2 log ˆ T 2.4 ˙ ďOplogc Tq. So combining both bounds in the numerator and the denominator, we have: EµrTcs ě 4 ∆2 log2 T ´ Oplog1`c Tq Oplogc Tq “ Ω ˆlog2´c T ∆2 ˙ . C Proofs of Main Results for KL-EOCP Now we are ready to prove the regret and sample complexity until commitment results for our KL-EOCP Algorithms, i.e., Algorithm. 3. Without loss of generality, suppose action 1 is the unique optimal action. Recall that for any time index t, UCBtpaq and LCBtpaq denotes be KL upper and lower confidence bound of action a as: UCBtpaq “ arg max µě¯rtpaq tNtpaqKLp¯rtpaq, µq ď lu , LCBtpaq “ arg min µď¯rtpaq tNtpaqKLp¯rtpaq, µq ď lu . Throughout the proof section, we let Wa,i be the i-th sample from pulling arm a the i-th time. Recall that Tc is the length of exploration phase. Let ¯ra,s be the empirical mean of arm a after it has been pulled s times. Moreover, we let: UCBs,a “ arg max µě¯rs,a tsKLp¯rs,a, µq ď lu , LCBs,a “ arg min µď¯rs,a tsKLp¯rs,a, µq ď lu . In order to incorporate this new definition of UCB and LCB, we need new sets of concentration inequalities, which will be summarized in the next subsection. The proof will be delayed to Sec. D. C.1 Concentration Inequalities for Natural Exponential Families Since we no longer assume the distributions rν1, ¨ ¨ ¨ , νAs are subgaussian any more, the concentration inequalities from Sec. B.1, especially Lemma. 3, no longer holds. Therefore, we need a set of new concentrations specialized in the natural exponential family regime. To be specific, we want to bound the probability that the KL divergence of empirical estimations and the true expectation is very large. First, we provide a concentration lemma which is analogous to the Hoeffding’s inequality widely used in the subgaussian scenario as follows: Lemma 5 Let W1, W2, ¨ ¨ ¨ , WT be identically and independently distributed random variables with com- mon expectation µ “ ErW1s and sampled from a distribution ν that belongs to a canonical exponential 32family P. For any s ď T, Let Ss “ řs i“1 Wi and ¯µs “ Ss s denote the sum and the empirical mean of the first s samples. For any δ ą 0, we have: P p¯µs ď µ, sKL p¯µs, µq ě δq ď expp´δq, (5a) P p¯µs ě µ, sKL p¯µs, µq ě δq ď expp´δq. (5b) Based on Lemma. 5 we propose the following lemmas characterizing the any-time concentration property of random variables. Lemma 6 Let W1, W2, ¨ ¨ ¨ , WT be identically and independently distributed random variables with com- mon expectation µ “ ErW1s and sampled from a distribution ν that belongs to a canonical exponential family P. For any s ď T, Let Ss “ řs i“1 Wi and ¯µs “ Ss s denote the sum and the empirical mean of the first s samples. Let T1 ď T2 ď T be two real numbers in R`. For any l ą 2, the following holds: P pDs P rT1, T2s, ¯µs ď µ, sKL p¯µs, µq ě lq ď min "T2 ´ T1 ` 1 expplq , el plog T2 ´ log T1q ` e expplq * , (6a) P pDs P rT1, T2s, ¯µs ě µ, sKL p¯µs, µq ě lq ď min "T2 ´ T1 ` 1 expplq , el plog T2 ´ log T1q ` e expplq * . (6b) Lemma 7 Let W1, W2, ¨ ¨ ¨ , WT be identically and independently distributed random variables with com- mon expectation µ “ ErW1s and sampled from a distribution ν that belongs to a canonical exponential family P. For any s ď T, Let Ss “ řs i“1 Wi and ¯µs “ Ss s denote the sum and the empirical mean of the first s samples. Let UCBs “ arg maxµě¯rs tsKLp¯µs, µq ď lu be the upper confidence bound for empirical mean. For any l ą 2, any T1 ď T, any µ1 ą µ, and any ε ą 0, the following holds: T1 ÿ s“1 P ` UCBs ě µ1˘ ď p1 ` εql KLpµ, µ1q ` β2pεq T β1pεq , (7) where β1pεq “ Opε2q and β2pεq “ Opε´2q are constants. C.2 Proof of Regret Optimality for KL-EOCP In this section, we provide a complete proof of Theorem. 4. We prove the following Theorem which characterizes the finite-time performance of Algorithm. 3, and Theorem. 4 can be derived from Theorem. 4 by letting T approaches infinity. Theorem 7 Under pre-determined stopping time setting for general bandits with time horizon T with T ě 16, the Regret of KL-EOCP algorithm in Algorithm. 3 can be upper bounded as: RegKL-EOCP µ pTq ď ÿ a:∆aą0 ˜ ∆a log T KLpµa, µ1q ` 10∆a log 3 4 T KLpµa, µ1q ¸ ` op1q. Remark: The asymptotic result in Theorem. 4 is clear from Theorem.7 by letting T increases to infinity, i.e., lim sup TÑ8 RegKL-EOCP µ pTq log T ď ÿ a:∆aą0 ∆a KLpµa, µ1q. 33Proof. From the regret decomposition lemma (Lattimore and Szepesvári, 2020, Lemma 4.5), we can decompose the regret of Algorithm. 3 to the number of pulls for each sub-optimal arm as follows: RegµpTq “ ÿ a:∆aą0 ∆aErNT paqs. Then, the key to bound the total regret is to bound the number of pulls for each sub-optimal arms. Since our KL-EOCP algorithm has a clear separation of exploration and exploitation phases, so for any sub-optimal action a, we can bound its pulls in different phases as follows: ErNapTqs “ E rN2pTcqs looooomooooon I1 `pT ´ Tcq Ppˆa “ aq looomooon I2 , where recall that Tc is the end of exploration phase and ˆa is the arm that the algorithm commits to during exploitation phase. Bounding I1: We first bound I1, which is similar to the proof of bounding the regret for the traditional KL-UCB algorithm (Garivier and Cappé, 2011). We have: I1 “ E « Tc ÿ t“1 1At“a ff “ 1 ` E « Tc ÿ t“A`1 1At“a1UCBt´1paqěµ1 ff looooooooooooooooooomooooooooooooooooooon A1 ` E « Tc ÿ t“A`1 1At“a1UCBt´1paqăµ1 ff looooooooooooooooooomooooooooooooooooooon A2 , Notice that At “ a indicates UCBt´1p1q ă UCBt´1paq due to the dynamic of UCB exploration, we can bound the last term as: A2 ď Tc ÿ t“A`1 E “ 1UCBt´1p1qăµ1 ‰ “ Tc ÿ t“A`1 P pUCBt´1p1q ă µ1q Event tUCBt´1p1q ă µ1u means that at time step t, the mean estimation ¯rt´1p1q of the optimal arm from previous pulls is lower than µ1 ´ bt´1p1q, which incurs a large deviation, so we can bound this event with a union over all time steps: tUCBt´1p1q ă µ1u Ă ␣ Dt1 P rA, Tcq, UCBt1p1q ă µ1 ( , which doesn’t depend on time step t any more, so we have: Tc ÿ t“A`1 P pUCBt´1p1q ă µ1q ď Tc ÿ t“A`1 P ` Dt1 P rA, Tcq, UCBt1p1q ă µ1 ˘ “pTc ´ AqP ` Dt1 P rA, Tcq, UCBt1p1q ă µ1 ˘ For two time steps Tc ă t2, if there is no pulls for arm 1 between them, then the term UCBt1p1q will remain the same through t1 P rTc, t2s. So instead of counting on the time steps, we can count the number of pulls for arm 1 instead. We have: ␣ Dt1 P rA, Tcq, UCBt1p1q ă µ1 ( Ă tDs P r1, NTcpaqq , UCBs,1 ă µ1u . Notice that NTcpaq ď Tc ´ A ` 1, so we can bound the probability as follows: P ` Dt1 P rA, Tcq, UCBt1p1q ă µ1 ˘ ďP pDs P r1, Tc ´ A ` 1q, UCBs,1 ă µ1q “P ˆ Ds P r1, Tc ´ A ` 1q, max µě¯rs,1 " KLp¯rs,1, µq ď l s * ă µ1 ˙ . 34Notice that for any s and under event tUCBs,1 ă µ1u, we have µ1 ě UCBs,1 ě ¯rs,1. By definition of UCBs,1, we also have KL p¯rs,1, UCBs,1q “ l s. Therefore, we come to the conclusion that KL p¯rs,1, µq ě l s. using Eq. (6a) of Lemma 6, we have: P ˆ Ds P r1, Tc ´ A ` 1q, max µě¯rs,1 " KLp¯rs,1, µq ď l s * ă µ1 ˙ ďP pDs P r1, Tc ´ A ` 1q, sKL p¯rs,1, µq ě lq ďTc ´ A expplq . Using the fact that expplq ě T log4 T when T ě 16 and putting the above bound back to the bound of A2, we have: A2 ď pTc ´ Aq2 T log4 T “ ˆ 8Aplog T`4?2 log Tq KL2 min ˙2 T log2 T “ o ˆ 1 T ˙ , where the last inequality is due to the fact that when T ě 3, ?log T ď log T. Next, we attempt to bound the middle term A1 as follows: A1 “ E « Tc ÿ t“A`1 1At“a1UCBt´1paqěµ1 ff “E « Tc ÿ t“A`1 1At“a1UCBNt´1paq,aěµ1 ff Notice that the number of pulls Nt´1paq will only increase by 1 every time there is new pull, i.e., when At “ a. Otherwise, the term inside summation is 0. So instead of counting on the time step t, we can count over the number of pulls over arm a as follows: E « Tc ÿ t“A`1 1At“a1UCBNt´1paq,aěµ1 ff “ E » – NTcpaq ÿ s“1 1UCBs,aěµ1 fi fl ď Tc ÿ s“1 P pUCBs,a ě µ1q , where the last inequality is due to NTcpaq ď Tc. Using Lemma. 7, we can bound the sum of probabilities. Specifically, for any ε ą 0, there exists two constants β1pεq “ Opε2q and β2pεq “ Opε´2q such that: Tc ÿ s“1 P pUCBs,a ě µ1q ď p1 ` εql KLpµa, µ1q ` β2pεq T β1pεq . Let ϵ “ log´ 1 4 T, and when T ě 16, we have log T ě ?log T and 4?2 log T ď 5 log 3 4 T, so we have: Tc ÿ s“1 P pUCBs,a ě µ1q ď log T KLpµa, µ1q ` 10 log 3 4 T KLpµa, µ1q ` op1q. Therefore, combining the bounds on A1 and A2, we can bound I1 as follows: I1 ď log T KLpµa, µ1q ` 10 log 3 4 T KLpµa, µ1q ` op1q. Bounding I2: After the exploration phase, recall that we select the arm with largest LCB to commit to. The idea of proof is very similar to the regret bound in (Yang et al., 2022a) and our proof of 35Theorem. 5. We can first bound the probability of selecting a sub-optimal arm a as: P pˆa “ aq ďP pLCBTcpaq ě LCBTcp1qq “P ˆ LCBTcpaq ě LCBTcp1q, NTcp1q ą 4l KLmin ˙ ` P ˆ LCBTcpaq ě LCBTcp1q, NTcp1q ď 4l KLmin ˙ ď P ˆ LCBTcpaq ě LCBTcp1q, NTcp1q ą 4l KLmin ˙ looooooooooooooooooooooooooooomooooooooooooooooooooooooooooon B1 ` P ˆ NTcp1q ď 4l KLmin ˙ looooooooooooomooooooooooooon B2 . We first bound the term B2 which states that during the first exploration phase with UCB exploration, the optimal arm is under-pulled, which means that one of the sub-optimal arms have been pulled with larger number of times. Therefore, we have: P ˆ NTcp1q ď 4l KLmin ˙ “ P ˜ ÿ a:∆aą0 NTcpaq ą Tc ´ 4l KLmin ¸ . Recall that Tc “ 4Al KLmin ` A, so we have: P ˆ NTcp1q ď 4l KLmin ˙ “P ˜ ÿ a:∆aą0 NTcpaq ą 4pA ´ 1ql KLmin ` A ¸ ďP ˆ Da ą 1, NTcpaq ą 4l KLmin ` 1 ˙ . Then by union bound, we have: P ˆ Da ą 1, NTcpaq ą 4l KLmin ` 1 ˙ ď ÿ a:∆aą0 P ˆ NTcpaq ą 4l KLmin ` 1 ˙ . Consider a fixed sub-optimal arm a, then for each probability inside the summation, we have: P ˆ NTcpaq ą 4l KLmin ` 1 ˙ ďP ˆ Dt P rA ` 1, Tcs, Nt´1paq “ R 4l KLmin V , At “ a ˙ ďP ˆ Dt P rA ` 1, Tcs, Nt´1paq “ R 4l KLmin V , UCBt´1p1q ď UCBt´1paq ˙ ď P ˆ Dt P rA ` 1, Tcs, Nt´1paq “ R 4l KLmin V , µ1 ď UCBt´1paq ˙ looooooooooooooooooooooooooooooooooooooomooooooooooooooooooooooooooooooooooooooon B3 ` P pDt P rA ` 1, Tcs, UCBt´1p1q ď µ1q looooooooooooooooooooooomooooooooooooooooooooooon B4 . Notice that B4 can be bounded from concentration lemma. First, we switch the count from time step t to the number of pulls for arm 1 since the empirical estimation ¯rt´1p1q and count for pulls Nt´1p1q won’t 36change unless there is a new pull. Then, we will apply Eq. (4a) to the probability as follows: B4 ďP pDs P r1, Tc ´ A ` 1q, UCBs,1 ď µ1q ďP pDs P r1, Tc ´ A ` 1q, ¯rs,1 ď µ1, sKLp¯rs,1, µ1q ě lq ďTc ´ A exp l , where the second inequality is because under event tUCBs,1 ď µ1u and according to the definition of UCBs,1, we have ¯rs,1 ď UCBs,1 ď µ1, and since sKLp¯rs,1, UCBs,1q “ l with µ1 ą UCBs,1, we have sKLp¯rs,1, µ1q ě l. The last inequality uses Lemma. 6. Similar to the procedure of bounding I1, notice that expplq ě T log4 T, we have: B4 ď 4A ` log T ` 4?2 log T ˘ KLmin 1 T log4 T ď 20A KLminT log3 T . On the other hand, let γ “ Q 4l KLmin U , and we also change the count from time step t to the number of pulls for arm a. Then, B3 can be expressed as follows: B3 “ P ` Dt P rA ` 1, Tc ´ 1s, Nt´1paq “ γ, µ1 ď UCBNt´1paq,a ˘ “ P pµ1 ď UCBγ,aq . For any pair of means x, y P r0, 1s, define KL`px, yq “ KLpx, yq1xăy. Then, we have: P pµ1 ď UCBγ,aq ď P ˆ KL`p¯rγ,a, µ1q ď l γ ˙ . Notice that γ “ Q 4l KLmin U ě 4l KLpµa,µ1q, we can also bound B3 as follows: B3 ď P ˆ KL`p¯rγ,a, µ1q ď KLpµa, µ1q 4 ˙ . Recall that µ1 a P pµa, µ1q such that KLpµ1 a, µ1q “ KLpµa,µ1q 4 , so we have ¯rγ,a ě µ1 a under the event that ! KL`p¯rγ,a, µ1q ď KLpµa,µ1q 4 ) , and thus KLp¯rγ,a, µaq ě KLpµ1 a, µaq. By Lemma. 5, we have: P ˆ KL`p¯rγ,a, µ1q ď KLpµa, µ1q 4 ˙ ď P ` KLp¯rγ,a, µaq ě KLpµ1 a, µaq ˘ ď expp´γKLpµ1 a, µaqq. Notice that γKLprpγq, µaq ě 4lKLpµ1 a,µaq KLmin ě l by the definition of KLmin, we then have: B3 ď expp´lq ď 1 T log4 T . The last inequality is due to the fact that expplq ě T log4 T. So combine B3 and B4 together, we can bound B2 as follows: B2 ď ÿ a:∆aą0 pB3 ` B4q ď A ˆ 40A ∆2 minT log3 T ` 1 T log4 T ˙ “ o ˆ 1 T ˙ . 37Next, we attempt to bound B1. Notice that B1 can also be bounded as follows: B1 “P ˆ LCBTcpaq ě LCBTcp1q, NTcp1q ą 4l KLmin ˙ ď P ˆ µa ě LCBTcp1q, NTcp1q ą 4l KLmin ˙ loooooooooooooooooooooooomoooooooooooooooooooooooon B5 ` P pµa ď LCBTcpaqq loooooooooomoooooooooon B6 . The term B6 can be expressed by counting the number of pulls of arm a as follows: B6 ďP pDs P r1, Tc ´ A ` 1s, LCBs,a ě µaq “P pDs P r1, Tc ´ A ` 1s, ¯rs,a ě µa, sKL p¯rs,a, µaq ě lq ďTc ´ A ` 1 expplq , where in the last inequality, we can apply Lemma. (6). Since log T ě ?2 log T and expplq ě 1 T log4 T when T ě 16, we have: B6 ď 4A ` log T ` 4?2 log T ˘ KLminT log4 T ` 1 T log4 T “ o ˆ 1 T ˙ . For any pair of means x, y P r0, 1s, define KL´px, yq “ KLpx, yq1xąy. Similarly, term B5 can be expressed as: B5 ďP ˆ Ds ą 4l KLmin , µa ě LCBs,1 ˙ ďP ˆ Ds ą 4l KLmin , sKL´ p¯rs,1, µaq ď l ˙ ďP ˆ Ds ą 4l KLmin , KL´ p¯rs,1, µaq ď KLpµa, µ1q 4 ˙ . Recall that µ1 a P pµa, µ1q satisfy KL pµ1 a, µaq “ KLpµa,µ1q 4 , so under event tKL´ p¯rs,1, µaq ď KLpµa,µ1q 4 u we can conclude ¯rs,1 ď µ1 a, which means KL p¯rs,1, µ1q ě KL pµ1 a, µ1q, so we have: P ˆ Ds ą 4l KLmin , KL´ p¯rs,1, µaq ď KLpµa, µ1q 4 ˙ ďP ˆ Ds ą 4l KLmin , ¯rs,1 ď µ1, KL p¯rs,1, µ1q ě KL ` µ1 a, µ1 ˘˙ ďP ˆ Ds ą 4l KLmin , ¯rs,1 ď µ1, sKL p¯rs,1, µ1q ě 4lKL pµ1 a, µ1q KLmin ˙ ď el log T ` e exp ´ 4lKLpµ1a,µ1q KLmin ¯, where the last inequality comes from Lemma. 6. By the definition of KLmin, we know that 4lKLpµ1 a,µ1q KLmin ě l, so we have: B5 ď el log T ` e expplq ď 10e log2 T T log4 T “ o ˆ 1 T ˙ . 38Therefore, collecting the bounds for B5 and B6 and B2, we have a bound for I2 as follows: I2 ď B1 ` B2 ď B5 ` B6 ` B2 “ o ˆ 1 T ˙ . Finally, putting the bounds on I1 and I2 together, we have: ErNapTqs ď I1 ` TI2 ď log T KLpµa, µ1q ` 10 log 3 4 T KLpµa, µ1q ` op1q. Therefore, by the regret decomposition lemma, we can bound the total regret as follows: RegKL-EOCP µ pTq ď ÿ a:∆aą0 ˜ ∆a log T KLpµa, µ1q ` 10∆a log 3 4 T KLpµa, µ1q ¸ ` op1q. D Proof of Concentration Inequalities In this section, we provide the proof of the concentration inequalities presented in previous sections. D.1 Proof of Lemma. 3 The proof of Lemma. 3 relies on the maximal inequalities in Lemma. 1 and Lemma. 2, and the famous Hoeffding’s inequality which we state here for the sake of completeness. Lemma 8 (Hoeffding’s Inequality) Let pWiqT i“1 be i.i.d. σ-sub-Gaussian random variables with ErW1s “ 0, we have: P ˜řT i“1 Wi T ě δ ¸ ď exp ˆ ´Tδ2 2σ2 ˙ . Eq. (4a) of Lemma. 3: We now prove Eq. (4a) of Lemma. 3. We will first base our proof on the anytime concentration on union bound, which is tight only when T2 ´ T1 is relatively small. This procedure will result in the first term in the minimum at the RHS. To prove a stronger result when T2 ´ T1 is relatively large, we resort to the technique of "peeling device" which divide the time horizon into exponential grids, where we will perform maximal inequality inside each grid and a union bound over the grids. This will result in the second term in the minimum at the RHS. We first perform union bound on s as follows: P ˜ Ds P pT1, T2s, řs i“1 Wi s ` c 2l s ď 0 ¸ “P ˜ Ds P pT1, T2s, sÿ i“1 Wi ` ? 2ls ď 0 ¸ ď T2 ÿ j“T1`1 P ˜ s “ j, sÿ i“1 Wi ` ? 2ls ď 0 ¸ . Then, we can bound each probability with Hoeffding’s concentration inequality as follows: P ˜ s “ j, sÿ i“1 Wi ` ? 2ls ď 0 ¸ “ P ˜ jÿ i“1 Wi ` a 2jl ď 0 ¸ ď exp ˆ ´p?2jlq2 2j ˙ “ 1 expplq. 39Therefore, summing up the probabilities, we will have: P ˜ Ds P pT1, T2s, řs i“1 Wi s ` c 2l s ď 0 ¸ ď T2 ÿ j“T1`1 1 expplq “ T2 ´ T1 expplq . (8) Next, we apply the peeling method to prove the second inequality. Take β ą 1 to be a constant. Let M “ tlogβ T2 T1 u, we apply peeling method on s and divide the time horizon over exponential grids rT1, T1βs, rT1β, T1β2s, ¨ ¨ ¨ , rT1βM, T2s as follows: P ˜ Ds P pT1, T2s, řs i“1 Wi s ` c 2l s ď 0 ¸ “P ˜ Ds P pT1, T2s, sÿ i“1 Wi ` ? 2ls ď 0 ¸ ď M ÿ j“0 P ˜ Ds P rT1βj, T1βj`1s, sÿ i“1 Wi ` ? 2ls ď 0 ¸ . Since at each grid, s ě T1βj, so we can upper bound each probability as: P ˜ Ds P rT1βj, T1βj`1s, sÿ i“1 Wi ` ? 2ls ď 0 ¸ ď P ˜ Ds P rT1βj, T1βj`1s, sÿ i“1 Wi ` a 2T1βjl ď 0 ¸ . Let β “ l l´1, then according to the anytime concentration inequality from Lemma. 1, we have: P ˜ Ds P rT1βj, T1βj`1s, sÿ i“1 Wi ` a 2T1βjl ď 0 ¸ ď exp ˆ ´ 2T1βjl 2T1βj`1 ˙ “ exp ˆ ´ l β ˙ “ e expplq. Then, summing up the probabilities, we can bound the total probability as follows: P ˜ Ds P pT1, T2s, řs i“1 Wi s ` c 2l s ď 0 ¸ ď epM ` 1q expplq ď e plog T2 ´ log T1q log β expplq ` e exp l, where the last inequality is due to the definition of M “ tlogβ T2 T1 u. Notice that when l ě 2, log β “ log ´ l l´1 ¯ ě 1 l , so we can further upper bound the probability as: P ˜ Ds P pT1, T2s, řs i“1 Wi s ` c 2l s ď 0 ¸ ď el plog T2 ´ log T1q expplq ` e exp l. (9) Finally, combining the two bounds Eq. (8) and Eq. (9) together, we can prove the first result of Lemma. 3 as follows: P ˜ Ds P pT1, T2s, řs i“1 Wi s ` c 2l s ď 0 ¸ ď min "T2 ´ T1 expplq , el plog T2 ´ log T1q expplq ` e exp l * . Eq. (4b) of Lemma. 3: Next, we prove Eq. (4b) of Lemma. 3. Our result is only based on performing union bound, but one can also modify the proof of Eq. (4a) with peeling trick to prove Eq. (4b). However, the result from peeling trick is no better than simply performing union bound, at least not order-wise 40better. So for simplicity, we apply union bound to the probability as follows: P ˜ Ds P rT1, T2s, řs i“1 Wi s ` c 2l s ` δ ď 0 ¸ “P ˜ Ds P rT1, T2s, sÿ i“1 Wi ` ? 2ls ` δs ď 0 ¸ ď T2 ÿ j“T1 P ˜ s “ j, sÿ i“1 Wi ` ? 2ls ` δs ď 0 ¸ “ T2 ÿ j“T1 P ˜ jÿ i“1 Wi ` a 2jl ` δj ď 0 ¸ . Then, we can apply Hoeffding’s inequality to upper bound each probability as follows: P ˜ jÿ i“1 Wi ` a 2jl ` δj ď 0 ¸ ď exp ˜ ´ `?2jl ` δj ˘2 2j ¸ “ exp ˆ ´2jl ` 2?2jlδj ` δ2j2 2j ˙ ď expp´δ2 2 jq exp ` l ` δ?2T1l ˘, where the last inequality is due to j ě T1. So summing up all the probabilities, we can bound the anytime concentration as: P ˜ Ds P rT1, T2s, řs i“1 Wi s ` c 2l s ` δ ď 0 ¸ ď T2 ÿ j“T1 expp´δ2 2 jq exp ` l ` δ?2T1l ˘ ď 1 exp ` l ` δ?2T1l ˘ expp´δ2T1 2 q 1 ´ exp ´ ´δ2 2 ¯ ď 4 δ2 exp ˜ˆ? l ` δ b T1 2 ˙2¸. where the last step is due to 1 ´ e´ δ2 2 ě δ2 2 when δ P r0, ? 3s. Then we finish the proof of Eq. (4b) of Lemma. 3. Eq. (4c) of Lemma. 3: Finally, we prove Eq. (4a) of Lemma. 3 which bounds the summation of probability for deviation events over the time horizon. Since l ě T1δ2 2 , we define γ “ 2l δ2 ě T1. Then, we can bound the probabilities when s ď γ by 1 as follows: T2 ÿ s“T1`1 P ˜řs i“1 Wi s ` c 2l s ě δ ¸ ď γ ´ T1 ` T2 ÿ s“rγs P ˜řs i“1 Wi s ` c 2l s ě δ ¸ . When s ě rγs, we have: b 2l s “ δ b γ s . So, for each probability inside the summation, we have: P ˜řs i“1 Wi s ` c 2l s ě δ ¸ “ P ˜řs i“1 Wi s ě δ ´ c 2l s ¸ ď P ˆřs i“1 Wi s ě δ ˆ 1 ´ cγ s ˙˙ . 41Then, we can bound the probability with Hoeffding’s inequality as follows: P ˆřs i“1 Wi s ě δ ˆ 1 ´ cγ s ˙˙ ď exp ˜ ´sδ2 2 ˆ 1 ´ cγ s ˙2¸ “ exp ˆ ´δ2 2 `?s ´ ?γ ˘2 ˙ . Notice that the upper bound function expp´δ2 2 p?s ´ ?γq2q on the RHS is uni-modal when s ě γ. If a function fpsq is uni-modal, then we can bound the summation ř8 s“γ with the sum of maxs fpsq and integral ş8 γ fpsqds. Therefore, putting the upper bound back to the summation, we have: T2 ÿ s“rγs P ˜řs i“1 Wi s ` c 2l s ě δ ¸ ď T2 ÿ s“rγs exp ˆ ´δ2 2 `?s ´ ?γ ˘2 ˙ ď 8 ÿ s“rγs exp ˆ ´δ2 2 `?s ´ ?γ ˘2 ˙ ď1 ` ż 8 γ exp ˆ ´δ2 2 `?s ´ ?γ ˘2 ˙ ds “1 ` 2 δ2 ` ?2πγ δ . So the whole term can be bounded by: T2 ÿ s“T1`1 P ˜řs i“1 Wi s ` c 2l s ě δ ¸ ď γ ´ T1 ` 1 ` 2 δ2 ` ?2πγ δ “ 2l ` ? 4πl ` 2 δ2 ` 1 ´ T1, which completes the proof of Lemma. 3. D.2 Proof of Lemma. 5 We only prove the inequality when ¯µs ď µ. The other inequality can be proved exactly the same way. For every λ P R, let ϕµpλq “ log ErexppλX1qs which is well-defined and finite by assumption. Let W λ 0 “ 1 and for s ě 1, we define W λ t “ exp pλSs ´ sϕµpλqq. We show that ` W λ s ˘ sě1 is a martingale with respect to the σ-field Fs “ σpW1, ¨ ¨ ¨ , Wsq. In fact, E ” W λ s`1|Fs ı “E rexp pλSs ` λWs`1 ´ sϕµpλq ´ ϕµpλqq |Fss “ exp pλSs ´ sϕµpλqq E rexp pλWs`1 ´ ϕµpλqq |Fss “W λ s E rexp pλWs`1qs E exp rλX1s “W λ s , where the second equality is because Ss is measurable w.r.t. Fs, and the third equality uses the fact that Ws`1 is independent w.r.t. Fs. Th last equality is due to the i.i.d. nature of W1 and Ws`1. Let x P r0, µs be such that KLpx, µq “ δ{s, and let λpxq “ θx ´ θµ. It is worth-noting that since θµ is a monotonically non-decreasing function since its inverse function µ “ b1pθq is monotonically non-decreasing. So we have λpxq ď 0 since x ď µ. Observe that: ¯µs ď µ, KL p¯µs, µq ě δ s, and, x ď µ, KL px, µq “ δ s. 42Then it holds that x ě ¯µs. Notice that for natural parameter exponential family, ϕµpλq “ bpλ`θµq´bpθµq. Hence on the event t¯µs ă µu X tsKL p¯µs, µq ě δu, we have: λpxq¯µs ´ ϕµpλpxqq ě λpxqx ´ ϕµpλpxqq “ x pθx ´ θµq ´ bpθxq ` bpθµq “ KLpx, µq “ δ s, where the first inequality is because λpxq ă 0, and the second last equality uses the expression of KL divergence for natural exponential families. Putting everything together, we have: P p¯µs ď µ, sKL p¯µs, µq ě δq ďP ˆ λpxq¯µs ´ ϕµpλpxqq ě δ s ˙ “P ´ W λpxq s ě δ ¯ ďErW λpxq s s expp´δq, where the last inequality uses Markov inequality. Since W λpxq s is a martingale, so we have: P p¯µs ď µ, sKL p¯µs, µq ě δq ď ErW λpxq 0 s expp´δq “ expp´δq. D.3 Proof of Lemma 6 We only prove the inequality when ¯µs ď µ. The other inequality can be proved exactly the same way. The proof of Lemma/ 6 partly resembles the Proof of Lemma. 5. However, to show an any-time concentration bound, we need either the union bound or a peeling trick. We first apply union bound as follows: P pDs P pT1, T2s, ¯µs ď µ, sKL p¯µs, µq ě lq ď T2 ÿ s“T1`1 P p¯µs ď µ, sKL p¯µs, µq ě lq ďpT2 ´ T1q expp´lq, which obtains the first term in the RHS of Lemma. 6. Next, we apply the peeling trick. For every λ P R, let ϕµpλq “ log ErexppλX1qs which is well-defined and finite by assumption. Let W λ 0 “ 1 and for s ě 1, we define W λ t “ exp pλSs ´ sϕµpλqq. Recall that ` W λ s ˘ sě1 is a martingale with respect to the σ-field Fs “ σpW1, ¨ ¨ ¨ , Wsq. Take β ą 1 to be a constant. Let M “ tlogβ T2 T1 u, we apply peeling method on s and divide the time horizon over exponential grids rT1, T1βs, rT1β, T1β2s, ¨ ¨ ¨ , rT1βM, T2s as follows: P pDs P pT1, T2s, ¯µs ď µ, sKL p¯µs, µq ě lq ď M ÿ i“0 P ` Ds P rT1βi, T1βi`1s, ¯µs ď µ, sKL p¯µs, µq ě l ˘ . Let si “ T1βi and let x ď µ such that sKLpx, µq “ l. let λpxq “ θx ´ θµ < 0. Then, we have KLpx, µq “ λpxqx ´ ϕµpλpxqq. Consider z such that zi ă µ and KLpzi, µq “ l si , so we have when s P rsi, si`1s KLp¯µs, µq ě l s ě l si`1 “ KLpzi`1, µq So we can conclude that ¯µs ď zi`1 Also, we have: KLpzi`1, µq “ l si`1 “ 1 β l si ě 1 β l s. 43Therefore, we have: λpzi`1q¯µs ´ ϕµpλpzi`1qq ě λpzi`1qzi`1 ´ ϕµpλpzi`1qq “ KLpzi`1, µq ě l βs So we can bound each probability as: P pDs P rzi, zi`1s, ¯µs ď µ, sKL p¯µs, µq ě δq ďP ˆ λpzi`1q¯µs ´ ϕµpλpzi`1qq ě l βs ˙ “P ˆ W λpzi`1q s ě exp ˆ l β ˙˙ ďErW λpzi`1q s s exp ˆ ´ l β ˙ , where the last inequality uses Markov inequality. Since W λpzi`1q s is a martingale, so we have: P pDs P rzi, zi`1s, ¯µs ď µ, sKL p¯µs, µq ě δq ď ErW λpzi`1q 0 s exp ˆ ´ l β ˙ “ e expplq, where in the last step, we choose β “ l l´1. Then, summing up the probabilities, we can bound the total probability as follows: P pDs P pT1, T2s, ¯µs ď µ, sKL p¯µs, µq ě lq ď M ÿ j“0 e expplq “ epM ` 1q expplq ď e plog T2 ´ log T1q log β expplq ` e exp l, where the last inequality is due to the definition of M “ tlogβ T2 T1 u. Notice that when l ě 2, log β “ log ´ l l´1 ¯ ě 1 l , so we can further upper bound the probability as: P pDs P pT1, T2s, ¯µs ď µ, sKL p¯µs, µq ě lq ď el plog T2 ´ log T1q expplq ` e exp l. (10) D.4 Proof of Lemma. 7 Our proof is based on the analysis of Theorem. 2 of (Garivier and Cappé, 2011). For any pair of means x, y P r0, 1s, define KL`px, yq “ KLpx, yq1xăy. Then for a fixed s, under event tUCBs ě µ1u we have either µ1 ă ¯µs, or µ1 ą ¯µs but sKLp¯µs, µ1q ď l. So in general we can conclude that sKL`p¯µs, µ1q ď l. Then we can bound the sum of probabilities as follows: T1 ÿ s“1 P ` UCBs ě µ1˘ ď T1 ÿ s“1 P ` sKL`p¯µs, µ1q ď l ˘ . Define γ “ p1`εql KL`pµ,µ1q “ p1`εql KLpµ,µ1q, then if T1 ą γ, we can bound the first γ terms in the summation with 1. If otherwise T1 ď γ, the whole summation is bounded by γ. So without loss with generality, assume γ ă T1 and let ε ą 0 be a constant, we have: T1 ÿ s“1 P ` sKL`p¯µs, µ1q ď l ˘ ďγ ` T1 ÿ s“rγs P ` sKL`p¯µs, µ1q ď l ˘ ďγ ` T1 ÿ s“rγs P ` γKL`p¯µs, µ1q ď l ˘ “γ ` T1 ÿ s“rγs P ˆ KL`p¯µs, µ1q ď KLpµ, µ1q 1 ` ε ˙ 44where the second inequality is due to KL`p¯µs, µ1q ą 0. For any s, let rpεq P pµ, µ1q such that KLprpεq, µ1q “ KLpµ,µ1q 1`ε . if KL`p¯µs, µ1q ď KLpµ,µ1q 1`ε , we have ¯µs ě rpεq. Hence, P ˆ KL`p¯µs, µ1q ď KLpµ, µ1q 1 ` ε ˙ ď P p¯µs ě µ, KLp¯µs, µq ě KLprpεq, µqq ď expp´sKLprpεq, µqq, where the last inequality uses Lemma. 5. So we can bound the sum of probabilities as follows: T1 ÿ s“1 P ` sKL`p¯µs, µ1q ď l ˘ ď γ ` 8 ÿ s“rγs expp´sKLprpεq, µqq ď γ ` expp´γKLprpεq, µqq 1 ´ expp´KLprpεq, µqq. Notice that expp´γKLprpεq, µqq “ exp ´ ´l p1`εqKLprpεq,µq KLpµ,µ1q ¯ ď T ´β1pεq, where β1pεq “ p1`εqKLprpεq,µq KLpµ,µ1q . Let β2pεq “ 1 1´expp´KLprpεq,µqq. It is easy to check that rpεq “ µ ` Opεq, so we have β1pεq “ Opε2q and β2pεq “ Opε´2q. So we have: T1 ÿ s“1 P ` UCBs ě µ1˘ ď T1 ÿ s“1 P ` sKL`p¯µs, µ1q ď l ˘ ď p1 ` εql KLpµ, µ1q ` β2pεq T β1pεq . Remark: Let ε “ l´ 1 4 , we have: T1 ÿ s“1 P ` UCBs ě µ1˘ ď p1 ` εql KLpµ, µ1q ` β2pεq T β1pεq ď l ` l 3 4 KLpµ, µ1q ` O ¨ ˝ ? l exp ´? l ¯ ˛ ‚. 45
Area-norm COBRA on Conditional Survival Prediction Rahul Goswami1,2 and Arabin Kr. Dey2,3 1*Department, of Mathematics, IIT Guwahati, Guwahati, 781039, Assam, India. 2Department, of Mathematics, IIT Guwahati, Guwahati, 781039, Assam, India. Contributing authors: rahul.goswami@iitg.ac.in; arabin.k.dey@iitg.ac.in; Abstract The paper explores a different variation of combined regression strategy to cal- culate the conditional survival function. We use regression based weak learners to create the proposed ensemble technique. The proposed combined regression strategy uses proximity measure as area between two survival curves. The pro- posed model shows a construction which ensures that it performs better than the Random Survival Forest. The paper discusses a novel technique to select the most important variable in the combined regression setup. We perform a simu- lation study to show that our proposition for finding relevance of the variables works quite well. We also use three real-life datasets to illustrate the model. Keywords: Cox’s model, Weak learners, Survival Tree, Combined Regression Strategy 1 Introduction Prediction of Conditional survival function is an important relevant information for doctors, clinicians, and insurance companies. Function prediction has a large number of applications in many areas of science and technology. This is a challenging problem. Researchers attempted to provide a reasonable generic solution over decades. After the Cox proportional hazard model ([1], [2]), one of the breakthrough contributions to this problem is Random Survival Forest by Ishawaran [3]. The solution encourages researchers to explore the problem through different other neural network models [[4], [5], [4, 5]] and ensemble techniques which include XGboost, Adaboost ([6]) etc. In 1 arXiv:2309.00417v1 [cs.LG] 1 Sep 2023technical language, this is a problem of constructing a functional regression. Goswami et al ([7], [8]) provide different solutions to this problem in a combined regression setup. However, the solution has issues and demands a more detailed exploration to get a complete idea of the different possible implementations and their benefits. The paper addresses a different variation of the COBRA which works for certain aspects and certain types of datasets. The paper proposes to use a different proximity measure for this combined regression strategy and a specific set of weak learners which ensures that the proposed algorithm works much better than the usual Random Survival Forest. It ensures the algorithm will work at least as well as a random survival forest if the dataset consists of the censored observation up to a certain level. All implementation in this paper assumes datasets are sufficiently large. This ensures the proximity measure to include a reasonable number of points in the pre- diction set. It uses Kaplan Meier estimation ([9]) or Nelson-Aalen [10, 11] estimator to estimate the survival function or cumulative hazard function respectively based on all similar observations. We use both concordance measure and integrated Brier score in a censored setup to tune the hyperparameters of the model and use them to under- stand the goodness of fit for each dataset used in this paper. The approach does not use similar weak learners as used by Goswami et al. The weak learner’s choices in this paper follow the same as the original combined regression paper with its similar extension in functional regression setup. Thus we take each base learner as a survival predictor. We use different survival predictors instead of just one type of survival pre- dictor used by Goswami et al. : (1) Cox model (ridge) [12] (2) Cox model (lasso) [13] (3) random survival forest [14] (4) KNN survival ([15], [16] ) (5) Survival Tree [14]. These are exact functional extensions of the following models (1) Ridge (2) Lasso (3) Random Forest [14] (4) KNN [17] (5) Decision tree used respectively in original cobra for usual regression. We observe the choice can help us to ensure that the algorithm works better than a Random survival forest for sufficiently large datasets with lesser censored data. The experimental results open a large number of issues and questions. We address the issues in other research articles. The relevance of the covariates is also an important aspect of survival analysis. Earlier statistical models like Cox Proportional Hazard Model [2] and Accelerated Failure Time Model [18] have a clear idea of the covariates, which are relevant for the happening of the event of interest at a specific time. The importance of the covariates in predicting the survival function measures the relevance of the covari- ates. Random Survival Forest [3] uses the out-of-bag error ([14], [3], [19], [20], [21]) and minimal depth procedure. Feature importance is crucial for providing the inter- pretability of the model. Independent approaches like LIME [22] and Shapley Value [23] use variable importance concepts in ordinary regression setup. The approaches like SurvLIME ([24, 25], [26]) and SurvSHAP ([27], [28]) serves the similar purpose of bringing interpretability of survival model through measuring the importance of features. In this paper, we propose a novel technique to find out the most important variable. It can also find a score to rank the importance. The technique exploits the cobra structure to find the variable importance. The proposition has support on a simulated framework. 2The organization of the paper is as follows. Section 2 provides a brief overview of the notations and basic framework of survival analysis and original combined regressions strategy used for regression. We describe the proposed framework for aggregating the survival-based functional models in Section 3. Numerical results of the experiments conducted to evaluate the performance of the proposed framework are available in Section 4. A novel proposition for ranking the variables or selecting the most important variable exploiting the Cobra Structure is available in Section 5. Finally, Section 6 concludes the paper and discusses future research directions. 2 Background Important Notations and Technical Specification of the Problem Our goal is to develop individual survival function. Each patient i have a d-dimensional covariate xi ∈ X which represents the unique characteristics of the patient. Here, X is the covariate space. An outcome of interest Ti ∈ R+, representing the time of occurrence of the event of interest. The patient may drop out of the study before the event of interest occurs. We denote this dropout time or censored time as Ci ∈ R+. The observations carry another variable, δi ∈ {0, 1}. δi = 1 when event of interest is available, and δi = 0 if the event of interest is not available. We can write S(t | xi) as S(t | xi) = P(Ti > t | xi) (1) It represents the probability of survival of the patient i beyond time t given the covariate xi. Note that, our aim is to estimate the survival function S specific to the patient i, not the unconditional survival function.. The relationship between the patient covariates and survival function is to be estimated by a dataset Dn comprising of n individuals assumed to be drawn from the random tuple (Xi, δiTi +(1−δi)Ci, δi) . For simplicity we denote the random tuple (Xi, δiTi + (1 − δi)Ci, δi) by (Xi, Yi, δi). Since, we do not observe both Ti and Ci concurrently in practice, we denote the observed time by Yi = δiTi + (1 − δi)Ci Combined Regression Strategy for Usual Regression Framework Let’s consider the regression setting, where Dn = (X1, Y1), · · · , (Xn, Yn) are indepen- dent and identically distributed samples from (X, Y ) ∈ Rp ×R. We assume EY 2 < ∞. In a typical regression setup we are interested in estimating the conditional expectation of Y given X = x i.e E(Y | X = x). In COBRA setup, we split the the dataset Dn into two parts, one part Dk = (X1, Y1) · · · (Xk, Yk) and the other part Dl = (Xk+1, Yk+1), · · · , (Xn, Yn) where l = n − k ≥ 1. We denote Dl = (X1, Y1), · · · , (Xl, Yl). Suppose, we have M competing estimators, referred to as machines in COBRA setup. Each machine m is trained on Dk and referred as rk1, rk2, · · · , rkM, here we 3note that rki is a machine trained only on Dk and capable of estimating E(Y | X = x) for any x ∈ Rp. Then the estimate of E(Y | X = x) by COBRA is given by ˆECOBRA(Y | X = x) = l � i=1 Wn,i(x)Yi (2) where Wn,i(x) is the weight of the ith sample in Dl and is given by Wn,i(x) = I(∩M m=1 | rk,m(Xi) − rk,m(x) |≤ ϵ) �l j=1 I(∩M m=1 | rk,m(Xj) − rk,m(x) |≤ ϵ) (3) where, ϵ is a user specified parameter. The intuition behind the weight is that if the ith sample is close to the query point x in all the machines, then the weight of the ith sample is 1 and if the ith sample is far from the query point x in any machine, then the weight of the ith sample is 0. This condition for consensus of a fraction of the machines brings a new parameter α, which tells us how many machines should agree on the closeness of the ith sample to the query point x.The weight of the ith sample is given by Wn,i(x) = I(�M m=1 I(| rk,m(Xi) − rk,m(x) |≤ ϵ) ≥ α) �l j=1 I(�M m=1 I(| rk,m(Xj) − rk,m(x) |≤ ϵ) ≥ α) (4) COBRA gives a good approximation of the conditional expectation of Y given X = x, there is both theoretical and experimental proof that COBRA perform better that individual machine in terms of quadratic loss. 3 Proposed Algorithm for Functional Regression The structure of the survival data takes the shape of Dn : {Xi, Yi, δi}n i=1, where Xi signifies the covariate under consideration. Correspondingly, Yi represents the lesser of the event of interest occurrence time and the censoring time while δi serves as the indicator that offers insight into whether we observe the event of interest or not. Let us consider of the set survival models M = {m1, m2 . . . m|M|}. Each model of the set M provides a survival function. Since we use five fold cross validation, we divide the dataset into five parts, where each part represents test set and rest as train set. Suppose, we denote the train data as Dn′ which is of size n ′ and split the dataset Dn′ into Dk and Dl such that k + l = n ′. Let us assume the train and test dataset as Dk = {(Xi, Yi, δi)}k i=1 and Dl = {(Xi, Yi, δi)}n ′ i=k+1. We use the subdivision of the dataset Dk to train the base models, which splits out individual survival function for a given query point x. Sk,m(t | x) denotes the estimated the survival function based on mth base model, and k is the dataset on which we train the base model. We define a function as follows: Γ(x; Xj, M, ϵ, α) = I � M � m=1 I(d(Sk,m(t | x), Sk,m(t | Xj)) ≤ ϵ) ≥| M | α � (5) 4The k subscript notify that the prediction of the base models are trained on the dataset Dk. Here, I is the indicator function, which is 1 if the condition is true and 0 if the condition is false. ϵ is the threshold distance, α is the fraction of base models in the in consensus of ϵ proximity of the query point x. basically I(.) is the indicator function, which is 1 if the ith individual is in the (ϵ, α)−proximity of the query point x, otherwise 0. In this paper we take the following area-norm to calculate the distance function : d(Si(t), Sj(t)) = 1 t � t 0 | Si(t) − Sj(t) | dt (6) However, computing this area-norm is computationally expensive. We use the following Euler approximation as distance function between survival function : d(Si(t), Sj(t)) = 1 tn′ − t1 n ′ � i=1 | Si(t) − Sj(t) | (ti − ti−1) (7) The other part of the dataset Dl aggregate the survival function of the base mod- els. We calculate the event time, event count and survivor count using the following equations, YΓ(x; h) = � Yj : j ≥ k ∩ δjΓ(x; Xj, M, ϵ, α) = 1 � (8) DΓ(t | x; h) = n ′ � j=k Γ(x; Xj, M, ϵ, α)δjI(Yj = t) (9) RΓ(t | x; h) = n ′ � j=k Γ(x; Xj, M, ϵ, α)I(Yj ≥ t) (10) Then using the equations 8, 9 and 10, we can construct the aggregated survival function ˆSCOBRA(t | x; h) using the following equation ˆSCOBRA(t | x; h) = � t′∈YΓ(x;h) � 1 − DΓ(t ′ | x; h) RΓ(t ′ | x; h) �I(t ′≤t) (11) The proposed algorithm is available in Algorithm 1. 4 Dataset Descriptions and Numerical Experiments 4.1 Datasets We evaluate the performance of the proposed method on the following datasets: • METABRIC : The Molecular Taxonomy of Breast Cancer International Con- sortium (METABRIC) dataset comprises of gene expression profiles and clinical 5Algorithm 1 Overall flow of the proposed Algorithm Require: Dn : Dataset of n individuals Dn′ : Dataset of n ′ train individuals ϵ : Threshold distance α : Fraction of base models in the consensus of ϵ proximity of the query point x. M : Number of base models Ensure: ˆS(t | x) : Estimated survival function of the query point x at time t 1: Partition the whole data into five parts. Take each part as test set. 2: Split train data Dn′ into Dk and Dl such that k + l = n ′ 3: Train M base models on Dk and predict the survival function Sk,m(t | x) for m = 1, 2 . . . M 4: Calculate the distance d(Sk,m(t | x), Sk,m(t | xi)) for m = 1, 2 . . . M and i = k + 1, k + 2, · · · , n ′ using 7 5: Calculate the indicator function Γi(x) for i = k + 1, k + 2, · · · , n ′ using 5 6: Calculate the estimated survival function ˆS(t | x) using 11 attributes aimed at discerning breast cancer subtypes. Among the 1,981 patients included, 888 (44.8%) had complete follow-up data until mortality, while the remain- ing 1,093 (55.2%) were right-censored. Our analysis focuses on 21 publicly accessible clinical features, including critical indicators such as tumor size and lymph node involvement, with detailed descriptions available in Bilal et al. (2013) [29]. Miss- ing values underwent imputation to ensure data integrity, with real-valued features substituted by their mean and categorical features by their mode. We one-hot code the categorical variables, facilitating their integration into subsequent analytical methodologies. • FLCHAIN : The FLCHAIN3 [[30], [31]] dataset, introduced by Dispenzieri et al. [30], constitutes a publicly accessible resource designed to investigate the intricate association between serum free light chain levels and mortality. This dataset encom- passes a diverse array of covariates, including age, gender, serum creatinine levels, and the presence of monoclonal gammopathy, all of which are pivotal factors in com- prehending the dynamics of this relationship. This dataset is a critical resource for elucidating the multifaceted interplay between serum free light chains and pertinent covariates in the context of mortality studies. • RECID : The Recidivism4 [32] dataset is a publicly available dataset, widely utilised in criminology and social sciences, aimed at exploring the intricate dynamics of recidivism – defined as the re-offending or re-incarceration of individuals previ- ously involved with the criminal justice system. This dataset offers a comprehensive array of covariates essential for the analysis of recidivism, including demographic information such as age, gender, socio-economic factors and criminal history vari- ables like prior convictions and offence type. This dataset serves as a fundamental resource for gaining insights into the multifaceted determinants of recidivism and contributes to informed policy and intervention strategies in criminal justice studies. 6Table 1 Datasets Dataset Total Observations # Features∗ % Censored Time Range METABRIC 1981 53 44.78 3 - 9218 FLCHAIN 7874 11 27.55 0 - 5220 RECID 1445 16 61.80 0 - 4.3994 ∗ The total number of features after preprocessing 4.2 Experimental Setup We conduct the following experiments to evaluate the performance of the proposed method. The experiment uses he five fold cross validation are reported for each dataset, 4 and 3. We tune all parameters for each dataset using hyperopt [33]. The parameters are given in table 2. Table 2 Parameters tuned with 1000 trials using hyperopt Parameter Tuning Range ϵ 1e-300 - 0.9 α 1/5,2/5,3/5,4/5,1 l/n 0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9 4.3 Performance Metrics We evaluate the performance of the proposed method using the following two metrics: Concordance Concordance [34] index is popular to evaluate the model performance in survival mod- els/functional regression setup. We define concordance index with right censored data. The expression is as follows : Ctd(t) = Pr( ˆSi(ti) > ˆSj(ti) | di = 1, t ≤ tj, ti < tj) where Si(ti) is the predicted survival probability of the ith individual at time ti and Sj(ti) is the predicted survival probability of the jth individual at time ti. The time- dependent concordance index given above measure the level of agreement between the predicted survival probabilities of two individuals and actual event time of the individuals. It is defined between 0.5 and 1, where 0.5 random prediction and 1 is perfect prediction, represents that every prediction is in agreement with the actual event time. 7Integrated Brier Score We use another benchmark as Integrated Brier Score. Since the data consists of right censored observation we use the following modified version of the Integrated Brier Score, we call it as IBSc. IBSc computed between time interval [t1, tmax] as IBSc = 1 tmax − t1 � tmax t1 BSc(t)dt . We estimate the integration numerically via trapezoidal rule. Expression of Brier Score for the censored observation (BSc) is : BSc(t) = 1 N N � i=1 � I(yi ≤ t, δi = 1)(0 − ˆS(t | xi))2 ˆG(yi) + I(yi > t)(1 − ˆS(t | xi))2 ˆG(t) � where, ˆS(t | xi) is the predicted conditional function and 1 ˆ G(t) is the inverse probability of the censoring weight. We assume C > t, where C is the censoring point. Base Models The algorithm need base models to calculate the estimation on dataset Dl. We have used the following base models: 1. Survival Tree : Survival Tree is a tree based method for survival analysis. We split the data into different groups based on the log-rank test statistic during the construction of survival tree. The log-rank statistic tells us how different the survival curves of two groups are. Further, we grow the tree recursively, till it meets a certain stopping criteria. 2. Random Survival Forest : Random Survival Forest [3] is an extension Survival Tree, The output of Random Survival Forest is the average of the output of Survival Tree, obtained from different bootstrap samples of the data. 3. Cox Model (Lasso and Ridge) : Cox Model is a semi-parametric model for survival analysis, which is based on the proportional hazard assumption. The Cox Model is given by h(t | x) = h0(t) exp(βT x) (12) where h(t | x) is the hazard function of an individual with covariate x at time t, h0(t) is the baseline hazard function and β is the parameter vector. The Cox Model is usually estimated using partial likelihood method. Regularisation brings two variations in the Cox Model, known as Lasso and Ridge Cox Model. 4. K-Nearest Neighbour Survival : K-Nearest Neighbour Survival ( [15]) is a non-parametric method to estimate conditional survival function. In K-Nearest Neighbour Regression, the prediction of the query point is the average of the K-Nearest Neighbours of the query point. Suppose, we have the following right censored survival data, ((X1, Y1, δ1), · · · , (Xn, Yn, δn)) ∈ X × R+ × {0, 1} Then for specified Kernel function K : R+ → R+, and bandwidth h > 0, we can measure 8the proximity between observation Xi and Xj by K � ρ(Xi,Xj) h � , where ρ is the dis- tance function. We can create unique event time, event count and survivor count as follows YK(x; h) = � Yj : j ∈ [n] ∩ δjK �ρ(x, Xj) h � > 0 � DK(t | x; h) = n � j=1 K �ρ(x, Xj) h � δjI(Yj = t) RK(t | x; h) = n � j=1 K �ρ(x, Xj) h � I(Yj ≥ t) Then the survival function is given by ˆSK(t | x; h) = � t′∈YK(x;h) � 1 − DK(t ′ | x; h) RK(t ′ | x; h) �I(t ′≤t) (13) The K-Nearest Neighbour Survival can be derived from the above equation by replacing the kernel function K by the I, where mathematical expression of I(x; k, Xi)1 is as follows : I(x; k, Xi) = I � n � j=1,j̸=i I(ρ(x, Xi) ≤ ρ(x, Xj)) ≥ k � (14) KNN survival implementation is available by George Chen at https://github.com/georgehc/npsurvival. Numerical Results on the Real Datasets Table 3 provides the information of the concordance of all the five models on the three datasets, whereas Table 4 describes the integrated brier score of all the five models. We use 5 fold cross validation to obtain the result We observe that our proposed construction of Combined regression strategy provides maximum concordance index and minimum integrated brier score for all datasets. 5 Covariate Relevance The relevance of the covariates is also an important aspect of survival analysis. COBRA Survival has an inbuilt mechanism to measure the relevance of the covari- ates. We use a logistic regression to find the relevance of the covariates for a query point x. A detailed overview of logistic regression is available at [36], on the indicator function 15, where i = k + 1, k + 2, · · · , n ′. 1Taking I(·) = 1∀i will lead to a population Kaplan-Meier Estimate 9Table 3 Concordance (Higher is Better) Dataset METABRIC FLCHAIN RECID Survival Tree 0.553 0.780 0.544 Random Survival Forest 0.669 0.899 0.629 Cox Model (Lasso) 0.626 0.886 0.599 Cox Model (Ridge) 0.589 0.901 0.551 Kernel Survival Model 0.561 0.829 0.540 proposed 0.677 0.911 0.630 Table 4 Integrated Brier Scoe (Lower is better) Dataset METABRIC FLCHAIN RECID Survival Tree 0.278 0.065 0.262 Random Survival Forest 0.167 0.046 0.175 Cox Model (Lasso) 0.160 0.079 0.183 Cox Model (Ridge) 0.162 0.143 .189 Kernel Survival Model 0.203 0.054 0.211 proposed 0.148 0.043 0.175 The logistic regression is given by log � Pr(Γi(x) = 1) 1 − Pr(Γi(x) = 1) � = β0 + β1x1,i + β2x2,i + · · · + βpxp,i∀i = k + 1, k + 2, · · · , n ′ The coefficients β1, β2 . . . βp are the relevance of the covariates x1, x2 . . . xp respectively. We recall the definition of −(·) : Γ(x; Xj, M, ϵ, α) = I � M � m=1 I(d(Sk,m(t | x), Sk,m(t | Xj)) ≤ ϵ) ≥| M | α � (15) The k subscript notifies that the prediction of the base models takes training on the dataset Dk. Here, I is the indicator function, which is 1 if the condition is true and 0 if the condition is false. ϵ is the threshold distance, α is thetakes fraction of base models in the consensus of ϵ proximity of the query point x. basically I(.) is the indicator function, which is 1 if the ith individual is in the (ϵ, α)−proximity of the query point x, otherwise 0. Simulation Study We use a synthetic population to demonstrate COBRA Survival’s ability to discrimi- nate between different factors and determine their relative importance. Our strategy 10incorporates a non-linear data-generating procedure supported by the following link rule2 Λ(Xi) = 2 + log(13X0,i + 5X1,i + 7X2,i) + X3,i We have generated 2000 sample, where 40 percent of the sample are censored. The censoring time is generated by taking uniform distribution between 0 and time of event. The time of event is generated by following weibuill distribution Ti ∼ W(2, Λ(Xi)) There are 9 covariates generated, where X0 , X1 , X2 , X3 have non-linear effect on true event time, whereas X4 , X5 , X6 , X7 , X8 have no effect on true event time. This is done to show that COBRA Survival can identify the covariates which have effect on true event time, and the covariates which have no effect on true event time. The covariates are generated from uniform distribution. The estimates of the covariate relevance is given in Figure 1. Fig. 1 Relevance of the covariates 2Similar Link rule is taken by Bellot and van der Schaar 116 Conclusion A large number of variations of cobra structure can not able to beat the performance of Random Survival Forest even with weak learners taken as Random Survival Forest. The performance of the Cobra depends on the appropriate choice of its parameters. We observe that the proposed method in the paper works well for all three datasets. It ensures much better performance in most of the cases. Large censoring may degrade the performance. Also, a lesser number of observations in proximity points affect the performance. Our proposed model in the variable selection method works very well in the simulated framework. However, this requires special attention in a separate research. We may pursue further research on weighted version, problems on smaller observations, and more on variable importance in this context. The work is in progress. References [1] Cox, D.R.: Partial likelihood. Biometrika 62(2), 269–276 (1975) [2] Cox, D.R.: Regression models and life-tables. Journal of the Royal Statistical Society: Series B (Methodological) 34(2), 187–202 (1972) [3] Ishwaran, H., Kogalur, U.B., Blackstone, E.H., Lauer, M.S.: Random survival forests (2008) [4] Faraggi, D., Simon, R.: A neural network model for survival data. Statistics in medicine 14(1), 73–82 (1995) [5] Katzman, J.L., Shaham, U., Cloninger, A., Bates, J., Jiang, T., Kluger, Y.: Deepsurv: personalized treatment recommender system using a cox proportional hazards deep neural network. BMC medical research methodology 18(1), 1–12 (2018) [6] Bellot, A., Schaar, M.: Boosted trees for risk prognosis. In: Machine Learning for Healthcare Conference, pp. 2–16 (2018). PMLR [7] Goswami, R., Dey, A.K.: Concordance based survival cobra with regression type weak learners. arXiv preprint arXiv:2209.11919 (2022) [8] Goswami, R., Dey, A.K.: Integrated brier score based survival cobra–a regression based approach. arXiv preprint arXiv:2210.12006 (2022) [9] Kaplan, E.L., Meier, P.: Nonparametric estimation from incomplete observations. Journal of the American statistical association 53(282), 457–481 (1958) [10] Aalen, O.: Nonparametric inference for a family of counting processes. The Annals of Statistics, 701–726 (1978) [11] Nelson, W.: Theory and applications of hazard plotting for censored failure data. 12Technometrics 14(4), 945–966 (1972) [12] Simon, N., Friedman, J., Hastie, T., Tibshirani, R.: Regularization paths for cox’s proportional hazards model via coordinate descent. Journal of statistical software 39(5), 1 (2011) [13] Tibshirani, R.: The lasso method for variable selection in the cox model. Statistics in medicine 16(4), 385–395 (1997) [14] Breiman, L.: Random forests. Machine learning 45, 5–32 (2001) [15] Chen, G.: Nearest neighbor and kernel survival analysis: Nonasymptotic error bounds and strong consistency rates. In: International Conference on Machine Learning, pp. 1001–1010 (2019). PMLR [16] Beran, R.: Nonparametric regression with randomly censored survival data (1981) [17] Fix, E., Hodges, J.L.: Discriminatory analysis: Nonparametric discrimination: Small sample performance (1952) [18] Kalbfleisch, J.D., Prentice, R.L.: The Statistical Analysis of Failure Time Data. John Wiley & Sons, ??? (2011) [19] Ishwaran, H.: Variable importance in binary regression trees and forests (2007) [20] Gr¨omping, U.: Variable importance assessment in regression: linear regression versus random forest. The American Statistician 63(4), 308–319 (2009) [21] Genuer, R., Poggi, J.-M., Tuleau-Malot, C.: Variable selection using random forests. Pattern recognition letters 31(14), 2225–2236 (2010) [22] Ribeiro, M.T., Singh, S., Guestrin, C.: ”why should i trust you?” explaining the predictions of any classifier. In: Proceedings of the 22nd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining, pp. 1135–1144 (2016) [23] Strumbelj, E., Kononenko, I.: An efficient explanation of individual classifications using game theory. The Journal of Machine Learning Research 11, 1–18 (2010) [24] Kovalev, M.S., Utkin, L.V., Kasimov, E.M.: Survlime: A method for explaining machine learning survival models. Knowledge-Based Systems 203, 106164 (2020) [25] Kovalev, M.S., Utkin, L.V.: A robust algorithm for explaining unreliable machine learning survival models using the kolmogorov–smirnov bounds. Neural Networks 132, 1–18 (2020) 13[26] Utkin, L.V., Kovalev, M.S., Kasimov, E.M.: Survlime-inf: A simplified modifi- cation of survlime for explanation of machine learning survival models. arXiv preprint arXiv:2005.02387 (2020) [27] Alabdallah, A., Pashami, S., R¨ognvaldsson, T., Ohlsson, M.: Survshap: A proxy- based algorithm for explaining survival models with shap. In: 2022 IEEE 9th International Conference on Data Science and Advanced Analytics (DSAA), pp. 1–10 (2022). IEEE [28] Krzyzi´nski, M., Spytek, M., Baniecki, H., Biecek, P.: Survshap (t): Time- dependent explanations of machine learning survival models. Knowledge-Based Systems 262, 110–234 (2023) [29] Bilal, E., Dutkowski, J., Guinney, J., Jang, I.S., Logsdon, B.A., Pandey, G., Sauerwine, B.A., Shimoni, Y., Moen Vollan, H.K., Mecham, B.H., et al.: Improv- ing breast cancer survival analysis through competition-based multidimensional modeling. PLoS computational biology 9(5), 1003047 (2013) [30] Dispenzieri, A., Katzmann, J.A., Kyle, R.A., Larson, D.R., Therneau, T.M., Colby, C.L., Clark, R.J., Mead, G.P., Kumar, S., Melton III, L.J., et al.: Use of nonclonal serum immunoglobulin free light chains to predict overall survival in the general population. Mayo Clinic Proceedings 87(6), 517–523 (2012) [31] Kyle, R.A., Therneau, T.M., Rajkumar, S.V., Larson, D.R., Plevak, M.F., Offord, J.R., Dispenzieri, A., Katzmann, J.A., Melton III, L.J.: Prevalence of monoclonal gammopathy of undetermined significance. New England Journal of Medicine 354(13), 1362–1369 (2006) [32] Wooldridge, J.M., et al.: Recid. Boston College Department of Economics, ??? (2000) [33] Bergstra, J., Yamins, D., Cox, D.D., et al.: Hyperopt: A python library for opti- mizing the hyperparameters of machine learning algorithms. In: Proceedings of the 12th Python in Science Conference, vol. 13, p. 20 (2013). Citeseer [34] Gerds, T.A., Kattan, M.W., Schumacher, M., Yu, C.: Estimating a time- dependent concordance index for survival prediction models with covariate dependent censoring. Statistics in medicine 32(13), 2173–2184 (2013) [35] P¨olsterl, S.: scikit-survival: A library for time-to-event analysis built on top of scikit-learn. The Journal of Machine Learning Research 21(1), 8747–8752 (2020) [36] Cramer, J.S.: The origins of logistic regression. Tinbergen Institute Working Paper (2002) 14
Advancing Personalized Federated Learning: Group Privacy, Fairness, and Beyond Filippo Galli1,3*, Kangsoo Jung2*, Sayan Biswas2,4*, Catuscia Palamidessi2,4* and Tommaso Cucinotta3* 1Scuola Normale Superiore, Pisa, Italy. 2INRIA, Palaiseau, France. 3Scuola Superiore Sant’Anna, Pisa, Italy. 4´Ecole Polytechnique, Palaiseau, France. *Corresponding author(s). E-mail(s): filippo.galli@sns.it; gangsoo.zeong@inria.fr; sayan.biswas@inria.fr; catuscia@lix.polytechnique.fr; tommaso.cucinotta@santannapisa.it; Abstract Federated learning (FL) is a framework for training machine learning models in a distributed and collaborative manner. During training, a set of participating clients process their data stored locally, sharing only updates of the statistical model’s parameters obtained by minimizing a cost function over their local inputs. FL was proposed as a stepping-stone towards privacy-preserving machine learn- ing, but it has been shown to expose clients to issues such as leakage of private information, lack of personalization of the model, and the possibility of having a trained model that is fairer to some groups of clients than to others. In this paper, the focus is on addressing the triadic interaction among personalization, privacy guarantees, and fairness attained by trained models within the FL framework. Differential privacy and its variants have been studied and applied as cutting- edge standards for providing formal privacy guarantees. However, clients in FL often hold very diverse datasets representing heterogeneous communities, making it important to protect their sensitive and personal information while still ensur- ing that the trained model upholds the aspect of fairness for the users. To attain this objective, a method is put forth that introduces group privacy assurances through the utilization of d-privacy (aka metric privacy). d-privacy represents a localized form of differential privacy that relies on a metric-oriented obfuscation approach to maintain the original data’s topological distribution. This method, besides enabling personalized model training in a federated approach and pro- viding formal privacy guarantees, possesses significantly better group fairness 1 arXiv:2309.00416v1 [cs.LG] 1 Sep 2023measured under a variety of standard metrics than a global model trained within a classical FL template. Theoretical justifications for the applicability are pro- vided, as well as experimental validation on real-world datasets to illustrate the working of the proposed method. Keywords: Federated Learning, Metric Privacy, Personalized Models, Fairness 1 Introduction The widespread collection of user data in modern machine learning has raised concerns regarding privacy violations and the potential disclosure of sensitive personal infor- mation [1, 2]. To address these concerns, Federated Learning [3] was introduced as a collaborative machine learning paradigm, where users’ devices train a global predic- tive model without transmitting raw data to a central server. While FL offers promises of preserving user privacy and maintaining model performance, the heterogeneity of data distributions among clients can lead to challenges such as reduced model utility and convergence issues during training. In response, personalized federated learning approaches have emerged, aiming to tailor models to clusters of users with similar data distributions [4–6]. Furthermore, it has been demonstrated that avoiding the release of users’ raw data alone does not provide sufficient protection against potential privacy violations [7–9]. To address this issue, researchers have explored the application of differential privacy (DP) [10, 11] to federated learning, providing privacy guarantees for users participat- ing in the optimization process. DP mechanisms introduce randomness in the model updates released by clients, making each user’s contribution to the final model proba- bilistically indistinguishable up to a certain likelihood factor. To bound this factor, the domain of secrets (i.e., the parameter space in FL) is artificially constrained, either to offer central [12, 13] or local DP guarantees [14, 15]. However, constraining the opti- mization process to a subset of Rn can have negative effects, such as when the optimal model parameters for a particular cluster of users lie outside such a bounded domain. To address the challenges of personalization and local privacy protection, this work proposes the adoption of a more general notion of DP called d-privacy or metric-based privacy [16] which has been in the spotlight of late mainly in the context of location- privacy [17–19]. This concept of privacy does not require a bounded domain and provides guarantees based on the distance between any two points in the parameter space. Therefore, assuming that clients with similar data distributions have simi- lar optimal fitting parameters, d-privacy offers strong indistinguishability guarantees. Conversely, privacy guarantees degrade gracefully for clients with significantly different data distributions. In addition to addressing privacy concerns in personalised FL as was studied in [20], this work extends the analysis and investigates the impact of the proposed method on fairness aspects in federated model training. As machine learning-based decision systems become more prevalent, it has become apparent that many of these 2systems exhibit gender and racial biases that disproportionately affect minority pop- ulations [21, 22]. Therefore, beyond protecting user privacy, it is crucial to explore cutting-edge machine learning algorithms that can potentially mitigate this perva- sive lack of fairness among participating clients. However, systems aiming to protect privacy while ensuring fairness often involve a trade-off between the two [23]. This trade-off arises because privacy protection techniques based on DP tend to minimize the impact of outliers or minorities within the overall dataset. In other words, the application of d-privacy, a metric-based generalization of DP, to personalized FL could potentially compromise the fairness of the machine learning model. Building upon [20], this paper presents extensive experimental results demonstrating that the use of per- sonalized FL under group privacy guarantees not only significantly improves fairness compared to the classical (non-personalized) FL framework, but it also maintains a relatively small trade-off between privacy and fairness. In summary, the contributions of this paper are the following: it extends the work pursued in [20] (points 1 and 2) and it investigates the implications of our proposal on the fairness of the model (point 3): 1. A novel algorithm is put forward for collaborative training of machine learning models, leveraging advanced techniques for model personalization and addressing user privacy concerns by formalizing privacy guarantees in terms of d-privacy. 2. This research focuses on studying the Laplace mechanism under Euclidean distance, and providing a closed-form expression for its generalization in Rn, as well as an efficient sampling procedure. 3. It shows that personalized federated learning under formal privacy guarantees improves group fairness significantly compared to the non-personalized federated learning framework and, hence, establishes that this method enhances the trade-off between privacy and fairness. The rest of this paper is organized as follows. Section 2 introduces the relevant foun- dations of federated learning, differential privacy, and fairness notions. Section 3 discusses the related works for our research. Section 4 explains the proposed algorithm for personalized federated learning with group privacy. Section 5 illustrates how the proposed method works in terms of privacy and fairness, and Section 6 provides our concluding remarks. 2 Background 2.1 Personalized Federated Learning The problem of personalized federated learning falls within the framework of stochas- tic optimization, and the notation from [4] is adopted here to determine the set of minimizers θ∗ j ∈ Rn with j ∈ {1, . . . , k} of the cost functions F(θj) = Ez∼Dj [f(θj; z)] , (1) where {D1, . . . , Dk} are the data distributions which cannot be accessed directly but only through a collection of client datasets Zc = {z|z ∼ Dj, z ∈ D} for some j ∈ 3{1, . . . , k} with c ∈ C = {1, . . . , N} the set of clients, and D a generic domain of data points. C is partitioned in k disjoint sets S∗ j = {c ∈ C | ∀z ∈ Zc, z ∼ Dj} ∀ j ∈ {1, . . . , k} (2) The mapping c → j is unknown and it is necessary to rely on estimates Sj of the membership of Zc to compute the empirical cost functions ˜F(θj) = 1 |Sj| � c∈Sj ˜Fc(θj; Zc); ˜Fc(θj; Zc) = 1 |Zc| � zi∈Zc f(θ; zi) (3) The cost function f : Rn × D �→ R≥0 is applied on z ∈ D, parametrized by the vector θj ∈ Rn. Thus, the optimization aims to find, ∀ j ∈ {1, . . . , k}, ˜θ∗ j = arg min θj ˜F(θj) (4) 2.2 Privacy d-privacy, introduced in [16], extends the concept of differential privacy (DP) to any domain X, which represents the original data space and is equipped with a distance measure d: X 2 �→ R≥0, along with a space of secrets Y. A random mechanism R : X �→ Y is considered ε-d-private if, for any x1, x2 ∈ X and measurable S ⊆ Y, the inequality in Equation (5) holds: P [R(x1) ∈ S] ≤ eεd(x1,x2)P [R(x2) ∈ S] (5) It is important to note that when X corresponds to the domain of databases and d represents the distance based on the Hamming graph of their adjacency relation, Equation (5) aligns with the standard definition of DP in [10, 11]. However, in this study, θ ∈ Rn is considered as both the domain X and the space of secrets Y. The primary motivation behind employing d-privacy is to preserve the topology of the parameter distributions among clients. Specifically, it aims to ensure that clients with similar model parameters in the non-privatized space X will communicate approximate model parameters in the privatized space Y, on average. 2.3 Fairness With the recent surge of interest in building ethical ways to train machine learning models, the topic of fairness in machine learning has been in the spotlight and, corre- spondingly, various metrics and algorithms to quantify and establish fairness in model training have been studied from a variety of perspectives and in different contexts [24– 26]. Most fairness metrics consider the simple case of having a privileged group and an unprivileged group in the population. Under this assumption, typically one attribute of the dataset is selected as a sensitive attribute (e.g., gender, race, etc.) that defines 4the privileged and the unprivileged groups. The goal of fairness in machine learn- ing is to ensure fair and non-discriminated results regardless of the membership in a sensitive attribute. The two main notions of fairness considered by the community are individual fairness and group fairness: Individual fairness [27] claims that simi- lar individuals should be treated similarly, and group fairness requires that different demographic subgroups should receive equal treatment with respect to their sensitive attributes. While both notions of fairness are important, this work focus on group fairness because our goal is to analyze and mitigate the potential bias against certain groups (e.g. demographic groups) through personalization techniques. The following metrics are considered for evaluating group fairness as a part of this work. In the rest of the paper, ˆY = 1, ˆY = 0 is used to represent the positive and negative prediction respectively, and S = 1, S = 0 to represent the privileged and unprivileged group. The simplest notion of fairness to be proposed was demographic parity [27]. Definition 2.1. Demographic parity is achieved by a system when the prediction ˆY of the target label Y is statistically independent of the sensitive attributes S, i.e., P � ˆY = 1|S = 1 � = P � ˆY = 1|S = 0 � (6) Imposing demographic parity has often a strong negative impact on accuracy, and, consequently, more refined notions were proposed afterwards. In particular, equalized odds and equal opportunity [28]. Definition 2.2. A system satisfies equalized odds if its prediction ˆY is conditionally independent of the sensitive attribute S given the target label Y , P � ˆY = 1|Y = y, S = 1 � = P � ˆY = 1|Y = y, S = 0 � , y ∈ {0, 1} (7) In other words, the notion of equalized odds requires the privileged and unprivi- leged groups to have equal true positive rates and equal false positive rates. Equal opportunity is a relaxation of equalized odds, in the sense that it only requires equal true positive rates across the groups. Definition 2.3. Equal opportunity is satisfied by a system if its prediction ˆY is conditionally independent of the sensitive attribute S given the target label Y P � ˆY = 1|Y = 1, S = 1 � = P � ˆY = 1|Y = 1, S = 0 � (8) In practice, however, it is difficult to obtain perfect equality for any of the afore- mentioned notions. Hence, typically the aim is to minimize the absolute value of the difference between the privileged and unprivileged groups, rather than requiring this difference to be exactly zero. For instance, the demographic parity difference is defined as ���P � ˆY = 1|S = 1 � − P � ˆY = 1|S = 0 ���� (9) and similarly for the equalized odd difference and equal opportunity difference. 5[13] [33] [14] [20] This Work Central Privacy ✓ ✓ ✓ ✓ ✓ Local Privacy × ✓ ✓ ✓ ✓ Personalization × ✓ × ✓ ✓ Mild Assumptions on Training ✓ × ✓ ✓ ✓ Fairness analysis × × × × ✓ Table 1: Qualitative comparison with the most relevant prior research on the topic. 3 Related Works Federated optimization has demonstrated suboptimal performance when the local datasets consist of samples from non-congruent distributions, resulting in the inability to simultaneously minimize both client-level and global objectives. In previous stud- ies [4–6], researchers examined various meta-algorithms for personalization, but the assertion of preserving user privacy relies solely on clients releasing updated mod- els or model updates, rather than transferring raw data to the server, which can have significant consequences. To address this issue, several works have focused on the privatization of the (federated) optimization algorithm within the framework of DP [12, 13, 29, 30], which adopt DP to provide defences against an honest-but-curious adversary. However, even in this setting, there is no guarantee of protection against sample reconstruction from the local datasets using client updates, as highlighted in [9]. Various strategies have been explored to offer local privacy guarantees, either through cryptographic approaches [31] or within the framework of local DP [14, 32, 33]. Specif- ically, in [33], the authors tackle the problem of personalized and locally differentially private federated learning, but only for the case of simple convex, 1-Lipschitz cost func- tions of the inputs. It is worth noting that this assumption is unrealistic in the majority of machine learning models and excludes many statistical modelling techniques, par- ticularly neural networks. Finally, some research focused on designing architectures capable of providing private computing environments for remote users [34], often mak- ing use of trusted platform modules, secure processors [35], or similar mechanisms [36] improving efficiency by enforcing encryption on network transmissions, rather than memory accesses. For example, the latter work conceptualizes an architecture that could be leveraged to deploy a server that can only reveal the data being processed to clients that instantiated the server. It shall be noted, however, that cryptographic guarantees of security are orthogonal to the privacy notions of differential privacy and its generalizations. To summarize and provide context around this work, Table 1 pro- vides a qualitative evaluation of relevant research and how the contributions presented in this paper fit among them. Of late, a great deal of attention has been devoted to studying and understanding the aspects of fairness in machine learning [23, 37–42]. Most of the research on fairness focuses on developing techniques to mitigate bias in machine learning algorithms. These techniques can be categorized into three main approaches: pre-processing, in- processing, and post-processing. Pre-processing techniques [43, 44] aim to generate a less biased dataset by modifying the values or adjusting the sampling process. In 6the case of in-processing techniques [45, 46], the objective function is optimized while taking into account discrimination-aware regularizers. Post-processing techniques [47, 48] involve adjusting the trained model to produce fairer outcomes. However, it is worth noting that the majority of these studies primarily target centralized machine learning models as opposed to FL. Furthermore, there is a lack of research exploring the interplay between accuracy and fairness [40, 41] or privacy and fairness [23, 49]. In particular, to the best of our knowledge, disproportionately fewer works have focused on investigating the relationship between privacy and fairness. [23] formally proved that privacy and fairness can be at odds with each other with non-trivial accuracy. A few recent works on group fairness in FL have emerged [38, 39] but they do not consider the facet of privacy-fairness trade-off. 4 An Algorithm for Private and Personalized Federated Learning Algorithm 1 aims to enable personalized federated learning while ensuring local privacy guarantees to preserve group privacy. In this context, locality refers to the sanitization of client information before it is shared with the server, while group privacy pertains to the notion of indistinguishability within a specific neighbourhood of clients, defined based on a particular distance metric. To clarify our terminology, we provide definitions for neighbourhood and group as follows: Definition 4.1. For any model parameterized by θ0, ∈, Rn, the r-neighbourhood is defined as the set of points in the parameter space that are within an L2 distance of r or less from θ0, i.e., θ ∈ Rn : ∥θ0 − θ∥2 ≤ r. Clients whose models are parameterized by θ ∈ Rn within the same r-neighbourhood are considered to be part of the same group or cluster. Algorithm 1 is inspired by the Iterative Federated Clustering Algorithm (IFCA) proposed in [4] and extends it by incorporating formal privacy guarantees. The key modifications include the introduction of the SanitizeUpdate function, as described in Algorithm 2, and the utilization of k-means for server-side clustering of the updated models. 4.1 The Laplace mechanism under Euclidean distance in Rn The SanitizeUpdate function in Algorithm 2 is based on a generalization of the Laplace mechanism to Rn under the Euclidean distance, which was originally intro- duced in [50] for geo-indistinguishability in R2. The decision to utilize the L2 norm as the distance measure serves two main purposes. First, clustering is performed on the vector space Rn of parameters, using the k-means algorithm, which relies on the Euclidean distance. By defining clusters or groups of users based on the proximity of their model parameters using the L2 norm, the procedure needs a d-privacy mechanism that obscures the reported values within each group while enabling the server to distinguish among users belonging to different clusters. Second, the use of equidistant noise vectors in the L2 norm for sanitizing the parameters ensures equiprobability by construction. This property leads to the same 7Algorithm 1 An algorithm for personalized federated learning with formal privacy guarantees in local neighbourhoods. Input: number of clusters k; initial hypotheses θ(0) j , j ∈ {1, . . . , k}; number of rounds T; number of users per round U; number of local epochs E; local step size s; user batch size Bs; noise multiplier ν; local dataset Zc held by user c. for t = {0, 1, . . . , T − 1} do ▷ Server-side loop C(t) ← SampleUserSubset(U) BroadcastParameterVectors(C(t); θ(t) j , j ∈ {1, . . . , k}) for c ∈ C(t) do in parallel ▷ Client-side loop ¯j = arg minj∈{1,...,k} Fc(θ(t) j ; Zc) θ(t) ¯j,c ← LocalUpdate(θ(t) ¯j ; s; E; Zc) ˆθ(t) ¯j,c ← SanitizeUpdate(θ(t) ¯j,c; ν) end for {S1, . . . , Sk} = k-means(ˆθ(t) ¯j,c, c ∈ C(t); θ(t) j , j ∈ {1, . . . , k}) θ(t+1) j ← 1 |Sj| � c∈Sj ˆθ(t) ¯j,c, ∀j ∈ {1, . . . , k} end for Algorithm 2 SanitizeUpdate obfuscates a vector θ ∈ Rn, with a Laplacian noise tuned on the radius of a certain neighbourhood and centered in 0. function SanitizeUpdate(θ(t) ¯j ; θ(t) ¯j,c; ν) δ(t) c = θ(t) ¯j,c − θ(t) ¯j ε = n ν∥δ(t) c ∥ Sample ρ ∼ L0,ε(x) ˆθ(t) ¯j,c = θ(t) ¯j,c + ρ return ˆθ(t) ¯j,c end function bound on the increase of the cost function in first-order approximation, as demon- strated in Proposition 4.2. The Laplace mechanism under Euclidean distance in the general space Rn is formally defined in Proposition 4.1. Proposition 4.1. Let Lε : Rn �→ Rn be the Laplace mechanism with distribution Lx0,ε(x) = P [Lε(x0) = x] = Ke−εd(x,x0) with d(.) being the Euclidean distance. If ρ ∼ Lx0,ε(x), then: 1. Lx0,ε is ε-d-private and K = εnΓ( n 2 ) 2π n 2 Γ(n) 2. ∥ρ∥2 ∼ γε,n(r) = εne−εrrn−1 Γ(n) 3. The ith component of ρ has variance σ2 ρi = n+1 ε2 where Γ(n) is the Gamma function defined for positive reals as � ∞ 0 tn−1e−t dt which reduces to the factorial function whenever n ∈ N. 8Proof. The proof can be found in Appendix A of [20]. Proposition 4.2. Let y = f(x, θ) be the fitting function of a machine learning model parameterized by θ, and (X, Y ) = Z the dataset over which the RMSE loss function F(Z, θ) is to be minimized, with x ∈ X and y ∈ Y . If ρ ∼ L0,ε, the bound on the increase of the cost function does not depend on the direction of ρ, in first-order approximation, and: ∥F(Z, θ + ρ)∥2 − ∥F(Z, θ)∥2 ≤ ∥Jf(X, θ)∥2 ∥ρ∥2 + o(∥Jf(X, θ) · ρ∥2) (10) Proof. The proof can be found in Appendix A of [20]. The results in Proposition 4.1 allow to reduce the problem of sampling a point from Laplace to i) sampling the norm of such point according to the result in Item 2 of Proposition 4.1 and then ii) sample uniformly a unit (directional) vector from the hypersphere in Rn. Much like DP, d-privacy provides a means to compute the total privacy parameters in case of repeated queries, a result known as the Compositionality Theorem for d-privacy. Theorem 4.1. Let Ki be (εi)-d-private mechanism for i ∈ {1, 2}. Then their independent composition is (ε1 + ε2)-d-private. Proof. The proof can be found in Appendix A of [20]. 4.2 A Heuristic for defining the Neighbourhood of a Client During the t-th iteration, when a user c invokes the SanitizeUpdate procedure in Algorithm 2, it has already received a set of hypotheses, optimized θ(t) ¯j (the one that fits best its data distribution), and got θ(t) ¯j,c. It is reasonable to assume that clients whose datasets are sampled from the same underlying data distribution D¯j will perform an update similar to δ(t) c . Therefore, points which are within the δ(t) c -neighbourhood of ˆθ(t) ¯j,c are forced to be indistinguishable. To provide this guarantee, the Laplace mechanism is tuned such that the points within the neighbourhood are ε∥δ(t) c ∥2 differentially private. By choosing ε = n/(νδ(t) c ), it results in ε∥δ(t) c ∥2 = n/ν, where ν is referred to as the noise multiplier. Notably, a larger value of ν corresponds to a stronger privacy guarantee. This is because the norm of the noise vector sampled from the Laplace distribution follows the distribution specified in Proposition 4.1, with an expected value of E [γε,n(r)] = n/ε. 5 Experiments The following Section discusses a number of experimental validations of Algorithm 1 on different tasks and datasets. Detailed experimental settings are discussed in Appendix B of [20], but we provide here an overview of the hardware and software stacks: All the following experiments are run on a local server running Ubuntu 20.04.3 LTS with an AMD EPYC 7282 16-Core processor, 1.5TB of RAM and 8× NVIDIA A100 GPUs. 9Python and PyTorch are the main software tools adopted for simulating the federation of clients and their corresponding collaborative training. 5.1 Characterizing privacy In this Section, we aim to evaluate and assess the trade-off in training personalized federated learning models under formal local privacy guarantees. 5.1.1 Synthetic Data Data is generated according to k = 2 different distributions: y = xT θ∗ i + u and u ∼ Uniform [0, 1), ∀i ∈ {1, 2} and θ∗ 1 = [+5, +6]T , θ∗ 2 = [+4, −4.5]T . We then assess how training progresses as we move from the Federated Averaging [51] (Figure 1a, 1b, 1c), to IFCA (Figure 1d, 1e, 1f), and finally Algorithm 1 (Figure 1g, 1h, 1i). When utilizing Federated Averaging, a noticeable issue arises: relying on a single hypothesis fails to capture the diversity present in the data distributions. As a result, the final parameters tend to settle somewhere between the optimal parameter values (see Figure 1b). Conversely, employing IFCA demonstrates that having multiple initial hypotheses enhances performance, particularly when clients possess heterogeneous data. This is evident from the nearly overlapping optimized client parameters with the true optimal parameters (see Figure 1e). By adopting our algorithm instead, not only do we provide formal guarantees, but we also achieve remarkable outcomes in terms of proximity to the optimal parameters (see Figure 1h) and reduction of the loss function (see Figure 1i). To assess privacy infringement, Figure 2 illustrates the maximum level of privacy leakage incurred by clients per cluster. 5.1.2 Hospital Charge Data This experiment utilizes the Hospital Charge Dataset obtained from the Centers for Medicare and Medicaid Services of the US Government [52]. Here, the healthcare providers are regarded as the clients who participate in training a machine learning model through federated learning. The objective is to predict the cost of a medical service based on its location in the country and the specific procedure involved. To evaluate the trade-off between privacy, personalization, and accuracy, we explore various numbers of initial hypotheses since the number of underlying data distribu- tions is unknown a priori. Accuracy is assessed at different levels of the noise multiplier ν. Notably, using Algorithm 1 with only one hypothesis yields the Federated Averag- ing algorithm. As depicted in Figure 3, employing multiple hypotheses significantly reduces the RMSE loss function, particularly when transitioning from one to three hypotheses. Furthermore, we emphasize that increasing the number of hypotheses also helps mitigate the impact of the noise multiplier, even at high levels (as shown on the right-hand side of the figure). This highlights the importance of adopting for- mal privacy guarantees when a slight increase in the cost function is acceptable. The empirical distribution of privacy leakage among clients involved in a specific train- ing configuration is illustrated in Figure 4. Table 2 presents privacy leakage statistics across multiple rounds and configurations. 100 1 2 3 4 5 θ i, 0 −4 −2 0 2 4 6 θ i, 1 real client parameters optimal parameters θ * 1 , θ * 0 (a) First round −2 0 2 4 6 8 θ i, 0 −4 −2 0 2 4 6 θ i, 1 real client parameters optimal parameters θ * 1 , θ * 0 (b) Best round 0 4 8 12 round 1.8 2.0 2.2 2.4 2.6 2.8 RMSE 1.802 (c) Validation loss 0 1 2 3 4 5 θ i, 0 −4 −2 0 2 4 6 θ i, 1 real client parameters optimal parameters θ * 1 , θ * 0 (d) First round −2 0 2 4 6 8 θ i, 0 −4 −2 0 2 4 6 θ i, 1 real client parameters optimal parameters θ * 1 , θ * 0 (e) Best round 0 4 8 12 16 20 24 28 round 0.0 0.5 1.0 1.5 2.0 2.5 RMSE 0.021 (f) Validation loss −4 −2 0 2 4 θ i, 0 −4 −2 0 2 4 6 θ i, 1 real client parameters optimal parameters θ * 1 , θ * 0 sanitized client parameters (g) First round −2.5 0.0 2.5 5.0 7.5 10.0 θ i, 0 −4 −2 0 2 4 6 8 10 θ i, 1 real client parameters optimal parameters θ * 1 , θ * 0 sanitized client parameters (h) Best round 0 4 8 12 16 round 0.0 0.5 1.0 1.5 2.0 2.5 3.0 RMSE 0.093 (i) Validation loss Fig. 1: (From [20]) Learning federated linear models with: (a, b, c) one initial hypothe- sis and non-sanitized communication, (d, e, f) two initial hypotheses and non-sanitized communication, (g, h, i) two initial hypotheses and sanitized communication. The first two figures of each row show the parameter vectors released by the clients to the server. 11Fig. 2: (From [20]) Synthetic data: max privacy leakage among clients. Privacy leakage is constant when clients with the largest privacy leakage are not sampled (by chance) to participate in those rounds. 0 0.1 1 2 3 5 Noise Multiplier 0.80 0.92 1.04 1.16 1.28 1.40 1.52 1.64 1.76 1.88 2.00 RMSE Model Performance T rends Initial Hypotheses 7 5 3 1 Fig. 3: (From [20]) RMSE for models trained with Algorithm 1 on the Hospital Charge Dataset. Error bars show ±σ, with σ the empirical standard deviation. Lower RMSE values are better for accuracy. 5.1.3 FEMNIST Image Classification This task involves character recognition from images using the FEMNIST dataset [53]. When selecting the range of noise multipliers ν, the resulting privacy leakage ε∥δ(t) c ∥2 = n/ν would be exceptionally large, given the CNN’s n = 206590 12Fig. 4: (From [20]) Hospital charge data: the empirical distribution of the privacy budget over the clients for ν = 3, 5 initial hypotheses, seed = 3, r is the radius of the neighbourhood, the total number of clients is 2062. Hypotheses ν 7 5 3 1 0 −, − −, − −, − −, − 0.1 517.0, 1551.0 418.0, 1342.0 473.0, 1386.0 528.0, 1540.0 1 36.3, 126.5 40.7, 127.6 44.0, 138.6 49.5, 147.4 2 15.4, 57.8 14.3, 54.5 22.0, 69.3 21.5, 66.6 3 7.7, 32.3 8.4, 36.7 12.5, 40.0 12.1, 40.0 5 5.7, 21.3 5.9, 22.0 5.5, 21.6 5.3, 20.9 Table 2: (From [20]) Hospital charge data: median and maximum local privacy bud- gets over the whole set of clients, averaged over 10 runs with different seeds. ν = 0 means no privacy guarantee. parameters. Consequently, this renders the mechanism incapable of providing mean- ingful theoretical privacy guarantees. This issue is commonly encountered with local privacy mechanisms [54], as the expected value of the noise vector’s norm, E [γε,n(r)], exhibits a linear dependence on n: n/ε. However, it is still possible to evaluate, in practice, whether this specific general- ization of the Laplace mechanism can effectively defend against a particular attack known as DLG [9]. The outcomes of varying noise multiplier values are presented in Figure 5, and Table 3 provides additional details. Notably, when ν = 10−3, the ground truth image can be fully reconstructed. Partial reconstruction remains possible up to ν = 10−1. However, for ν ≥ 1, experimental results demonstrate the failure of the DLG attack to reconstruct input samples when the communication between the client and server is protected by the mechanism outlined in Proposition 4.1. 13Cross Entropy loss RMSE loss ν Average Accuracy Standard Deviation Average Accuracy Standard Deviation 0 0.832 ± 0.012 0.801 ± 0.001 0.001 0.843 ± 0.006 0.813 ± 0.014 0.01 0.832 ± 0.017 0.805 ± 0.008 0.1 0.834 ± 0.026 0.808 ± 0.019 1 0.834 ± 0.014 0.814 ± 0.012 3 0.835 ± 0.017 0.825 ± 0.010 5 0.812 ± 0.016 0.787 ± 0.003 10 0.692 ± 0.002 0.687 ± 0.014 15 0.561 ± 0.005 0.622 ± 0.003 Table 3: (From [20]) Effects of increasing the noise multiplier on the validation accuracy and standard deviation. 100 101 102 103 iterations 10−5 10−3 10−1 101 103 105 gradient match loss function Noise Multiplier 0.001 0.01 0.1 1 3 5 10 15 Fig. 5: (From [20]) Effects of the Laplace mechanism in Proposition 4.1 with different noise multipliers as a defence strategy against the DLG attack. 5.2 Fairness analysis In this section, we analyze how group fairness improves with the personalization of the trained models under d-privacy guarantees when there are two groups with different data distributions. Experiments were performed on synthetic data and the FEMNIST image classification dataset that was used in Section 5.1. To ensure a thorough evalua- tion, we considered a variety of group fairness metrics in the experiments. In particular, 14we measured the fairness with respect to equal opportunity [28], equalized odds [28], and demographic parity [27] as explained in Section 2.3. In particular, in Figures 7 and 8, the X-axis denotes the noise multiplier ν rep- resenting the amount of d-private noise added to the local updates as explained in Section 4.2 and the Y -axis denotes the absolute value of the difference in fairness between the privileged and unprivileged groups with respect to the different metrics of group fairness that we considered. 5.2.1 Synthetic data Fig. 6: The first two plots from the left illustrate the spatial distribution of the samples in g1 and g2, respectively, and the third plot shows g1 and g2 superimposed together in the same space. Synthetic data was generated in a method similar to that in Section 5.1.1 with the following modifications to enable us to investigate the aspect of group fairness fostered by our method: i) Total number of users is 1000 and each user holds 10 samples. 800 15users have data that is generated according to distributions y = xT θ1 + u and u ∼ Uniform [0, 1), ∀i ∈ {1, 2}, and set as a privileged majority group g1. The remaining 200 users have data that is generated according to distribution y = xT θ2 + 15 + u and u ∼ Uniform [0, 1), ∀i ∈ {1, 2}, and set as an unprivileged minority group g2. In this case, the sensitive attribute considered to evaluate fairness is the group id G where G ∈ {g1, g2}. ii) For binary classification, we set labels by using the z = Sigmoid(Y ), ∀ y, ˆy ∈ Y . In the case of g1, we assign the label 1 if the value of z is greater than or equal to 0.5 and assign the label 0 otherwise. On the other hand, in the case of g2, the label 1 is assigned when the z = Sigmoid(Y − 15), ∀ y, ˆy ∈ Y is less than or equal to 0.5, and the label 0 is assigned otherwise. This setting is to simulate a situation in which discrimination occurs depending on sensitive attributes in the real world such as minorities would have experienced a higher loan rejection rate than white applicants with the same property [55]. Thus, in our experiment, label 1 could be interpreted as “loan approved” and label 0 as “loan denied”. The data generated in this way are shown in Figure 6. We compared the fairness for two cases: one with a single hypothesis (no person- alization) and the other with the number of hypotheses as 2 (with personalization) in the framework of Algorithm 1. The experimental results are demonstrated in Figure 7. The results illustrated by Figure 7 assert that the personalization of models (i.e., Algorithm 1) enhances the group fairness under all the metrics and the levels of formal privacy guarantees compared to that of the non-personalized model. A major reason behind this significant improvement of fairness by the personalized model is that unlike the non-personalized model, which trains using data from both groups that are biased towards the majority group g1, the personalized model training optimizes for each group’s data distribution without disregarding the effect of the minority group g2. We also observe that fairness deteriorates as the value of the noise multiplier increases, as we would expect. This is presumably due to the decreasing influence of the minority group g2 as the amount of noise insertion increases. This is consistent with the philosophy behind and the definition of DP and its variants. Furthermore, interestingly we observe that the personalized model ensures better fairness than the non-personalized model even with the highest level of privacy protection. This shows that personalization in FL under d-privacy can be a comprehensive solution towards privacy-preserving and ethical machine learning as it provides both privacy guarantees and enhanced fairness. 5.2.2 FEMNIST Image Classification To evaluate the fairness of our method on real datasets, we considered FEMNIST image classification dataset in the same form as in Section 5.1.3. As in experiments performed with the synthetic data in Section 5.2.1, the size of the groups considered privileged and unprivileged were different denoting the existence of a majority and a minority in the population. In this part, the rotated images are set as the unprivileged group g2 with a total number of sampled users of 382 forming only 20% of all users. and the un-rotated images are used to represent the privileged group g1 with a total number of users of 1736. Like in the case of synthetic data considered before, the 160.1 1.0 2.0 4.0 Noise multiplier 0.04 0.06 0.08 0.10 0.12 0.14 0.16 0.18 Equal opportunity difference global personal (a) Equal opportunity difference 0.1 1.0 2.0 4.0 Noise multiplier 0.05 0.10 0.15 0.20 0.25 0.30 Equalized odds difference global personal (b) Equalized odds difference 0.1 1.0 2.0 4.0 Noise multiplier 0.00 0.05 0.10 0.15 0.20 0.25 Demographic parity difference global personal (c) Demographic parity difference Fig. 7: The figure shows the comparison between the personalized and non- personalized models for (from left) equal opportunity, equalized odds, and demo- graphic parity, respectively. Experiments were performed for noise multipliers ν of 0.1, 1, 2, and 4. For all the metrics of fairness and the values of the noise multiplier, the per- sonalized model is seen to possess improved fairness over the non-personalized model. group membership was used to denote the sensitive attribute. In the case of g1, we assign label 1 if the FEMNIST image label is even and 0 if it is odd. And for the g2, we assign label 0 if the FEMNIST image label is even and assign 1 if it is odd. The experimental results are given by Figure 8. We observe that the personalized model training harbours significantly better group fairness across all metrics compared to its non-personalized counterpart. The change in fairness due to the amount of noise added was not as notable as in the case of the synthetic dataset but it was still observed to deteriorate with an increase in 170.1 1.0 2.0 4.0 Noise multiplier 0.04 0.06 0.08 0.10 0.12 0.14 Equal opportunity difference global personal (a) Equal opportunity difference 0.1 1.0 2.0 4.0 Noise multiplier 0.02 0.04 0.06 0.08 0.10 Equalized odds difference global personal (b) Equalized odds difference 0.1 1.0 2.0 4.0 Noise multiplier 0.04 0.06 0.08 0.10 0.12 0.14 0.16 0.18 Demographic parity difference global personal (c) Demographic parity difference Fig. 8: The figure shows the comparison between the personalized and non- personalized models for equal opportunity equalized odds, and demographic parity. Experiments were performed for noise multipliers ν of 0.1, 1, 2, and 4. For all metrics of fairness and values of the noise multiplier, the personalized model improved fairness over the non-personalized model. the value of the noise multiplier. Personalized model training in FL under the high- est level of privacy is still observed to have better fairness across all the metrics than (non-personalized) models trained in a classical FL framework even with no privacy, similar to what we observed in the experiments with the synthetic data. 6 Conclusion This work builds upon our previous research on personalized federated learning with metric privacy guarantees. To ensure the privacy of ML model parameters during 18transmission, we employ d-privacy techniques for sanitization. The objective of this process is to generate personalized models that converge to optimal parameters, cater- ing to the diverse datasets present in the federated learning setting. Given the presence of multiple, unknown data distributions among the individuals participating in the federated learning process, we make a reasonable assumption of a mixture of these dis- tributions. To effectively aggregate clients with similar data distributions, we employ a clustering approach using k-means on the sanitized parameter vectors. This method proves suitable because d-private mechanisms preserve the underlying topology of the true value domain. Notably, our mechanism shows particular promise for machine learning models with a relatively small number of parameters. Although the formal privacy guarantees diminish with larger models, experimental results demonstrate the effectiveness of the Laplace mechanism against the DLG attack. In addition to metric privacy guarantees, we also evaluate the fairness of machine learning models trained using personalized federated learning and d-privacy. Our study assesses various group fairness metrics, including equal opportunity, equalized odds, and demographic parity. The consistent findings demonstrate that personalized models significantly improve group fairness across all evaluated metrics and privacy levels. Moreover, they, unlike non-personalized models, optimize for each group’s specific data distribution, effectively mitigating biases towards the majority group. Consequently, significant advancements in fairness are achieved through this approach. The level of fairness is influenced by the incorporation of d-private noise in the local updates. As the noise increases, the influence of the minority group decreases, resulting in a deterioration of fairness. This behaviour aligns with the principles of differential privacy and the expected impact of noise addition on group fairness. Remarkably, even with the highest level of privacy protection, personalized models consistently maintain superior fairness compared to non-personalized models. This observation highlights the potential of personalized model training in federated learning under d-privacy as a comprehensive solution for privacy-preserving and ethical machine learning. By offer- ing privacy guarantees alongside enhanced fairness, personalized models demonstrate their effectiveness in balancing these critical aspects. 7 Conflict of Interest On behalf of all authors, the corresponding author states that there is no conflict of interest. References [1] Le M´etayer, D., De, S.J.: PRIAM: a Privacy Risk Analysis Methodology. In: Livraga, G., Torra, V., Aldini, A., Martinelli, F., Suri, N. (eds.) Data Privacy Management and Security Assurance. Springer, Heraklion, Greece (2016). https: //hal.inria.fr/hal-01420983 [2] NIST: NIST Privacy Framework Core (2021). https://www.nist.gov/system/ files/documents/2021/05/05/NIST-Privacy-Framework-V1.0-Core-PDF.pdf 19[3] McMahan, B., Moore, E., Ramage, D., Hampson, S., Arcas, B.A.: Communication-efficient learning of deep networks from decentralized data. In: Artificial Intelligence and Statistics, pp. 1273–1282 (2017). PMLR [4] Ghosh, A., Chung, J., Yin, D., Ramchandran, K.: An efficient framework for clustered federated learning. Advances in Neural Information Processing Systems 33, 19586–19597 (2020) [5] Mansour, Y., Mohri, M., Ro, J., Suresh, A.T.: Three approaches for personal- ization with applications to federated learning. arXiv preprint arXiv:2002.10619 (2020) [6] Sattler, F., M¨uller, K.-R., Samek, W.: Clustered federated learning: Model- agnostic distributed multitask optimization under privacy constraints. IEEE transactions on neural networks and learning systems 32(8), 3710–3722 (2020) [7] Hitaj, B., Ateniese, G., Perez-Cruz, F.: Deep models under the gan: informa- tion leakage from collaborative deep learning. In: Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, pp. 603–618 (2017) [8] Nasr, M., Shokri, R., Houmansadr, A.: Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning. In: 2019 IEEE Symposium on Security and Privacy (SP), pp. 739–753 (2019). IEEE [9] Zhu, L., Liu, Z., Han, S.: Deep leakage from gradients. Advances in Neural Information Processing Systems 32 (2019) [10] Dwork, C., McSherry, F., Nissim, K., Smith, A.: Calibrating noise to sensitivity in private data analysis. In: Halevi, S., Rabin, T. (eds.) Theory of Cryptography, pp. 265–284. Springer, Berlin, Heidelberg (2006) [11] Dwork, C., Kenthapadi, K., McSherry, F., Mironov, I., Naor, M.: Our data, our- selves: Privacy via distributed noise generation. In: Vaudenay, S. (ed.) Advances in Cryptology - EUROCRYPT 2006, pp. 486–503. Springer, Berlin, Heidelberg (2006) [12] Andrew, G., Thakkar, O., McMahan, B., Ramaswamy, S.: Differentially pri- vate learning with adaptive clipping. Advances in Neural Information Processing Systems 34 (2021) [13] McMahan, H.B., Ramage, D., Talwar, K., Zhang, L.: Learning differentially private recurrent language models. In: International Conference on Learning Representations (2018). https://openreview.net/forum?id=BJ0hF1Z0b 20[14] Truex, S., Liu, L., Chow, K.-H., Gursoy, M.E., Wei, W.: Ldp-fed: Federated learn- ing with local differential privacy. In: Proceedings of the Third ACM International Workshop on Edge Systems, Analytics and Networking, pp. 61–66 (2020) [15] Zhao, Y., Zhao, J., Yang, M., Wang, T., Wang, N., Lyu, L., Niyato, D., Lam, K.-Y.: Local differential privacy-based federated learning for internet of things. IEEE Internet of Things Journal 8(11), 8836–8853 (2020) [16] Chatzikokolakis, K., Andr´es, M.E., Bordenabe, N.E., Palamidessi, C.: Broadening the scope of differential privacy using metrics. In: International Symposium on Privacy Enhancing Technologies Symposium, pp. 82–102 (2013). Springer [17] Biswas, S., Palamidessi, C.: PRIVIC: A privacy-preserving method for incremen- tal collection of location data (2023) [18] Fernandes, N., McIver, A., Palamidessi, C., Ding, M.: Universal optimality and robust utility bounds for metric differential privacy. In: 2022 IEEE 35th Computer Security Foundations Symposium (CSF), pp. 348–363 (2022). https://doi.org/10. 1109/CSF54842.2022.9919647 [19] Atmaca, U.I., Biswas, S., Maple, C., Palamidessi, C.: A privacy preserving querying mechanism with high utility for electric vehicles (2022) [20] Galli, F., Biswas, S., Jung, K., Cucinotta, T., Palamidessi, C.: Group Privacy for Personalized Federated Learning. In: Proceedings of the 9th International Conference on Information Systems Security and Privacy - ICISSP, pp. 252–263 (2023). https://doi.org/10.5220/0011885000003405 . SciTePress - INSTICC [21] Berk, R., Heidari, H., Jabbari, S., Kearns, M., Roth, A.: Fairness in criminal justice risk assessments: The state of the art. Sociological Methods & Research 50(1), 3–44 (2021) [22] Chouldechova, A.: Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big data 5(2), 153–163 (2017) [23] Agarwal, S.: Trade-Offs between fairness and privacy in machine learning. IJCAI 2021 Workshop on AI for Social Good. 2021 (2022) [24] Verma, S., Rubin, J.: Fairness definitions explained. In: Proceedings of the International Workshop on Software Fairness, pp. 1–7 (2018) [25] Hanna, R., Linden, L.: Measuring discrimination in education. Technical report, National Bureau of Economic Research (2009) [26] Makhlouf, K., Zhioua, S., Palamidessi, C.: On the applicability of machine learning fairness notions. ACM SIGKDD Explorations Newsletter 23(1), 14–23 (2021) 21[27] Dwork, C., Hardt, M., Pitassi, T., Reingold, O., Zemel, R.: Fairness through awareness. In: Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, pp. 214–226 (2012) [28] Hardt, M., Price, E., Srebro, N.: Equality of opportunity in supervised learning. Advances in neural information processing systems 29 (2016) [29] Abadi, M., Chu, A., Goodfellow, I., McMahan, H.B., Mironov, I., Talwar, K., Zhang, L.: Deep learning with differential privacy. In: Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, pp. 308– 318 (2016) [30] Geyer, R.C., Klein, T., Nabi, M.: Differentially private federated learning: A client level perspective. arXiv preprint arXiv:1712.07557 (2017) [31] Bonawitz, K.A., Ivanov, V., Kreuter, B., Marcedone, A., McMahan, H.B., Patel, S., Ramage, D., Segal, A., Seth, K.: Practical secure aggregation for federated learning on user-held data. In: NIPS Workshop on Private Multi-Party Machine Learning (2016). https://arxiv.org/abs/1611.04482 [32] Agarwal, N., Suresh, A.T., Yu, F.X.X., Kumar, S., McMahan, B.: cpsgd: Communication-efficient and differentially-private distributed sgd. Advances in Neural Information Processing Systems 31 (2018) [33] Hu, R., Guo, Y., Li, H., Pei, Q., Gong, Y.: Personalized federated learning with differential privacy. IEEE Internet of Things Journal 7(10), 9530–9539 (2020) [34] Bonawitz, K.A., Ivanov, V., Kreuter, B., Marcedone, A., McMahan, H.B., Patel, S., Ramage, D., Segal, A., Seth, K.: Practical secure aggregation for federated learning on user-held data. In: NIPS Workshop on Private Multi-Party Machine Learning (2016). https://arxiv.org/abs/1611.04482 [35] Chhabra, S., Solihin, Y., Lal, R., Hoekstra, M.: An analysis of secure processor architectures. Transactions on computational science VII, 101–121 (2010) [36] Cucinotta, T., Cherubini, D., Jul, E.: Confidential execution of cloud services. In: CLOSER, pp. 616–621 (2014) [37] Chhabra, A., Masalkovait˙e, K., Mohapatra, P.: An overview of fairness in clustering. IEEE Access 9, 130698–130720 (2021) [38] Ezzeldin, Y.H., Yan, S., He, C., Ferrara, E., Avestimehr, S.: Fairfed: Enabling group fairness in federated learning. In: 1st NeurIPS Workshop on New Frontiers in Federated Learning (2021). https://arxiv.org/abs/1611.04482 [39] Chu, L., Wang, L., Dong, Y., Pei, J., Zhou, Z., Zhang, Y.: Fedfair: Training fair models in cross-silo federated learning. arXiv preprint arXiv:2109.05662 (2021) 22[40] Menon, A.K., Williamson, R.C.: The cost of fairness in binary classification. In: Conference on Fairness, Accountability and Transparency, pp. 107–118 (2018). PMLR [41] Wick, M., Tristan, J.-B., et al.: Unlocking fairness: a trade-off revisited. Advances in neural information processing systems 32 (2019) [42] Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., Galstyan, A.: A survey on bias and fairness in machine learning. ACM Computing Surveys (CSUR) 54(6), 1–35 (2021) [43] Biswas, S., Rajan, H.: Fair preprocessing: towards understanding compositional fairness of data transformers in machine learning pipeline. In: Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pp. 981–993 (2021) [44] Kamiran, F., Calders, T.: Data preprocessing techniques for classification without discrimination. Knowledge and information systems 33(1), 1–33 (2012) [45] Wan, M., Zha, D., Liu, N., Zou, N.: In-processing modeling techniques for machine learning fairness: A survey. ACM Transactions on Knowledge Discovery from Data 17(3), 1–27 (2023) [46] Hashimoto, T., Srivastava, M., Namkoong, H., Liang, P.: Fairness without demo- graphics in repeated loss minimization. In: International Conference on Machine Learning, pp. 1929–1938 (2018). PMLR [47] Petersen, F., Mukherjee, D., Sun, Y., Yurochkin, M.: Post-processing for individ- ual fairness. Advances in Neural Information Processing Systems 34, 25944–25955 (2021) [48] Noriega-Campero, A., Bakker, M.A., Garcia-Bulle, B., Pentland, A.: Active fair- ness in algorithmic decision making. In: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pp. 77–83 (2019) [49] Cummings, R., Gupta, V., Kimpara, D., Morgenstern, J.: On the compatibility of privacy and fairness. In: Adjunct Publication of the 27th Conference on User Modeling, Adaptation and Personalization, pp. 309–315 (2019) [50] Andr´es, M.E., Bordenabe, N.E., Chatzikokolakis, K., Palamidessi, C.: Geo- indistinguishability: Differential privacy for location-based systems. In: Proceed- ings of the 2013 ACM SIGSAC Conference on Computer & Communications Security, pp. 901–914 (2013) [51] Koneˇcn´y, J., McMahan, H.B., Yu, F.X., Richtarik, P., Suresh, A.T., Bacon, D.: Federated learning: Strategies for improving communication efficiency. In: NIPS Workshop on Private Multi-Party Machine Learning (2016). https://arxiv.org/ 23abs/1610.05492 [52] CMMS: Centers for Medicare and Medicaid Services. Accessed: 2022-09-21 (2021). https://www.cms.gov/mmrr/News/mmrr-news-2013-03-hosp-chg-data.html [53] Caldas, S., Duddu, S.M.K., Wu, P., Li, T., Koneˇcn`y, J., McMahan, H.B., Smith, V., Talwalkar, A.: Leaf: A benchmark for federated settings. Workshop on Federated Learning for Data Privacy and Confidentiality (2019) [54] Bassily, R., Nissim, K., Stemmer, U., Guha Thakurta, A.: Practical locally private heavy hitters. In: Guyon, I., Luxburg, U.V., Bengio, S., Wal- lach, H., Fergus, R., Vishwanathan, S., Garnett, R. (eds.) Advances in Neural Information Processing Systems, vol. 30. Curran Associates, Inc., Red Hook, NY, USA (2017). https://proceedings.neurips.cc/paper/2017/file/ 3d779cae2d46cf6a8a99a35ba4167977-Paper.pdf [55] Bartlett, R., Morse, A., Stanton, R., Wallace, N.: Consumer-lending discrimina- tion in the fintech era. Journal of Financial Economics 143(1), 30–56 (2022) 24
