Fast and Regret Optimal Best Arm Identification: Fundamental Limits and Low-Complexity Algorithms Qining Zhang University of Michigan, Ann Arbor qiningz@umich.edu Lei Ying University of Michigan, Ann Arbor leiying@umich.edu Abstract This paper considers a stochastic multi-armed bandit (MAB) problem with dual objectives: (i) quick identification and commitment to the optimal arm, and (ii) reward maximization throughout a sequence of T consecutive rounds. Though each objective has been individually well-studied, i.e., best arm identification for (i) and regret minimization for (ii), the simultaneous realization of both objectives remains an open problem, despite its practical importance. This paper introduces Regret Optimal Best Arm Identification (ROBAI) which aims to achieve these dual objectives. To solve ROBAI with both pre-determined stopping time and adaptive stopping time requirements, we present the EOCP algorithm and its variants respectively, which not only achieve asymptotic optimal regret in both Gaussian and general bandits, but also commit to the optimal arm in Oplog Tq rounds with pre-determined stopping time and Oplog2 Tq rounds with adaptive stopping time. We further characterize lower bounds on the commitment time (equivalent to sample complexity) of ROBAI, showing that EOCP and its variants are sample optimal with pre-determined stopping time, and almost sample optimal with adaptive stopping time. Numerical results confirm our theoretical analysis and reveal an interesting “over-exploration” phenomenon carried by classic UCB algorithms, such that EOCP has smaller regret even though it stops exploration much earlier than UCB (Oplog Tq versus OpTq), which suggests over-exploration is unnecessary and potentially harmful to system performance. 1 Introduction The stochastic multi-armed bandit problem (MAB) (Thompson, 1933), which models a wide range of applications including online recommendations (Yang et al., 2022a; Kohli et al., 2013; Zeng et al., 2016), job assignments (Liu et al., 2020; Huang et al., 2023; Yang et al., 2022b), clinical trials (Villar et al., 2015; Aziz et al., 2021), and etc, is a sequential decision-making process between an agent and an environment which consists of a number of actions. Most existing studies, say Auer et al. (2002); Agrawal and Goyal (2012); Garivier and Cappé (2011); Lai et al. (1985); Katehakis and Robbins (1995); Kaufmann et al. (2012b), formulate MAB as a regret minimization problem, where the agent aims to maximize the cumulative reward through interacting with the environment for a consecutive of T rounds. The UCB algorithm Auer et al. (2002) and its variants are among the most popular MAB algorithms for regret minimization, which exhibit outstanding performances both theoretically and empirically, not only in bandits but also have been used in reinforcement learning. However, the UCB algorithms do not commit to a single action and continue to change the action based on reward signals received. Since one of the ultimate goals of MAB is to find the optimal arm, it would be ideal if the algorithm can commit to an arm quickly without sacrificing the regret. In fact, in practice, a number of applications such as occupational decisions Phillips et al. (1984), medicine release and pandemic control Higuchi (1963); Zhang et al. (2022), and long-term investments Siegel (2021), require or prefer quick commitment to 1 arXiv:2309.00591v1 [cs.LG] 1 Sep 2023Algorithm Regret Setting Optimality Commitment Time Confidence UCB 2`op1q ∆ log T Gaussian Optimal T N/A KL-UCB ∆`op1q KLpµ2,µ1q log T General Optimal T N/A TS ∆`op1q KLpµ2,µ1q log T General Optimal T N/A BAI-ETC 4`op1q ∆ log T Gaussian Not Optimal Oplog Tq ˜OpT ´1q DETC 2`op1q ∆ log T Gaussian Optimal Ωplog2 Tq OpT ´1q UCBα 2α2`op1q ∆ log T Gaussian Not Optimal Oplog Tq ˜OpT ´1q EOCP (Ours) 2`op1q ∆ log T Gaussian Optimal Oplog Tq O ` T ´1˘ EOCP-UG (Ours) 2`op1q ∆ log T Gaussian Optimal Oplog2 Tq O ` T ´1˘ KL-EOCP (Ours) ∆`op1q KLpµ2,µ1q log T General Optimal Oplog Tq O ` T ´1˘ LB (pre-determined) 2`op1q ∆ log T Gaussian Oplogc Tq Oplog Tq OpT ´1q LB (adaptive) 2`op1q ∆ log T Gaussian Oplogc Tq Oplog2´c Tq OpT ´1q Table 1: Caparison under 2-armed bandits with algorithms in the literature: UCB (Garivier et al., 2016), KL-UCB (Garivier and Cappé, 2011), TS (Kaufmann et al., 2012b), BAI-ETC (Garivier et al., 2016), DETC (Jin et al., 2021a), UCBα (Degenne et al., 2019). ∆ “ |µ1 ´ µ2| is the expected reward difference, KLpµ2, µ1q is the Kullback-Leibler divergence between reward distributions, and α ą 1. EOCP and KL-EOCP use a pre-determined stopping time and require the knowledge of ∆, while EOCP-UG uses adaptive stopping time. Both LBs represent the commitment time lower bound under Gaussian bandits for regret optimal algorithms with Oplogc Tq finite-time regret violation and OpT ´1q confidence. an action instead of continuous exploration. This motivated us to consider a MAB problem with dual objectives: (i) quick identification and commitment to the optimal arm, and (ii) minimization of the cumulative regret throughout a sequence of T consecutive rounds. Regret Optimal Best Arm Identification: The lack of commitment in traditional regret mini- mization formulation motivates us to propose a new viewpoint towards online decision-making called regret optimal best arm Identification (ROBAI), which intends to manage two goals at the same time: minimizing regret while committing to an action quickly. Specifically, it is ideal for the agent to quickly commit to the optimal action which has the highest expected reward while minimizing the exploration regret. To solve ROBAI, we need to answer three fundamental questions: (1) how should the learner explore actions while maintaining low regret performance (exploration strategy)? (2) when should the learner stops exploration and commit to an action (stopping time)? and (3) which action to commit to when the exploration ends (action selection strategy)? All three components need to be designed together to make the algorithm most efficient. The fundamental question this paper addresses is: Can we design an efficient algorithm that is both regret optimal and identifies the optimal action quickly, and what are the fundamental limits of such algorithms? Connection to Best Arm Identification: One approach people may take to solve ROBAI is the best arm identification (BAI) algorithm, which focuses on identifying the best arm with a minimum number of samples (rounds) and then commit to the selected action (arm). However, since BAI focuses on sample complexity (or commitment time), the algorithms for BAI often explore sub-optimal actions too aggressively and too often, leading to large regret. As shown in Garivier et al. (2016), the regret is at least twice as large as the regret under UCB asymptotically. Modifications shown in Jin et al. (2021a) may lead to better regret performance, but they also make the algorithm too complicated to find the optimal action quickly. Moreover, these algorithms adapted from BAI often exhibit poor empirical regret performances as shown in our numerical experiments of Fig. 1. 2Our Contributions: We propose an algorithm called Explore Optimistically then Commit Pes- simistically (EOCP) to solve the ROBAI problem. It first uses an optimistic modified UCB algorithm to explore actions with a slightly larger exploration function , and then commits to actions according to a pessimistic LCB algorithm when the exploration ends. The exploration and action identification strategies are respectively motivated by the inflated bonus trick Degenne et al. (2019) and the principle of pessimism from the literature of offline bandits (Rashidinejad et al., 2021; Li et al., 2022a; Xiao et al., 2021) and offline reinforcement learning (Li et al., 2022b; Shi et al., 2022; Jin et al., 2021b). Our greatest contributions include designing new stopping rules with both pre-determined stopping time (vanilla EOCP) and adaptive stopping time (the EOCP-UG variant), which provably balance the trade-off between regret minimization and optimal action identification. We theoretically show that both algorithms are asymptotically regret optimal in Gaussian bandits. Moreover, EOCP and EOCP-UG algorithms commit to the optimal action in Oplog Tq and Oplog2 Tq number of rounds respectively, both with OpT ´1q confidence. We further characterize the fundamental commitment time (sample complexity until commitment) limits of optimal action identification for regret optimal algorithms, which shows that Oplog Tq number of samples is always required with pre-determined stopping time, and Oplog2´c Tq number of samples is required with adaptive stopping time if the finite-time regret of an algorithm does not exceed its asymptotic regret by Oplogc Tq. This shows that EOCP is sample optimal and EOCP-UG is nearly sample optimal. We also proposed an improved algorithm called KL-EOCP to achieve regret optimality in general bandits. To the best of our knowledge, KL-EOCP is the first algorithm that not only achieves asymptotic regret optimality in general bandits but also commits to the optimal action in Oplog Tq rounds, which also matches the commitment time lower bound. The more detailed comparison between existing algorithms and our proposed algorithms with lower bounds are summarized in Tab. 1. Numerical experiments confirm the superiority of our proposed algorithm and show an interesting “over-exploration” phenomenon carried by UCB algorithms. As shown in Fig. 1, our EOCP algorithm reduces more than 20% of regret compared to the vanilla UCB algorithm by finding and committing to the optimal action early. 2 Preliminaries Stochastic Multi-armed Bandits: A stochastic multi-armed bandit problem is an online decision- making process between an agent and an environment for a consecutive of T number of rounds. At each round t P t1, 2, ¨ ¨ ¨ , Tu, the agent can choose an action At among a set of A actions (arms) denoted by A “ t1, 2, ¨ ¨ ¨ , Au to interact with the environment. Each action a is associated with a probability distribution νa and we denote the set of distributions as ν “ tν1, ¨ ¨ ¨ , νAu with respective expectations µ “ tµ1, ¨ ¨ ¨ , µAu which is unknown to the agent a priori. The expectations are assumed to be bounded so without loss of generality, we have µa P r0, 1s for any action a. After the agent chooses an action, say action At at round t, it will observe an independent reward rt which is sampled from the distribution νAt associated with the action At that it chooses. We define the optimal action a˚ to be the action which has the highest expected reward, i.e., µa˚ “ arg maxaPA µa, and it is unique. For simplicity, let ∆a “ |µa˚ ´ µa| P p0, 1s to be the gap in terms of expected reward between the optimal action and a sub-optimal action a, and we use ∆min “ mina:∆aą0 |µa˚ ´µa| to denote the minimum reward gap among sub-optimal actions. Regret: The goal of the agent is to maximize the expected cumulative reward from the total T rounds of interactions with the environment, i.e., to maximize Err1 `r2 `¨ ¨ ¨`rT s, where the expectation is taken over all randomness. The performance of any MAB algorithm Alg chosen by the agent is usually 3measured by the cumulative Regret up to round T defined as follows: RegAlg µ pTq “ Tµa˚ ´ Eµ « Tÿ t“1 rt ff , where the subscript µ denotes the bandit instance represented by the reward expectations. Maximizing reward is equivalent to minimizing the cumulative regret. For most of the algorithms to achieve this goal, the agent will make action-choosing decisions based on two statistics maintained and updated at each round for every action: the empirical mean ¯rtpaq and the number of pulls Ntpaq in previous rounds. They are defined as Ntpaq “ řt k“1 1At“a, ¯rtpaq “ 1 Ntpaq řt k“1 rt1At“a. The theoretical regret limit of any algorithm Alg is studied and characterized in (Lai et al., 1985), which shows: lim inf TÑ8 RegAlg µ pTq log T ě ÿ a:∆aą0 ∆a KLpνa, νa˚q, (1) where KLp¨, ¨q denotes the Kullback–Leibler divergence between two distributions. We call an algorithm Alg regret optimal (asymptotically) if the asymptotic regret performance of Alg achieves this lower bound. Therefore, whether an algorithm is regret asymptotic optimal will depend on the distributions ν of the rewards. Specifically, for Gaussian bandits, KL divergence between distributions Npµa, 1q and Npµa1, 1q is simply pµa ´ µa1q2{2. So the asymptotic regret rate lower bound in the RHS for Gaussian bandits would be 2 ř a:∆aą0 ∆´1 a . Commitment: In ROBAI, commitment to a single action ˆa (ideally, the optimal action) is required. After a stopping time Tc, the agent will not be allowed to switch actions and will commit to the same action until the end. We consider two categories of commitment: the pre-determined stopping-time setting and the adaptive stopping time setting. The pre-determined stopping time requires Tc to be pre-specified before the first round of interaction, while the adaptive stopping criterion requires Tc to be a measurable stopping time. How quickly the agent commits is measure by the Sample Complexity until Commitment (also called commitment time) which is the expected number of rounds until commitment, i.e., SCCAlg µ pTq “ EµrTcs. We also care about the confidence of action identification, which is the probability that the agent commits to a sub-optimal action, i.e., Pµpˆa ‰ a˚q. ROBAI Problem Formulation: We use ΠRO to denote the class of regret optimal algorithms which commits to the optimal action with confidence lower than OpT ´1q, i.e., ΠRO “ # Alg ˇˇˇˇˇ lim sup TÑ8 RegAlg µ pTq log T ď ÿ a:∆aą0 ∆a KLpνa, νa˚q and Pµpˆa ‰ a˚q “ O ` T ´1˘ + . ROBAI aims to design a regret optimal algorithm Alg P ΠRO to minimize the commmitment time: min SCCAlg µ pTq “ EµrTcs, s.t., Alg P ΠRO. 3 Low-Complexity Algorithms for ROBAI In this section, we propose a low-complexity algorithms called EOCP with pre-determined stopping time to solve ROBAI. Then, we propose its variant called EOCP-UG with adaptive stopping time. 3.1 The Pre-determined Stopping Time Setting The pre-determined stopping time setting is motivated by real-world applications such as A/B testings in medical experiments with budget limits (Siroker and Koomen, 2015). In these applications, the number 4of testers is usually pre-determined before the trial starts. Therefore, we require the agent to pre-specify the stopping time Tc before the first round and assume it knows the strictly positive reward gap ∆min between the optimal action and sub-optimal actions1. We then propose EOCP in Algorithm. 1. Algorithm 1 EOCP with Pre-determined Stopping Time Require: Exploration function l. 1: Let Tc “ 8Al ∆2 min ` A be the pre-determined stopping time. 2: Initialize by pulling each arm a once. 3: for t “ A ` 1 : Tc do 4: Set uncertainty bonus bt´1paq “ b 2l Nt´1paq, and UCBt´1paq “ ¯rt´1paq ` bt´1paq. 5: Take action At “ arg maxa UCBt´1paq. // UCB Exploration 6: end for 7: Set bonus bTcpaq “ b 2l NTcpaq, and LCBTcpaq “ ¯rTcpaq ´ bTcpaq. 8: For t P rTc ` 1, Ts, commit to action ˆa “ arg maxa LCBTcpaq. // LCB Commitment In EOCP, the agent will spend the first A rounds in exploring each arm once as a start. Our choice of exploration strategy is a modified version of UCB algorithm where the agent will choose the action with the largest upper confidence bound in terms of empirical reward. The exploration function l controls the intensity of exploration to achieve the optimal trade-off between reducing uncertainty for action identification and minimizing regret. After the pre-determined stopping time Tc, the agent will commit to an action which has the largest lower confidence bound of empirical reward. This LCB commitment strategy is inspired by the principle of pessimism from the literature of offline learning (Rashidinejad et al., 2021; Li et al., 2022a; Xiao et al., 2021; Li et al., 2022b; Shi et al., 2022; Jin et al., 2021b), where the empirical reward of each action is penalized by the amount of uncertainty to combat the imbalanced data coverage of actions in the offline dataset. It is also shown that the pessimistic principle works well when the data coverage are concentrated on the optimal action, i.e., the optimal action has the largest number of pulls. This trait of the LCB algorithm matches the trait of UCB exploration, which the optimal action will be chosen much more often than sub-optimal actions in exploration. So by designing such a proper Tc, we will be able to achieve the best of both worlds: a low-regret exploration of UCB algorithm, and a fast best arm identification through the choice of LCB algorithm. 3.2 The Adaptive Stopping Time Setting In this setting, the minimum reward gap ∆min is unknown to the agent a priori, so there is no hope to design a pre-determined stopping time. Instead, we design our stopping criterion based on the samples collected from the explorations, which lead to the fact that Tc is a measurable stopping time. We propose our EOCP-UG algorithm in Algorithm 2 corresponding to unknown gap. In EOCP-UG, we use the same UCB exploration and LCB best action identification strategies as in the pre-determined stopping time setting. The only difference compared to EOCP comes from the new stopping rule based on the number of pulls Ntpaq for each action. Specifically, the exploration ends if there is an imbalanced fraction of Ntpaq among all actions, that is, one action has l times more pulls than other actions in previous rounds. Here, l is the exploration function and we will select l to be slightly larger than logpTq in later sections. The intuition of such stopping criterion comes from the characteristics of UCB exploration, i.e., as round t increases, the algorithm will slowly adapt to choosing the optimal action more often. When the fraction between the number of pulls for the optimal action and any sub-optimal action is large enough, the optimal action will be identifiable. Note that in action 1The results can be generalized to knowing a lower bound of ∆min. 5Algorithm 2 EOCP-UG with Adaptive Stopping Time Require: Exploration function l. 1: Initialize by pulling each arm a once. 2: while maxa mina1 Nt´1paq ´ lNt´1pa1q ď 1 do 3: Set uncertainty bonus bt´1paq “ b 2l Nt´1paq, UCBt´1paq “ ¯rt´1paq ` bt´1paq. 4: Take action At “ arg maxa UCBt´1paq. // UCB Exploration 5: end while 6: Let Tc Ð t ´ 1, bonus bTcpaq “ b 2l NTcpaq, and LCBTcpaq “ ¯rTcpaq ´ bTcpaq. 7: For t P rTc ` 1, Ts, commit to the action ˆa “ arg maxa LCBTcpaq. // LCB Commitment identification, we can simply choose the action which has the largest number of pulls NTcpaq when we stop, and obtain exactly the same performance guarantees. However, to keep it consistent with the pre-determined stopping time setting, we use the LCB commitment. 4 Main Results In this section, we assume the distributions tν1, ¨ ¨ ¨ , νAu come from a Gaussian family2, which means νa associated with action a follows a Gaussian distribution with mean µa and unit variance, i.e., νa „ Npµa, 1q. Inspired by the inflated bonus from (Degenne et al., 2019), we choose the exploration function l to be logpTq ` 4 a 2 logpTq which is slightly larger than logpTq used in vanilla UCB algorithms. This slight inflation will help us identify the optimal action quickly while controlling regret. 4.1 Regret Optimality for Gaussian Bandits with Pre-Determined Stopping Time In Theorem. 1, we present the theoretical regret performance guarantee of the EOCP algorithm: Theorem 1 If we choose l “ logpTq ` 4 a 2 logpTq, the expected regret of the EOCP algorithm in Algorithm. 1 with pre-determined stopping time is asymptotically upper bounded by: lim sup TÑ8 RegEOCP µ pTq log T ď ÿ a:∆aą0 2 ∆a . It is clear that under Gaussian bandit setting, EOCP is asymptotically regret optimal. The commitment time and confidence level guarantees can be extracted from the setup of Algorithm. 1 itself and the proof of Theorem. 1. Recall that in Algorithm. 1, we pre-determined the length of exploration Tc to be Op∆´2 minlq. The follow corollary characterizes these parts of theoretical performance: Corollary 1 If we choose l “ logpTq ` 4 a 2 logpTq, the expected commitment time for EOCP in Algo- rithm. 1 is given by SCCEOCP µ pTq “ Op∆´2 min log Tq, and the confidence level is OpT ´1q. The complete proofs of Theorem. 1 and Corollary. 1 are provided in the supplementary material. In order to upper bound the cumulative regret of T rounds, we divide the total regret into the regret accumulated in exploration and the regret accumulated in commitment. Bounding Regret from Exploration: To bound the regret accumulated in exploration and since we use a variant of UCB exploration, we follow the standard procedure of proofs for UCB algorithms, e.g., proof of Theorem 8 from. (Garivier et al., 2016). Then, this procedure results in a order Oplq dominating 2The results can be easily generalized to sub-Gaussian distributions. 6regret term, which is OplogpTqq by the choice of our exploration function. Through carefully applying any-time concentration inequalities, we are able to show that the constant in front of this dominating regret term is exactly the constant we obtained in Theorem. 1. Bounding Regret from Committing to the Wrong Action: As for the regret accumulated from commitment, the key is to prove the OpT ´1q confidence level upper bound presented in Corollary. 1. We follow a procedure similar to the proof of Theorem 1 in (Yang et al., 2022a) to utilize the adaptivity of UCB exploration and the pessimistic LCB commitment. We first show that with high probability, the number of pulls Ntpaq for any sub-optimal actions in the exploration phase is upper bounded. This is because after certain number of pulls, the uncertainty bonus bt´1paq for any sub-optimal action will be so small that the upper confidence bound UCBt´1paq cannot be larger than µa˚, thus less than the upper confidence bound of the optimal action. Therefore, sub-optimal actions will not be chosen in future rounds. After our carefully designed Tc, we make sure that the optimal action has the largest number of pulls NTcpaq among all actions, thus its bonus is so small so that its lower confidence bound LCBTcpa˚q is larger than the expectations µa of any sub-optimal action a, and thus larger than the lower confidence bound of other actions. So with high probability, we will commit to the optimal action. Then, the OpT ´1q confidence level will provide us with a constant regret in commitment,and the overall dominating regret comes from exploration. Combining both bounds, we are able to show the regret performance upper bound in Theorem. 1. 4.2 Regret Optimality for Gaussian Bandits with Adaptive Stopping Time We present the regret performance of EOCP-UG in Algorithm. 2 in Theorem. 2. Compared to Theorem. 1, EOCP-UG has the same performance guarantee as EOCP, which implies the knowledge of ∆min does not affect regret. So, EOCP-UG is also regret optimal in Gaussian bandit settings. Theorem 2 If we choose l “ logpTq ` 4 a 2 logpTq, the expected regret of the EOCP-UG algorithm in Algorithm. 2 with adaptive stopping time is asymptotically upper bounded by: lim sup TÑ8 RegEOCP-UG µ pTq log T ď ÿ a:∆aą0 2 ∆a . With adaptive stopping, the sample complexity until commitment is not a pre-determined value. However, we can still extract similar guarantee along with the confidence level from the proof of Theorem. 2. We present these results in the following corollary: Corollary 2 If we choose l “ logpTq ` 4 a 2 logpTq, the sample complexity until commitment for EOCP- UG algorithm in Algorithm. 2 is asymptotically upper bounded by: lim sup TÑ8 SCCEOCP-UG µ pTq log2 T ď ÿ a:∆aą0 8 ∆2a , and the confidence level is upper bounded by OpT ´1q. Proof Roadmap: The complete proofs of Theorem. 2 and Corollary 2 are provided in the supplementary material. Compared to the proof of Theorem. 1, the major difference lies in bounding the regret of commitment. Similarly, we require to bound the probability of committing to a sub-optimal action. We first show that when the agent stops exploration according to Line 2 of Algorithm. 2, the action which has the maximum number of pulls is the optimal action with high probability. Then, we show that under this event we will to commit to the optimal action if we use LCB commitment. 7Magic Choice of Exploration Function: Bounding the regret in both exploration and commitment requires a delicate analysis with any-time concentration inequalities. Our choice of exploration function l plays an important role which manages the trade-off between low-regret exploration and high-probability optimal action commitment. If the exploration function is too large, i.e., if l “ 2 log T, the regret in exploration will not be optimal. If the exploration function is too small, i.e., if l “ log T, the probability of committing to the wrong action can not be bounded by OpT ´1q. Our magic choice of exploration function l achieves the best of both worlds. Loss of SCC from Unknown Gap: Even though we have the same regret performance, the guarantee for sample complexity until commitment is now Oplog2pTqq which is worse than OplogpTqq in the pre-determined stopping time setting. So, is this order fundamental with adaptive stopping time? In the next section, we provide the answer by investigating the theoretical limits of commitment time for asymptotic regret optimal algorithms. 4.3 Fundamental Limits of Sample Complexity until Commitment In order to answer the question regarding the fundamental sample complexity until commitment for regret optimal algorithms with both pre-determined and adaptive stopping times, we provide the following theorem in a 2-armed Gaussian bandit model, where the reward gap between the two actions is ∆, and the optimal regret is 2∆´1 logpTq asymptotically: Theorem 3 (Information-Theoretic Limits of SCC) Consider a 2-armed Gaussian bandits, for any asymptotically regret optimal algorithm Alg which has c-logarithm regret violation, i.e., there exists c P p0, 1q such that when T is large enough, the following relationship always holds: ˇˇˇˇRegAlg µ pTq ´ 2 logpTq ∆ ˇˇˇˇ “ Oplogc Tq. In order to guarantee OpT ´1q confidence level, the sample complexity until commitment with pre- determined stopping time is lower bounded by Tc “ Ωp∆´2 logpTqq, and with adaptive stopping time, the sample complexity until commitment is lower bounded by: EµrTcs “ Ωp∆´2 log2´cpTqq. Proof Roadmap: The proof is provided in the supplementary material. The proof idea relies on the well-known “transportation” lemma (Kaufmann et al., 2016, Lemma. 1) originally derived to prove the theoretical limits of best arm identification algorithms. This lemma characterizes the expected number of pulls for each action by hypothesis testing between the original bandit problem and another bandit instance with a different optimal action. Then by finding a proper bandit instance and combining the lemma with regret optimal algorithms, we will be able to prove Theorem. 3 for both settings. Sample Optimality: It is shown by Corollary. 1 and Theorem. 3 together that our proposed EOCP algorithm from Algorithm. 1 achieves the optimal commitment time with Oplog Tq with pre-determined stopping time. However, with adaptive stopping time, our EOCP-UG algorithm has 1{2-logarithm regret violation and Oplog2 Tq commitment time indicated by the proof of Theorem. 2. However, Theorem. 3 implies Oplog1.5 Tq commitment time lower bound for such algorithms. Even though EOCP-UG is not exactly sample optimal, we conjure that this gap comes from our analysis techniques which make one of the bounds (maybe both) not tight. 5 Regret Optimality for General Bandits Even though EOCP is applicable in sub-Gaussian bandits, it is not regret optimal beyond Gaussian bandits. Comparing Theorem. 1 and the fundamental regret limit (1) with Pinsker’s inequality, it is clear 8that the lower bound is smaller than our upper bound even asymptotically. To close this gap, we propose an improved algorithm called KL-EOCP which is provably regret optimal in general bandits. Natural Exponential Family: We assume the reward distributions of each action belong to a natural exponential family, i.e., P “ tpνθqθPΘ : dνθ{dξ “ exppθx ´ bpθqqhpxqu, where Θ Ă R is the set of all parameters θ such that the expectation µ is positive and bounded, i.e., µ P r0, 1s. ξ is some reference measure on R and b : Θ Ñ R is a convex twice differentiable function. This distribution νθ can also be parameterized by its expectation µ “ b1pθq, the derivative of bp¨q, and for every µ we denote by νµ the unique distribution in P with expectation µ and by θµ its corresponding parameter. Gaussian distribution with unit variance is an example of this family. Moreover, the Kullback-Leibler divergence from νθ1 to νθ2 (with a little abuse of notation) can be expressed as (Garivier and Cappé, 2011): KLpµ1, µ2q “ KLpνθ1, νθ2q “ bpθ2q ´ bpθ1q ´ b1pθ1qpθ2 ´ θ1q. The set of exponential family bandit models ν “ pνθ1, ¨ ¨ ¨ , νθAq can be characterized by the expectations of the actions µ “ pµ1, ¨ ¨ ¨ , µAq. We assume for all λ P R, and θ P Θ the moment generating function Mνθpλq “ EνθrexppλWqs for the distribution νθ is well-defined and is finite. Algorithm: Analog to ∆min in Algorithm. 1, the KL-EOCP algorithm requires the knowledge of a strictly positive “minimum KL divergence”, denoted as KLmin, which captures the minimum reward distri- bution gap (distance) between the optimal action and any sub-optimal action. Considering the asymmetric nature of the KL divergence, we define KLmin as follows: KLmin “ mina‰a˚ min tKLpµa, µ1q, 4KLpµ1 a, µaqu , where µ1 a P pµa, µ1q and 4KLpµ1 a, µ1q “ KLpµa, µ1q. The term 4KLpµ1 a, µaq reflects the skew of KL diver- gence when the two distributions are switched. A lower bound to KLmin can be computed given the knowledge of minimum reward gap ∆min with the exponential family, and we only require the knowledge of such lower bound. The KL-EOCP algorithm is summarized in Algorithm. 3. It designs the UCB and LCB bonuses based on the KL divergence of the reward distributions. In general, these designs would lead to smaller confidence intervals with the same OpT ´1q probability guarantee as first shown in (Garivier and Cappé, 2011). If KLmin is not known, we can combine Algorithm. 3 with the adaptive stopping time of EOCP-UG to deal with this setting. Algorithm 3 KL-EOCP with Pre-Determined Stopping Time Require: Exploration function l. 1: Let Tc “ 4Al KL2 min ` A be the length of the exploration phase. 2: Initialize by pulling each arm a once. 3: for t “ A ` 1 : Tc do 4: Set upper confidence bound: UCBt´1paq “ arg max µě¯rt´1paq tNt´1paqKLp¯rt´1paq, µq ď lu . 5: Take action At “ arg maxa UCBt´1paq. // UCB Exploration 6: end for 7: Set lower confidence bound: LCBTcpaq “ arg min µď¯rTcpaq tNTcpaqKLp¯rTcpaq, µq ď lu . 8: For t P rTc ` 1, Ts, commit to action ˆa “ arg maxa LCBTcpaq. // LCB Commitment Regret Optimality in General Bandits: The theoretical regret performance of the KL-EOCP 9100 101 102 103 104 105 106 Number of Rounds 0 10 20 30 40 50 60 70 80 Regret EOCP EOCP-UG UCB BAI-ETC DETC (a) Gaussian Bandits 100 101 102 103 104 105 Number of Rounds 0 10 20 30 40 50 60 Regret EOCP EOCP-UG KL-EOCP UCB KL-UCB BAI-ETC DETC (b) Bernoulli Bandits Figure 1: Comparison of regret performance of EOCP with variants and existing algorithms in the literature. The gap ∆ between the two arms is 0.5 and results are averaged over 105 iterations. is sumarized in the following Theorem. To the best of our knowledge, it is the first result achieving asymptotic regret optimality in general bandit problems with commitment. Theorem 4 If we choose l “ logpTq`4 a 2 logpTq, when the reward distributions ν of each action belong to the same natural exponential family, the expected regret of the KL-EOCP algorithm in Algorithm. 3 is asymptotically upper bounded by: lim sup TÑ8 RegµpTq log T ď ÿ a:∆aą0 ∆a KLpµa, µ1q. The complete proof of Theorem. 4 is provided in the supplementary materials. Even though the proof roadmap is similar to the proof of Theorem. 1, the major difference comes from the use of a tighter concentration lemma modified from (Garivier and Cappé, 2011, Theorem. 11) which captures the low probability event when the KL divergence of the empirical mean is far away from its expectation. The sample complexity until commitment and confidence level guarantees can also be extracted from the proof. Specifically, SCCKL-EOCP µ pTq “ OpKL´1 min log Tq and the confidence level is upper bounded by OpT ´1q. 6 Numerical Experiments In this section, we study the empirical performance of our proposed EOCP algorithm with variants compared to existing algorithms in the literature, including BAI-ETC Garivier et al. (2016), UCB Auer et al. (2002), KL-UCB Garivier and Cappé (2011), and DETC Jin et al. (2021a) in both Gaussian and Bernoulli bandit settings. In the Gaussian setting, we test all the algorithms on a two-armed bandit problem with distributions Npµi, 1q for arm i “ 1, 2 with a total of 106 rounds, and in the Bernoulli bandit setting, we test the algorithms with distribution Berpµiq for arm i “ 1, 2 with a total of 105 rounds. We set µ1 “ 0.7 and µ2 “ 0.2, so the gap between the arms is ∆ “ 0.5. The results are averaged over 106 iterations and shown in Fig. 1. In the Gaussian bandit setting, it shows that both BAI-ETC and DETC algorithms exhibit unsatisfac- torily high regret. On the contrary, our proposed algorithms EOCP and EOCP-UG have lower final regret 10even compared to the popular UCB algorithm, surprisingly. Even though our algorithms use a larger exploration function than the vanilla UCB algorithm, and accumulate regret more quickly in exploration, i.e., approximately the first 1000 rounds, it allows us to commit to the optimal action. Our algorithms almost find the optimal action in all simulated traces which give rise to the very slowly-increasing behavior of regret in the commitment phase. This phenomenon coincides with our theoretical analysis which shows that the regret in commitment is Op1q. However, the UCB algorithm continues to explore sub-optimal actions, so its regret continues to grow when the EOCP algorithm has already committed to the optimal action. Numerically, the EOCP algorithm reduces 20% of the final regret compared to UCB through early commitment, and we conjure that preventing the “over-exploration” phenomenon of UCB algorithm is behind the reason for such empirical regret reduction. Comparing the commitment time of EOCP-UG and EOCP, we can see that both algorithms stop exploration at approximately 1000 rounds. This means that not knowing the gap information won’t harm the empirical sample complexity until commitment too much, which in turn may imply that our theoretical analysis of commitment time upper bound in Corollary. 2 is not tight enough. The same trend can be witnessed from results of Bernoulli bandits. However, in Bernoulli bandits, KL-UCB and KL-EOCP algorithms have a much better regret performance than other algorithms, which shows the knowledge of the reward distribution family improves the performance significantly. 7 Conclusion We studied ROBAI which intends to both minimize regret and commit to the optimal action. We proposed EOCP with variants, which combine UCB exploration and LCB commitment with novel stopping criterion in both pre-determined and adaptive settings. We showed that both EOCP and EOCP-UG are regret asymptotic optimal in Gaussian bandits with Oplog Tq and Oplog2 Tq commitment time respectively, almost matching the theoretical limits we derived. For general bandits, we proposed KL-EOCP which is provably regret optimal. Numerical experiments confirmed the superiority of our algorithms and revealed the “over-exploration” phenomenon of UCB algorithms. References Agrawal, S. and Goyal, N. (2012). Analysis of thompson sampling for the multi-armed bandit problem. In Conference on learning theory, pages 39–1. JMLR Workshop and Conference Proceedings. Auer, P., Cesa-Bianchi, N., and Fischer, P. (2002). Finite-time analysis of the multiarmed bandit problem. Machine learning, 47:235–256. Aziz, M., Kaufmann, E., and Riviere, M.-K. (2021). On multi-armed bandit designs for dose-finding clinical trials. The Journal of Machine Learning Research, 22(1):686–723. Degenne, R., Nedelec, T., Calauzènes, C., and Perchet, V. (2019). Bridging the gap between regret minimization and best arm identification, with application to a/b tests. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1988–1996. PMLR. Garivier, A. and Cappé, O. (2011). The kl-ucb algorithm for bounded stochastic bandits and beyond. In Proceedings of the 24th annual conference on learning theory, pages 359–376. JMLR Workshop and Conference Proceedings. Garivier, A. and Kaufmann, E. (2016). Optimal best arm identification with fixed confidence. In Conference on Learning Theory, pages 998–1027. PMLR. 11Garivier, A., Lattimore, T., and Kaufmann, E. (2016). On explore-then-commit strategies. Advances in Neural Information Processing Systems, 29. Higuchi, T. (1963). Mechanism of sustained-action medication. theoretical analysis of rate of release of solid drugs dispersed in solid matrices. Journal of pharmaceutical sciences, 52(12):1145–1149. Huang, J., Golubchik, L., and Huang, L. (2023). Queue scheduling with adversarial bandit learning. arXiv preprint arXiv:2303.01745. Jin, T., Xu, P., Xiao, X., and Gu, Q. (2021a). Double explore-then-commit: Asymptotic optimality and beyond. In Conference on Learning Theory, pages 2584–2633. PMLR. Jin, Y., Yang, Z., and Wang, Z. (2021b). Is pessimism provably efficient for offline rl? In International Conference on Machine Learning, pages 5084–5096. PMLR. Katehakis, M. N. and Robbins, H. (1995). Sequential choice from several populations. Proceedings of the National Academy of Sciences, 92(19):8584–8585. Kaufmann, E., Cappé, O., and Garivier, A. (2012a). On bayesian upper confidence bounds for bandit problems. In Artificial intelligence and statistics, pages 592–600. PMLR. Kaufmann, E., Cappé, O., and Garivier, A. (2016). On the complexity of best arm identification in multi-armed bandit models. Journal of Machine Learning Research, 17:1–42. Kaufmann, E., Korda, N., and Munos, R. (2012b). Thompson sampling: An asymptotically optimal finite-time analysis. In Algorithmic Learning Theory: 23rd International Conference, ALT 2012, Lyon, France, October 29-31, 2012. Proceedings 23, pages 199–213. Springer. Kohli, P., Salek, M., and Stoddard, G. (2013). A fast bandit algorithm for recommendation to users with heterogenous tastes. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 27, pages 1135–1141. Lai, T. L., Robbins, H., et al. (1985). Asymptotically efficient adaptive allocation rules. Advances in applied mathematics, 6(1):4–22. Lattimore, T. and Szepesvári, C. (2020). Bandit algorithms. Cambridge University Press. Li, G., Ma, C., and Srebro, N. (2022a). Pessimism for offline linear contextual bandits using lp confidence sets. arXiv preprint arXiv:2205.10671. Li, G., Shi, L., Chen, Y., Chi, Y., and Wei, Y. (2022b). Settling the sample complexity of model-based offline reinforcement learning. arXiv preprint arXiv:2204.05275. Liu, X., Li, B., Shi, P., and Ying, L. (2020). Pond: Pessimistic-optimistic online dispatching. arXiv preprint arXiv:2010.09995. Nie, G., Agarwal, M., Umrawal, A. K., Aggarwal, V., and Quinn, C. J. (2022). An explore-then-commit algorithm for submodular maximization under full-bandit feedback. In Uncertainty in Artificial Intelligence, pages 1541–1551. PMLR. Phillips, S. D., Pazienza, N. J., and Walsh, D. J. (1984). Decision making styles and progress in occupational decision making. Journal of Vocational Behavior, 25(1):96–105. 12Rashidinejad, P., Zhu, B., Ma, C., Jiao, J., and Russell, S. (2021). Bridging offline reinforcement learning and imitation learning: A tale of pessimism. Advances in Neural Information Processing Systems, 34:11702–11716. Russo, D. and Van Roy, B. (2016). An information-theoretic analysis of thompson sampling. The Journal of Machine Learning Research, 17(1):2442–2471. Russo, D. J., Van Roy, B., Kazerouni, A., Osband, I., Wen, Z., et al. (2018). A tutorial on thompson sampling. Foundations and Trends® in Machine Learning, 11(1):1–96. Shi, L., Li, G., Wei, Y., Chen, Y., and Chi, Y. (2022). Pessimistic q-learning for offline reinforcement learning: Towards optimal sample complexity. arXiv preprint arXiv:2202.13890. Siegel, J. J. (2021). Stocks for the long run: The definitive guide to financial market returns & long-term investment strategies. McGraw-Hill Education. Siroker, D. and Koomen, P. (2015). A/B testing: The most powerful way to turn clicks into customers. John Wiley & Sons. Srikant, R. and Ying, L. (2013). Communication networks: an optimization, control, and stochastic networks perspective. Cambridge University Press. Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3-4):285–294. Villar, S. S., Bowden, J., and Wason, J. (2015). Multi-armed bandit models for the optimal design of clinical trials: benefits and challenges. Statistical science: a review journal of the Institute of Mathematical Statistics, 30(2):199. Xiao, C., Wu, Y., Mei, J., Dai, B., Lattimore, T., Li, L., Szepesvari, C., and Schuurmans, D. (2021). On the optimality of batch policy optimization algorithms. In International Conference on Machine Learning, pages 11362–11371. PMLR. Yang, Z., Liu, X., and Ying, L. (2022a). Exploration. exploitation, and engagement in multi-armed bandits with abandonment. In 2022 58th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 1–2. Yang, Z., Srikant, R., and Ying, L. (2022b). Maxweight with discounted ucb: A provably stable scheduling policy for nonstationary multi-server systems with unknown statistics. arXiv preprint arXiv:2209.01126. Yekkehkhany, A., Arian, E., Hajiesmaili, M., and Nagi, R. (2019). Risk-averse explore-then-commit algorithms for finite-time bandits. In 2019 IEEE 58th Conference on Decision and Control (CDC), pages 8441–8446. IEEE. Yekkehkhany, A., Arian, E., Nagi, R., and Shomorony, I. (2021). A cost–based analysis for risk–averse explore–then–commit finite–time bandits. IISE Transactions, 53(10):1094–1108. Zeng, C., Wang, Q., Mokhtari, S., and Li, T. (2016). Online context-aware recommendation with time varying multi-armed bandit. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pages 2025–2034. Zhang, Q., Wei, H., Wang, W., and Ying, L. (2022). On low-complexity quickest intervention of mutated diffusion processes through local approximation. In Proceedings of the Twenty-Third International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing, pages 141–150. 13A Related Works In this section, we provide a more detailed review of related works. We first review two classic problem formulations of the multi-armed bandit model: the regret minimization problem and the best arm identification problem. Then we review previous works on explore-then-commit algorithms which takes commitment into account. Finally, we review the offline stochastic bandit literature which motivated our choice of pessimistic principle in action identification. A.1 Regret Minimization The theoretical limits of regret minimization have been revealed by Lai et al. (1985); Katehakis and Robbins (1995), which shows that the expected regret of any algorithm is lower bounded when horizon T approaches infinity: lim inf TÑ8 RegAlg µ pTq log T ě ÿ a:∆aą0 ∆a KLpνa, νa˚q, where KLp¨, ¨q denotes the Kullback–Leibler divergence between two distributions. Based on the asymptotic lower bound, we say an algorithm is asymptotically regret optimal if its regret performance achieves the regret lower bound asymptotically. Two sets of algorithms prevail in the regret minimization literature. One is the family of UCB algorithms Auer et al. (2002); Garivier and Cappé (2011); Kaufmann et al. (2012a), which reflects the principal of optimism in action selection to encourage exploration. To be specific, the UCB algorithms will select the action which has the largest upper confidence bound of reward estimation. This upper confidence bound represents the highest possible expected reward given the samples collected from previous rounds under a high probability event. Usually the additional bonus from the empirical mean to the upper confidence bound for a specific action decreases as the number of pulls increases. Therefore, actions which have not been tried frequently in previous rounds will have larger bonus. This trait encourages exploration. By designing the bonus carefully, one can find the optimal trade-off between exploration and exploitation. The other set of prevailing algorithms is the family of Thompson Sampling Algorithms Thompson (1933); Russo et al. (2018); Agrawal and Goyal (2012); Kaufmann et al. (2012b); Russo and Van Roy (2016). The TS algorithm assumes each action is associated with a posterior distribution given the reward feedback from previous rounds. Then the agent will collect one virtual sample from each posterior distribution and choose the action which has the largest virtual sample. As the number of pulls grows, the posterior distribution will be more and more concentrated around the expectation so the virtual sample will also be closer to its expectation. On the other hand, the remaining randomness encourages exploration of other under-explored actions. However, both algorithms do not commit to a single action because their action selection policies have to be re-evaluated at each round based on new observations. A.2 Best Arm Identification The best arm identification algorithms consist of three components: an action sampling rule deciding which action to choose at each round, a stopping rule deciding a time τ to stop collecting new samples, and a decision rule which outputs a best action candidate ˆa. We call an algorithm δ-PAC if the probability of outputting a sub-optimal action is less than δ, i.e., Pµpˆa ‰ 1q ď δ. Here, δ is also called the confidence level. The performance of best arm identification algorithms is measured by both the sample complexity until identifying the optimal action and the confidence level. The theoretical limits for best arm identification algorithms are also well studied in Garivier and Kaufmann (2016); Kaufmann et al. (2016), which shows that any δ-PAC algorithm would incur at least Oplog δ´1q sample complexity 14asymptotically. The constant in front of the logarithmic term depends on the bandit instance, i.e., the reward expectation and distribution of every action. Based on the this lower bound, the authors of Garivier and Kaufmann (2016) proposed the Track and Stop (TAS) algorithm which proved to be asymptotic optimal if the distribution associated with each action is from a single parameter natural exponential family. The TAS algorithm estimates the expectation of each action and at the same time calculates the optimal proportion of pulls for each action so that the optimal action is identifiable from the sub-optimal ones. Then, it designs a feedback control dynamic to track this proportion, similar to the Max-weight dynamic in queueing systems Srikant and Ying (2013). The stopping criterion of TAS is based on a generalized likelihood test. When the optimal action is identifiable from sub-optimal ones, the optimal action candidate ˆa is chosen to be the action which has the largest empirical reward mean. A.3 Explore-Then-Commit Algorithms A natural bridge between regret minimization and best arm identification problems, which also takes into account action commitment, is the explore-then-commit algorithms (Lattimore and Szepesvári, 2020; Garivier et al., 2016; Jin et al., 2021a; Nie et al., 2022; Yekkehkhany et al., 2019, 2021).Researchers have long been hoping that such algorithms will achieve the best of both worlds: maintaining low regret and identifying the optimal action quickly. In these algorithms, a clear separation of exploration phase and exploitation phase exists. At each round, the agent will only make action selection decisions based on the samples collected from the exploration phase, and commitment to a single action during the exploitation phase is required. No samples from the exploitation phase can be utilized although the agent may determine the length of both phases. It is clear that explore-then-commit algorithms can be easily designed from best arm identification algorithms, i.e., one would first run the best arm identification algorithm in the exploration phase, and then commit to the action ˆa found by the algorithm. However, it is shown in (Garivier et al., 2016) that these type of BAI-ETC algorithms is essentially regret sub-optimal. To be specific, the regret lower bound of such algorithms is asymptotically twice as large as the upper bound of optimal regret minimization algorithms such as UCB and Thompson Sampling even with careful tuning. A recent work (Jin et al., 2021a) provides a double explore-then-commit algorithm called DETC which is shown to be asymptotically regret optimal. But the algorithm itself is very complex and requires multiple stages of exploration and exploitation. This trait makes its sample complexity to identify the optimal action large, i.e., Ωplog2 Tq. Moreover, in empirical studies, the DETC algorithm incurs very large regret which is no match for the vanilla UCB algorithm. Another work (Degenne et al., 2019) proposes an exploration algorithm with inflated UCB, which can be naturally adapted to an ETC algorithm. Even though the sample complexity is order optimal, i.e., Oplog Tq, the regret performance is essentially sub-optimal due to the inflation of UCB bonus. A.4 Offline Stochastic Bandits and Reinforcement Learning Our action identification policy is inspired by the principle of pessimism widely adopted in offline stochastic bandit problems Li et al. (2022a); Xiao et al. (2021); Rashidinejad et al. (2021) and reinforcement learning problems Li et al. (2022b); Jin et al. (2021b); Shi et al. (2022). In offline bandit problems, the agent is not allowed to interact with the environment at will. Instead, it is provided with a training dataset which contains action and reward pairs collected from the same bandit problem. Based on this dataset, the agent is asked to choose the optimal action. It is shown in Rashidinejad et al. (2021) that greedily selecting the action with largest empirical mean would fail to produce the optimal action in some bandit problems. This failure results from the randomness of samples in the dataset and the imbalanced number of samples for each action. Instead, choosing the action with largest lower confidence bound to combat the imbalanced uncertainty between estimations of different actions leads to better performance as justified 15in Xiao et al. (2021); Rashidinejad et al. (2021). Similar to the UCB algorithm, the penalty term between the empirical mean and the lower confidence bound for each action decreases as the number of pulls increases, so the action which has not been tried frequently will suffer large penalty. In this way, the agent will avoid selecting an action which has large uncertainty to enhance stability. Our proposed algorithm incorporates the idea of pessimism in action identification. B Proofs of Main Results for EOCP and EOCP-UG In this section, we provide the proofs of main results presented in Section. 4. Throughout the proof section, we let Wa,i be the i-th sample from pulling arm a the i-th time. Let ¯ra,s be the empirical mean of arm a after it has been pulled s times. Before proving the theorems, we provide several lemmas which gathers specific large deviation results useful in our analyses. Then we prove the regret performance based on the concentration lemmas. B.1 Concentration Inequalities We first present two concentration lemmas which characterize the sum and mean of empirical realizations of independent sub-Gaussian random variables as follows: Lemma 1 (Theorem 9.2 in (Lattimore and Szepesvári, 2020)) Let W1, W2, ¨ ¨ ¨ , WT be a sequence of independent σ-subgaussian random variables with ErW1s “ 0. Then, for any δ ą 0, we have: P ˜ Ds ď T, sÿ i“1 Wi ě δ ¸ ď exp ˆ ´ δ2 2Tσ2 ˙ . (2) Lemma 2 (Lemma C.3 in (Jin et al., 2021a)) Let T1 ď T2 ď T be two real numbers in R`. Let W1, W2, ¨ ¨ ¨ , WT be a sequence of identically and independently distributed random variable according to a σ-subgaussian distribution with ErW1s “ 0. Then, for any δ ą 0, we have: P ˆ DT1 ď s ď T2, řs i“1 Wi s ě δ ˙ ď exp ˆ ´T1δ2 2σ2 ˙ (3) The proofs of aforementioned concentration lemmas can be found in the references respectively. Now we present the concentration results for our design of confidence bonuses as follows: Lemma 3 Let W1, W2, ¨ ¨ ¨ , WT be identically and independently distributed 1-sub-Gaussian random variables with ErW1s “ 0. Let T1 ď T2 ď T, then the following holds: (a) if l ě 2, P ˜ Ds P pT1, T2s, řs i“1 Wi s ` c 2l s ď 0 ¸ ď min tT2 ´ T1, el plog T2 ´ log T1q ` eu expplq ; (4a) (b) if δ P p0, ? 3s, P ˜ Ds P rT1, T2s, řs i“1 Wi s ` c 2l s ` δ ď 0 ¸ ď 4 δ2 exp ˜ˆ? l ` δ b T1 2 ˙2¸; (4b) (c) if l ě T1δ2 2 , T2 ÿ s“T1`1 P ˜řs i“1 Wi s ` c 2l s ě δ ¸ ď 2l ` ? 4πl ` 2 δ2 ` 1 ´ T1. (4c) The proof of the Lemma. 3 will be delayed to Sec. D. 16B.2 Proof of Regret Optimality for EOCP with Pre-Determined Stopping Time We prove the following Theorem which characterizes the finite-time performance of Algorithm. 1. Theorem. 1 can be directly derived from Theorem. 1 by letting T approaches infinity. Theorem 5 If l “ logpTq ` 4 a 2 logpTq and T ě 16, the expected regret of the EOCP algorithm in Algorithm. 1 with pre-determined stopping time is upper bounded by: RegEOCP µ pTq ď ÿ a:∆aą0 ˆ2 log T ∆a ` p8 ` ? 20πq?log T ∆a ` 2 ∆a ` ∆a ˙ ` op1q. Remark: The asymptotic result in Theorem. 1 is clear from Theorem.5 by letting T increases to infinity, i.e., lim sup TÑ8 RegEOCP µ pTq log T ď lim sup TÑ8 ÿ a:∆aą0 ˆ 2 ∆a ` p8 ` ? 20πq ∆a ?log T ` 2 ∆a log T ` ∆a log T ˙ ` op1q “ ÿ a:∆aą0 2 ∆a . Proof. Without loss of generality, assume action 1 is the unique optimal action. From the regret decomposition lemma (Lattimore and Szepesvári, 2020, Lemma 4.5), we can decompose the regret of Algorithm. 1 to the number of pulls for each sub-optimal arm as follows: RegEOCP µ pTq “ ÿ a:∆aą0 ∆aErNT paqs. Then, the key to bound the total regret is to bound the number of pulls for each sub-optimal arms. Since our EOCP algorithm has a clear separation of exploration and exploitation phases, so for any sub-optimal action a, we can bound its pulls in different phases as follows: ErNapTqs ď E rN2pTcqs looooomooooon I1 `pT ´ Tcq Ppˆa “ aq looomooon I2 , where Tc is the end of exploration phase and ˆa is the action that the algorithm commits to. Notice that the bound on I2 gives the confidence level result provided in Corollary. 1. Bounding I1: We first bound I1, which is similar to the proof of bounding the regret for the traditional UCB algorithm (Garivier and Cappé, 2011; Auer et al., 2002; Garivier et al., 2016). We have: I1 “ E « Tc ÿ t“1 1At“a ff “ 1 ` E « Tc ÿ t“A`1 1At“a1UCBt´1paqěµ1 ff looooooooooooooooooomooooooooooooooooooon A1 ` E « Tc ÿ t“A`1 1At“a1UCBt´1paqăµ1 ff looooooooooooooooooomooooooooooooooooooon A2 . Notice that At “ a indicates UCBt´1p1q ă UCBt´1paq due to the dynamic of UCB exploration, we can bound the last term as: A2 ď Tc ÿ t“A`1 E “ 1UCBt´1p1qăµ1 ‰ “ Tc ÿ t“A`1 P pUCBt´1p1q ă µ1q . 17Event tUCBt´1p1q ă µ1u means that at round t, the mean estimation ¯rt´1p1q of the optimal arm from previous pulls is lower than µ1 ´ bt´1p1q, which incurs a large deviation, so we can bound this event with a union over all rounds in the exploration phase: tUCBt´1p1q ă µ1u Ă ␣ Dt1 P rA, Tcq, UCBt1p1q ă µ1 ( , which doesn’t depend on round number t any more, so we have: Tc ÿ t“A`1 P pUCBt´1p1q ă µ1q ď Tc ÿ t“A`1 P ` Dt1 P rA, Tcq, UCBt1p1q ă µ1 ˘ “pTc ´ AqP ` Dt1 P rA, Tcq, UCBt1p1q ă µ1 ˘ “pTc ´ AqP ˜ Dt1 P rA, Tcq, ¯rt1p1q ` d 2l Nt1p1q ă µ1 ¸ It is worth-noting that if there is no pulls for arm 1 between a time interval, its empirical mean and number of pulls will remain the same. So we have: # Dt1 P rA, Tcq, ¯rt1p1q ` d 2l Nt1p1q ă µ1 + Ă # Ds P r1, NTcpa˚qq , ¯rs,1 ` c 2l s ă µ1 + Notice that NTcp1q ď Tc ´ A ` 1, so we can bound the probability as follows: P ˜ Dt1 P rA, Tcq, ¯rt1p1q ` d 2l Nt1p1q ă µ1 ¸ ďP ˜ Ds P r1, Tc ´ A ` 1q, ¯rs,1 ´ µ1 ` c 2l s ă 0 ¸ ďP ˜ Ds P r1, Tc ´ A ` 1q, řs i“1pW1,i ´ µ1q i ` c 2l s ă 0 ¸ ďTc ´ A ` 1 expplq , where the last inequality uses Eq. (4a) of Lemma 3 and the fact that W1,i ´ µ1 is 1-sub-Gaussian with zero mean. Notice that ?log T ě log log T when T is large and we have: expplq “ exp ´ log T ` 4 a 2 log T ¯ ě expplog T ` 4 log log Tq “ 1 T log4 T . So putting the above bound back to the bound of A2, we have: A2 ď pTc ´ A ` 1q2 T log2 T “ ˆ 8Aplog T`4?2 log Tq ∆2 min ` 1 ˙2 T log4 T “ o ˆ 1 T ˙ , where the last inequality is due to the fact that when T ě 3, ?log T ď log T. Next, we attempt to bound the middle term A1 as follows: A1 “ E « Tc ÿ t“A`1 1At“a1UCBt´1paqěµ1 ff “E « Tc ÿ t“A`1 1At“a1¯rt´1paq` b 2l Nt´1paq ěµ1 ff 18Notice that the number of pulls Ntpaq will only increase by 1 every time there is new pull, i.e., when At “ a. Otherwise, the term inside summation is 0. So instead of counting on the time step t, we can count over the number of pulls over arm a as follows: E « Tc ÿ t“A`1 1At“a1¯rt´1paq` b 2l Nt´1paq ěµ1 ff “E » – NTcpaq ÿ s“1 1¯rs,a` b 2l s ěµ1 fi fl ď Tc ÿ s“1 P ˜ ¯rs,a ´ µa ` c 2l s ě ∆a ¸ , where the last inequality is due to NTcpaq ď Tc. Then, by Eq. (4c) of Lemma. 3, we have: Tc ÿ s“1 P ˜ ¯rs,a ´ µa ` c 2l s ě ∆a ¸ “ Tc ÿ s“1 P ˜řs i“1pWa,i ´ µaq i ` c 2l s ě ∆a ¸ ď 2 ` log T ` 4?2 log T ˘ ` b 4π ` log T ` 4?2 log T ˘ ` 2 ∆2a ` 1 ď2 log T ∆2a ` p8 ` ? 20πq?log T ∆2a ` 2 ∆2a ` 1, where the first inequality is due to the fact that Wa,i ´ µa is 1-sub-Gaussian with zero mean, and the last inequality is due to ?2 log T ď log T when T ě 9. Therefore, combining the bounds on A1 and A2, we can bound I1 as follows: I1 ď 2 log T ∆2a ` p8 ` ? 20πq?log T ∆2a ` 2 ∆2a ` 1 ` o p1q . Bounding I2: The bound on I2 gives the confidence level result provided in Corollary. 1. After the exploration phase, recall that we select the arm with the largest lower confidence bound to commit to. The idea of bounding I2 is very similar to the regret bound in (Yang et al., 2022a) where since we use UCB to explore in the exploration phase, the optimal arm should be pulled very often such that its bonus becomes very small when exploration phase ends. Then selecting the arm with largest LCB will ensure we select the optimal arm with high probability. This is because the LCB of the optimal arm is larger than the true means of any sub-optimal arms, and the true means of sub-optimal arms are larger than their respective LCB, both with high probability. We can first bound the probability of selecting a sub-optimal arm a as: P pˆa “ aq ďP pLCBTcpaq ě LCBTcp1qq ď P ˆ LCBTcpaq ě LCBTcp1q, NTcp1q ą 8l ∆2 min ˙ loooooooooooooooooooooooooooomoooooooooooooooooooooooooooon B1 ` P ˆ NTcp1q ď 8l ∆2 min ˙ loooooooooooomoooooooooooon B2 We first bound the term B2 which states that during the first exploration phase with UCB exploration, the optimal arm is under-pulled, which means that one of the sub-optimal arms have been pulled with larger number of times. Therefore, we have: P ˆ NTcp1q ď 8l ∆2 min ˙ “ P ˜ ÿ a:∆aą0 NTcpaq ą Tc ´ 8l ∆2 min ¸ . Recall that Tc “ 8Al ∆2 min ` A, so we have: P ˆ NTcp1q ď 8l ∆2 min ˙ “P ˜ ÿ a:∆aą0 NTcpaq ą 8pA ´ 1ql ∆2 min ` A ¸ ď P ˆ Da ą 1, NTcpaq ą 8l ∆2 min ` 1 ˙ . 19Then by union bound, we have: P ˆ Da ą 1, NTcpaq ą 8l ∆2 min ` 1 ˙ ď ÿ a:∆aą0 P ˆ NTcpaq ą 8l ∆2 min ` 1 ˙ . Consider a fixed sub-optimal arm a, then for each probability inside the summation, we have: P ˆ NTcpaq ą 8l ∆2 min ` 1 ˙ ďP ˆ Dt P rA ` 1, Tcs, Nt´1paq “ R 8l ∆2 min V , At “ 2 ˙ ďP ˆ Dt P rA ` 1, Tcs, Nt´1paq “ R 8l ∆2 min V , UCBt´1p1q ď UCBt´1paq ˙ ď P ˆ Dt P rA ` 1, Tcs, Nt´1paq “ R 8l ∆2 min V , µ1 ď UCBt´1paq ˙ looooooooooooooooooooooooooooooooooooooomooooooooooooooooooooooooooooooooooooooon B3 ` P pDt P rA ` 1, Tcs, UCBt´1p1q ď µ1q looooooooooooooooooooooomooooooooooooooooooooooon B4 . Notice that B4 can be bounded from concentration lemma. First, we switch the count from time step t to the number of pulls for arm 1 since the empirical estimation ¯rt´1p1q and count for pulls Nt´1p1q won’t change unless there is a new pull. Then, we will apply Eq. (4a) to the probability as follows: B4 “P ˜ Dt P rA ` 1, Tcs, ¯rt´1p1q ´ µ1 ` d 2l Nt´1p1q ď 0 ¸ ďP ˜ Ds P r1, Tc ´ A ` 1q, ¯rs,1 ´ µ1 ` c 2l s ď 0 ¸ ďP ˜ Ds P r1, Tc ´ A ` 1q, řs i“1pW1,i ´ µ1q s ` c 2l s ď 0 ¸ ďTc ´ A exp l , where the last inequality is due to the fact that W1,i ´ µ1 is 1-sub-Gaussian with zero mean. Similar to the procedure of bounding I1, notice that expplq ě T log4 T, we have: B4 ď 8A ` log T ` 4?2 log T ˘ ∆2 min 1 T log4 T “ o ˆ 1 T ˙ . On the other hand, let γ “ Q 8l ∆2 min U , and we also change the count from time step t to the number of pulls for arm a. Then, B3 can be expressed as follows: B3 “P ˜ Dt P rA ` 1, Tc ´ 1s, Nt´1paq “ γ, µ1 ď ¯rt´1paq ` d 2l Nt´1paq ¸ “P ˜ ¯rγ,a ` d 2l γ ą µ1 ¸ Notice that γ ě 8l ∆2 min ě 8l ∆2a , so we have b 2l γ ď b 2l∆2a 8l “ ∆a 2 . Then B3 can be bounded as follows: B3 ď P ˆ ¯rγ,a ` ∆a 2 ą µ1 ˙ “ P ˆ ¯rγ,a ´ µa ą ∆a 2 ˙ ď exp ˆ ´γ∆2 a 8 ˙ ď 1 expplq ď 1 T log4 T , 20where the first inequality uses Hoeffding’s inequality and the second inequality uses the lower bound on γ mentioned above. The last inequality is due to the fact that expplq ě T log4 T. So combine B3 and B4 together, we can bound B2 as follows: B2 ď ÿ a:∆aą0 pB3 ` B4q “ o ˆ 1 T ˙ . Next, we attempt to bound B1. Notice that B1 can also be bounded as follows: B1 “P ˆ LCBTcpaq ě LCBTcp1q, NTcp1q ą 8l ∆2 min ˙ ď P ˆ µa ě LCBTcp1q, NTcp1q ą 8l ∆2 min ˙ looooooooooooooooooooooomooooooooooooooooooooooon B5 ` P pµa ď LCBTcpaqq loooooooooomoooooooooon B6 . The term B6 can be expressed by counting the number of pulls of arm a as follows: B6 “P ˜ µa ď ¯rTcpaq ´ d 2l NTcpaq ¸ ďP ˜ Ds P r1, Tc ´ A ` 1s, µa ´ ¯rs,a ` c 2l s ď 0 ¸ “P ˜ Ds P r1, Tc ´ A ` 1s, řs i“1pµa ´ Wa,iq s ` c 2l s ď 0 ¸ . Notice that pµa ´ Wa,iq is 1-sub-Gaussian with zero mean, so we can apply concentration lemma Eq. (4a) from Lemma. (3) as follows: B6 ď Tc ´ A ` 1 expplq “ 8A ` log T ` 4?2 log T ˘ ∆2 min expplq ` 1 expplq “ o ˆ 1 T ˙ , where the last inequality is due to log T ě ?2 log T and expplq ě T log4 T when T ě 9. Similarly, term B5 can be expressed as: B5 “ P ˜ µa ě ¯rTcp1q ´ d 2l NTcp1q, NTcp1q ą 8l ∆2 min ¸ . First notice that when NTcp1q ą 8l ∆2 min , we have the bonus term bTcp1q “ b 2l NTcp1q ď ∆min 2 ď ∆a 2 . Then we have: B5 ď P ˆ µa ě ¯rTcp1q ´ ∆a 2 , NTcp1q ą 8l ∆2 min ˙ “ P ˆ µ1 ´ ¯rTcp1q ě ∆a 2 , NTcp1q ą 8l ∆2 min ˙ Then it is equivalent to count over the number of pulls for arm 1: B5 ďP ˆ Ds P „ 8l ∆2 min , T ȷ , µ1 ´ ¯rs,1 ě ∆a 2 ˙ “P ˆ Ds P „ 8l ∆2 min , T ȷ , řs i“1pµ1 ´ W1,iq s ě ∆a 2 ˙ . 21Then, by the maximal concentration Eq. (3) from Lemma. 2, we can bound B5 as follows: P ˆ Ds P „ 8l ∆2 min , T ȷ , řs i“1pµ1 ´ W1,iq s ě ∆a 2 ˙ ď exp ¨ ˝´ 8l ∆2 min ` ∆a 2 ˘2 2 ˛ ‚“ 1 expplq ď 1 T log4 T . Therefore, collecting the bounds for B5 and B6, we have a bound for B1 as follows: B1 ď B5 ` B6 “ o ˆ 1 T ˙ . And therefore from the bounds of B1 and B2, we can bound term I2 as follows: I2 ď B1 ` B2 “ o ˆ 1 T ˙ . This means the the confidence of selecting the wrong action to commit to is opT ´1q as indicated in Corollary. 1. Finally, putting the bounds on I1 and I2 together, we have: ErNapTqs ďI1 ` TI2 ď 2 log T ∆2a ` p8 ` ? 20πq?log T ∆2a ` 2 ∆2a ` 1 ` o p1q . Therefore, by the regret decomposition lemma, we can bound the total regret as follows: RegµpTq ď ÿ a:∆aą0 ˆ2 log T ∆a ` p8 ` ? 20πq?log T ∆a ` 2 ∆a ` ∆a ` o p1q ˙ “ ÿ a:∆aą0 ˆ2 log T ∆a ` p8 ` ? 20πq?log T ∆a ` 2 ∆a ` ∆a ˙ ` op1q. B.3 Proof of Regret Optimality for EOCP-UG with Adaptive Stopping Time We prove the following theorem which characterizes the finite-time performance of Algorithm. 2. Theo- rem. 2 can be derived by letting T approaches infinity. Theorem 6 Under Gaussian bandits time horizon T ě 16, the Regret of EOCP algorithm in Algorithm. 2 with adaptive stopping time can be upper bounded as: RegEOCP-UG µ pTq ď ÿ a:∆aą0 ˆ2 log T ∆a ` p8 ` ? 20πq?log T ∆a ˙ ` Op1q. Remark: The asymptotic result in Theorem. 2 is clear from Theorem.6 by letting T increases to infinity, i.e., lim sup TÑ8 RegEOCP-UG µ pTq log T ď lim sup TÑ8 ÿ a:∆aą0 ˆ 2 ∆a ` p8 ` ? 20πq ∆a ?log T ˙ ` Oplog´1pTqq “ ÿ a:∆aą0 2 ∆a . Proof. Without loss of generality, let action 1 be the unique optimal action. The first step for proving the regret performance is regret decomposition lemma (Lattimore and Szepesvári, 2020, Lemma 4.5). We also decompose the regret of Algorithm. 2 into the number of pulls for each sub-optimal arm as follows: RegµpTq “ ÿ a:∆aą0 ∆aErNT paqs. 22Then, for a specific sub-optimal arm a, we bound the number of pulls. Since our EOCP-UG algorithm has a clear separation of exploration and exploitation phases, we can bound the pulls in the two phases respectively. However, the unique characteristic of unknown gap scenario is we don’t have a fixed end time of exploration phase. Recall that Tc ď T is the stopping time that the exploration phase ends and ˆa is the arm we choose for commitment, so we can decompose the number of pulls into two phases as: ErNapTqs “ ErNapTcqs ` ErpT ´ Tcq1ˆa“as ď ErNapTcqs loooomoooon I1 `T Ppˆa “ aq looomooon I2 . Notice that upper bound of I2 gives the confidence level result in Corollary. 2. We then bound the two terms I1 and I2 separately. In order to simplify the proof, we first prove a lemma which characterizes a high probability upper bound for the stopping time Tc. This lemma also proves the sample complexity to commitment result in Corollary. 2. The proof of Lemma. 4 will be delayed. Lemma 4 Under Gaussian bandits with time horizon T, our stopping time Tc for exploration of Algo- rithm. 2 is upper bounded with high probability: P ˜ Tc ě ÿ a:∆aą0 8pl ` 1q2 ∆2a ` Apl ` 2q ¸ ď 10eA T log2 T . Bounding I1: With the help of Lemma. 4, we have a high probability upper bound for the exploration phase. We let T u c “ ř a:∆aą0 8pl`1q2 ∆2a ` Apl ` 2q to be the high probability upper bound of our stopping time to end the exploration phase. Notice that NapTcq ď T, we have: I1 “ErNapTcq1TcěT u c s ` ErNapTcq1TcďT u c s ď TP pTc ě T u c q ` ErNapTcq1TcďT u c s ď ErNapTcq1TcďT u c s looooooooomooooooooon I3 ` 10eA log2 T . Then, bounding I3 is similar to bounding I1 for Theorem. 5. We decompose I3 as follows: I3 “E « 1TcďT u c Tc ÿ t“1 1At“a ff “1 ` E « 1TcďT u c Tc ÿ t“A`1 1At“a1UCBt´1paqěµ1 ff loooooooooooooooooooooooomoooooooooooooooooooooooon A1 ` E « 1TcďT u c Tc ÿ t“A`1 1At“a1UCBt´1paqăµ1 ff loooooooooooooooooooooooomoooooooooooooooooooooooon A2 , In order to bound A1 and A2, we assume there is a virtual process that after the end time Tc of exploration phase, it continues to select the arm with largest UCB and receive the corresponding reward until time T u c . This is only a virtual process used in our proof, while in reality our algorithm will stop exploration after stopping time Tc. We will use E1 and P1 to denote the expectation and probability over this virtual process. Then, we can bound A2 as follows: A2 ďE « 1TcďT u c Tc ÿ t“A`1 1UCBt´1p1qăµ1 ff ď E1 « T u c ÿ t“A`1 1UCBt´1p1qăµ1 ff “ T u c ÿ t“A`1 P1 pUCBt´1p1q ă µ1q . where the first inequality is because At “ a indicates UCBt´1p1q ă UCBt´1paq due to the dynamic of UCB exploration. The second inequality is because E and E1 are totally the same for the first Tc time 23steps. This step also allows us to bound A2 over the events on a different probability measure E1 and P1. Event tUCBt´1p1q ă µ1u means that at time step t, the mean estimation ¯rt´1p1q of the optimal arm from previous pulls is lower than µ1 ´ bt´1p1q, which incurs a large deviation, so we can bound this event with a union over all time steps: tUCBt´1p1q ă µ1u Ă ␣ Dt1 P rA, T u c q, UCBt1p1q ă µ1 ( , which doesn’t depend on time step t any more, so we have: T u c ÿ t“A`1 P1 pUCBt´1p1q ă µ1q ď T u c ÿ t“A`1 P1 ` Dt1 P rA, T u c q, UCBt1p1q ă µ1 ˘ “pT u c ´ AqP1 ` Dt1 P rA, T u c q, UCBt1p1q ă µ1 ˘ “pT u c ´ AqP1 ˜ Dt1 P rA, T u c q, ¯rt1p1q ` d 2l Nt1p1q ă µ1 ¸ . If there is no pulls for arm 1 in a time interval, the empirical mean and number of pulls will remain the same. So instead of counting on the time steps, we can count the number of pulls for arm 1: # Dt1 P rA, T u c q, ¯rt1p1q ` d 2l Nt1p1q ă µ1 + Ă # Ds P “ 1, NT u c paq ˘ , ¯rs,1 ` c 2l s ă µ1 + . Notice that NT u c paq ď T u c ´ A ` 1, so we can bound the probability as follows: P1 ˜ Dt1 P rA, T u c q, ¯rt1p1q ` d 2l Nt1p1q ă µ1 ¸ ďP1 ˜ Ds P r1, T u c ´ A ` 1q, ¯rs,1 ´ µ1 ` c 2l s ă 0 ¸ ďP1 ˜ Ds P r1, T u c ´ A ` 1q, řs i“1pW1,i ´ µ1q i ` c 2l s ă 0 ¸ ďT u c ´ A expplq , where the last inequality uses Eq. (4a) of Lemma 3 and the fact that W1,i ´ µ1 is 1-sub-Gaussian with zero mean. Notice that expplq ě T log4 T, so putting the above bound back to the bound of A2, we have: A2 ď ´ř a:∆aą0 8l2 ∆2a ` Al ´ A ¯2 T log4 T ď ´ř a:∆aą0 200 log2 T ∆2a ` 5A log T ¯2 T log4 T “ OpT ´1q. where the first inequality is due to the fact that when T ě 3, ?log T ď log T. Next, we attempt to bound A1 as follows: A1 “ E « 1TcďT u c T u c ÿ t“A`1 1At“a1UCBt´1paqěµ1 ff ďE1 « T u c ÿ t“A`1 1At“a1¯rt´1paq` b 2l Nt´1paq ěµ1 ff , where the inequality is also due to the fact that the virtual and real processes are identical before time Tc. Notice that the number of pulls Ntpaq will only increase by 1 every time there is new pull, i.e., when 24At “ a. Otherwise, the term inside summation is 0. So instead of counting on the time step t, we can count over the number of pulls over arm a as follows: E1 « T u c ÿ t“A`1 1At“a1¯rt´1paq` b 2l Nt´1paq ěµ1 ff “E1 » – NT u c paq ÿ s“1 1¯rs,a` b 2l s ěµ1 fi fl ď T u c ÿ s“1 P1 ˜ ¯rs,a ´ µa ` c 2l s ě ∆a ¸ , where the last inequality is due to NT u c paq ď T u c . Then, by Eq. (4c) of Lemma. 3, we have: T u c ÿ s“1 P1 ˜ ¯rs,a ´ µa ` c 2l s ě ∆a ¸ “ T u c ÿ s“1 P1 ˜řs i“1pWa,i ´ µaq i ` c 2l s ě ∆a ¸ ď 2 ` log T ` 4?2 log T ˘ ` b 4π ` log T ` 4?2 log T ˘ ` 2 ∆2a ` 1 ď2 log T ∆2a ` p8 ` ? 20πq?log T ∆2a ` 2 ∆2a ` 1, where the first inequality is due to the fact that Wa,i ´ µa is 1-sub-Gaussian with zero mean, and the last inequality is due to ?2 log T ď log T when T ě 9. Therefore, combining the bounds on A1 and A2, we can bound I3 as follows: I3 ď 2 log T ∆2a ` p8 ` ? 20πq?log T ∆2a ` 2 ∆2a ` 1 ` OpT ´1q. Therefore, a similar bound can be established on I1 as follows: I1 ď 2 log T ∆2a ` p8 ` ? 20πq?log T ∆2a ` 2 ∆2a ` 1 ` 10eA log2 T ` OpT ´1q. Bounding I2: Recall that our stopping criterion is when there exists an arm ˜a whose number of pulls is significantly larger than other arms, i.e., NTcp˜aq ě l maxa‰˜a NTcpaq. Therefore, its bonus bTcp˜aq should be very small compared to other arms. Also recall that we select the arm ˆa with highest LCB to commit to, so the proof follows two steps. First we will show that with high probability the arm ˜a with most number of pulls is the best arm. Then we will show that under this circumstance, the maximum LCB arm is also the best arm. Therefore, consider an arbitrary sub-optimal arm a: P p˜a “ aq ď P pNTcpaq ě rlNTcp1qs ` 1q . Notice that if NTcpaq ě rlNTcp1qs ` 1 at the end time Tc, we must have pulled arm a at time step Tc. This also means that the number of pulls NTc´1paq for arm a after time step Tc ´ 1 is exactly rlNTcp1qs and and NTc´1p1q “ NTcp1q. It also means that arm a has the largest UCB. So we have: P pNTcpaq ě rlNTcp1qs ` 1q ďP pNTc´1paq “ rlNTc´1p1qs , UCBTc´1paq ě UCBTc´1p1qq ďP ˜ Dt ď Tc, Ntpaq “ rlNtp1qs , ¯rtpaq ` d 2l Ntpaq ě ¯rtp1q ` d 2l Ntp1q ¸ 25Notice that we can separate the two random quantities with union bound as follows: P pNTcpaq ě rlNTcp1qs ` 1q ď P ˜ Dt ď Tc, ¯rtpaq ě µa ` d 2l Ntpaq ¸ looooooooooooooooooooooomooooooooooooooooooooooon B1 ` P ˜ Dt ď Tc, Ntpaq “ rlNtp1qs , µa ` 2 d 2l Ntpaq ě ¯rtp1q ` d 2l Ntp1q ¸ loooooooooooooooooooooooooooooooooooooooooooomoooooooooooooooooooooooooooooooooooooooooooon B2 We first bound B1. Notice that we can switch from counting of time step t to count the number of pulls for arm a. It is clear that Ntpaq ď Tc ď T when t ď Tc, so we have: B1 ďP ˜ Ds ď T, ¯rs,a ě µa ` c 2l s ¸ “ P ˜ Ds ď T, řs i“1pµa ´ Wa,iq s ` c 2l s ď 0 ¸ ďel log T ` e exp l , where the last inequality uses Eq. (4a) of Lemma. 3. Also notice that expplq ě T log4 T, so we have: B1 ď 10e log2 T ` e T log4 T “ o ˆ 1 T ˙ . Next, we bound the term B2, we can rearrange the terms inside B2 as follows: B2 “ P ˜ Dt ď Tc, Ntpaq “ rlNtp1qs , ¯rtp1q ´ µ1 ` ˆ 1 ´ 2 ? l ˙ d 2l Ntp1q ` ∆a ď 0 ¸ . Denote α “ 1 ´ 2 ? l, and switch the count from time step t to the number of pulls for arm 1, we can bound B2 as follows: B2 ďP ˜ Ds ď T, ¯rs,1 ´ µ1 ` c 2α2l s ` ∆a ď 0 ¸ “ P ˜ Ds ď T, řs i“1pW1,i ´ µ1q s ` c 2α2l s ` ∆a ď 0 ¸ ď 4 ∆2a exp pα2lq. When T is large enough, we have α2l “ l ´ 4 ? l ` 4 ě log T ` 4, so we have: B2 ď 4 ∆2aT . Therefore, combining B1 and B2 together, we can bound the probability as follows: P p˜a “ aq ď P pNTcpaq ě rlNTcp1qs ` 1q ď B1 ` B2 ď O ˆ 1 T ˙ . Recall that arm 1 is the optimal arm. Therefore, by a union bound, we can characterize the probability that of event t˜a ‰ 1u as follows: P p˜a ‰ 1q ď ÿ a:∆aą0 P p˜a “ aq ď O ˆ 1 T ˙ . 26Next, we investigate the probability of choosing the wrong arm ˆa to commit to. Recall that for the arm which we commit to, we choose the one with the largest LCB. Consider any sub-optimal arm a, if we wrongly choose the arm ˆa “ a, it means its LCB should be larger than the LCB of the optimal arm, which with high probability has the largest number of pulls. So, we have: P pˆa “ aq ď P pˆa “ a, ˜a “ 1q ` P p˜a ‰ 1q ď P pLCBTcpaq ě LCBTcp1q, ˜a “ 1q ` O ˆ 1 T ˙ . When ˜a “ 1, arm 1 has l times more pulls than arm a when the exploration phase stops, so we have: P pLCBTcpaq ě LCBTcp1q, ˜a “ 1q ďP ˜ ¯rTcpaq ´ d 2l NTcpaq ě ¯rTcp1q ´ d 2l NTcp1q, NTcp1q ě lNTcpaq ¸ . Similarly, we separate the two random variables as follows: P pLCBTcpaq ě LCBTcp1q, ˜a “ 1q ďP ˜ ¯rTcpaq ´ d 2l NTcpaq ě µ1 ´ 2 d 2l NTcp1q, NTcp1q ě lNTcpaq ¸ ` P ˜ ¯rTcp1q ď µ1 ´ d 2l NTcp1q ¸ ď P ˜ pµa ´ ¯rTcpaqq ` ˆ 1 ´ 2 ? l ˙ d 2l NTcpaq ` ∆a ď 0 ¸ looooooooooooooooooooooooooooooooooomooooooooooooooooooooooooooooooooooon B3 ` P ˜ ¯rTcp1q ď µ1 ´ d 2l NTcp1q ¸ loooooooooooooooooomoooooooooooooooooon B4 . For B4, notice that Tc is a random stopping time, so we bound the probability over all time step when the number of pulls is larger than l as follows: B4 ď P ˜ Ds P rl, Ts, ¯rs,1 ´ µ1 ` c 2l s ď 0 ¸ “ P ˜ Ds P rl, Ts, řs i“1pW1,i ´ µ1q s ` c 2l s ď 0 ¸ . Since W1,i ´ µ1 is 1-subgaussian, we can use Eq. (4a) from the concentration Lemma. 3 to bound B4 as follows: B4 ď P ˜ Ds P rl, Ts, řs i“1pW1,i ´ µ1q s ` c 2l s ď 0 ¸ ď el log T ` e exp l “ o ˆ 1 T ˙ , where the last inequality is due to ?2 log T ď log T when T ě 9 and expplq ě T log4 T. On the other hand, bounding B3 is similar to the proof of probability upper bound regarding ˜a. recall that α “ ´ 1 ´ 2 ? l ¯ , and notice that Tc is a random variable, so we use a union over all possible arm pulls to bound the event on time step Tc as follows: B3 ďP ˜ Ds ď T, pµa ´ ¯rs,aq ` c 2α2l s ` ∆a ď 0 ¸ ďP ˜ Ds ď T, řs i“1pµa ´ Wa,iq s ` c 2α2l s ` ∆a ď 0 ¸ ď 4 ∆2a exp pα2lq, 27where the last inequality uses the second concentration inequality Eq. (4b) from Lemma. 3 and the fact that µa ´ Wa,i is 1-subgaussian. Then when T is large enough, we have: B3 ď 4 ∆2aT . Combining B3 and B4, we can bound the probability that we select the wrong arm as: P pLCBTcpaq ě LCBTcp1q, ˜a “ 1q ď B3 ` B4 ď 4 ∆2aT ` o ˆ 1 T ˙ ď O ˆ 1 T ˙ . Therefore, we finally bound I2 as follows: I2 “ Ppˆa “ aq ď P pˆa “ a, ˜a “ 1q ` P p˜a ‰ 1q “ O ˆ 1 T ˙ . Here, the bound on I2 shows that the confidence level of our algorithm is OpT ´1q indicated in Corollary. 2. Combining I1 and I2, we can bound ErNapTqs in finite time as follows: ErNapTqs ď I1 ` TI2 ď 2 log T ∆2a ` p8 ` ? 20πq?log T ∆2a ` Op1q. So by the regret decomposition lemma, we can bound the regret performance as: RegEOCP-UG µ pTq ď ÿ a:∆aą0 ˆ2 log T ∆a ` p8 ` ? 20πq?log T ∆a ˙ ` Op1q. B.4 Proof of Lemma. 4 and Corollary. 2 Consider a specific sub-optimal arm a. We first show that it can only be pulled Oplog Tq with high probability during the exploration phase due to the dynamic of our exploration strategy, i.e., the UCB exploration strategy. To be specific, we intend to show the following inequality: P ˆ Dt ď Tc, Ntpaq ě 8l ∆2a ` 1 ˙ ď 10e T log2 T . Since during the exploration phase, we select the arm with the largest UCB to explore, if there exists a time t which the number of pulls for action a is larger than 8l ∆2a ` 1, which means that there exists a time t1 P rA, tq, where at time t1 ` 1 the number of pulls for previous rounds Nt1paq is exactly 8l ∆2a and arm a has the largest UCB. Therefore, we can bound the probability as: P ˆ Dt ď Tc, Ntpaq ě 8l ∆2a ` 1 ˙ ď P ˆ Dt1 ď t ď T, Nt1paq “ 8l ∆2a , a “ arg max a1 UCBt1pa1q ˙ . Therefore, the UCB of arm a should be larger than the UCB of the optimal arm 1, i.e., UCBt1paq ě UCBt1p1q. So we can further derive an upper bound as follows: P ˆ Dt1 ď t ď T, Nt1paq “ R 8l ∆2a V , a “ arg max a1 UCBt1pa1q ˙ ďP ˆ Dt1 ď T, Nt1paq “ R 8l ∆2a V , UCBt1paq ě UCBt1p1q ˙ “P ˜ Dt1 ď T, Nt1paq “ R 8l ∆2a V , ¯rt1paq ` d 2l Nt1paq ě ¯rt1p1q ` d 2l Nt1p1q ¸ ďP ˜ Dt1 ď T, ¯rR 8l ∆2a V ,a ` ∆a 2 ě ¯rt1p1q ` d 2l Nt1p1q ¸ . 28Then, we can separate the two empirical means ¯rR 8l ∆2a V ,a and ¯rt1p1q with a union bound as follows: P ˜ Dt1 ď T, ¯rR 8l ∆2a V ,a ` ∆a 2 ě ¯rt1p1q ` d 2l Nt1p1q ¸ ď P ˜ Dt1 ď T, ¯rt1p1q ` d 2l Nt1p1q ď µa ` ∆a ¸ looooooooooooooooooooooooooomooooooooooooooooooooooooooon A1 ` P ˜ ¯rR 8l ∆2a V ,a ě µa ` ∆a 2 ¸ looooooooooooooomooooooooooooooon A2 . The term A2 can be easily bounded through Hoeffding’s inequality as: A2 “ P ¨ ˚ ˚ ˝ ř R 8l ∆2a V i“1 pWa,i ´ µaq Q 8l ∆2a U ě ∆a 2 ˛ ‹‹‚ď exp ˆ ´ R 8l ∆2a V ∆2 a 8 ˙ ď 1 expplq, where the first inequlity is due to the fact that Wa,i ´ µa is 1-subgaussian. For term A1, we notice that the empirical estimation ¯rt1p1q and the counter Nt1p1q will remain the same if there is no pull for arm 1, so we can switch the count of time step t1 to the count of number of pulls for N1 tp1q. To be specific, A1 ď P ˜ Ds ď T, ¯rs,1 ` c 2l s ď µa ` ∆a ¸ “ P ˜ Ds ď T, řs i“1pW1,i ´ µ1q s ` c 2l s ď 0 ¸ . Since W1,i ´ µ1 is 1-subgaussian, we can use the concentration result Eq. (4a) of Lemma. 3 to bound A1 as follows: P ˜ Ds ď T, řs i“1pW1,i ´ µ1q s ` c 2l s ď 0 ¸ ď el log T ` e expplq . Therefore, we have can bound the probability of overpull for sub-optimal arm a as follows: P ˆ Dt ď Tc, Ntpaq ě 8l ∆2a ` 1 ˙ ď A1 ` A2 ď eplog T ` 4?2 log Tq log T ` 2e expplq ď 10e T log2 T , where the third inequality is due to log T ě ?2 log T when T ě 9 and the last inequality is due to expplq ě T log4 T. Recall that in Algorithm. 2, the exploration phase will stop if there exists an arm ˜a whose pulls NTcp˜aq at the stopping time have exceeds l times of all other arms, i.e., l max a‰˜a NTcpaq ` 2 ě NTcp˜aq ą l max a‰˜a NTcpaq ` 1. So if Tc is larger than ř a:∆aą0 8pl`1q2 ∆2a ` Apl ` 2q, it means that there exists at least one sub-optimal arm a whose pulls NTcpaq is larger than 8l ∆2a ` 1. So we have: P ˜ Tc ě ÿ a:∆aą0 8pl ` 1q2 ∆2a ` Apl ` 2q ¸ ďP ˆ Da : ∆a ą 0, NTcpaq ě 8l ∆2a ` 1 ˙ ď ÿ a:∆aą0 P ˆ Dt ď Tc, Ntpaq ě 8l ∆2a ` 1 ˙ ď 10eA T log2 T , 29where the second inequality, we use union bound over a. Then Corollary. 2 can be easily proved with: SCCEOCP-UG µ pTq “ErTcs “E « Tc1Tcěř a:∆aą0 8pl`1q2 ∆2a `Apl`2q ff ` E « Tc1Tcăř a:∆aą0 8pl`1q2 ∆2a `Apl`2q ff ďTP ˜ Tc ě ÿ a:∆aą0 8pl ` 1q2 ∆2a ` Apl ` 2q ¸ ` ÿ a:∆aą0 8pl ` 1q2 ∆2a ` Apl ` 2q ď ÿ a:∆aą0 8 log2 T ` 80 log 3 2 T ` 200 log T ∆2a ` 6A log T ` 10eA log2 T . Therefore, taking T to infinity, we have: lim sup TÑ8 SCCEOCP-UG µ pTq log2 T “ ÿ a:∆aą0 8 ∆2a . B.5 Proof of Theorem. 3 We first prove the fundamental limits of sample complexity until commitment in the pre-determined stopping time setting. Suppose the bandit problem (instance) is as follows: the reward expectation of action 1 is µ1 and the reward expectation of action 2 is µ2. We assume µ1 ą µ2 and ∆ “ µ1 ´ µ2. Consider another bandit instance with reward expectations λ1 and λ2 for the two actions respectively, and λ1 ` ∆ ď λ2. By the “transportation” lemma (Kaufmann et al., 2016, Lemma. 1) when T ě 10, we have for any stopping time τ that: EµrN1pτqsKLpµ1, λ1q ` EµrN2pτqsKLpµ2, λ2q ě log ˆ T 2.4 ˙ . Since under Gaussian bandit, the KL divergence is simply the squared norm, we have: EµrN1pτqspµ1 ´ λ1q2 2 ` EµrN2pτqspµ2 ´ λ2q2 2 ě log ˆ T 2.4 ˙ . Since the inequality holds for any λ1 and λ2 such that that λ1 ` ∆ ď λ2, we can minimize the LHS to obtain tighter bounds. So minimizing the LHS over λ1 and λ2 gives: λ1 “ µ1EµrN1pτqs ` pµ2 ´ ∆qEµrN2pτqs EµrN1pτqs ` EµrN2pτqs , λ2 “ pµ1 ` ∆qEµrN1pτqs ` µ2EµrN2pτqs EµrN1pτqs ` EµrN2pτqs . So plug the optimization result into the “transportation” lemma, we have: 2∆2EµrN2pτqs2 pEµrN1pτqs ` EµrN2pτqsq2 ` 2∆2EµrN1pτqs2 pEµrN1pτqs ` EµrN2pτqsq2 ě log ˆ T 2.4 ˙ . Therefore, rearranging the terms in the inequality, we can derive a lower bound on EµrN1pτqs as follows: EµrN1pτqs ě 2 log ` T 2.4 ˘ EµrN2pτqs 2∆2EµrN2pτqs ´ 2 log ` T 2.4 ˘. 30Let τ “ Tc be the fixed length of exploration. Thus, we can derive the following lower bound on the expectation of stopping time as follows: EµrTcs “ EµrN1pTcqs ` EµrN2pTcqs ě 2∆2 pEµrN2pTcqsq2 2∆2EµrN2pTcqs ´ 2 log ` T 2.4 ˘. According to our assumption, the algorithm has c-logarithm regret violation with OpT ´1q confidence, which means the the number of pulls for the sub-optimal action is upper and lower bounded when T is large enough. So we have: ˇˇˇˇEµ rN2pTqs ´ 2 logpTq ∆2 ˇˇˇˇ “ Oplogc Tq. Recall that ˆa is the action the algorithm chooses to commit to and action 1 is the optimal action in instance µ, so we have E rN2pTqs ď Eµ rN2pTcqs ` TPµ pˆa ‰ 1q. Since the algorithm has OpT ´1q confidence, so we have Pµ pˆa ‰ 1q “ OpT ´1q and Eµ rN2pTqs ď Eµ rN2pTcqs ` Op1q. So we can lower bound the numerator as follows: 2∆2 pEµrN2pTcqsq2 ě2∆2 pEµrN2pTqs ´ Op1qq2 ě2∆2 ˆ 4 ∆4 log2 T ´ Oplog1`c Tq ˙ “ 8 ∆2 log2 T ´ Oplog1`c Tq. Notice that EµrN2pTcqs ď EµrN2pTqs.For the denominator, we can derive an upper bound similarly as follows: 2∆2EµrN2pTcqs ´ 2 log ˆ T 2.4 ˙ ď2∆2EµrN2pTqs ´ 2 log ˆ T 2.4 ˙ ď2∆2 2 ∆2 log T ` Oplogc Tq ´ 2 log ˆ T 2.4 ˙ ď2 log T ` Oplogc Tq, where the second inequality uses the fact that the algorithm has c-logarithm regret violation. So combining both bounds in the numerator and the denominator, we have: EµrTcs ě 8 ∆2 log2 T ´ Oplog1`c Tq 2 log T ` Oplogc Tq “ Ω ˆlog T ∆2 ˙ , which concludes the proof for the pre-determined stopping time setting. In the adaptive stopping time setting, we can use the same procedure to prove the lower bound of sample complexity until commitment, so we also create another bandit instance with reward expectations λ1 and λ2. Unlike in the pre-determined stopping time setting, now λ1 and λ2 satisfies λ1 ď λ2. We also use the “transportation” lemma and optimize over λ1 and λ2 to get a lower bound for ErTcs as follows: EµrTcs “ EµrN1pTcqs ` EµrN2pTcqs ě ∆2 pEµrN2pTcqsq2 ∆2EµrN2pTcqs ´ 2 log ` T 2.4 ˘. Similar to the proof of pre-determined stopping time setting, we utilize the fact that Eµ rN2pTcqs ď Eµ rN2pTqs ď Eµ rN2pTcqs ` Op1q and the algorithm has c-logarithm regret violation to bound the 31numerator and denominators separately. For the numerator, we have: ∆2 pEµrN2pTcqsq2 ě∆2 pEµrN2pTqs ´ Op1qq2 ě∆2 ˆ 4 ∆4 log2 T ´ Oplog1`c Tq ˙ “ 4 ∆2 log2 T ´ Oplog1`c Tq. For the denominator, we use the fact that the algorithm has c-logarithm regret violation: ∆2EµrN2pTcqs ´ 2 log ˆ T 2.4 ˙ ď∆2 2 ∆2 log T ` Oplogc Tq ´ 2 log ˆ T 2.4 ˙ ďOplogc Tq. So combining both bounds in the numerator and the denominator, we have: EµrTcs ě 4 ∆2 log2 T ´ Oplog1`c Tq Oplogc Tq “ Ω ˆlog2´c T ∆2 ˙ . C Proofs of Main Results for KL-EOCP Now we are ready to prove the regret and sample complexity until commitment results for our KL-EOCP Algorithms, i.e., Algorithm. 3. Without loss of generality, suppose action 1 is the unique optimal action. Recall that for any time index t, UCBtpaq and LCBtpaq denotes be KL upper and lower confidence bound of action a as: UCBtpaq “ arg max µě¯rtpaq tNtpaqKLp¯rtpaq, µq ď lu , LCBtpaq “ arg min µď¯rtpaq tNtpaqKLp¯rtpaq, µq ď lu . Throughout the proof section, we let Wa,i be the i-th sample from pulling arm a the i-th time. Recall that Tc is the length of exploration phase. Let ¯ra,s be the empirical mean of arm a after it has been pulled s times. Moreover, we let: UCBs,a “ arg max µě¯rs,a tsKLp¯rs,a, µq ď lu , LCBs,a “ arg min µď¯rs,a tsKLp¯rs,a, µq ď lu . In order to incorporate this new definition of UCB and LCB, we need new sets of concentration inequalities, which will be summarized in the next subsection. The proof will be delayed to Sec. D. C.1 Concentration Inequalities for Natural Exponential Families Since we no longer assume the distributions rν1, ¨ ¨ ¨ , νAs are subgaussian any more, the concentration inequalities from Sec. B.1, especially Lemma. 3, no longer holds. Therefore, we need a set of new concentrations specialized in the natural exponential family regime. To be specific, we want to bound the probability that the KL divergence of empirical estimations and the true expectation is very large. First, we provide a concentration lemma which is analogous to the Hoeffding’s inequality widely used in the subgaussian scenario as follows: Lemma 5 Let W1, W2, ¨ ¨ ¨ , WT be identically and independently distributed random variables with com- mon expectation µ “ ErW1s and sampled from a distribution ν that belongs to a canonical exponential 32family P. For any s ď T, Let Ss “ řs i“1 Wi and ¯µs “ Ss s denote the sum and the empirical mean of the first s samples. For any δ ą 0, we have: P p¯µs ď µ, sKL p¯µs, µq ě δq ď expp´δq, (5a) P p¯µs ě µ, sKL p¯µs, µq ě δq ď expp´δq. (5b) Based on Lemma. 5 we propose the following lemmas characterizing the any-time concentration property of random variables. Lemma 6 Let W1, W2, ¨ ¨ ¨ , WT be identically and independently distributed random variables with com- mon expectation µ “ ErW1s and sampled from a distribution ν that belongs to a canonical exponential family P. For any s ď T, Let Ss “ řs i“1 Wi and ¯µs “ Ss s denote the sum and the empirical mean of the first s samples. Let T1 ď T2 ď T be two real numbers in R`. For any l ą 2, the following holds: P pDs P rT1, T2s, ¯µs ď µ, sKL p¯µs, µq ě lq ď min "T2 ´ T1 ` 1 expplq , el plog T2 ´ log T1q ` e expplq * , (6a) P pDs P rT1, T2s, ¯µs ě µ, sKL p¯µs, µq ě lq ď min "T2 ´ T1 ` 1 expplq , el plog T2 ´ log T1q ` e expplq * . (6b) Lemma 7 Let W1, W2, ¨ ¨ ¨ , WT be identically and independently distributed random variables with com- mon expectation µ “ ErW1s and sampled from a distribution ν that belongs to a canonical exponential family P. For any s ď T, Let Ss “ řs i“1 Wi and ¯µs “ Ss s denote the sum and the empirical mean of the first s samples. Let UCBs “ arg maxµě¯rs tsKLp¯µs, µq ď lu be the upper confidence bound for empirical mean. For any l ą 2, any T1 ď T, any µ1 ą µ, and any ε ą 0, the following holds: T1 ÿ s“1 P ` UCBs ě µ1˘ ď p1 ` εql KLpµ, µ1q ` β2pεq T β1pεq , (7) where β1pεq “ Opε2q and β2pεq “ Opε´2q are constants. C.2 Proof of Regret Optimality for KL-EOCP In this section, we provide a complete proof of Theorem. 4. We prove the following Theorem which characterizes the finite-time performance of Algorithm. 3, and Theorem. 4 can be derived from Theorem. 4 by letting T approaches infinity. Theorem 7 Under pre-determined stopping time setting for general bandits with time horizon T with T ě 16, the Regret of KL-EOCP algorithm in Algorithm. 3 can be upper bounded as: RegKL-EOCP µ pTq ď ÿ a:∆aą0 ˜ ∆a log T KLpµa, µ1q ` 10∆a log 3 4 T KLpµa, µ1q ¸ ` op1q. Remark: The asymptotic result in Theorem. 4 is clear from Theorem.7 by letting T increases to infinity, i.e., lim sup TÑ8 RegKL-EOCP µ pTq log T ď ÿ a:∆aą0 ∆a KLpµa, µ1q. 33Proof. From the regret decomposition lemma (Lattimore and Szepesvári, 2020, Lemma 4.5), we can decompose the regret of Algorithm. 3 to the number of pulls for each sub-optimal arm as follows: RegµpTq “ ÿ a:∆aą0 ∆aErNT paqs. Then, the key to bound the total regret is to bound the number of pulls for each sub-optimal arms. Since our KL-EOCP algorithm has a clear separation of exploration and exploitation phases, so for any sub-optimal action a, we can bound its pulls in different phases as follows: ErNapTqs “ E rN2pTcqs looooomooooon I1 `pT ´ Tcq Ppˆa “ aq looomooon I2 , where recall that Tc is the end of exploration phase and ˆa is the arm that the algorithm commits to during exploitation phase. Bounding I1: We first bound I1, which is similar to the proof of bounding the regret for the traditional KL-UCB algorithm (Garivier and Cappé, 2011). We have: I1 “ E « Tc ÿ t“1 1At“a ff “ 1 ` E « Tc ÿ t“A`1 1At“a1UCBt´1paqěµ1 ff looooooooooooooooooomooooooooooooooooooon A1 ` E « Tc ÿ t“A`1 1At“a1UCBt´1paqăµ1 ff looooooooooooooooooomooooooooooooooooooon A2 , Notice that At “ a indicates UCBt´1p1q ă UCBt´1paq due to the dynamic of UCB exploration, we can bound the last term as: A2 ď Tc ÿ t“A`1 E “ 1UCBt´1p1qăµ1 ‰ “ Tc ÿ t“A`1 P pUCBt´1p1q ă µ1q Event tUCBt´1p1q ă µ1u means that at time step t, the mean estimation ¯rt´1p1q of the optimal arm from previous pulls is lower than µ1 ´ bt´1p1q, which incurs a large deviation, so we can bound this event with a union over all time steps: tUCBt´1p1q ă µ1u Ă ␣ Dt1 P rA, Tcq, UCBt1p1q ă µ1 ( , which doesn’t depend on time step t any more, so we have: Tc ÿ t“A`1 P pUCBt´1p1q ă µ1q ď Tc ÿ t“A`1 P ` Dt1 P rA, Tcq, UCBt1p1q ă µ1 ˘ “pTc ´ AqP ` Dt1 P rA, Tcq, UCBt1p1q ă µ1 ˘ For two time steps Tc ă t2, if there is no pulls for arm 1 between them, then the term UCBt1p1q will remain the same through t1 P rTc, t2s. So instead of counting on the time steps, we can count the number of pulls for arm 1 instead. We have: ␣ Dt1 P rA, Tcq, UCBt1p1q ă µ1 ( Ă tDs P r1, NTcpaqq , UCBs,1 ă µ1u . Notice that NTcpaq ď Tc ´ A ` 1, so we can bound the probability as follows: P ` Dt1 P rA, Tcq, UCBt1p1q ă µ1 ˘ ďP pDs P r1, Tc ´ A ` 1q, UCBs,1 ă µ1q “P ˆ Ds P r1, Tc ´ A ` 1q, max µě¯rs,1 " KLp¯rs,1, µq ď l s * ă µ1 ˙ . 34Notice that for any s and under event tUCBs,1 ă µ1u, we have µ1 ě UCBs,1 ě ¯rs,1. By definition of UCBs,1, we also have KL p¯rs,1, UCBs,1q “ l s. Therefore, we come to the conclusion that KL p¯rs,1, µq ě l s. using Eq. (6a) of Lemma 6, we have: P ˆ Ds P r1, Tc ´ A ` 1q, max µě¯rs,1 " KLp¯rs,1, µq ď l s * ă µ1 ˙ ďP pDs P r1, Tc ´ A ` 1q, sKL p¯rs,1, µq ě lq ďTc ´ A expplq . Using the fact that expplq ě T log4 T when T ě 16 and putting the above bound back to the bound of A2, we have: A2 ď pTc ´ Aq2 T log4 T “ ˆ 8Aplog T`4?2 log Tq KL2 min ˙2 T log2 T “ o ˆ 1 T ˙ , where the last inequality is due to the fact that when T ě 3, ?log T ď log T. Next, we attempt to bound the middle term A1 as follows: A1 “ E « Tc ÿ t“A`1 1At“a1UCBt´1paqěµ1 ff “E « Tc ÿ t“A`1 1At“a1UCBNt´1paq,aěµ1 ff Notice that the number of pulls Nt´1paq will only increase by 1 every time there is new pull, i.e., when At “ a. Otherwise, the term inside summation is 0. So instead of counting on the time step t, we can count over the number of pulls over arm a as follows: E « Tc ÿ t“A`1 1At“a1UCBNt´1paq,aěµ1 ff “ E » – NTcpaq ÿ s“1 1UCBs,aěµ1 fi fl ď Tc ÿ s“1 P pUCBs,a ě µ1q , where the last inequality is due to NTcpaq ď Tc. Using Lemma. 7, we can bound the sum of probabilities. Specifically, for any ε ą 0, there exists two constants β1pεq “ Opε2q and β2pεq “ Opε´2q such that: Tc ÿ s“1 P pUCBs,a ě µ1q ď p1 ` εql KLpµa, µ1q ` β2pεq T β1pεq . Let ϵ “ log´ 1 4 T, and when T ě 16, we have log T ě ?log T and 4?2 log T ď 5 log 3 4 T, so we have: Tc ÿ s“1 P pUCBs,a ě µ1q ď log T KLpµa, µ1q ` 10 log 3 4 T KLpµa, µ1q ` op1q. Therefore, combining the bounds on A1 and A2, we can bound I1 as follows: I1 ď log T KLpµa, µ1q ` 10 log 3 4 T KLpµa, µ1q ` op1q. Bounding I2: After the exploration phase, recall that we select the arm with largest LCB to commit to. The idea of proof is very similar to the regret bound in (Yang et al., 2022a) and our proof of 35Theorem. 5. We can first bound the probability of selecting a sub-optimal arm a as: P pˆa “ aq ďP pLCBTcpaq ě LCBTcp1qq “P ˆ LCBTcpaq ě LCBTcp1q, NTcp1q ą 4l KLmin ˙ ` P ˆ LCBTcpaq ě LCBTcp1q, NTcp1q ď 4l KLmin ˙ ď P ˆ LCBTcpaq ě LCBTcp1q, NTcp1q ą 4l KLmin ˙ looooooooooooooooooooooooooooomooooooooooooooooooooooooooooon B1 ` P ˆ NTcp1q ď 4l KLmin ˙ looooooooooooomooooooooooooon B2 . We first bound the term B2 which states that during the first exploration phase with UCB exploration, the optimal arm is under-pulled, which means that one of the sub-optimal arms have been pulled with larger number of times. Therefore, we have: P ˆ NTcp1q ď 4l KLmin ˙ “ P ˜ ÿ a:∆aą0 NTcpaq ą Tc ´ 4l KLmin ¸ . Recall that Tc “ 4Al KLmin ` A, so we have: P ˆ NTcp1q ď 4l KLmin ˙ “P ˜ ÿ a:∆aą0 NTcpaq ą 4pA ´ 1ql KLmin ` A ¸ ďP ˆ Da ą 1, NTcpaq ą 4l KLmin ` 1 ˙ . Then by union bound, we have: P ˆ Da ą 1, NTcpaq ą 4l KLmin ` 1 ˙ ď ÿ a:∆aą0 P ˆ NTcpaq ą 4l KLmin ` 1 ˙ . Consider a fixed sub-optimal arm a, then for each probability inside the summation, we have: P ˆ NTcpaq ą 4l KLmin ` 1 ˙ ďP ˆ Dt P rA ` 1, Tcs, Nt´1paq “ R 4l KLmin V , At “ a ˙ ďP ˆ Dt P rA ` 1, Tcs, Nt´1paq “ R 4l KLmin V , UCBt´1p1q ď UCBt´1paq ˙ ď P ˆ Dt P rA ` 1, Tcs, Nt´1paq “ R 4l KLmin V , µ1 ď UCBt´1paq ˙ looooooooooooooooooooooooooooooooooooooomooooooooooooooooooooooooooooooooooooooon B3 ` P pDt P rA ` 1, Tcs, UCBt´1p1q ď µ1q looooooooooooooooooooooomooooooooooooooooooooooon B4 . Notice that B4 can be bounded from concentration lemma. First, we switch the count from time step t to the number of pulls for arm 1 since the empirical estimation ¯rt´1p1q and count for pulls Nt´1p1q won’t 36change unless there is a new pull. Then, we will apply Eq. (4a) to the probability as follows: B4 ďP pDs P r1, Tc ´ A ` 1q, UCBs,1 ď µ1q ďP pDs P r1, Tc ´ A ` 1q, ¯rs,1 ď µ1, sKLp¯rs,1, µ1q ě lq ďTc ´ A exp l , where the second inequality is because under event tUCBs,1 ď µ1u and according to the definition of UCBs,1, we have ¯rs,1 ď UCBs,1 ď µ1, and since sKLp¯rs,1, UCBs,1q “ l with µ1 ą UCBs,1, we have sKLp¯rs,1, µ1q ě l. The last inequality uses Lemma. 6. Similar to the procedure of bounding I1, notice that expplq ě T log4 T, we have: B4 ď 4A ` log T ` 4?2 log T ˘ KLmin 1 T log4 T ď 20A KLminT log3 T . On the other hand, let γ “ Q 4l KLmin U , and we also change the count from time step t to the number of pulls for arm a. Then, B3 can be expressed as follows: B3 “ P ` Dt P rA ` 1, Tc ´ 1s, Nt´1paq “ γ, µ1 ď UCBNt´1paq,a ˘ “ P pµ1 ď UCBγ,aq . For any pair of means x, y P r0, 1s, define KL`px, yq “ KLpx, yq1xăy. Then, we have: P pµ1 ď UCBγ,aq ď P ˆ KL`p¯rγ,a, µ1q ď l γ ˙ . Notice that γ “ Q 4l KLmin U ě 4l KLpµa,µ1q, we can also bound B3 as follows: B3 ď P ˆ KL`p¯rγ,a, µ1q ď KLpµa, µ1q 4 ˙ . Recall that µ1 a P pµa, µ1q such that KLpµ1 a, µ1q “ KLpµa,µ1q 4 , so we have ¯rγ,a ě µ1 a under the event that ! KL`p¯rγ,a, µ1q ď KLpµa,µ1q 4 ) , and thus KLp¯rγ,a, µaq ě KLpµ1 a, µaq. By Lemma. 5, we have: P ˆ KL`p¯rγ,a, µ1q ď KLpµa, µ1q 4 ˙ ď P ` KLp¯rγ,a, µaq ě KLpµ1 a, µaq ˘ ď expp´γKLpµ1 a, µaqq. Notice that γKLprpγq, µaq ě 4lKLpµ1 a,µaq KLmin ě l by the definition of KLmin, we then have: B3 ď expp´lq ď 1 T log4 T . The last inequality is due to the fact that expplq ě T log4 T. So combine B3 and B4 together, we can bound B2 as follows: B2 ď ÿ a:∆aą0 pB3 ` B4q ď A ˆ 40A ∆2 minT log3 T ` 1 T log4 T ˙ “ o ˆ 1 T ˙ . 37Next, we attempt to bound B1. Notice that B1 can also be bounded as follows: B1 “P ˆ LCBTcpaq ě LCBTcp1q, NTcp1q ą 4l KLmin ˙ ď P ˆ µa ě LCBTcp1q, NTcp1q ą 4l KLmin ˙ loooooooooooooooooooooooomoooooooooooooooooooooooon B5 ` P pµa ď LCBTcpaqq loooooooooomoooooooooon B6 . The term B6 can be expressed by counting the number of pulls of arm a as follows: B6 ďP pDs P r1, Tc ´ A ` 1s, LCBs,a ě µaq “P pDs P r1, Tc ´ A ` 1s, ¯rs,a ě µa, sKL p¯rs,a, µaq ě lq ďTc ´ A ` 1 expplq , where in the last inequality, we can apply Lemma. (6). Since log T ě ?2 log T and expplq ě 1 T log4 T when T ě 16, we have: B6 ď 4A ` log T ` 4?2 log T ˘ KLminT log4 T ` 1 T log4 T “ o ˆ 1 T ˙ . For any pair of means x, y P r0, 1s, define KL´px, yq “ KLpx, yq1xąy. Similarly, term B5 can be expressed as: B5 ďP ˆ Ds ą 4l KLmin , µa ě LCBs,1 ˙ ďP ˆ Ds ą 4l KLmin , sKL´ p¯rs,1, µaq ď l ˙ ďP ˆ Ds ą 4l KLmin , KL´ p¯rs,1, µaq ď KLpµa, µ1q 4 ˙ . Recall that µ1 a P pµa, µ1q satisfy KL pµ1 a, µaq “ KLpµa,µ1q 4 , so under event tKL´ p¯rs,1, µaq ď KLpµa,µ1q 4 u we can conclude ¯rs,1 ď µ1 a, which means KL p¯rs,1, µ1q ě KL pµ1 a, µ1q, so we have: P ˆ Ds ą 4l KLmin , KL´ p¯rs,1, µaq ď KLpµa, µ1q 4 ˙ ďP ˆ Ds ą 4l KLmin , ¯rs,1 ď µ1, KL p¯rs,1, µ1q ě KL ` µ1 a, µ1 ˘˙ ďP ˆ Ds ą 4l KLmin , ¯rs,1 ď µ1, sKL p¯rs,1, µ1q ě 4lKL pµ1 a, µ1q KLmin ˙ ď el log T ` e exp ´ 4lKLpµ1a,µ1q KLmin ¯, where the last inequality comes from Lemma. 6. By the definition of KLmin, we know that 4lKLpµ1 a,µ1q KLmin ě l, so we have: B5 ď el log T ` e expplq ď 10e log2 T T log4 T “ o ˆ 1 T ˙ . 38Therefore, collecting the bounds for B5 and B6 and B2, we have a bound for I2 as follows: I2 ď B1 ` B2 ď B5 ` B6 ` B2 “ o ˆ 1 T ˙ . Finally, putting the bounds on I1 and I2 together, we have: ErNapTqs ď I1 ` TI2 ď log T KLpµa, µ1q ` 10 log 3 4 T KLpµa, µ1q ` op1q. Therefore, by the regret decomposition lemma, we can bound the total regret as follows: RegKL-EOCP µ pTq ď ÿ a:∆aą0 ˜ ∆a log T KLpµa, µ1q ` 10∆a log 3 4 T KLpµa, µ1q ¸ ` op1q. D Proof of Concentration Inequalities In this section, we provide the proof of the concentration inequalities presented in previous sections. D.1 Proof of Lemma. 3 The proof of Lemma. 3 relies on the maximal inequalities in Lemma. 1 and Lemma. 2, and the famous Hoeffding’s inequality which we state here for the sake of completeness. Lemma 8 (Hoeffding’s Inequality) Let pWiqT i“1 be i.i.d. σ-sub-Gaussian random variables with ErW1s “ 0, we have: P ˜řT i“1 Wi T ě δ ¸ ď exp ˆ ´Tδ2 2σ2 ˙ . Eq. (4a) of Lemma. 3: We now prove Eq. (4a) of Lemma. 3. We will first base our proof on the anytime concentration on union bound, which is tight only when T2 ´ T1 is relatively small. This procedure will result in the first term in the minimum at the RHS. To prove a stronger result when T2 ´ T1 is relatively large, we resort to the technique of "peeling device" which divide the time horizon into exponential grids, where we will perform maximal inequality inside each grid and a union bound over the grids. This will result in the second term in the minimum at the RHS. We first perform union bound on s as follows: P ˜ Ds P pT1, T2s, řs i“1 Wi s ` c 2l s ď 0 ¸ “P ˜ Ds P pT1, T2s, sÿ i“1 Wi ` ? 2ls ď 0 ¸ ď T2 ÿ j“T1`1 P ˜ s “ j, sÿ i“1 Wi ` ? 2ls ď 0 ¸ . Then, we can bound each probability with Hoeffding’s concentration inequality as follows: P ˜ s “ j, sÿ i“1 Wi ` ? 2ls ď 0 ¸ “ P ˜ jÿ i“1 Wi ` a 2jl ď 0 ¸ ď exp ˆ ´p?2jlq2 2j ˙ “ 1 expplq. 39Therefore, summing up the probabilities, we will have: P ˜ Ds P pT1, T2s, řs i“1 Wi s ` c 2l s ď 0 ¸ ď T2 ÿ j“T1`1 1 expplq “ T2 ´ T1 expplq . (8) Next, we apply the peeling method to prove the second inequality. Take β ą 1 to be a constant. Let M “ tlogβ T2 T1 u, we apply peeling method on s and divide the time horizon over exponential grids rT1, T1βs, rT1β, T1β2s, ¨ ¨ ¨ , rT1βM, T2s as follows: P ˜ Ds P pT1, T2s, řs i“1 Wi s ` c 2l s ď 0 ¸ “P ˜ Ds P pT1, T2s, sÿ i“1 Wi ` ? 2ls ď 0 ¸ ď M ÿ j“0 P ˜ Ds P rT1βj, T1βj`1s, sÿ i“1 Wi ` ? 2ls ď 0 ¸ . Since at each grid, s ě T1βj, so we can upper bound each probability as: P ˜ Ds P rT1βj, T1βj`1s, sÿ i“1 Wi ` ? 2ls ď 0 ¸ ď P ˜ Ds P rT1βj, T1βj`1s, sÿ i“1 Wi ` a 2T1βjl ď 0 ¸ . Let β “ l l´1, then according to the anytime concentration inequality from Lemma. 1, we have: P ˜ Ds P rT1βj, T1βj`1s, sÿ i“1 Wi ` a 2T1βjl ď 0 ¸ ď exp ˆ ´ 2T1βjl 2T1βj`1 ˙ “ exp ˆ ´ l β ˙ “ e expplq. Then, summing up the probabilities, we can bound the total probability as follows: P ˜ Ds P pT1, T2s, řs i“1 Wi s ` c 2l s ď 0 ¸ ď epM ` 1q expplq ď e plog T2 ´ log T1q log β expplq ` e exp l, where the last inequality is due to the definition of M “ tlogβ T2 T1 u. Notice that when l ě 2, log β “ log ´ l l´1 ¯ ě 1 l , so we can further upper bound the probability as: P ˜ Ds P pT1, T2s, řs i“1 Wi s ` c 2l s ď 0 ¸ ď el plog T2 ´ log T1q expplq ` e exp l. (9) Finally, combining the two bounds Eq. (8) and Eq. (9) together, we can prove the first result of Lemma. 3 as follows: P ˜ Ds P pT1, T2s, řs i“1 Wi s ` c 2l s ď 0 ¸ ď min "T2 ´ T1 expplq , el plog T2 ´ log T1q expplq ` e exp l * . Eq. (4b) of Lemma. 3: Next, we prove Eq. (4b) of Lemma. 3. Our result is only based on performing union bound, but one can also modify the proof of Eq. (4a) with peeling trick to prove Eq. (4b). However, the result from peeling trick is no better than simply performing union bound, at least not order-wise 40better. So for simplicity, we apply union bound to the probability as follows: P ˜ Ds P rT1, T2s, řs i“1 Wi s ` c 2l s ` δ ď 0 ¸ “P ˜ Ds P rT1, T2s, sÿ i“1 Wi ` ? 2ls ` δs ď 0 ¸ ď T2 ÿ j“T1 P ˜ s “ j, sÿ i“1 Wi ` ? 2ls ` δs ď 0 ¸ “ T2 ÿ j“T1 P ˜ jÿ i“1 Wi ` a 2jl ` δj ď 0 ¸ . Then, we can apply Hoeffding’s inequality to upper bound each probability as follows: P ˜ jÿ i“1 Wi ` a 2jl ` δj ď 0 ¸ ď exp ˜ ´ `?2jl ` δj ˘2 2j ¸ “ exp ˆ ´2jl ` 2?2jlδj ` δ2j2 2j ˙ ď expp´δ2 2 jq exp ` l ` δ?2T1l ˘, where the last inequality is due to j ě T1. So summing up all the probabilities, we can bound the anytime concentration as: P ˜ Ds P rT1, T2s, řs i“1 Wi s ` c 2l s ` δ ď 0 ¸ ď T2 ÿ j“T1 expp´δ2 2 jq exp ` l ` δ?2T1l ˘ ď 1 exp ` l ` δ?2T1l ˘ expp´δ2T1 2 q 1 ´ exp ´ ´δ2 2 ¯ ď 4 δ2 exp ˜ˆ? l ` δ b T1 2 ˙2¸. where the last step is due to 1 ´ e´ δ2 2 ě δ2 2 when δ P r0, ? 3s. Then we finish the proof of Eq. (4b) of Lemma. 3. Eq. (4c) of Lemma. 3: Finally, we prove Eq. (4a) of Lemma. 3 which bounds the summation of probability for deviation events over the time horizon. Since l ě T1δ2 2 , we define γ “ 2l δ2 ě T1. Then, we can bound the probabilities when s ď γ by 1 as follows: T2 ÿ s“T1`1 P ˜řs i“1 Wi s ` c 2l s ě δ ¸ ď γ ´ T1 ` T2 ÿ s“rγs P ˜řs i“1 Wi s ` c 2l s ě δ ¸ . When s ě rγs, we have: b 2l s “ δ b γ s . So, for each probability inside the summation, we have: P ˜řs i“1 Wi s ` c 2l s ě δ ¸ “ P ˜řs i“1 Wi s ě δ ´ c 2l s ¸ ď P ˆřs i“1 Wi s ě δ ˆ 1 ´ cγ s ˙˙ . 41Then, we can bound the probability with Hoeffding’s inequality as follows: P ˆřs i“1 Wi s ě δ ˆ 1 ´ cγ s ˙˙ ď exp ˜ ´sδ2 2 ˆ 1 ´ cγ s ˙2¸ “ exp ˆ ´δ2 2 `?s ´ ?γ ˘2 ˙ . Notice that the upper bound function expp´δ2 2 p?s ´ ?γq2q on the RHS is uni-modal when s ě γ. If a function fpsq is uni-modal, then we can bound the summation ř8 s“γ with the sum of maxs fpsq and integral ş8 γ fpsqds. Therefore, putting the upper bound back to the summation, we have: T2 ÿ s“rγs P ˜řs i“1 Wi s ` c 2l s ě δ ¸ ď T2 ÿ s“rγs exp ˆ ´δ2 2 `?s ´ ?γ ˘2 ˙ ď 8 ÿ s“rγs exp ˆ ´δ2 2 `?s ´ ?γ ˘2 ˙ ď1 ` ż 8 γ exp ˆ ´δ2 2 `?s ´ ?γ ˘2 ˙ ds “1 ` 2 δ2 ` ?2πγ δ . So the whole term can be bounded by: T2 ÿ s“T1`1 P ˜řs i“1 Wi s ` c 2l s ě δ ¸ ď γ ´ T1 ` 1 ` 2 δ2 ` ?2πγ δ “ 2l ` ? 4πl ` 2 δ2 ` 1 ´ T1, which completes the proof of Lemma. 3. D.2 Proof of Lemma. 5 We only prove the inequality when ¯µs ď µ. The other inequality can be proved exactly the same way. For every λ P R, let ϕµpλq “ log ErexppλX1qs which is well-defined and finite by assumption. Let W λ 0 “ 1 and for s ě 1, we define W λ t “ exp pλSs ´ sϕµpλqq. We show that ` W λ s ˘ sě1 is a martingale with respect to the σ-field Fs “ σpW1, ¨ ¨ ¨ , Wsq. In fact, E ” W λ s`1|Fs ı “E rexp pλSs ` λWs`1 ´ sϕµpλq ´ ϕµpλqq |Fss “ exp pλSs ´ sϕµpλqq E rexp pλWs`1 ´ ϕµpλqq |Fss “W λ s E rexp pλWs`1qs E exp rλX1s “W λ s , where the second equality is because Ss is measurable w.r.t. Fs, and the third equality uses the fact that Ws`1 is independent w.r.t. Fs. Th last equality is due to the i.i.d. nature of W1 and Ws`1. Let x P r0, µs be such that KLpx, µq “ δ{s, and let λpxq “ θx ´ θµ. It is worth-noting that since θµ is a monotonically non-decreasing function since its inverse function µ “ b1pθq is monotonically non-decreasing. So we have λpxq ď 0 since x ď µ. Observe that: ¯µs ď µ, KL p¯µs, µq ě δ s, and, x ď µ, KL px, µq “ δ s. 42Then it holds that x ě ¯µs. Notice that for natural parameter exponential family, ϕµpλq “ bpλ`θµq´bpθµq. Hence on the event t¯µs ă µu X tsKL p¯µs, µq ě δu, we have: λpxq¯µs ´ ϕµpλpxqq ě λpxqx ´ ϕµpλpxqq “ x pθx ´ θµq ´ bpθxq ` bpθµq “ KLpx, µq “ δ s, where the first inequality is because λpxq ă 0, and the second last equality uses the expression of KL divergence for natural exponential families. Putting everything together, we have: P p¯µs ď µ, sKL p¯µs, µq ě δq ďP ˆ λpxq¯µs ´ ϕµpλpxqq ě δ s ˙ “P ´ W λpxq s ě δ ¯ ďErW λpxq s s expp´δq, where the last inequality uses Markov inequality. Since W λpxq s is a martingale, so we have: P p¯µs ď µ, sKL p¯µs, µq ě δq ď ErW λpxq 0 s expp´δq “ expp´δq. D.3 Proof of Lemma 6 We only prove the inequality when ¯µs ď µ. The other inequality can be proved exactly the same way. The proof of Lemma/ 6 partly resembles the Proof of Lemma. 5. However, to show an any-time concentration bound, we need either the union bound or a peeling trick. We first apply union bound as follows: P pDs P pT1, T2s, ¯µs ď µ, sKL p¯µs, µq ě lq ď T2 ÿ s“T1`1 P p¯µs ď µ, sKL p¯µs, µq ě lq ďpT2 ´ T1q expp´lq, which obtains the first term in the RHS of Lemma. 6. Next, we apply the peeling trick. For every λ P R, let ϕµpλq “ log ErexppλX1qs which is well-defined and finite by assumption. Let W λ 0 “ 1 and for s ě 1, we define W λ t “ exp pλSs ´ sϕµpλqq. Recall that ` W λ s ˘ sě1 is a martingale with respect to the σ-field Fs “ σpW1, ¨ ¨ ¨ , Wsq. Take β ą 1 to be a constant. Let M “ tlogβ T2 T1 u, we apply peeling method on s and divide the time horizon over exponential grids rT1, T1βs, rT1β, T1β2s, ¨ ¨ ¨ , rT1βM, T2s as follows: P pDs P pT1, T2s, ¯µs ď µ, sKL p¯µs, µq ě lq ď M ÿ i“0 P ` Ds P rT1βi, T1βi`1s, ¯µs ď µ, sKL p¯µs, µq ě l ˘ . Let si “ T1βi and let x ď µ such that sKLpx, µq “ l. let λpxq “ θx ´ θµ < 0. Then, we have KLpx, µq “ λpxqx ´ ϕµpλpxqq. Consider z such that zi ă µ and KLpzi, µq “ l si , so we have when s P rsi, si`1s KLp¯µs, µq ě l s ě l si`1 “ KLpzi`1, µq So we can conclude that ¯µs ď zi`1 Also, we have: KLpzi`1, µq “ l si`1 “ 1 β l si ě 1 β l s. 43Therefore, we have: λpzi`1q¯µs ´ ϕµpλpzi`1qq ě λpzi`1qzi`1 ´ ϕµpλpzi`1qq “ KLpzi`1, µq ě l βs So we can bound each probability as: P pDs P rzi, zi`1s, ¯µs ď µ, sKL p¯µs, µq ě δq ďP ˆ λpzi`1q¯µs ´ ϕµpλpzi`1qq ě l βs ˙ “P ˆ W λpzi`1q s ě exp ˆ l β ˙˙ ďErW λpzi`1q s s exp ˆ ´ l β ˙ , where the last inequality uses Markov inequality. Since W λpzi`1q s is a martingale, so we have: P pDs P rzi, zi`1s, ¯µs ď µ, sKL p¯µs, µq ě δq ď ErW λpzi`1q 0 s exp ˆ ´ l β ˙ “ e expplq, where in the last step, we choose β “ l l´1. Then, summing up the probabilities, we can bound the total probability as follows: P pDs P pT1, T2s, ¯µs ď µ, sKL p¯µs, µq ě lq ď M ÿ j“0 e expplq “ epM ` 1q expplq ď e plog T2 ´ log T1q log β expplq ` e exp l, where the last inequality is due to the definition of M “ tlogβ T2 T1 u. Notice that when l ě 2, log β “ log ´ l l´1 ¯ ě 1 l , so we can further upper bound the probability as: P pDs P pT1, T2s, ¯µs ď µ, sKL p¯µs, µq ě lq ď el plog T2 ´ log T1q expplq ` e exp l. (10) D.4 Proof of Lemma. 7 Our proof is based on the analysis of Theorem. 2 of (Garivier and Cappé, 2011). For any pair of means x, y P r0, 1s, define KL`px, yq “ KLpx, yq1xăy. Then for a fixed s, under event tUCBs ě µ1u we have either µ1 ă ¯µs, or µ1 ą ¯µs but sKLp¯µs, µ1q ď l. So in general we can conclude that sKL`p¯µs, µ1q ď l. Then we can bound the sum of probabilities as follows: T1 ÿ s“1 P ` UCBs ě µ1˘ ď T1 ÿ s“1 P ` sKL`p¯µs, µ1q ď l ˘ . Define γ “ p1`εql KL`pµ,µ1q “ p1`εql KLpµ,µ1q, then if T1 ą γ, we can bound the first γ terms in the summation with 1. If otherwise T1 ď γ, the whole summation is bounded by γ. So without loss with generality, assume γ ă T1 and let ε ą 0 be a constant, we have: T1 ÿ s“1 P ` sKL`p¯µs, µ1q ď l ˘ ďγ ` T1 ÿ s“rγs P ` sKL`p¯µs, µ1q ď l ˘ ďγ ` T1 ÿ s“rγs P ` γKL`p¯µs, µ1q ď l ˘ “γ ` T1 ÿ s“rγs P ˆ KL`p¯µs, µ1q ď KLpµ, µ1q 1 ` ε ˙ 44where the second inequality is due to KL`p¯µs, µ1q ą 0. For any s, let rpεq P pµ, µ1q such that KLprpεq, µ1q “ KLpµ,µ1q 1`ε . if KL`p¯µs, µ1q ď KLpµ,µ1q 1`ε , we have ¯µs ě rpεq. Hence, P ˆ KL`p¯µs, µ1q ď KLpµ, µ1q 1 ` ε ˙ ď P p¯µs ě µ, KLp¯µs, µq ě KLprpεq, µqq ď expp´sKLprpεq, µqq, where the last inequality uses Lemma. 5. So we can bound the sum of probabilities as follows: T1 ÿ s“1 P ` sKL`p¯µs, µ1q ď l ˘ ď γ ` 8 ÿ s“rγs expp´sKLprpεq, µqq ď γ ` expp´γKLprpεq, µqq 1 ´ expp´KLprpεq, µqq. Notice that expp´γKLprpεq, µqq “ exp ´ ´l p1`εqKLprpεq,µq KLpµ,µ1q ¯ ď T ´β1pεq, where β1pεq “ p1`εqKLprpεq,µq KLpµ,µ1q . Let β2pεq “ 1 1´expp´KLprpεq,µqq. It is easy to check that rpεq “ µ ` Opεq, so we have β1pεq “ Opε2q and β2pεq “ Opε´2q. So we have: T1 ÿ s“1 P ` UCBs ě µ1˘ ď T1 ÿ s“1 P ` sKL`p¯µs, µ1q ď l ˘ ď p1 ` εql KLpµ, µ1q ` β2pεq T β1pεq . Remark: Let ε “ l´ 1 4 , we have: T1 ÿ s“1 P ` UCBs ě µ1˘ ď p1 ` εql KLpµ, µ1q ` β2pεq T β1pεq ď l ` l 3 4 KLpµ, µ1q ` O ¨ ˝ ? l exp ´? l ¯ ˛ ‚. 45