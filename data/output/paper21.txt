Calibrated Explanations for Regression Tuwe L¨ofstr¨om1*, Helena L¨ofstr¨om2,3, Ulf Johansson1, Cecilia S¨onstr¨od1, Rudy Matela1 1*Department of Computing, J¨onk¨oping University, Box 1026, J¨onk¨oping, 55111, Sweden. 2J¨onk¨oping International Business School, J¨onk¨oping University, Box 1026, J¨onk¨oping, 551 11, Sweden. 3Department of Information Technology, University of Bor˚as, Bor˚as, 501 90, Sweden. *Corresponding author(s). E-mail(s): tuwe.lofstrom@ju.se; Contributing authors: helena.lofstrom@ju.se; ulf.johansson@ju.se; cecilia.sonstrod@ju.se; rudy.matela@ju.se; Abstract Artificial Intelligence (AI) is often an integral part of modern decision support sys- tems (DSSs). The best-performing predictive models used in AI-based DSSs lack transparency. Explainable Artificial Intelligence (XAI) aims to create AI systems that can explain their rationale to human users. Local explanations in XAI can provide information about the causes of individual predictions in terms of feature importance. However, a critical drawback of existing local explanation methods is their inability to quantify the uncertainty associated with a feature’s impor- tance. This paper introduces an extension of a feature importance explanation method, Calibrated Explanations (CE), previously only supporting classifica- tion, with support for standard regression and probabilistic regression, i.e., the probability that the target is above an arbitrary threshold. The extension for regression keeps all the benefits of CE, such as calibration of the prediction from the underlying model with confidence intervals, uncertainty quantification of fea- ture importance, and allows both factual and counterfactual explanations. CE for standard regression provides fast, reliable, stable, and robust explanations. CE for probabilistic regression provides an entirely new way of creating probabilistic explanations from any ordinary regression model and with a dynamic selection of thresholds. The performance of CE for probabilistic regression regarding stabil- ity and speed is comparable to LIME. The method is model agnostic with easily understood conditional rules. An implementation in Python is freely available 1 arXiv:2308.16245v2 [cs.LG] 1 Sep 2023on GitHub and for installation using pip making the results in this paper easily replicable. Keywords: Explainable AI, Feature Importance, Calibrated Explanations, Conformal Predictive Systems, Uncertainty Quantification, Regression, Probabilistic Regression, Counterfactual Explanations 1 Introduction In recent times, Decision Support Systems (DSSs) in various domains such as retail, sport, or defence have been incorporating Artificial Intelligence (AI) extensively [1]. However, the predictive models used in AI-based DSSs generally lack transparency and only provide probable results [2, 3]. This can result in misuse (when users rely on it excessively) or disuse (when users do not rely on it enough) [4, 5]. The lack of transparency has led to the development of explainable artificial intel- ligence (XAI), which aims to create AI systems capable of explaining their reasoning to human users. The goal of explanations is to support users in identifying incorrect predictions, especially in critical areas such as medical diagnosis [6]. An explanation provided by XAI should highlight the underlying model’s strengths and weaknesses and predict how it will perform in the future [2, 7]. Regarding explanations in XAI, there are two types: local and global. Local explanations focus on the reasons behind individual predictions, while global expla- nations provide information about the entire model [8–10]. Despite the apparent strength stemming from the possibility of providing explanations for each instance, local explanations typically have some drawbacks. For example, they can lack robust- ness, meaning that minor differences in the instance can lead to significantly different explanations, or be instable, meaning that the same model and instance may result in different explanations [11, 12]. Lack of robustness and instability create issues when evaluating the quality of the explanations. Metrics like fidelity, which measure how well an explanation captures the behaviour of the underlying model, do not give an accurate picture of explanation quality since they depend heavily on the details of the explanation method [9, 11, 13–18]. Furthermore, even the best explanation tech- niques offer limited insight into model uncertainty and reliability. Recent research has emphasized uncertainty estimation’s role in enhancing the transparency of underlying models [11, 19]. Although achieving well-calibrated uncertainty has been underscored as a critical factor in fostering transparent decision-making, [19] points out the chal- lenges and complexities of obtaining accurately calibrated uncertainty estimates for complex problems. Moreover, as indicated by [11], the focus has predominantly leaned towards adopting a well-calibrated underlying model (such as Bayesian) rather than relying on calibration techniques. The probability estimate that most classifiers output is commonly used as an indi- cator of the likelihood of each class in local explanation methods for classification. However, it is widely recognized that these classifiers are often poorly calibrated, 2resulting in probability estimates that do not faithfully represent the actual proba- bility of correctness [20]. Specialized calibration techniques such as Platt Scaling [21] and Venn-Abers (VA) [22] have been proposed to tackle these shortcomings. The VA method generates a probability range associated with each prediction, which can be refined into a properly calibrated probability estimate utilizing regularisation. When employing the VA approach for decision-making, it is essential to recognize that the technique provides intervals for individual classes. These intervals quantify the uncertainty within the probability estimate, offering valuable insights from an explana- tory standpoint. The breadth of the interval directly corresponds to the model’s level of uncertainty, with a narrower interval signifying heightened confidence in the probabil- ity estimate. In comparison, a broader interval indicates more substantial uncertainty in said estimates. Furthermore, this uncertainty information can be extended to the features, given that the feature weights are informed by the prediction’s probability estimate. Being able to quantify the uncertainty can improve the quality and usefulness of explanations in XAI. Recently, a local explanation method, Calibrated Explana- tions (CE), utilizing the intervals provided by VA to estimate feature uncertainty was introduced for classification [23]. Existing explanation methods most commonly focus on explaining decisions from classifiers, despite the fact that regression is widely used in highly critical situations. Due to the lack of specialized explanation techniques for regression, applying methods designed for classification on regression problems is not unusual, highlighting the need for well-founded explanations methods for regression [24]. The aim of this study is to propose an explanation method - with the same pos- sibility of quantifying uncertainty of feature weights as VA, through CE, provides for classification - for a regression context. The conformal prediction framework [25] pro- vides several different techniques for quantifying uncertainty in a regression context. In this paper, the Conformal Predictive Systems (CPSs) technique [26] for uncertainty estimation is used in CE to allow creation of calibrated explanations with uncertainty estimation for regression. Using CPS is not only a very flexible technique, providing a rich toolbox to be used for uncertainty quantification, but it also allows for estimat- ing the probability that the target is above any user-defined threshold. Based on this, a new form of probabilistic explanation for regression is also proposed in this paper. These approaches are user-friendly and model-agnostic, making them easy to use and applicable to diverse underlying models. In summary, this paper introduces extensions of CE aimed at regression, with the following characteristics: • Fast, reliable, stable and robust feature importance explanations for regression. • Calibration of the predictions from the underlying model through the application of CPSs. • Arbitrary forms of uncertainty quantification of the predictions from the underly- ing model and the feature importance weights through querying of the conformal predictive distribution (CPD) derived from the CPS. • Possibility of creating explanations on the probability of the prediction exceeding a user-defined threshold. 3• Rules with straightforward interpretation in relation to the feature values and the target. • Possibility to generate counterfactual rules with uncertainty quantification of the expected predictions (or probability of exceeding a threshold). 2 Background 2.1 Post-Hoc Explanation Methods The research area of XAI research can be broadly categorised into two main types: developing inherently interpretable and transparent models and utilising post-hoc methods to explain opaque models. Post-hoc explanation techniques seek to construct simplified and interpretable models that reveal the relationship between feature values and the model’s predictions. These explanations, which can be either local or global, often leverage visual aids such as pixel representations, feature importance plots, or word clouds. These visuals emphasise the features, pixels, or words accountable for causing the model’s predictions [9, 27]. Two distinct approaches of explanations exist: factual explanations, where a fea- ture value directly influences the prediction outcome, and counterfactual explanations, which explore the potential impact on predictions when altering a feature’s values [28–30]. Importantly, counterfactual explanations are intrinsically local. They are particularly human-friendly, mirroring how human reasoning operates [27]. 2.2 Essential Characteristics of Explanations Creating high-quality explanations in XAI requires a multidisciplinary approach that draws knowledge from both Human-Computer Interaction (HCI) and Machine Learn- ing (ML) fields. The quality of an explanation method depends on the goals it addresses, which may vary. For instance, assessing how users appreciate the expla- nation interface differs from evaluating if the explanation accurately mirrors the underlying model [31]. However, specific characteristics are universally desirable for post-hoc explanation methods. It is crucial that an explanation method accurately reflects the underlying model, which is closely related to the concept that an explana- tion method should have a high level of fidelity to the underlying model [11]. Therefore, a reliable explanation must have feature weights that correspond accurately to the actual impact on the estimates to correctly reflect the model’s behaviour. In other words, it should be well-calibrated [19]. Stability and robustness are two additional critical features of explanation meth- ods [7, 18, 32]. Stability refers to the consistency of the explanations [11, 14]; the same instance and model should produce identical explanations across multiple runs. On the other hand, robustness refers to the ability of an explanation method to pro- duce consistent results even when an instance undergoes small perturbations [7] or other circumstances change. Therefore, the essential characteristics of an explanation method in XAI are that it should be reliable, stable, and robust. 42.3 Explanations for classification and regression Distinguishing between explanations for classification and regression lies in the nature of the insights they offer. In classification, the task involves predicting the specific class an instance belongs to from a set of predefined classes. The accompanying probabil- ity estimates reflect the model’s confidence level for each class. Various explanation techniques have been developed for classifiers to clarify the rationale behind the class predictions. Notable methods include SHAP [33], LIME [34], and Anchor [35]. These techniques delve into the factors that contribute to the assignment of a particular class label. Typically, the explanations leverage the concept of feature importance, e.g., words in textual data or pixels in images. In regression, the paradigm shifts as there are no predetermined classes or cate- gorical values. Instead, each instance is associated with a numerical value, and the prediction strives to approximate this value. Consequently, explanations for regression models cannot rely on the framework of predefined classes. Nevertheless, explana- tion techniques designed for classifiers, as mentioned above, can often be applied to regression problems, provided these methods concentrate on attributing features to the predicted instance’s output. 2.4 Calibrated Explanations for Classification (CEC) Below is an introduction to CEC [23], which provides the foundation upon which this paper is contributing1. In the following descriptions, a factual explanation is composed of a calibrated prediction from the underlying model accompanied by an uncertainty interval and a collection of factual feature rules, each composed of a feature weight with an uncertainty interval and a factual condition, covering that feature’s instance value. Counterfactual explanations only contain a collection of counterfactual feature rules, each composed of a prediction estimate with an uncertainty interval and a counter- factual condition, covering alternative instance values for the feature. The prediction estimate represents a probability estimate for classification, whereas for regression, the prediction estimate will be expressed as a potential prediction. 2.4.1 Venn-Abers predictors Probabilistic predictors offer class labels and associated probability distributions. Val- idating these predictions is challenging, but calibration focuses on aligning predicted and observed probabilities [25]. The goal is well-calibrated models where predicted probabilities match actual accuracy. Venn predictors [36] produce multi-probabilistic predictions, converted to confidence-based probability intervals. Inductive Venn prediction [37] involves a Venn taxonomy, categorizing calibration data for probability estimation. Within each category, the estimated probability for test instances falling into a category is the relative frequency of each class label among all calibration instances in that category. Venn-Abers predictors (VA) [22] offer automated taxonomy optimization via iso- tonic regression, thus introducing dynamic probability intervals. A two-class scoring 1The Python implementation can be accessed at github.com/Moffran/calibrated explanations or installed through: pip install calibrated-explanations 5classifier assigns a prediction score s(xi) to a test object xi. A higher score implies higher belief in the positive class. In order to calibrate a model, some data must be set aside and used as a calibration set. Consequently, split the training set {z1, . . . , zi, . . . , zn}, with objects xi and labels yi, into a proper training set ZT and a calibration set {z1, . . . , zl}. Train a scoring classifier on ZT to compute s for {x1, . . . , xl, xn+1}. Inductive VA prediction follows these steps: 1. Derive isotonic calibrators g0 and g1 using {{s1, y1}, . . . , {sl, yl}, {sn+1, yn+1 = 0}} and {{s1, y1}, . . . , {sl, yl}, {sn+1, yn+1 = 1}}, respectively. 2. The probability interval for yn+1 = 1 is [g0(sn+1), g1(sn+1)] (henceforth referred to as [Pl, Ph], representing the lower and higher bounds of the interval). 3. Obtain a regularized probability estimate for yn+1 = 1 using the recommendation by [22]: P = Ph 1 − Pl + Ph In summary, VA produce a calibrated (regularized) probability estimate P together with a probability interval with a lower and upper bound [Pl, Ph]. 2.4.2 Factual Explanations for Classification Assuming we have a scoring classifier trained with the appropriate training set ZT , and we want to generate a local explanation for a test instance xn+1. We classify the set of features F into two categories: categorical features C and numerical features N. Let Vf denote all feature values for a feature f ∈ F let and fv denote the index of value v for feature f. The feature value held by the test instance xn+1 for a particular feature is denoted f ′ v. Using VA as calibrator, producing a probability interval [Pl, Ph] and a calibrated probability estimate P for the test instance xn+1, the explanation process follows these steps: 1. Define a discretizer for numerical features that sets thresholds and conditions (≤ , >) for features in N. LIME discretizers or their sub-classes described below are used as discretizers. For categorical features, rules are based on identity conditions (=). 2. For each feature f ∈ F: • If f ∈ C a) Iterate through all possible categorical values v ∈ Vf. Create one per- turbed instance per feature value v except the original value by replacing the original value xf n+1, resulting in a perturbed instance xfv n+1 = v. Here, fv denotes the index of value v for feature f. b) Calculate and record the probability intervals [Pfv l , Pfv h ] and the cali- brated probability estimate Pfv for the perturbed instances. c) Form a factual condition covering the instance value v as f = v. 6• If f ∈ N a) Utilize the discretizer’s thresholds to identify the nearest lower or upper threshold around the feature value xf n+1. Divide all possible feature val- ues in the calibration set for feature f into two groups Vf, separated by the lower or upper condition thresholda. b) Extract the 25th, 50th, and 75th percentiles within each group to get percentile values pv. c) For each group, iterate over the percentile values pv and create a per- turbed instance by substituting the feature value xf n+1 with one value fpv at a time, yielding the perturbed instance xfpv n+1 = pv. Apply the calibrator to the perturbed instance and record the probability intervals [Pfpv l , Pfpv h ] and the calibrated probability estimate Pfpv. d) Before proceeding to the next group, calculate an average over all percentile values within the group. This yields a probability interval [Pfv l , Pfv h ] and a calibrated probability estimate Pfv for each group. e) Let f ′ v denote the index of the group containing the feature value xf n+1 from the test instance. f) Form a factual condition covering the instance value v based on the Discretizer used. The rule will be either f ≤ v or f > v. Finalize Step 2: Form a feature rule for feature f composed of a factual condition covering the instance value and the feature weight. Calculate the feature weight (and interval weights) for feature f as the difference between P and the average of all Pfv (and [Pfv l , Pfv h ]) except for index f ′ v. This is because P = Pf ′ v. The weights for the calibrated prediction and the lower and upper bounds are computed as follows: wf = P − 1 |Vf| − 1 � ∀fv\f ′v Pfv, (1) wf l = P − 1 |Vf| − 1 � ∀fv\f ′v Pfv l , (2) wf h = P − 1 |Vf| − 1 � ∀fv\f ′ v Pfv h . (3) aWhen creating counterfactual rules, both a lower and an upper condition threshold may be used to enable counterfactual rules representing the possibility of both smaller and larger values. Since feature weights are derived from calibrated probabilities, they would lose clarity if feature rules allowed interval formats like 0 < feature f ≤ 2. The reason is that probabilities for values below the interval (feature f < 0) may differ significantly from probabilities for values above the interval (feature f > 2), making averages hard to interpret. Hence, the discretizer is typically binary for normal use of CE. Two binary discretizers are implemented to complement the discretizers existing in LIME: 7• A simple binary discretizer (BinaryDiscretizer) that uses the median of all calibration set values for any numerical feature. It’s like QuartileDiscretizer and DecileDiscretizer in LIME, but uses only the median. • A binary entropy discretizer (BinaryEntropyDiscretizer) is similar to LIME’s EntropyDiscretizer, except it employs a decision tree with depth limited to 1, forcing a binary split based on a threshold from the calibration set. When using a binary discretizer on numeric features, two groups are formed with one representing f ′ v. This simplifies equation (1) to wf = P − P¬f ′ v, along with corresponding adjustments to equations (2) and (3). Each individual rule only conveys the contribution of an individual feature. To counteract this shortcoming, conjoined rules can be derived to estimate the joint contribution between combinations of features. This is done separately from the generation of the feature rules. 2.4.3 Counterfactual Explanations for Classification Using the CE definition above, generating counterfactual rules becomes straight- forward. When employing Counterfactual Calibrated Explanations for classification (CCEC), it is advisable to use non-binary discretizers for numeric features. Thus, for- mation of both ≤-rules and >-rules will be allowed. For categorical features, one rule per alternative categorical value will be formed. The EntropyDiscretizer in LIME is the recommended choice for CCEC. Each feature rule’s expected probability inter- val is already established as [Pfv l , Pfv h ], following the CE process in step 2, defining one feature rule for each alternative instance value. The condition will be similar as in Step 2, but for the alternative instance value v. Equation (1)’s feature weights are mainly employed to sort counterfactual rules by impact. The calibrated probability estimate Pfv is normally neglected in counterfactual rules for classification. 3 Calibrated Explanations for Regression The basic idea in CEC is that each factual and counterfactual explanation is derived using three calibrated values: The calibrated probability and the probability interval represented by the lower and upper bound. For regression, there are two natural use cases that are commonly occurring. The most obvious is predicting the continues target value directly, i.e., standard regression, and another common use case is predicting the probability of the target being below (or above) a given threshold, basically viewing the problem as a binary classification problem. CPSs produce CPDs, which are cumulative distribution functions. These distri- butions can be used for various purposes, such as deriving prediction intervals for specified confidence levels or obtaining the probability of the true target falling below or above any threshold. CPSs are extending conformal regression. 83.1 Conformal Regression Conformal predictors (CPs) [25] offer predictive confidence by generating prediction regions, which encompass the true target with a specified probability. These regions are sets of class labels for classification or prediction intervals for regression. Errors arise when the true target falls outside the region, yet CPs are automati- cally valid under exchangeability, yielding an error rate of ϵ over time. Thus, the key evaluation criterion is efficiency, gauged by the region’s size and sharpness for greater insight. Conformal regressors (CRs), specifically an inductive (split) CR, follows these steps: 1. Divide the data into a proper training set ZT and a calibration set {z1, . . . , zl}. 2. Fit an underlying regression model h to ZT . 3. Define nonconformity as the absolute error |yi − h(xi)|. 4. Compute nonconformity scores for {z1, . . . , zl} and sort them in descending order to obtain α1 ≤ α2 ≤ ... ≤ αl. 5. Assign an ϵ, e.g., 0.01, 0.05, or 0.1. 6. Calculate the (1 − ϵ)-percentile nonconformity score, αs, where index s = ⌊ϵ(l + 1)⌋. 7. For a new instance xi, the prediction interval is h(xi) ± αs. To individualize intervals, the normalized nonconformity function augments noncon- formity with σi and β. These adapt intervals based on predicted difficulty σi for each yi. Normalized nonconformity is |yi−h(xi)| σi+β , and the interval is h(xi) ± αs(σi + β). This approach yields individualized prediction intervals, accommodating prediction difficulty and enhancing region informativeness. 3.2 Conformal Predictive Systems The process of creating (normalized) inductive CPSs closely resembles the forma- tion of inductive conformal regressors. The primary distinction lies in calculating nonconformity scores using actual errors, defined as: f (zi) = yi − h (xi) , (4) or normalized errors: f (zi) = yi − h (xi) σi + β , (5) where σi, xi, and β retain their prior definitions. The prediction for a test instance xi (potentially with an estimated difficulty σi) then becomes the following CPD: Q(y) = � i+τ l+1 , if y ∈ � C(i), C(i+1) � , for i ∈ {0, ..., l} i′−1+(i′′−i′+2)τ l+1 , if y = C(i), for i ∈ {1, ..., l} (6) where C(1), . . . , C(l) are obtained from the calibration scores α1, . . . , αl, sorted in increasing order: C(i) = h (x) + αi or, when using normalization: C(i) = h (x) + σαi 9Fig. 1: A CPD with three different intervals representing 90% confidence are defined: Lower-bounded interval: more than the 10th percentile; Two-sided interval: between the 5th and the 95th percentiles; Upper-bounded interval: less than the 90th percentile. The black dotted lines indicate how to determine the probability of the true target being smaller than 0.5, which in this case would be approximately 80%. with C(0) = −∞ and C(l+1) = ∞. τ is sampled from the uniform distribution U(0, 1) and its role is to allow the P-values of target values to be uniformly distributed. i′′ is the highest index such that y = C(i′′), while i′ is the lowest index such that y = C(i′) (in case of ties). For a specific value y, the function returns the estimated probability P(Y ≤ y), where Y is a random variable corresponding to the true target. Given a CPD, a two-sided prediction interval for a chosen significance level ϵ can be obtained by [C⌊(ϵ/2)(l+1)⌋, C⌈(1−ϵ/2)(l+1)⌉]. One-sided prediction intervals can be obtained by [C⌊ϵ(l+1)⌋, ∞] for a lower-bounded interval, and by [−∞, C⌈(1−ϵ)(l+1)⌉] for an upper-bounded interval. Similarly, a point prediction corresponding to the median of the distribution can be obtained by (C⌈0.5(l+1)⌉ + C⌊0.5(l+1)⌋)/2. The median predic- tion can be seen as a calibration of the underlying models prediction. Unless the model is biased, the median will tend to be very close to the prediction of the underlying model. Figure 1 illustrates how the CPD can form one-sided and two-sided confidence intervals. It also illustrates how the probability of the true target falling below a given threshold can be determined. Or, conversely, it illustrates what threshold a specific probability does correspond to. 10Compared to a CR, also able to provide valid confidence intervals from the under- lying model, a CPS offers richer opportunities to define intervals and probabilities through querying of the CPD. There are several different ways that difficulty (σ) can be estimated, such as: • The (Euclidean) distances to the k nearest neighbors. • The standard deviation of the targets of the k nearest neighbors. • The absolute errors of the k nearest neighbors. • The variance of the predictions of the constituent models, in case the underlying model is an ensemble. 3.3 Factual and Counterfactual Explanations for Regression (CER and CCER) In order to get CER, the probability intervals [Pl, Ph] and a calibrated probability estimate P from VA are exchanged for a confidence interval and the median which are derived from the CPD. The confidence interval is defined by user-selected lower and upper percentiles and allows dynamic selection of arbitrary confidence intervals. Thus, for the algorithm to produce factual and counterfactual rules in the same way as for classification, the only thing that needs to be adjusted in the algorithm described in section 2.4.2 is to exchange the calibrator from VA to CPS. Since the confidence interval from CPS is based on the user-provided percentiles, the lower and upper percentiles are two necessary additional parameters. By default, the lower and upper percentiles are [5th, 95th], resulting in a two-sided 90% confidence interval derived from the CPD. One-sided intervals can in practice be handled as a two-sided interval with either −∞ or ∞ assigned as lower or upper percentiles. The calibrated probability estimate used in classification is exchanged for the median from the CPD, which in practice represents a calibration of the underlying model’s prediction, neutralizing any systematic bias in the underlying model. Consequently, using a CPS effectively enables CER with uncertainty quantification of both the prediction from the underlying model and each feature rule. More formally, the confidence interval and the median is derived as follows: 1. Use the calibration set to calculate the calibration residuals ri = yi − h(xi), i ∈ {1, . . . , l}. 2. Fit a ConformalPredictiveSystem model cps using the residuals. 3. Obtain the median and interval values [mn+1, ln+1, hn+1] = cps(h(xn+1), percentiles = [50th, Pth l , Pth h ]) using the 50th, the lower Pth l and the higher Pth h percentiles. 4. To create CER following the procedure described in section 2.4.2 above, substitute P and [Pl, Ph] from the VA calibrator with mn+1 and [ln+1, hn+1] from the CPS. 5. To make the weights have a natural interpretation for regression, with a positive weight indicating that the feature is positively contributing to the prediction, 11equations (1)-(3) are inverted: wf =   1 |Vf| − 1 � ∀fv\f ′ v Pfv   − mn+1, (7) wf l =   1 |Vf| − 1 � ∀fv\f ′v Pfv l   − mn+1, (8) wf h =   1 |Vf| − 1 � ∀fv\f ′v Pfv h   − mn+1. (9) It is important to realize that since the input to the CER differ from CEC, not being probability estimates but instead actual predicted values, the CER will result in feature weights indicating changes in prediction rather than changes in probabilities. If a difficulty estimator is used to get explanations based on normalized CPDs, σ is calculated using DifficultyEstimator in crepes.extras and passed along to cps both when fitting and obtaining median and interval values. A minor difference between classification and regression is related to the dis- cretizers that can be used. As both the BinaryEntropyDiscretizer and the EntropyDiscretizer require categorical target values for the calibration set, the BinaryDiscretizer and the DecileDiscretizer are recommended instead. These are automatically assigned based on the kind of problem and explanation that is extracted. 3.4 Factual and Counterfactual Probabilistic Calibrated Explanations for Regression (PCER and CPCER) The simplest approach when trying to predict the probability that a target value is below (or above) a threshold is to treat the problem as a binary classification problem, with the target defined as ˙yi = � 1 if yi ≤ t 0 if yi > t, (10) where y are the regression targets, t the threshold, and ˙y the binary classification target. To obtain the probability, some form of probabilistic classifier is used. If several different thresholds are of interest, or if each test instance needs a dynamic threshold based on some contextual information, this approach is infeasible. The CPS makes it possible to query any regular regression model for the probability of the target falling below any given threshold. This effectively eliminates the need to treat the problem as a classification problem. Utilizing this strength to create explanations is straightforward if it is only the probability that is of interest. However, achieving a calibrated explanation with uncer- tainty quantification for this scenario is not as straightforward as creating factual and counterfactual explanations for classification or regression. There is no obvious 12equivalent to the probability interval created by VA in classification or the confidence interval derived from a CPS in regression. The fact that probabilistic predictions for regression can be achieved by viewing it as a classification problem holds a key to a solution. VA need a score s for both the calibration and the test instances. By using a CPS as a probabilistic scoring function for both calibration and test instances, it becomes possible to use VA to calibrate the probability and provide a probability interval. The score used is the probability (from a CPD) of calibration and test instances being above the given threshold. The isotonic regressors used by VA also need a binary target for the calibration set, which is defined using equation (10). Since the CPS is defined using the calibration set, the probabilities achieved on the same calibration set will be biased and consequently not be entirely trustworthy. To counteract that, the probability for each calibration instance is achieved by defining a CPS with all other calibration instances. More formally, the scores are derived as follows: 1. Use the calibration set to calculate the calibration residuals R = {r1, . . . , rl} where ri = yi − h(xi), i ∈ {1, . . . , l}. 2. Fit a ConformalPredictiveSystem model cps using the residuals R. 3. Define the score for the test instance xn+1 as sn+1 = cps(xn+1, threshold = t). 4. For each calibration instance xi, for i ∈ {1, ..., l}: a) Use the residuals R\i, i.e., the residuals for all calibration instances except instance i, to fit a ConformalPredictiveSystem model cps\i. b) Calculate the score si = cps\i(xi, t) representing the probability of yi ≤ t. c) Let ˙yi = yi ≤ t represent the categorical target for calibration instance xi 5. Use s as scores and ˙y as targets to define a VA calibrator, producing probability intervals [Pl, Ph] and a calibrated probability estimate P for yn+1 ≤ t and create a calibrated explanation using the description in section 2.4.2. The solution presented above is preferable since it avoids bias when calculating the scores for the calibration set. However, it is also very computationally expensive. A much faster but somewhat biased solution would be to use the same cps to get the scores s for both the calibration and test instances. This solution is used for normalized PCER and CPCER, to achieve reasonable computational performance. The same discretizers as used for CER and CCER (see section 3.3) needs to be applied for PCER and CPCER, as it is motivated by the problem type. Furthermore, if normalized CPSs are to be used, σ is calculated using DifficultyEstimator in crepes.extras and passed along to cps and cps\i both when fitting and obtaining probability scores. 3.5 Summary of Calibrated Explanations With the two solutions proposed here, Calibrated Explanations provide a number of possible use cases, which are summarized in Table 1. The general structure of factual and counterfactual explanations composed of lists of feature rules with conditions and feature weights or feature prediction estimates with confidence intervals (as described in Section 2.4) is general. 13Probabilistic Explanation Characteristics Classification Regression Regression Regular Factual Only prediction CI TI 5TI + 5LI + 5UI 5TI Uncertainty Factual Rule + prediction CI TI 5TI 5TI Counterfactual Only rule CI TI 5TI + 5LI + 5UI 5TI Table 1: Summary of available ways of using CE. CI means confidence intervals, TI means two-sided intervals, LI means lower-bounded one-sided intervals, UI means upper-bounded one-sided intervals, and the multiplying factor (5) indicates the number of alternative ways to use normalization (no normalization + the four types of normalization listed in Section 3.2) 3.6 Quality of Calibrated Explanations for Regression The median from a CPD based on the calibration data can be seen as a form of cali- bration of the underlying model’s prediction, since it may adjust the prediction on the test instance to match what has previously been seen on the calibration set. The cal- ibration will primarily affect systematic bias in the underlying model. Consequently, since CE calibrates the underlying model, it will create calibrated predictions and explanations. In addition, VA provides uncertainty quantification of both the probabil- ity estimates from the underlying model and the feature importance weights through the intervals for PCER. By using equality rules for categorical features and binary rules for numerical features (as recommended above), interpreting the meaning of a rule with a corresponding feature weight in relation to the target and instance value is straightforward and unambiguous and follows the same logic as for classification. The explanations are reliable because the rules straightforwardly define the rela- tionship between the calibrated outcome and the feature weight (for CER and PCER) or feature prediction estimate (for CCER and CPCER). The explanations are robust, i.e., consistent, as long as the feature rules cover any perturbations in feature val- ues. Variation in predictions, e.g. when training using different training sets, can be expected to result in some variation in feature rules, corresponding to the variation in predictions. Obviously, the method does not guarantee robustness for perturbations violating a feature rule condition. The CER and CCER explanations are stable as long as the same calibration set and model are used. Finally, depending on the size of the calibration set which is used to define a CPS, the generation of CER is, in most cases, faster than or at least comparable to existing solutions such as LIME and SHAP. Generating a PCER will be slower than CEC since both require a VA to be trained. Compared to CEC, PCER will have some additional overhead from using a CPS on each calibration instance as well. Finally, the calibrated predictions and their confidence intervals, which are an inte- gral part of factual CE, provide the same guarantees as the calibration model used, i.e., the same guarantees as VA for classification and CPSs for regression (or a combination of both for probabilistic regression). However, even if the uncertainty quantification in the form of intervals for the feature rules are also derived from the same calibration model, these feature rule intervals do not necessarily provide the same guarantees. The reason is that the perturbed instances used in Step 2 in Section 2.4.2 are artificial and the combination of feature values may not always exist naturally in the problem domain. Whenever that happens, the underlying model and the calibration model will 14indicate that it is a strange instance but may not estimate the degree of strangeness correctly as there is no evidence in the data to base a correct estimate on. 4 Method The implementation of both the regression and the probabilistic regres- sion solutions is expanding the calibrated-explanations Python package [23] and relies on the ConformalPredictiveSystem from the crepes pack- age [38]. By default, ConformalPredictiveSystem is used without normaliza- tion but DifficultyEstimator provided by crepes.extras is fully supported by calibrated-explanations, with normalization options corresponding to the list given at the end of Section 3.2. 4.1 Presentation of Calibrated Explanations trough Plots In this paper, three different kinds of plots for CE are presented. The first two are used when visualizing CER. These plots are inspired by LIME, especially the rules in LIME have been seen as providing valuable information in the explanations. • Regular explanations, providing CE without any uncertainty information. These explanations are directly comparable to other feature importance explanation techniques like LIME. • Uncertainty explanations, providing CE including uncertainty intervals to high- light both the importance of a feature and the amount of uncertainty connected with its estimated importance. For the reasons given in previous sections, CE is meant to use binary rules with factual explanations (even if all discretizers used by LIME can also be used by CE). One noteworthy aspect of CE is that the feature weights only show how each feature separately affects the outcome. It is possible to see pairwise combined weights through conjoined conjunctions of features (combining two or three different rules into a con- junctive feature rule). It is important to clarify that the feature weights do not convey the same meaning as in attribution-based explanations, like SHAP. The third kind of plot is a counterfactual plot showing preliminary prediction estimates for each feature when alternative feature values are used. Features rules are always ordered based on feature weight, starting with the most impactful rules. When plotting CE explanations, the user can choose to limit the num- ber of rules to show. Factual explanations have one rule per feature. Counterfactual explanations, where CE creates as many counterfactual rules as possible, may result in a much larger number of rules, especially for categorical with many categories. Internally, CE uses the same representation for both classification and regression. However, the plots visualizing the explanations have been adapted to suit the CER and PCER. 4.1.1 Calibrated Explanations Plots The same kind of plots exists for regression as for classification. Compared to the plots used for classification, the regression plots differ in two essential aspects. 15A common difference for both CER and CCER is that the feature weights represent changes in actual target values. For CER, this means that a feature importance of +100 means that the actual feature value contributes with +100 to the prediction. For a CCER, showing the prediction estimates with uncertainty intervals, the plot shows what the prediction is estimated to have been if the counterfactual condition would be fulfilled. A difference that only applies to the factual plots is that the top of the plot omits the probabilities for the different classes and instead shows the median m and the confidence interval [l, h] as the prediction. 4.1.2 Probabilistic Calibrated Explanations Plots Since the PCER represents feature importances as probabilities, just like CEC. The only difference needed for the plots for PCER compared to classification is to change the probabilities for a class label into probabilities for being below (P(y ≤ t)) or above (P(y > t)) the given threshold. 4.2 Experimental Setup The evaluation is divided into an introduction to CER, CCER, PCER, and CPCER through plots and an evaluation of performance. All plots are from the California Housing data set [39]. The underlying model in all experiments is a RandomForestRegressor from the sklearn package. Our proposed algorithm is claimed to be fast, reliable, stable, and robust. These claims requires validation in an evaluation of performance. The explanations are reli- able due to the validity of the uncertainty estimates used, i.e., the results achieved by querying the CPD, and from the uncertainty quantification of the feature weights or feature prediction estimates. Speed, stability and robustness will be evaluated in an experiment using the California Housing data set on a fixed set of test instances. Each experiment is repeated 100 times using 500 instances as a calibration set and 10 test instances. The target values were normalized, i.e., y ∈ [0, 1]. The following setups are evaluated: • CER: Factual explanation without normalization. • CER Var.: Factual explanation, with normalization based on the variance of the predictions of the constituent models in the underlying random forest regressor. • CCER: Counterfactual explanation without normalization. • PCER: Probabilistic factual explanation without normalization. The threshold is 0.5 for all instances, i.e., the mid-point of the interval of possible target values. • LIME: LIME explanation. • LIME CPS: LIME explanation using the median from a CPD as prediction. The CPS was based on the underlying random forest regressor. • Tree SHAP: SHAP explanation. The TreeExplainer class is used, which is imple- mented in C++ and optimized for tree-based models, such as the underlying random forest regressor. • SHAP CPS: SHAP explanation using the median from a CPD as prediction. The CPS was based on the underlying random forest regressor. Here, the Explainer class was used. 16The evaluated metrics are: • Stability means that multiple runs on the same instance and model should pro- duce consistent results. Stability is evaluated by generating explanations for the same predicted instances a 100 times with different random seeds. The largest variance in feature weight (or feature prediction estimate) can be expected among the most important features (by definition having higher absolute weights). The top feature for each test instance is identified as the feature being most impor- tant most often in the 100 runs (i.e., the mode of the feature ranks defined by the absolute feature weight). The variance for the top feature is measured over the 100 runs and the mean variance among the test instances is reported. • Robustness means that small variations in the input should not result in large variations in the explanations. Robustness is measured in a similar way as stabil- ity, but with the training and calibration set being randomly drawn and a new model being fitted for each run, creating a natural variation in the predictions of the same instances without having to construct artificial instances. Again, the variance of the top feature is used to measure robustness. The same setups as for stability are used except that each run use a new model and calibration set and that the random seed was set to 42 in all experiments. • Computational speed is compared between the setups regarding explanation gen- eration times (in seconds per instance). It was only the method call resulting in an explanation that was measured. Any overhead in initiating the explainer class has not been considered (it is assumed to be negligible). The closest equivalent to PCER would be to apply LIME and SHAP for classification to a thresholded classification model, as described in section 3.4. Since VA is comparably slow and PCER combines both CPSs and VA, with fitting and calls to a CPS for each calibration instance, it can be expected to be slow. 5 Results The results are divided into two parts: 1) a presentation of CE through plots, explain- ing and showcasing a number of different available ways CE can be used and viewed; and 2) an evaluation of performance with comparisons to LIME and SHAP. 5.1 Presentation of Calibrated Explanations through Plots In the following subsections, a number of introductory examples of CE are given for regression. First, factual and counterfactual explanations for regression are shown, followed by factual and counterfactual explanations for probabilistic regression. 5.1.1 Factual Calibrated Explanations for Regression The regular CER plot in Fig. 2 illustrates the calibrated prediction of the underlying model as the solid red line at the top bar together with the 90% confidence interval in light red. As can be seen, the house price is predicted to be ≈$285K and with 90% confidence, the price can be expected to be between [$215K-$370K]. Turning to the feature rules, the solid black line represents the median in the top-bar. The rule condi- tion is shown to the left and the actual instance value is shown to the right of the lower 17plot area. The fact that this house is located more northbound (latitude > 34.26) has a large negative impact on the price (reducing it with ≈$95K). On the other hand, since the median income is a bit higher (median income > 3.52), the price is pressed upwards with about $60K. Housing median age and population are two more features that clearly impact the price negatively. 100000 200000 300000 400000 500000 Prediction interval with 90% confidence Median prediction 100000 80000 60000 40000 20000 0 20000 40000 60000 Feature weights ocean_proximity = <1H OCEAN total_bedrooms > 429.50 total_rooms > 2078.00 households > 409.50 longitude <= -118.53 population > 1138.50 housing_median_age <= 28.00 median_income > 3.52 latitude > 34.26 Rules <1H OCEAN 564.0 3075.0 543.0 -121.98 1633.0 25.0 5.25 37.27 Instance values Fig. 2: A regular CER plot for the California Housing data set. The top-bar illustrates the median (the red line) and a confidence interval (the light red area), defined by the 5th and the 95th percentiles. The subplot below visualizes the weights associated with each feature. The weights indicate how much that rule contributes to the prediction. Negative weights indicate a negative impact on the prediction whereas positive weights indicate a positive impact. When one-sided intervals are used instead, it is only the top-bar that is affected when using a regular CER plots. Figures 3a and 3b illustrate an upper bounded and a lower bounded explanation for the same instance, with the identical feature rule subplot omitted. As can be seen, the median (solid red line) is the same as before, while the confidence interval stretches one entire side of the bar. The upper bound (≈$330K in Fig. 3a) is lower and the lower bound (≈$240K in Fig. 3b) is higher compared to the two-sided CER plot in Fig. 2. Fig. 4 illustrates an uncertainty plot for the same instance as before2. When includ- ing uncertainty quantification in the CER plot, the feature importance has a light colored area corresponding to the span of possible contribution within the confidence used. The grey area surrounding the solid black line represents the same confidence interval as seen in the top bar. As can be seen, the northbound location still has a large negative impact but the span of uncertainty about exactly how large the impact 2Uncertainty plots are not available for one-sided explanations, as the visualization becomes obscured and hard to interpret. However, the one-sided uncertainty interval for each feature rule is calculated and can be accessed and used if needed. 18100000 200000 300000 400000 500000 Prediction interval with 90% confidence Median prediction 100000 80000 60000 40000 20000 0 20000 40000 60000 Feature weights latitude > 34.26 & population > 1138.50 latitude > 34.26 & housing_median_age <= 28.00 population > 1138.50 & median_income > 3.52 longitude <= -118.53 & population > 1138.50 housing_median_age <= 28.00 & median_income > 3.52 latitude > 34.26 & median_income > 3.52 population > 1138.50 housing_median_age <= 28.00 median_income > 3.52 latitude > 34.26 Rules 37.27 1633.0 37.27 25.0 1633.0 5.25 -121.98 1633.0 25.0 5.25 37.27 5.25 1633.0 25.0 5.25 37.27 Instance values (a) Upper bounded explanation 100000 200000 300000 400000 500000 Prediction interval with 90% confidence Median prediction 100000 80000 60000 40000 20000 0 20000 40000 60000 Feature weights ocean_proximity = <1H OCEAN total_bedrooms > 429.50 total_rooms > 2078.00 households > 409.50 longitude <= -118.53 population > 1138.50 housing_median_age <= 28.00 median_income > 3.52 latitude > 34.26 Rules <1H OCEAN 564.0 3075.0 543.0 -121.98 1633.0 25.0 5.25 37.27 Instance values (b) Lower bounded explanation Fig. 3: The top-bars of one-sided CER plot with confidence intervals bounded by the 90th upper percentile (Fig. 3a) and the 10th lower percentile (Fig. 3b). The red solid line represents the median. The weights (and consequently the entire subplot visualizing weights) are the same for these one-sided explanations as in Fig. 2. covers about $150K, falling approximately within the interval [-$180K, -$30K]. The fact that part of the line is solid in color indicates that we can expect this feature to impact the price at least with -$30K, given the selected confidence level. Looking at the other features, we can see that all of them include the median in the uncertainty interval, meaning that with 90% confidence, these features may impact the price in both directions. Obviously, both median income and in particular housing median age are more likely to have a positive and negative impact, respectively. Since no normalization have been used with this example, all the intervals are similar in width. 5.1.2 Counterfactual Calibrated Explanations for Regression Turning to CCER, Fig. 5 shows a CCER plot for the same instance as before. Here, the solid line and the very light area behind it represent the median and the confidence interval of the calibrated prediction of the underlying model (i.e., the same as in Fig. 2). This is the ground truth that all the counterfactual feature rules should be contrasted against. Contrary to CER, none of the rules cover the instance values in the CCER plot. Instead, there are several examples of the same feature being present in multiple rules. Here the interpretation is that the solid line and lighter red bar for each rule is the expected median and confidence interval achieved if the instance would have had values according to the rule. As an example, with everything else the same but median income > 6.28, then the expected price would be ≈$405K with a confidence interval of [$340K, $490K]. It is also clear that if the house would have been located further south (latitude < 36.7), the price would go up, and if it would have been even further north (latitude > 37.6), the price would have gone down even further. So far, all examples have used a standard CPS to construct the explanations (both CER and CCER), with the result that all confidence intervals are almost equal-sized. In Fig. 6, a difficulty estimator based on the standard deviation of the targets of the k nearest neighbors is used. The normalization will both affect the calibration of the underlying model, creating confidence intervals with varying sizes between instances, 19100000 200000 300000 400000 500000 Prediction interval with 90% confidence Median prediction 150000 100000 50000 0 50000 100000 Feature weights ocean_proximity = <1H OCEAN total_bedrooms > 429.50 total_rooms > 2078.00 households > 409.50 longitude <= -118.53 population > 1138.50 housing_median_age <= 28.00 median_income > 3.52 latitude > 34.26 Rules <1H OCEAN 564.0 3075.0 543.0 -121.98 1633.0 25.0 5.25 37.27 Instance values Fig. 4: An uncertainty CER plot for the California Housing data set. The top bar is the same as in Fig. 2, showing the median and the [5th, 95th] percentiles confidence interval. In the subplot below, the uncertainty of the weights is highlighted, using the [5th, 95th] percentiles confidence interval in light red or blue for each feature. The weights still indicate how much that rule contributes to the prediction but with a confidence interval highlighting the span of uncertainty for the impact of the feature value and rule combined. 100000 200000 300000 400000 500000 Prediction interval with 90% confidence housing_median_age < 20.0 ocean_proximity = ISLAND housing_median_age > 25.0 latitude > 37.59 population < 1541.7999999999997 median_income < 5.08154 ocean_proximity = INLAND latitude < 36.734 longitude < -122.02 median_income > 6.27628 Counterfactual rules 25.0 <1H OCEAN 25.0 37.27 1633.0 5.25 <1H OCEAN 37.27 -121.98 5.25 Instance values Fig. 5: A CCER plot for the California Housing data set. The large lightest red area in the background is the confidence interval defined by the 5th and the 95th percentiles. Each row represents a counterfactual rule with an interval in darker red indicating what confidence intervals a breach according to the rule condition would result in. The confidence intervals for the counterfactual rules are also defined by the 5th and the 95th percentiles. The solid lines represent the median values. 20and the feature intervals. A crude assumption regarding the width of the feature inter- vals is that when the calibration set contains fewer instances covering an alternative feature value, the feature intervals will tend to be larger due to less information, and vice versa. This does not have to be the whole truth, as difficulty in this example is defined based on the standard deviation of the neighboring instances target values. As can be seen in Fig. 6, normalized CCER may generate rules resulting in both smaller and wider confidence intervals then the non-normalized rules. 100000 200000 300000 400000 500000 Prediction interval with 90% confidence total_bedrooms > 579.3 ocean_proximity = ISLAND housing_median_age > 25.0 latitude > 37.59 population < 1541.7999999999997 median_income < 5.08154 ocean_proximity = INLAND latitude < 36.734 longitude < -122.02 median_income > 6.27628 Counterfactual rules 564.0 <1H OCEAN 25.0 37.27 1633.0 5.25 <1H OCEAN 37.27 -121.98 5.25 Instance values Fig. 6: A normalized CCER plot comparable to Fig 5, resulting in rules with varied interval widths as a consequence of the normalization. Difficulty is estimated as the standard deviation of the targets of the k nearest neighbors. Similarly to CER, CCER can also be one-sided. Fig. 7 shows an upper-bounded explanation with 90% confidence. The interpretation of the first rule is that, with everything else as before, but median income > 6.28 the price will be below ≈$450K with 90% certainty. Since the same CPS is used, the median is still the same as for a two-sided explanation. 21100000 200000 300000 400000 500000 Prediction interval with 90% confidence housing_median_age < 20.0 ocean_proximity = ISLAND housing_median_age > 25.0 latitude > 37.59 population < 1541.7999999999997 median_income < 5.08154 ocean_proximity = INLAND latitude < 36.734 longitude < -122.02 median_income > 6.27628 Counterfactual rules 25.0 <1H OCEAN 25.0 37.27 1633.0 5.25 <1H OCEAN 37.27 -121.98 5.25 Instance values Fig. 7: A one-sided CCER plot for the California Housing data set. Confidence inter- vals are defined by the 90th upper percentile only. The interpretation is that with 90% certainty, the true value of the original instance will fall within the lightest red area. If the counterfactual rule had been true for each feature individually, the true value will fall within that feature’s darker red area with approximately 90% certainty. 5.1.3 Factual Probabilistic Calibrated Explanations for Regression P(y<=250000.00) 0.0 0.2 0.4 0.6 0.8 1.0 Probability P(y>250000.00) 0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Feature weights total_bedrooms > 429.50 households > 409.50 population > 1138.50 housing_median_age <= 28.00 latitude > 34.26 longitude <= -118.53 ocean_proximity = <1H OCEAN total_rooms > 2078.00 median_income > 3.52 Rules 564.0 543.0 1633.0 25.0 37.27 -121.98 <1H OCEAN 3075.0 5.25 Instance values Fig. 8: A regular PCER plot for the California Housing data set. The plot shows the probability of the prediction for this instance being above the given threshold ($250K in this case). The explanation is similar to a regular CEC plot with the main difference being that it shows the probabilities of being below or above the threshold and that the probabilities are given by the CPD. 22Fig. 8 shows a regular PCER plot for the same instance as above. In this plot, the possibility of querying the CPD about the probability of being below or above a given threshold is utilized. In this case, the threshold is set to a house price of $250K. Here, median income > 3.52 contributes strongly to the probability that the target is above $250K. P(y<=250000.00) 0.0 0.2 0.4 0.6 0.8 1.0 Probability P(y>250000.00) 0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Feature weights total_bedrooms > 429.50 households > 409.50 population > 1138.50 housing_median_age <= 28.00 latitude > 34.26 longitude <= -118.53 ocean_proximity = <1H OCEAN total_rooms > 2078.00 median_income > 3.52 Rules 564.0 543.0 1633.0 25.0 37.27 -121.98 <1H OCEAN 3075.0 5.25 Instance values Fig. 9: An uncertainty PCER for the same explanation as in Fig. 8. The plot includes uncertainties for the feature weights. In Fig. 9, the same explanation is shown with uncertainties. As can be seen, the size of the uncertainty varies a lot between features, depending on the calibration of the VA calibrator. 5.1.4 Counterfactual Probabilistic Calibrated Explanations for Regression Fig. 10 shows a normalized CPCER plot for the same instance. In this case, the normalization used was based on the variance of the predictions of the trees in the random forest. The most influential rule relates to median income, with a lower income increasing the probability for a lower price. The normalization will affect the feature probability estimates and confidence intervals and may consequently also result in a different ordering of rules. The final example, shown in Fig. 11, illustrates both conjunctive rules, combining two feature conditions in one rule, and normalization using the variance of the pre- dictions of the trees in the random forest. Here, the number of rules to plot has been increased to 15. Here we see that conjunctive rules often result in more influential rules than single condition rules, illustrated by the majority of rules being conjunctive. 230.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Probability of target being below 250000.00 median_income > 6.27628 ocean_proximity = NEAR BAY population > 1847.2 total_rooms < 2790.5999999999995 latitude < 36.734 housing_median_age < 20.0 longitude > -121.326 latitude > 37.59 ocean_proximity = INLAND median_income < 5.08154 Counterfactual rules 5.25 <1H OCEAN 1633.0 3075.0 37.27 25.0 -121.98 37.27 <1H OCEAN 5.25 Instance values Fig. 10: A normalized CPCER plot for the same instance as before. . 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Probability of target being below 250000.00 longitude < -122.02 & median_income < 5.08154 latitude < 36.734 & housing_median_age < 20.0 latitude < 36.734 & median_income < 5.08154 longitude > -121.326 latitude < 36.734 & ocean_proximity = INLAND latitude > 37.59 longitude < -122.02 & ocean_proximity = INLAND total_bedrooms < 487.79999999999995 & median_income < 5.08154 ocean_proximity = INLAND housing_median_age < 20.0 & ocean_proximity = INLAND total_rooms < 2790.5999999999995 & ocean_proximity = INLAND median_income < 5.08154 total_rooms < 2790.5999999999995 & median_income < 5.08154 total_bedrooms < 487.79999999999995 & ocean_proximity = INLAND housing_median_age < 20.0 & median_income < 5.08154 Counterfactual rules -121.98 5.25 37.27 25.0 37.27 5.25 -121.98 37.27 <1H OCEAN 37.27 -121.98 <1H OCEAN 564.0 5.25 <1H OCEAN 25.0 <1H OCEAN 3075.0 <1H OCEAN 5.25 3075.0 5.25 564.0 <1H OCEAN 25.0 5.25 Instance values Fig. 11: A normalized CPCER plot with conjunctive rules for the same instance as before. . Factual or counterfactual rules can be generated without normalization or with any of the normalization options available in DifficultEstimator in crepes.extras. 24Conjunctive rules can be added at any time after the explanations are generated. All the examples shown here are from the same instance and the same underlying model, to showcase a subset of available ways the proposed solutions can be used. Further examples can be found in the code repository. 5.2 Performance Evaluation Table 2 shows the results achieved regarding stability, robustness, and computational speed. Stability is measured using the mean variance when constructing explanations on the same instance using different random seeds, with lower values representing more stability. It is evident that both SHAP setups and all CE setups but PCER must be considered stable, since the mean variance is 0 (i.e., less than 1e − 31). LIME and PCER, on the other hand, has a non-negligible mean variance, meaning that they are not, in comparison, as stable. The reason for why PCER is less stable is related to the sensibility of the probabilities derived from the CPD. The reason for the sensibility is that a relatively small change in prediction can easily result in a comparably much larger change in probability for exceeding the threshold, especially if the target is close to the threshold (which is set to 0.5, i.e., the mid-point in the interval of possible target values). Explanations using the median from a CPD and explanations using the underlying model result in similar stability levels. Robustness is measured in a similar way as stability, but with a new model trained using different distributions of training and calibration instances between each run. The results achieved on robustness should be seen in relation to the variance in pre- dictions from the underlying model on the same instances. The reason is that if the predictions that the explanations are based on fluctuate, then we can expect a some- what similar degree of fluctuation in the feature weights as well, since they are defined using the predictions. The mean prediction variance is 4.7e−2. Even if all the CE setups have higher mean variance compared to LIME and SHAP (i.e., are being less robust), it is still lower compared to the mean prediction variance. Furthermore, the explana- tions produced by the CE setups do not only rely on the crisp feature weight used to measure the mean variance but also include the uncertainty interval, highlighting the degree of uncertainty associated with each feature weight. CER CER CCER PCER LIME LIME Tree SHAP Metric Var. CPS SHAP CPS Stability 0 0 0 1.9e-5 3.9e-5 3.2e-5 0 0 Robustness 4.2e-4 3.7e-4 1.9e-3 3.9e-3 8.2e-5 8.6e-5 1.4e-4 1.4e-4 Speed 0.37 0.66 0.51 4.20 3.08 3.11 0.06 0.56 Table 2: Evaluation of stability, robustness and computational speed Regarding the computational speed, it should come as no surprise that Tree SHAP (TreeExplainer), implemented in C++, is fast when applied to a tree-based model like the random forest regressor. It is about 9 times faster than when SHAP (Explainer) is applied on the median from CPD. In comparison, CER is slightly faster than SHAP CPS but clearly slower than Tree SHAP. CER is in turn about 8 times faster than LIME which takes on average around 3 seconds per instance. CER Var. 25(i.e., with normalization) should be expected to be slightly slower than CER, as the difficulty estimation requires some calculation. Also CCER, normally creating more rules than CER, is somewhat slower than CER. The slowest solution is PCER, having to calculate probabilities for all calibration instances as well as training two isotonic calibrators for each test instance. It is worth pointing out that PCER with normal- ization (not evaluated) becomes very much slower, as each calibration instance must apply a CPS. However, in the current implementation, the biased but faster solution discussed above are used for normalized PCER and CPCER. 6 Concluding Discussion This paper extends Calibrated Explanations (CE), previously introduced for classi- fication, with support for regression. Two primary use cases are identified: standard regression and probabilistic regression, i.e., measuring the probability of exceeding a threshold. The proposed solution relies on Conformal Predictive Systems (CPS), mak- ing it possible to meet the different requirements of the two identified use cases. The proposed solutions provide access to factual and counterfactual explanations with the possibility of conveying uncertainty quantification for the feature rules, just like CE for classification. In the paper, the solutions have been demonstrated using several plots, showcas- ing some of the many ways that the proposed solutions can be used. Furthermore, the paper also includes a comparison with some of the best-known state-of-the-art explanation methods (LIME and SHAP). The results demonstrate that the proposed solution for standard regression is both stable and robust. Furthermore, it is reason- ably fast, even if it cannot compete with the SHAP implementation in C++, optimized for tree models. The suggested solution is considered reliable for two reasons: 1) The calibration of the underlying model and 2) the uncertainty quantification, highlighting the degree of uncertainty of both prediction and feature weights. The solution proposed to build probabilistic explanations for regression does not share all the benefits seen for standard regression. The solution has comparable per- formance as LIME, even if it is somewhat slower than LIME. The main strength of this solution is that it provides the possibility of getting probabilistic explanations in relation to an arbitrary threshold from any standard regression model without having to impose any restrictions on the regression model. A Python implementation of the CE solution described in this paper is freely available with a BSD3-style license from: • Code repository: https://github.com/Moffran/calibrated explanations • PyPi package: https://pypi.org/project/calibrated-explanations/ • Documentation: https://calibrated-explanations.readthedocs.io/ Since it is on PyPI, it can be installed with pip install calibrated-explanations. The GitHub repository includes Python scripts to run the examples in this paper, making the results here easily replicable. The repository also includes several note- books with additional examples. This paper details calibrated-explanations as of version 0.0.24. 266.1 Future Work There are several directions for future work. Incorporating support for Mondrian CPSs, already supported by the crepes package, would be a natural first endeavor. One moti- vation for this is that Mondrian CPSs have been shown to remedy heteroscedasticity in the underlying model (e.g., often happening with ensemble models due to averaging affecting the boundery cases differently from the main mass of instances). There are room for improvement regarding the plot layout and providing addi- tional ways of visualization is a natural development in the future. This involves implementing support for explanations within image and text prediction, even if these improvements are more closely connected to classification problems. An interesting area to look into is how this technique can be adapted to expla- nations of time-series problems. How to capture and convey the dependency between different time steps pose an interesting challenge. Finally, the computational speed can probably be increased if implementing the core in C++ or by relying on fast languages being able to run Python code more effi- ciently, e.g., Mojo. The computational speed of probabilistic calibrated explanations can be dramatically improved by allowing the probability estimates of the calibra- tion set to be biased. Further evaluation of the practical impact of such a change is warranted, to better understand the implications of making that trade-off. Acknowledgments. The authors acknowledge the Swedish Knowledge Foundation and industrial partners for financially supporting the research and education environ- ment on Knowledge Intensive Product Realization SPARK at J¨onk¨oping University, Sweden. Projects: AFAIR grant no. 20200223 and PREMACOP grant no. 20220187. Helena L¨ofstr¨om is a PhD student in the Industrial Graduate School in Digital Retailing (INSiDR) at the University of Bor˚as, funded by the Swedish Knowledge Foundation, grant no. 20160035. References [1] Zhou, J., Gandomi, A.H., Chen, F., Holzinger, A.: Evaluating the quality of machine learning explanations: A survey on methods and metrics. Electronics 10(5), 593 (2021) [2] David Gunning: Explainable Artificial Intelligence. Web. DARPA (2017). https: //www.darpa.mil/attachments/XAIProgramUpdate.pdf Accessed 2019-08-29 [3] Ribeiro, M.T., Singh, S., Guestrin, C.: ”Why Should I Trust You?”: Explaining the Predictions of Any Classifier. In: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. KDD ’16, pp. 1135–1144. Association for Computing Machinery, New York, NY, USA (2016). https://doi.org/10.1145/2939672.2939778 [4] Alvarado-Valencia, J.A., Barrero, L.H.: Reliance, trust and heuristics in judgmen- tal forecasting. Computers in human behavior 36, 102–113 (2014) 27[5] Bu¸cinca, Z., Lin, P., Gajos, K.Z., Glassman, E.L.: Proxy tasks and subjective measures can be misleading in evaluating explainable ai systems. In: Proceedings of the 25th International Conference on Intelligent User Interfaces, pp. 454–464 (2020) [6] Gunning, D., Aha, D.W.: Darpa’s explainable artificial intelligence program. AI Magazine 40(2), 44–58 (2019) [7] Dimanov, B., Bhatt, U., Jamnik, M., Weller, A.: You shouldn’t trust me: Learning models which conceal unfairness from multiple explanation methods. Frontiers in Artificial Intelligence and Applications: ECAI 2020 (2020) [8] Guidotti, R., Monreale, A., Ruggieri, S., Turini, F., Giannotti, F., Pedreschi, D.: A survey of methods for explaining black box models. ACM computing surveys (CSUR) 51(5), 1–42 (2018) [9] Moradi, M., Samwald, M.: Post-hoc explanation of black-box classifiers using confident itemsets. Expert Systems with Applications 165, 113941 (2021) [10] Martens, D., Foster, P.: Explaining data-driven document classifications. MIS Quaterly 38(1), 73–100 (2014) [11] Slack, D., Hilgard, A., Singh, S., Lakkaraju, H.: Reliable post hoc explanations: Modeling uncertainty in explainability. Advances in neural information processing systems 34, 9391–9404 (2021) [12] Rahnama, A.H.A., Bostr¨om, H.: A study of data and label shift in the lime framework. arXiv preprint arXiv:1910.14421 (2019) [13] Hoffman, R.R., Mueller, S.T., Klein, G., Litman, J.: Metrics for explainable ai: Challenges and prospects. Technical report, DARPA Explainable AI Program (2018) [14] Carvalho, D.V., Pereira, E.M., Cardoso, J.S.: Machine learning interpretability: A survey on methods and metrics. Electronics 8(8), 832 (2019) [15] Adadi, A., Berrada, M.: Peeking inside the black-box: A survey on explainable artificial intelligence (xai). IEEE Access 6, 52138–52160 (2018) [16] Wang, D., Yang, Q., Abdul, A., Lim, B.Y.: Designing theory-driven user-centric explainable ai. In: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. CHI ’19, pp. 1–15. Association for Computing Machinery, New York, NY, USA (2019). https://doi.org/10.1145/3290605.3300831 . https: //doi.org/10.1145/3290605.3300831 [17] Mueller, S.T., Hoffman, R.R., Clancey, W., Emrey, A., Klein, G.: Explanation in 28human-ai systems: A literature meta-review, synopsis of key ideas and publica- tions, and bibliography for explainable ai. Technical report, DARPA Explainable AI Program (2019) [18] Agarwal, C., Krishna, S., Saxena, E., Pawelczyk, M., Johnson, N., Puri, I., Zitnik, M., Lakkaraju, H.: Openxai: Towards a transparent evaluation of model expla- nations. Advances in Neural Information Processing Systems 35, 15784–15799 (2022) [19] Bhatt, U., Antor´an, J., Zhang, Y., Liao, Q.V., Sattigeri, P., Fogliato, R., Melan¸con, G., Krishnan, R., Stanley, J., Tickoo, O., et al.: Uncertainty as a form of transparency: Measuring, communicating, and using uncertainty. In: Proceed- ings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, pp. 401–413 (2021) [20] Vovk, V.: Cross-conformal predictors. Annals of Mathematics and Artificial Intelligence 74, 9–28 (2015) [21] Platt, J., et al.: Probabilistic outputs for support vector machines and compar- isons to regularized likelihood methods. Advances in large margin classifiers 10(3), 61–74 (1999) [22] Vovk, V., Petej, I.: Venn-Abers predictors. arXiv preprint arXiv:1211.0025 (2012) [23] L¨ofstr¨om, H., L¨ofstr¨om, T., Johansson, U., S¨onstr¨od, C.: Calibrated Explanations: with Uncertainty Information and Counterfactuals (2023) [24] Letzgus, S., Wagner, P., Lederer, J., Samek, W., M¨uller, K.-R., Montavon, G.: Toward explainable artificial intelligence for regression models: A methodological perspective. IEEE Signal Processing Magazine 39(4), 40–58 (2022) [25] Vovk, V., Gammerman, A., Shafer, G.: Algorithmic Learning in a Random World. Springer, Berlin, Heidelberg (2005) [26] Vovk, V., Shen, J., Manokhin, V., Xie, M.: Nonparametric predictive distributions based on conformal prediction. Mach. Learn. 108(3), 445–474 (2019) [27] Molnar, C.: Interpretable Machine Learning, 2nd edn. Leanpub, ??? (2022). https: //christophm.github.io/interpretable-ml-book [28] Mothilal, R.K., Sharma, A., Tan, C.: Explaining machine learning classifiers through diverse counterfactual explanations. In: Proceedings of the 2020 Confer- ence on Fairness, Accountability, and Transparency, pp. 607–617 (2020) [29] Guidotti, R.: Counterfactual explanations and how to find them: literature review and benchmarking. Data Mining and Knowledge Discovery, 1–55 (2022) [30] Wachter, S., Mittelstadt, B., Russell, C.: Counterfactual explanations without 29opening the black box: Automated decisions and the gdpr. Harv. JL & Tech. 31, 841 (2017) [31] L¨ofstr¨om, H., Hammar, K., Johansson, U.: A meta survey of quality evaluation cri- teria in explanation methods. In: De Weerdt, J., Polyvyanyy, A. (eds.) Intelligent Information Systems, pp. 55–63. Springer, Cham (2022) [32] Alvarez-Melis, D., Jaakkola, T.S.: On the robustness of interpretability methods. arXiv preprint arXiv:1806.08049 (2018) [33] Lundberg, S.M., Lee, S.-I.: A unified approach to interpreting model predic- tions. In: Proceedings of the 31st International Conference on Neural Information Processing Systems, pp. 4768–4777 (2017) [34] Ribeiro, M.T., Singh, S., Guestrin, C.: ”why should i trust you?” explaining the predictions of any classifier. In: Proceedings of the 22nd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining, pp. 1135–1144 (2016) [35] Ribeiro, M.T., Singh, S., Guestrin, C.: Anchors: High-precision model-agnostic explanations. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol. 32 (2018) [36] Vovk, V., Shafer, G., Nouretdinov, I.: Self-calibrating probability forecasting. In: Advances in Neural Information Processing Systems, pp. 1133–1140 (2004) [37] Lambrou, A., Nouretdinov, I., Papadopoulos, H.: Inductive venn prediction. Annals of Mathematics and Artificial Intelligence 74(1), 181–201 (2015) [38] Bostr¨om, H.: crepes: a python package for generating conformal regressors and predictive systems. In: Johansson, U., Bostr¨om, H., An Nguyen, K., Luo, Z., Carlsson, L. (eds.) Proceedings of the Eleventh Symposium on Conformal and Probabilistic Prediction and Applications. Proceedings of Machine Learning Research, vol. 179. PMLR, ??? (2022) [39] Pace, R.K., Barry, R.: Sparse spatial autoregressions. Statistics & Probability Letters 33(3), 291–297 (1997) 30