arXiv:2308.16059v1 [stat.ML] 30 Aug 2023 A Parameter-Free Two-Bit Covariance Estimator with Improved Operator Norm Error Rate Junren Chen∗, Michael K. Ng† August 31, 2023 Abstract A covariance matrix estimator using two bits per entry was recently developed by Dirksen, Maly and Rauhut [Annals of Statistics, 50(6), pp. 3538-3562]. The estimator achieves near minimax rate for general sub-Gaussian distributions, but also suﬀers from two downsides: the- oretically, there is an essential gap on operator norm error between their estimator and sample covariance when the diagonal of the covariance matrix is dominated by only a few entries; prac- tically, its performance heavily relies on the dithering scale, which needs to be tuned according to some unknown parameters. In this work, we propose a new 2-bit covariance matrix estimator that simultaneously addresses both issues. Unlike the sign quantizer associated with uniform dither in Dirksen et al., we adopt a triangular dither prior to a 2-bit quantizer inspired by the multi-bit uniform quantizer. By employing dithering scales varying across entries, our estimator enjoys an improved operator norm error rate that depends on the eﬀective rank of the under- lying covariance matrix rather than the ambient dimension, thus closing the theoretical gap. Moreover, our proposed method eliminates the need of any tuning parameter, as the dithering scales are entirely determined by the data. Experimental results under Gaussian samples are provided to showcase the impressive numerical performance of our estimator. Remarkably, by halving the dithering scales, our estimator oftentimes achieves operator norm errors less than twice of the errors of sample covariance. 1 Introduction Given i.i.d. samples X1, ..., Xn from a zero-mean random vector X, a fundamental problem in multivariate analysis is to accurately estimate the covariance matrix Σ := E(XX⊤), which fre- quently arises in principle component analysis [22], regression analysis [16], ﬁnance [25], massive MIMO system [28], and so on. Despite the vast literature on covariance estimation, we study in this paper a less well-understood setting where we can only access samples coarsely quantized ∗J. Chen is with Department of Mathematics, The University of Hong Kong. The work was done when he was a visiting Ph.D. student at School of Computing, National University of Singapore. (Corresponding author. e-mail: chenjr58@connect.hku.hk) †M. K. Ng is with Department of Mathematics, Hong Kong Baptist University. (e-mail: michael-ng@hkbu.edu.hk) 1to a small number of bits. This setting is of particular interest in signal processing or dis- tributed learning where it is expensive or even impossible to acquire or transmit high-precision data (e.g., [3,6,18,28,38,39]). Deﬁning the ψ2 norm and L2 norm of a random variable X respec- tively as ∥X∥ψ2 = inf{t > 0 : E(exp(X2 t2 )) ≤ 2} and ∥X∥L2 = (E[X2])1/2, we follow prior works (e.g., [24,27,35,36]) and formulate sub-Gaussian random vector X in Deﬁnition 1. Deﬁnition 1. A d-dimensional random vector X is K-sub-Gaussian if ∥⟨X, v⟩∥ψ2 ≤ K∥⟨X, v⟩∥L2, ∀ v ∈ Rd. (1.1) For given K, we use the shorthand ˆK := K√log K throughout this work. In this paper, we propose a 2-bit covariance estimator1 for general sub-Gaussian X and establish the non-asymptotic guarantee, which indicates the required sample size for achieving operator norm error lower than some desired accuracy. 1.1 Related Works Under a direct 1-bit sign quantizer, many works have investigated covariance estimation from {sgn(Xi)}n i=1 under the name of "arcsin-law" (e.g., [5, 19, 34, 36]), but it is impossible to estimate the full covariance in this case as the magnitudes of the samples are completely lost.2 For estimating the full covariance Σ, as we pursue in this paper, one must introduce non-zero thresholds τi ∈ Rn called "dither" before the 1-bit quantization and collect sgn(Xi + τi) instead. Such a technique known as "dithering", with its aid on signal reconstruction observed in engineering works [20,26,29] and theory established in [17,30], recently again received much attention from researchers working on various estimation problems like compressed sensing, covariance estimation, matrix completion and reduced-rank regression (e.g., [2–4,7,10,11,23,32,37]). Speciﬁcally, for covariance estimation, some works proposed to use random Gaussian dithering [12–14] or deterministic ﬁxed dithering (e.g., [1,15]), but to our knowledge, these two lines of works are restricted to Gaussian samples and does not establish non-asymptotic result, thus provide little guarantee for the practical ﬁnite-sample setting. We will not further explain these works since our interest is on estimators for sub-Gaussian distributions with non-asymptotic guarantee. The work most relevant to ours is Dirksen et al. [10] who developed the ﬁrst 2-bit covariance estimator that applies to sub-Gaussian samples and enjoys non-asymptotic near minimax error rate. Let {τi1, τi2}n i=1 ∼ U [−λ, λ]d be i.i.d. uniform dithers independent of the K-sub-Gaussian {Xi}n i=1, they proposed to quantize the i-th sample Xi to ˙Xi1 = λ·sgn(Xi+τi1) and ˙Xi2 = λ·sgn(Xi +τi2), thus only retaining two bits per entry. Based on the observation E( ˙Xi1 ˙X⊤ i2) = Σ that holds if 1Throughout this work, 2-bit estimator refers to the one that only relies on two bits from each entry of {Xi}n i=1. 2Indeed, we can only hope to estimate the correlation matrix with all-ones diagonal, see [10, Sec. 1.2]). 2λ ≥ ∥Xi∥∞ [10, Lem. 15], the 2-bit estimator in [10] is deﬁned as �Σna = 1 2n n � i=1 ( ˙Xi1 ˙X⊤ i2 + ˙Xi2 ˙X⊤ i1), with    ˙Xi1 = λ · sgn(Xi + τi1) ˙Xi2 = λ · sgn(Xi + τi2) . (1.2) The non-masked case of [10, Thm. 4] implies the error rate that is near minimax optimal up to logarithmic factors [8]. Theorem 1. Suppose that X1, ..., Xn are i.i.d. copies of the zero-mean K-sub-Gaussian ran- dom vector X ∈ Rd, we consider the 2-bit estimator �Σna in (1.2) with uniform dithers τi1, τi2 ∼ U [−λ, λ]d. If λ2 = C(K) log n∥Σ∥∞ for some large enough constant C(K) depending on K, then it holds with probability at least 1 − 2d−10 that ∥ �Σna − Σ∥op ≲K � d∥Σ∥op∥Σ∥∞ log d(log n)2 n + d∥Σ∥∞ log n log d n . (1.3) The novel estimator in (1.2) represents a signiﬁcant progress in covariance estimation, which has already sparked interest and led to subsequent research endeavors: Chen et al. extended it to high- dimensional sparse case and heavy-tailed samples in [3, Sec. II], then also developed a multi-bit covariance estimator for heavy-tailed distribution with near minimax non-asymptotic guarantees in [2, Sec. 3.1]; Yang et al. [38] extended it to the complex domain and applied to massive MIMO system; more recently, Dirksen and Maly proposed its tuning-free version by using data-driven dithering [9]. 1.2 Two Downsides of the Estimator Due to Dirksen et al. Nonetheless, the estimator �Σna also suﬀers from two frustrating downsides. Theoretical Gap. Although the rate in (1.3) is near minimax over certain set of covariance matrices, it is essentially sub-optimal for covariance matrices whose diagonals are dominated by only a few entries, as described by Tr(Σ) ≪ d∥Σ∥∞. Indeed, for the behaviour of the sample covariance �Σ = 1 n �n i=1 XiX⊤ i on sub-Gaussian Xi, the operator norm error rate O �∥Σ∥op �� r(Σ) n + r(Σ) n �� depending on the eﬀective rank r(Σ) := Tr(Σ) ∥Σ∥op rather than the ambient dimension d has been established in the literature, see for instance [27, Prop. 3], [24, Coro. 2], [36, Thm. 9.2.4]; this rate is tight up to multiplicative constant for Gaussian samples [24, Thm. 4] and near optimal in a minimax sense [27, Thm. 2]. To be more precise, we present Theorem 2 which provides an incremental extension of [36, Thm. 9.2.4] towards independent samples that may not be identically distributed (this will be useful in the proof of our main theorem); it also slightly reﬁnes the dependence on K by the recent results from [21]. Theorem 2. Suppose that X1, ..., Xn are independent, zero-mean, K-sub-Gaussian d-dimensional random vectors sharing the same covariance matrix Σ ∈ R. Let r(Σ) := Tr(Σ) ∥Σ∥op , then for any u ≥ 0, 3with probability at least 1 − 3e−u we have ����� 1 n n � i=1 XiX⊤ i − Σ ����� op ≤ C∥Σ∥op   � ˆK2(r(Σ) + u) n + ˆK2(r(Σ) + u) n   . (1.4) Proof. The proof can be founded in Section 5. Therefore, the sample covariance satisﬁes an operator norm rate O �� Tr(Σ)∥Σ∥op n + Tr(Σ) n � that is essentially tighter than (1.3) when Tr(Σ) ≪ d∥Σ∥∞. To our knowledge, there has not been any attempt to close this gap between 2-bit covariance and the sample covariance based on full data. Tuning Parameter. More practically, while the behaviour of �Σna heavily relies on a suitable dithering scale λ, this parameter is in general hard to tune, e.g., its theoretical choice depends on K, ∥Σ∥∞. To alleviate this issue, a recent pre-print [9] proposed a tuning-free variant of �Σna that uses data-driven dithering to get rid of the dependence of dithering scales on ∥Σ∥∞, as contrasted to �Σna that is non-adaptive to the data. Particularly, they add an additional sample X0 and calculate λi0 = 1 i �i−1 k=1 ∥Xk∥∞, and then use a uniform dither with scale λi = C(K)λi0 √log i for the i-th sample Xi before the 1-bit quantization, where C(K) is a ﬁxed constant depending on K (but not on ∥Σ∥∞). More precisely, let {τi1, τi2}n i=1 iid ∼ U [−1, 1]d be independent of {Xi}n i=0, they collect ˙Xi1 = λi · sgn(Xi + λiτi1) and ˙Xi2 = λi · sgn(Xi + λiτi2) from each Xi, and then construct the estimator �Σa = 1 2n n � i=1 ( ˙Xi1 ˙X⊤ i2 + ˙Xi2 ˙X⊤ i1) analogously to (1.2).3 The main theorem of [9] implies that ∥ �Σa − Σ∥op ≲K � d∥Σ∥∞∥Σ∥op(log d)7(log n)2 n + d∥Σ∥∞(log d)4 log n n holds with probability at least 1 − 2d−10, which coincides with (1.3) up to logarithmic factors. Nonetheless, �Σa does not fully address the tuning issue since the unknown multiplicative constant still depends on K that could vary among diﬀerent distributions. Besides, as will be shown by the numerical examples below, �Σa does not perform well empirically. 1.2.1 Numerical Examples We pause to provide numerical results to demonstrate the aforementioned weaknesses of existing 2-bit estimators. We ﬁx (n, d) = (500, 10) and test Gaussian samples Xi ∼ N(0, Σ), for which the dithering scale is λ = C√log n in �Σna and λ = C � 1 i �i−1 k=1 ∥Xk∥∞ �√log i in �Σa. Based on 50 repetitions, we plot the curves of "operator norm error v.s. C" in Figure 1, where we also include the results of sample covariance �Σ and a new (non-adaptive) 2-bit estimator that we propose in 3Compared to �Σna, this estimator requires storing the additional {λi}n i=1 as 32-bit data, while such memory is relatively minor compared to the savings of quantization [9, Rem. 2]; this remark also applies to the our main estimator �Σ in Section 3 that additionally requires d full-precision scalars. 4Section 2. Let Σ(a, b, c) = (a − b)Id + b11⊤ + (c − a)e1e⊤ 1 be the covariance matrix with diagonal being [c, a, ..., a] and non-diagonal entries being b, we test the low-correlation case Σ = Σ(1, 0.2, 1) in Figure 1(a) and the high-correlation case Σ = Σ(1, 0.9, 1) in Figure 1(b). Further, we test Σ(1, 0.2, 10) (that changes the (1, 1)-th entry of Σ(1, 0.2, 1) to 10) in Figure 1(c) to simulate the setting of Tr(Σ) ≪ d∥Σ∥∞. Comparing (a) and (c) in Figure 1 clearly corroborates the performance gap between �Σna under Tr(Σ) ≪ d∥Σ∥∞ and �Σ and the dependence of "optimal C" (for �Σna) on ∥Σ∥∞. Consistent with the theoretical progress made by [9], the optimal C for �Σa roughly remain in [0.3, 0.4], but its numerical performance is in general much worse than �Σna, especially when the correlations are high or Tr(Σ) ≪ d∥Σ∥∞ (Figure 1(b)-(c)). We conjecture that this numerical degradation stems from the scaling √log i in the λi, which may facilitate theoretical analysis but could make the dithering scale of Xi1 considerably larger than Xi2 if i1 ≫ i2; this may not be sensible since there is no reason to believe that the entries of Xi1 are of magnitudes greater than those of Xi2. 0 0.5 1 1.5 0 0.5 1 1.5 2 2.5 3 3.5 4 (a) 0 0.5 1 1.5 2 0 1 2 3 4 5 6 0 0.5 1 1.5 2 2.5 0 2 4 6 8 10 (a) (b) (c) Figure 1: The curves of "operator norm error v.s. C", with the optimal C and the corresponding minimum error reported in the labels. 1.3 Our Contributions In this work, we develop a new 2-bit covariance estimator to address the above two issues. Our quantization procedure is an essential departure from [9,10] in the sense that we use triangular dither and a 2-bit quantizer reduced from the multi-bit uniform quantizer. Indeed, our main estimator is built upon a new non-adaptive 2-bit estimator �Σ2b inspired by the multi-bit estimator developed in our prior work [2]; compared to �Σna, this new 2-bit estimator has comparable theoretical guarantee and numerically performs better over Gaussian samples. Then, we modify the proposed �Σ2b by using dithering scales varying across diﬀerent entries of Xi that are entirely speciﬁed by the given data. This not only remove the tuning parameter but also allows for a tight operator norm error bound — as shown by our main theorem that presents a high-probability operator norm error rate O �∥Σ∥op �� r(Σ)(log d)2 n + r(Σ)(log d)2 n ��, which matches Theorem 2 for sample covariance up to (a 5small number of) logarithmic factors and hence closes the theoretical gap. We conclude this section by ﬁxing notations and providing an overview. Notations. We use regular letters to denote scalars, and boldface symbols are for vectors and matrices. For vector a = (ai) ∈ Rd we let ∥a∥∞ = max1≤i≤d |ai| and ∥a∥2 = (�d i=1 a2 i )1/2; for matrix A = (aij) ∈ Rd×d we let ∥A∥op be the operator norm and ∥A∥∞ = maxi,j∈[d] |aij|. Given symmetric A, B ∈ Rd×d, we write A ⪯ B if B − A is positive semi-deﬁnite. Recall that ∥X∥ψ2 has been deﬁned for a random variable X (before Deﬁnition 1), now we further deﬁne ∥X∥ψ2 := supv∈Sd−1 ∥⟨v, X⟩∥ψ2 for a random vector X ∈ Rd, where Sd−1 = {v ∈ Rd : ∥v∥2 = 1} is the standard Euclidean sphere, ⟨a, b⟩ = a⊤b is the inner product. We use C, c, C1, C2, ... to denote absolute constants whose value may vary from line to line. We denote multivariate Gaussian distribution with covariance matrix Σ by N(0, Σ), and the uniform distribution over W ⊂ Rd by U (W). Given two terms T1, T2, we write T1 ≲ T2 or T1 = O(T2) if T1 ≤ CT2 for some C, and conversely write T1 ≳ T2 or T1 = Ω(T2) if T1 ≥ cT2 for some c. We may also use notations like ≲K, ≳K to indicate that the implied constant may depend on K. We use T1 ≍ T2 to state that T1 = O(T2), T1 = Ω(T2) simultanesouly hold. In this work, the quantizers operate on vectors in an element-wise manner. In our problem setting, we let Xij be the j-th entry of the sample Xi, Σij be the (i, j)-th entry of the underlying covariance Σ. Overview. We propose a new 2-bit estimator �Σ2b in Section 2 that is comparable to �Σna. Our primary contribution, a parameter-free estimator with improved error rate, is presented in Section 3, where we also provide numerical examples to validate our theory. The proof of our main theorem (Theorem 5) is given in Section 4. We collect the deferred proofs in Section 5. The paper is concluded with some discussions in Section 6. 2 A New 2-bit Covariance Matrix Estimator 2.1 A Multi-Bit Covariance Estimator Using the uniform quantizer associated with triangular dithers, a multi-bit covariance estimator was developed in [2, Sec. 3.1]. We shall introduce this estimator (denoted by �Σmb) ﬁrst in order to inspire the new 2-bit estimator �Σ2b. We need some necessary preliminaries of the dithered uniform quantizer to get started. The (multi-bit) uniform quantizer with resolution λ is deﬁned for a ∈ R as Qλ(a) = λ(⌊ a λ⌋ + 1 2). It is now well-understood that dithering prior to uniform quantization beneﬁts signal recovery or parameter from diﬀerent aspects (e.g., [2, 4, 23, 31, 32, 37]); throughout this work, the dithering vectors (i.e., dithers) τi's are independent of everything else and have independent entries.4 Under uniform quantizer Qλ(·), while a uniform dither τi ∼ U [− λ 2 , λ 2]d is adopted in most prior works, the triangular dither given by the sum of two independent uniform dither τi ∼ U [− λ 2, λ 2]d +U [− λ 2 , λ 2]d was adopted in [2,4] to allow for covariance estimation. Our subsequent developments hinge on the 4For data-driven dithers with dithering scale depending on Xi's, we will describe the procedure as drawing τi's with constant scale ﬁrst and then rescaling the dithers (e.g., our description for �Σa above). 6following properties of the dithered quantizer Qλ(· + τ) with triangular dither. Lemma 1. For random or deterministic X ∈ Rd and some λ > 0, we let τ ∼ U [− λ 2, λ 2]d + U [− λ 2 , λ 2]d be independent of X and quantize X to ˙X = Qλ(X + τ). Then the quantization noise ξ = ˙X − X satisﬁes ∥ξ∥∞ ≤ 3λ 2 , E[ξ] = 0, ∥ξ∥ψ2 = O(λ), E(ξξ⊤) = λ2 4 Id. Proof. The proof can be found in Section 5. Lemma 1 allows for a covariance estimator based on Qλ(·+τ) with triangular dither τ. Specif- ically, we let {τi}n i=1 iid ∼ U [− λ 2, λ 2]d + U [− λ 2 , λ 2]d be independent of {Xi}n i=1, and then quantize Xi to ˙Xi = Qλ(Xi + τi). Deﬁne ξi := ˙Xi − Xi, then we have E( ˙Xi ˙X⊤ i ) = E(XiX⊤ i ) + E(ξiξ⊤ i ) + E(Xiξ⊤ i ) + E(ξiX⊤ i ) (i) = Σ + λ2 4 Id, (2.1) where we use E(ξiξ⊤ i ) = λ2 4 Id and Eξi|Xi(ξi) = 0 from Lemma 1 in (i). Equation (2.1) motivates the estimator [2] �Σmb = 1 n n � i=1 ˙Xi ˙X⊤ i − λ2 4 Id, with ˙Xi = Qλ(Xi + τi). (2.2) Note that [2, Sec. 3.1] focused on heavy-tailed Xi (assumed to have bounded fourth moments) and overcame the heavy-tailedness by incorporating an additional truncation step before the dithered quantization. As a result, it was mentioned in Remark 1 therein that "the results for sub-Gaussian distributions can be analogously established and are also new to the literature." To proceed, we present here the sub-Gaussian counterpart of [2, Thm. 3]. Theorem 3. Suppose that X1, ..., Xn are i.i.d. copies of the zero-mean K-sub-Gaussian random vector X ∈ Rd. Given λ > 0, using the triangular dithers τi ∼ U [− λ 2, λ 2]d + U [− λ 2, λ 2]d, the estimator �Σmb in (2.2) satisﬁes �� �Σmb − Σ �� op ≤ C � ∥Σ∥op + λ2 4 �   � ˆK2�r(Σ + λ2 4 Id) + u � n + ˆK2�r(Σ + λ2 4 Id) + u � n   with probability at least 1 − 3e−u, where r(Σ + λ2 4 Id) = Tr(Σ)+λ2d/4 ∥Σ∥op+λ2/4 is the eﬀective rank. Proof. In order to invoke Theorem 2 to bound ∥ �Σmb − Σ∥op, we ﬁrst write �Σmb − Σ = 1 n n � i=1 ˙Xi ˙X⊤ i − � Σ + λ2 4 Id � . We let ξi = Qλ(Xi +τi)−Xi be the quantization noise, then Lemma 1 gives ∥ξi∥ψ2 = O(λ). Thus, for any v ∈ Rd we have ∥⟨ ˙Xi, v⟩∥ψ2 ≤ ∥⟨Xi, v⟩∥ψ2 + ∥⟨ξi, v⟩∥ψ2 ≤ K∥⟨Xi, v⟩∥ψ2 + O(λ∥v∥2). (2.3) 7Note that (2.1) yields ∥⟨ ˙Xi, v⟩∥L2 = � v⊤ E[ ˙Xi ˙X⊤ i ]v = Ω(∥⟨Xi, v⟩∥L2) + Ω(λ∥v∥2), which together with (2.3) implies ∥⟨ ˙Xi, v⟩∥ψ2 = O(K + 1)∥⟨ ˙Xi, v⟩∥L2. Since we must have K = Ω(1), ˙Xi is O(K)-sub-Gaussian. The result follows by invoking Theorem 2. 2.2 A New 2-Bit Covariance Estimator Having introduced the multi-bit estimator �Σmb, we propose a new 2-bit estimator by properly restricting the number of bits needed for the uniform quantizer. For the quantization of a scalar a, we ﬁrst note that the uniform quantizer Qλ(·) reduces to the 1-bit quantizer sgn(·) up to a scaling if the resolution λ dominates the signal magnitude: Qλ(a) = λ 2 sgn(a), when |a| < λ. This remains true when the quantizer is associated with a uniform dither τ ∼ U [− λ 2, λ 2]: Qλ(a + τ) = λ 2 sgn(a + τ), when |a| < λ 2. Nonetheless, such relation between Qλ(·) and sgn(·) is no longer valid if a triangular dither τ ∼ U [− λ 2 , λ 2] + U [− λ 2 , λ 2] is adopted. Actually, as a triangular dither takes value on [−λ, λ], for a non-constant signal a, the dithered signal a + τ falls on more than two bins of Qλ(·) with positive probability, making it impossible to reduce Qλ(·) to 1-bit quantization. In fact, when τ ∼ U [− λ 2 , λ 2] + U [− λ 2, λ 2], Qλ(·) reduces to a 2-bit quantizer when signal magnitude is bounded by λ: Qλ(a + τ) = Qλ,2b(a + τ), when |a| < λ, (2.4) where Qλ,2b(·) is deﬁned as Qλ,2b(a) := −3λ 2 1(a < −λ) − λ 2 1(−λ ≤ a < 0) + λ 2 1(0 ≤ a < λ) + 3λ 2 1(a ≥ λ). (2.5) In essence, (2.4) states that under triangular dither τi, Qλ(· + τi) and Qλ,2b(· + τi) are close (or the same) if the signal magnitude is dominated by (or exactly bounded by) λ. This inspires us to modify �Σmb in (2.2) and propose the following new non-adaptive 2-bit covariance matrix estimator �Σ2b: �Σ2b = 1 n n � i=1 ˙Xi ˙X⊤ i − λ2 4 Id, with ˙Xi = Qλ,2b(Xi + τi). (2.6) Theorem 4. In the setting of Theorem 3, we let λ2 = CK2∥Σ∥∞ log(nd) with some large enough 8C. Then with probability at least 1 − 10(nd)−10, the 2-bit estimator �Σ2b satisﬁes ∥ �Σ2b − Σ∥op ≲ � ˆK2d∥Σ∥∞∥Σ∥op(log(nd))2 n + ˆK2d∥Σ∥∞ log(nd) n . (2.7) Proof. Our strategy is to show the two events "∥ �Σmb − Σ∥op is bounded as in (2.7)" (denoted by E1) and " �Σmb = �Σ2b" (denoted by E2) simultaneously hold with the promised probability. First, by invoking Theorem 3 with u = Tr(Σ) + λ2d/4 ∥Σ∥op + λ2/4 (i) = Ω(log(nd)) (here, (i) follows from ∥Σ∥op ≤ d∥Σ∥∞) and performing some algebra, we obtain that with proba- bility at least 1 − 3(nd)−10, ∥ �Σmb − Σ∥op ≲ � ˆK2d∥Σ∥∞∥Σ∥op log(nd) �1 + ∥Σ∥∞∥Σ∥−1 op log(nd) � n + ˆK2d∥Σ∥∞ log(nd) n . (2.8) By ∥Σ∥∞ ≤ ∥Σ∥op, this implies E1. Second, we note that E2 holds if Qλ(Xi + τi) = Qλ,2b(Xi + τi) for any i ∈ [n], and from (2.4) this can be implied by λ ≥ max1≤i≤n ∥Xi∥∞ = maxi,j |Xij|. As Xi is K-sub-Gaussian, for any (i, j) we have ∥Xij∥ψ2 ≤ K �Σjj ≤ K � ∥Σ∥∞. Thus, for any t ≥ 0 we have P � |Xij| ≥ t � ≤ 2 exp � − C1t2 K2∥Σ∥∞ � . Taking a union bound yields P � max i,j |Xij| ≥ t � ≤ 2nd exp � − C1t2 K2∥Σ∥∞ � . Letting t = C2K � ∥Σ∥∞ log(nd) with large enough C2, we obtain maxi,j |Xij| = O(K � ∥Σ∥∞ log(nd)) with probability at least 1 − 3(nd)−10. Under our choice λ2 = CK2∥Σ∥∞ log(nd) with suﬃciently large C, E2 thus holds with the promised probability. The proof is complete. Despite the distinct quantization processes and constructions, our new 2-bit estimator �Σ2b is comparable to �Σna in [10] in the following senses: 1) they both rely on two bits per entry; 2) they enjoy similar non-asymptotic operator norm error rate (comparing (1.3) and (2.7)); 3) they both involve a tuning parameter of the dithering scale λ that should be tuned according to (∥Σ∥∞, K). Thus, like �Σna, our �Σ2b still suﬀers from the theoretical gap when Tr(Σ) ≪ d∥Σ∥∞ and the tuning issue; this can be seen by comparing (a) and (c) of Figure 1, where we also include �Σ2b with λ = C√log n for comparison. On the other hand, in all three numerical examples �Σ2b notably outperforms �Σna, which suggests that our new 2-bit estimator maybe preferable for Gaussian 9samples. 3 A Parameter-Free Estimator with Improved Error Rate As an intuitive explanation on the gap between �Σna, �Σ2b and the full-data-based sample covariance, it was written on [10, P. 3544] that "in order to accurately estimate all diagonal entries of Σ, the (maximal) dithering level λ needs to be on the scale ∥Σ∥∞; if Tr(Σ) ≪ d∥Σ∥∞, then most of the diagonal entries are much smaller than ∥Σ∥∞, and hence λ is on a sub-optimal scale for these entries." In a nutshell, the common dithering scale λ should be large enough to accommodate the large entries and hence could be highly sub-optimal for other small entries. While [9] deployed data-driving dithering, the dithering scale remains the same across entries of a speciﬁc sample, thus their estimator suﬀers from the same gap. Indeed, as also reﬂected by the discussion above, a possible strategy to overcome the limitation is to adopt dithering scales adaptive to diﬀerent entries of Xi. Along this idea, we propose in this section our main estimator that simultaneously resolves the issues of sub-optimal rate and tuning parameter. Speciﬁcally, let λj be the dithering scale for the j-th entry {Xij : i ∈ [n]}, we will quantize Xij to ˙Xij = Qλj,2b(Xij + λjτij) with (normalised) triangular dither τij ∼ U [− 1 2, 1 2] + U [− 1 2, 1 2]. We comment that a good λj should represent a reasonable trade-oﬀ between bias and variance. In particular, (2.4) indicates that using λj ≪ |Xij| induces large bias between Qλj,2b(·) and Qλj(·) (and hence between our 2-bit estimator and its unbiased multi-bit counterpart), while λj ≫ |Xij| also results in slow concentration (e.g., reﬂected by ∥ξ∥ψ2 = O(λ) in Lemma 1). Fortunately, as we will see, the tightest unbiased choices λj,min = max 1≤i≤n |Xij|, j = 1, ..., d (3.1) are suﬃcient for closing the theoretical gap. Additionally, such λj,min is speciﬁed by the given data, so (3.1) also completely addresses the tuning issue. Now we are ready to precisely propose our estimator. Let the triangular dithers {τi}n i=1 iid ∼ U [− 1 2, 1 2]d + U [− 1 2, 1 2]d be independent of {Xi}n i=1, Λ = diag(λ1,min, ..., λd,min) where λj,min is given in (3.1), then we deﬁne QΛ,2b(·) := (Qλ1,min,2b(·), ..., Qλd,min,2b(·))⊤, which element-wisely quantizes Xi to ˙Xi = QΛ,2b(Xi + Λτi) : = � Qλ1,min,2b(Xi1 + λ1,minτi1), ..., Qλd,min,2b(Xid + λd,minτid) �⊤ (i) = � Qλ1,min(Xi1 + λ1,minτi1), ..., Qλd,min(Xid + λd,minτid) �⊤ ; (3.2) note that (i) follows from (2.4) and (3.1). Then, as will be shown in the proof of Theorem 5 by 10Lemma 1, we have E( ˙Xi ˙X⊤ i ) = Σ + 1 4Λ2, hence we propose our main estimator as �Σ = 1 n n � i=1 ˙Xi ˙X⊤ i − 1 4Λ2, with ˙Xi = QΛ,2b(Xi + Λτi). (3.3) Our parimary theoretical result is given as below. When omitting dependence on K and some factors of log d, there is no diﬀerence between our (3.4) for �Σ and (1.4) for sample covariance. Theorem 5. Suppose that X1, ..., Xn are i.i.d. copies of the zero-mean K-sub-Gaussian random vector X ∈ Rd. For estimating Σ = E(XX⊤) with eﬀective rank r(Σ) = Tr(Σ) ∥Σ∥op , under the scaling of n = Ω( ˆK2 log d), max{n, K} = O(d10), the proposed estimator �Σ satisﬁes ∥ �Σ − Σ∥op ≲ K2∥Σ∥op   � r(Σ)(log d)2 n + r(Σ)(log d)2 n   (3.4) with probability at least 1 − 10d−10 − 10e−10r(Σ). Speciﬁcally, if assuming further the scaling n ≳ r(Σ)(log d)2, then ∥ �Σ − Σ∥op ≲ K2 � Tr(Σ)∥Σ∥op(log d)2 n . (3.5) Proof. The key challenge in the proof is that the actual dithering vectors Λτi's depend on {Xi}n i=1 through the quantities {λj,min}d j=1; thus, when simultaneously handling the randomness of {Xi, τi}n i=1, { ˙Xi ˙X⊤ i }n i=1 are no longer i.i.d. matrices but correlated in a rather delicate man- ner, making the simple arguments in Theorem 3 invalid. To get started, we must decompose the quantized sample into ˙Xi = Xi + ξi, and recall that the properties of the quantization noise ξi are developed in Lemma 1. This allows us to divide the estimation error into three pieces as in (4.2). To bound each piece separately, we often need to ﬁrst deal with the randomness of {τi}n i=1 by conditioning on {Xi}n i=1 to render "independence", and then deal with {Xi}n i=1 that are again independent. In addition, we need to carefully take the technical tools of Theorem 2 or matrix Bernstein's inequality (Lemma 2) to allow for a sharp rate depending on the eﬀective rank r(Σ) instead of d (note that some other approaches like covering argument would be insuﬃcient). It is also worth mentioning that we put a bit eﬀorts on reﬁning the dependence on sub-Gaussian parameter K via the recent results of [21]. The complete proof is provided in Section 4. 3.1 Numerical Simulations In our simulations, we compare the operator norm errors of �Σna in (1.2), our new 2-bit estimator �Σ2b in (2.6), the sample covariance �Σ based on full data, and our parameter-free estimator �Σ in (3.3). Recall that in �Σ, our choice of dithering scales in (3.1) is unbiased and do not balance the bias and concentration. Though this is suﬃcient for closing the theoretical error rate gap, numerically it may be preferable to slightly lower the dithering scales to allow for a better trade-oﬀ between bias and variance. To validate this over Gaussian samples, let we shrink the dithering scale by a 11commonly factor s and consider the estimator �Σ = 1 n n � i=1 ˙Xi ˙X⊤ i − 1 4(sΛ)2, with ˙Xi = QsΛ,2b(Xi + sΛτi), (3.6) and we include �Σ(0.9), �Σ(0.7), �Σ(0.5) for comparison. Note that �Σ(1) = �Σ. The non-adaptive estimators �Σna, �Σ2b require a tuning parameter λ = C√log n for some C depending on (K, ∥Σ∥∞), and we provide the (near) optimal C numerically found in the case of (n, d) = (500, 10) (e.g., from Figure 1(a), under Σ(1, 0.2, 1) we take C = 0.7 for �Σna, C = 0.65 for �Σ2b), but note that it is hard to tune C to such extent in practice. In contrast, our key estimator �Σ does not require any parameter, and its more "balanced" version �Σ(s) is also user-friendly in that one can manually set s as a constant moderately less than 1 (e.g., s = 0.9, 0.7, 0.5 tested here). We will use Xi ∼ N(0, Σ) and obtain each data point from 50 repetitions. 200 400 600 800 1000 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 (a) 200 400 600 800 1000 0 1 2 3 4 5 6 (b) 200 400 600 800 1000 0 2 4 6 8 10 12 14 (c) 10 20 30 40 50 0 0.5 1 1.5 2 (d) 10 20 30 40 50 0 1 2 3 4 5 6 (e) 10 20 30 40 50 2 4 6 8 10 12 14 (f) Figure 2: The curves of "operator norm error v.s. n" and "operator norm error v.s. d". We ﬁrst ﬁx d = 10 and test n = 100 : 100 : 1000, with the results over Σ = Σ(1, 0.2, 1), Σ(1, 0.2, 10), Σ(1, 0.2, 25) reported in Figure 2(a)-(c), respectively. For Σ = Σ(1, 0.2, 1) that represents the case of Tr(Σ) = d∥Σ∥∞, in which (1.3), (2.7) and (3.4) almost coincide, �Σna, �Σ2b with optimal C achieves smaller errors than �Σ; while as explained, the dithering scales (3.1) may be (slightly) numerically sub-optimal and proper shrinkage could further lower the errors of �Σ; particularly, �Σ(0.5) performs comparably to �Σ2b with the best C. 12To corroborate our theoretical achievement, of more interest are the cases of Σ = Σ(1, 0.2, 10), Σ(1, 0.2, 25) where Tr(Σ) ≪ d∥Σ∥∞ and hence (3.4) essentially improves on (1.3), (2.7). From Figure 2(b)-(c), �Σ already outperforms �Σna and performs only slightly worse than �Σ2b, although the latter two involve unrealistic tuning. By shrinking the dithering scales, �Σ(0.7) and �Σ(0.5) provide operator norm errors notably lower than �Σ2b, thus validating the beneﬁt of data-driven dithering whose scales diﬀer across entries. To compare with sample covariance based on full data, we use the unlabeled black dashed curve to double the sample covariance's errors. Remarkably, the curve of �Σ(0.5) are almost always below the black dashed one, indicating that by incorporating a proper shrinkage the proposed estimator oftentimes achieves operator norm errors no greater than twice of the errors of sample covariance, as highlighted in the abstract. Furthermore, we also test a ﬁxed n = 500 and the increasing ambient dimension d = 5 : 5 : 50 under the above three covariance matrices. The results with similar implications are provided in Figure 2(d)-(f). 4 The Proof of Main Theorem The most important ingredient in our analysis is the following matrix Bernstein's inequality. Lemma 2. (Matrix Bernstein's Inequality, e.g., [33, Thm. 6.1.1]). Let S1, S2, ..., Sn be independent, zero-mean, d1 × d2 random matrices such that ∥Si∥op ≤ L almost surely for all i. Let σ2 := max ���� n � i=1 E(SiS⊤ i ) ��� op, ��� n � i=1 E(S⊤ i Si) ��� op � , then for some absolute constant c, for any t ≥ 0 we have P ���� n � i=1 Si ��� op ≥ t � ≤ (d1 + d2) exp � −c min � t2 σ2 , t L �� . We will also use the concentration of the norm of a sub-Gaussian vector. Lemma 3. (Adapted from [21, Thm. 4.1]) Let a = (a1, ..., an) ∈ Rn be a random vector with independent sub-Gaussian coordinates ai that satisfy E(a2 i ) = 1. Let A := max1≤i≤n ∥ai∥ψ2 and ˆA = A√log A, then for some absolute constant C we have ��∥a∥2 − √n �� ψ2 ≤ C ˆA, which further implies the following for some C1: P ���∥a∥2 − √n �� ≥ t � ≤ 2 exp � −C1t2 ˆA2 � , ∀ t ≥ 0. The proof of Theorem 5. We only need to prove (3.4) because (3.5) is a direct outcome of (3.4). 13To get started, we prove a useful observation: ˙Xi = QΛ,2b(Xi + Λτi) = ΛQ1(Λ−1Xi + τi). (4.1) Note that the ﬁrst equality is from (3.2), and now let us prove the second equality. To this end, we start from the last line of (3.2) and calculate the j-th entry of ˙Xi as ˙Xij = Qλj,min(Xij + λj,minτij) = λj,min �� Xij + λj,minτij λj,min � + 1 2 � = λj,min �� Xij λj,min + τij � + 1 2 � = λj,min · Q1 � xij λj,min + τij � , then (4.1) follows by noting that the last quantity in the above equation is just the j-th entry of ΛQ1(Λ−1Xi +τi). To proceed, we deﬁne ξi = Q1(Λ−1Xi +τi)−Λ−1Xi as the quantization noise, which provides ˙Xi = ΛQ1(Λ−1Xi + τi) = Xi + Λξi and further leads to �Σ − Σ = 1 n n � i=1 �Xi + Λξi ��Xi + Λξi �⊤ − 1 4Λ2 − Σ = � 1 n n � i=1 XiX⊤ i − Σ � + � 1 n n � i=1 Λξiξ⊤ i Λ⊤ − 1 4Λ2� + 1 n n � i=1 � Xiξ⊤ i Λ + ΛξiX⊤ i � := I1 + I2 + I3. (4.2) Thus, by triangular inequality we have ∥ �Σ − Σ∥op ≤ ∥I1∥op + ∥I2∥op + ∥I3∥op. Step 1. Bounding ∥I1∥op Theorem 2 gives that for any u1 ≥ 0, with probability at least 1 − 3e−u1 it holds that ∥I1∥op ≤ C∥Σ∥op   � ˆK2[r(Σ) + u1] n + ˆK2[r(Σ) + u1] n   . (4.3) Setting u1 = 10r(Σ), we obtain that ∥I1∥op ≲ ∥Σ∥op   � ˆK2r(Σ) n + ˆK2r(Σ) n   (4.4) holds with probability at least 1 − 3e−10r(Σ). Step 2. Bounding ∥I2∥op 2.1) Conditioning on {Xi}n i=1 and using the randomness of {τi}n i=1 Since ξi = Q1(Λ−1Xi + τi) − Λ−1Xi, conditioning on {Xi}n i=1, ξi's are independent of each 14other, and Lemma 1 gives E(ξiξ⊤ i ) = 1 4Id. Hence, by letting ηi = Λξi, I2 can be written as I2 = 1 n n � i=1 (Λξi)(Λξi)⊤ − E[(Λξi)(Λξi)⊤] = 1 n n � i=1 ηiη⊤ i − E[ηiη⊤ i ]. By Lemma 1 we have ∥ξi∥ψ2 = O(λ), and thus ∥⟨ηi, v⟩∥ψ2 = ∥⟨ξi, Λv⟩∥ψ2 = O(∥Λv∥2). Combin- ing with ∥⟨ηi, v⟩∥2 L2 = ∥⟨ξi, Λv⟩∥2 L2 = 1 4∥Λv∥2 2, we know that ηi's are O(1)-sub-Gaussian under Deﬁnition 1. Thus, Theorem 2 implies that with probability at least 1 − 3e−u2, it holds that ∥I2∥op ≲ � Tr(Λ2)∥Λ∥2op + ∥Λ∥4opu2 n + Tr(Λ2) + ∥Λ2∥opu2 n . Setting u2 = 10r(Σ), we obtain that ∥I2∥op ≲ � Tr(Λ2)∥Λ∥2op + ∥Λ∥4opr(Σ) n + Tr(Λ2) + ∥Λ2∥opr(Σ) n (4.5) holds with probability at least 1 − 3e−10r(Σ). 2.2) Dealing with the randomness of {Xi}n i=1 Recall that Λ = diag(λ1,min, ..., λd,min) where λj,min = max1≤i≤n |Xij|. Since ∥Xij∥ψ2 ≤ K � E|Xij|2 = K �Σjj, for any u3 ≥ 0 we have P(|Xij| ≥ u3) ≤ 2 exp � − C4u2 3 K2Σjj � , then a union bound over i ∈ [n] gives P �λj,min ≥ u3 � ≤ 2n exp � − C4u2 3 K2Σjj � . Note that n = O(d10) implies log n = O(log d), we set u3 = C5K �Σjj log d with suﬃciently large C5 to obtain that, with probability at least 1 − 2d−11 we have λj,min ≤ C6K �Σjj log d. Further, a union bound over j ∈ [d] yields that P � λj,min ≤ C6K � Σjj log d, ∀j ∈ [d] � ≥ 1 − 2d−10. (4.6) On this high-probability event, it immediately follows that ∥Λ∥op = max 1≤j≤d λj,min ≤ C6K max 1≤j≤n � Σjj log d = C6K � ∥Σ∥∞ log d, (4.7) Tr(Λ2) = d � j=1 λ2 j,min ≤ C2 6K2 d � j=1 Σjj log d = C2 6K2 Tr(Σ) log d. (4.8) 15Substituting (4.7), (4.8) into (4.5) yields that, with the promised probability, ∥I2∥op ≤ C7K2 log d   � Tr(Σ)∥Σ∥∞ n + Tr(Σ) n   . (4.9) Step 3. Bounding ∥I3∥op It suﬃces to bound �� 1 n �n i=1 Xiξ⊤ i Λ �� op since ∥I3∥op ≤ 2 �� 1 n �n i=1 Xiξ⊤ i Λ �� op. We utilize matrix Bernstein's inequality for this purpose. 3.1) Conditioning on {Xi}n i=1 and using the randomness of {τi}n i=1 Given {Xi : i ∈ [n]}, ξi's are independent, and E(ξi) = 0 implies E(Xiξ⊤ i Λ) = 0. Our goal is to bound the operator norm of the following independent sum of zero-mean matrices: ��� n � i=1 Wi ��� op, where Wi := 1 nXiξ⊤ i Λ. (4.10) Since ∥ξi∥∞ ≤ 3 2, we have ∥Λξi∥2 2 ≤ 9 4 Tr(Λ2), which yields ∥Wi∥op ≤ 1 n∥Xi∥2∥Λξi∥2 ≤ 3 � Tr(Λ2) 2n max 1≤i≤n ∥Xi∥2 := L. (4.11) Moreover, we estimate the matrix variance statistic. First, note that n � i=1 E(WiW ⊤ i ) = n � i=1 E �∥Λξi∥2 2 � n2 XiX⊤ i ⪯ 9 Tr(Λ2) 4n2 n � i=1 XiX⊤ i , which gives �� �n i=1 E(WiW ⊤ i ) �� op ≤ 9 Tr(Λ2) 4n2 �� �n i=1 XiX⊤ i �� op. Second, we have n � i=1 E(W ⊤ i Wi) = n � i=1 ∥Xi∥2 2 n2 E �Λξiξ⊤ i Λ � = �n i=1 ∥Xi∥2 2 4n2 Λ2, which yields �� �n i=1 E(W ⊤ i Wi) �� op = �n i=1 ∥Xi∥2 2 4n2 ∥Λ∥2 op. Thus, recalling (4.11) and deﬁne σ2 := 9 4n2 � Tr(Λ2) · ��� n � i=1 XiX⊤ i ��� op + ∥Λ∥2 op · n � i=1 ∥Xi∥2 2 � . (4.12) we can invoke matrix Bernstein's inequality (Lemma 2) to obtain P ���� n � i=1 Wi ��� op ≥ u4 � ≤ 2d exp � −C8 min �u2 4 σ2 , u4 L �� for any u4 ≥ 0. Setting u4 = C9[σ√log d + L log d] with large enough C9 yields P ���� n � i=1 Wi ��� op ≤ C9 �σ � log d + L log d � � ≥ 1 − 2d−10. (4.13) 163.2) Dealing with the randomness of {Xi}n i=1 Note that σ and L depend on {Xi}, so we still need to bound them using the randomness of {Xi}. Recall that, on the high-probability event in (4.6), ∥Λ∥op and Tr(Λ2) can be bounded as in (4.7)-(4.8). Combining with max 1≤i≤n ∥Xi∥2 = max 1≤i≤n � � � � n � j=1 X2 ij ≤ � � � � n � j=1 max 1≤i≤n X2 ij = � Tr(Λ2), we have L ≤ 3 Tr(Λ2) 2n ≲ K2 Tr(Σ) log d n . (4.14) For bounding σ2 in (4.12) we still need to control ∥ �n i=1 XiX⊤ i ∥op and �n i=1 ∥Xi∥2 2. Recalling from (4.4) that we have �� 1 n �n i=1 XiX⊤ i − Σ �� op ≲ ∥Σ∥op �� ˆ K2r(Σ) n + ˆ K2r(Σ) n � with the promised probability, and note that this implies ��� n � i=1 XiX⊤ i ��� op ≤ ��� n � i=1 XiX⊤ i − nΣ ��� op + n∥Σ∥op ≲ �n + ˆK2r(Σ) �∥Σ∥op. (4.15) To bound �n i=1 ∥Xi∥2 2, recall that Σjj = E(X2 ij) = ∥Xij∥2 L2, so we can write it as n � i=1 ∥Xi∥2 2 = n � i=1 d � j=1 X2 ij = d � j=1 Σjj � n � i=1 � Xij ∥Xij∥L2 �2� = d � j=1 Σjj∥Yj∥2 2, (4.16) where in the last equality we let Yj = ∥Xij∥−1 L2 �X1j, ..., Xnj �⊤. Observe that {Yj}d j=1 are random vectors with independent sub-Gaussian coordinates (since the coordinates are from independent samples) satisfying �� Xij ∥Xij∥L2 �� ψ2 ≤ K, hence we can invoke Lemma 3 and a union bound over 1 ≤ j ≤ d to obtain that, for any u5 ≥ 0, P � max 1≤j≤d ���∥Yj∥2 − √n ��� ≥ u5 � ≤ 2d exp � −C10u2 5 ˆK2 � . We let u5 = C11 ˆK√log d with suﬃciently large C11, then we obtain max1≤j≤d ��∥Yj∥2 − √n �� = O( ˆK√log d) with probability at least 1 − 2d−10, which implies max1≤j≤d ∥Yj∥2 ≲ √n under the scaling of n ≳ ˆK2 log d. Substituting this into (4.16), we arrive at n � i=1 ∥Xi∥2 2 ≤ d � j=1 ΣjjC12n = C12n · Tr(Σ). (4.17) Then, we combine (4.7), (4.8), (4.15), (4.17) to bound σ2 in (4.12) as σ2 ≲ K2∥Σ∥op Tr(Σ) log d n + K2 ˆK2[Tr(Σ)]2 log d n2 . (4.18) 17Further, substituting (4.14), (4.18) into (4.13), we obtain that ∥I3∥op ≤ 2 ��� n � i=1 Wi ��� op ≲ � K2∥Σ∥op Tr(Σ)(log d)2 n + K2 Tr(Σ)(log d)2 n (4.19) holds with probability at least 1 − 2d−10. Step 4. Combining Everything. Recall that ∥ �Σ−Σ∥op ≤ ∥I1∥op +∥I2∥op +∥I3∥op, by combining the bounds for I1, I2, I3 derived in (4.4), (4.9), (4.19), we obtain that ∥ �Σ − Σ∥op ≲ K2   � Tr(Σ)∥Σ∥op(log d)2 n + Tr(Σ)(log d)2 n   holds with the promised probability. Recalling Tr(Σ) = r(Σ)∥Σ∥op, the result follows. ■ 5 Deferred Proofs 5.1 The Proof of Theorem 2 The proof is adjusted from [36, Thm. 9.2.4]. The key technical tool is the matrix deviation inequality with optimal dependence on K developed in [21]. Lemma 4. (Corollary 1.2 in [21]) Consider A = [a1, ..., an]⊤ ∈ Rn×d with independent isotropic rows ai's satisfying max1≤i≤n ∥ai∥ψ2 ≤ K and a bounded subset T ⊂ R with radius rad(T ) := supx∈T ∥x∥2, Gaussian width ω(T ) := E supx∈T ⟨g, x⟩ for g ∼ N(0, Id). Let ˆK = K√log K, then for any t ≥ 0, with probability at least 1 − 3e−t2 we have sup x∈T ��∥Ax∥2 − √n∥x∥2 �� ≤ C ˆK �ω(T ) + t · rad(T ) �. The proof of Theorem 2. We ﬁrst consider the case of rank(Σ) = d and let Yi = Σ− 1 2Xi. Note that Yi is isotropic since E(YiY ⊤ i ) = E(Σ− 1 2 XiX⊤ i Σ− 1 2 ) = Id, and further, its ψ2 norm is bounded by K because ∥Yi∥ψ2 = sup a∈Sd−1 ∥⟨a, Σ− 1 2 Xi⟩∥ψ2 = sup a∈Sd−1 ∥⟨Σ− 1 2 a, Xi⟩∥ψ2 ≤ sup a∈Sd−1 K∥⟨Σ− 1 2 a, Xi⟩∥L2 = sup a∈Sd−1 K∥⟨a, Yi⟩∥L2 = K. 18Then, we let Y = [Y1, ..., Yn]⊤ ∈ Rn×d and can write the operator norm error as ����� 1 n n � i=1 XiX⊤ i − Σ ����� op = sup a∈Sd−1 �����a⊤ � 1 n n � i=1 XiX⊤ i − Σ � a ����� = sup a∈Sd−1 1 n ����� n � i=1 ���Yi, Σ 1 2a ���2 − n ��Σ 1 2 a ��2 2 ����� = sup b∈Σ 1 2 Sd−1 1 n ���∥Y b∥2 2 − n∥b∥2 2 ��� ≤ sup b∈Σ 1 2 Sd−1 1 n ���∥Y b∥2 − √n∥b∥2 ��� ���∥Y b∥2 + √n∥b∥2 ���. (5.1) Now we invoke Lemma 4 to obtain that, for any t ≥ 0, with probability at least 1 − 3e−t2 sup b∈Σ 1 2 Sd−1 ���∥Y b∥2 − √n∥b∥2 ��� ≤ C ˆK � ω(Σ 1 2 Sd−1) + t · rad(Σ 1 2 Sd−1) � . (5.2) Note that rad(Σ 1 2 Sd−1) ≤ ∥Σ∥ 1 2op, and let g ∼ N(0, Id) we have ω �Σ 1 2 Sd−1� = E sup a∈Sd−1⟨g, Σ 1 2a⟩ = E ��Σ 1 2 g �� 2 ≤ � E∥Σ 1 2g∥2 2 = � Tr(Σ). Thus, the right-hand side of (5.2) is bounded by C ˆK �� Tr(Σ) + t � ∥Σ∥op �, and it further implies sup b∈Σ 1 2 Sd−1 ���∥Y b∥2 + √n∥b∥2 ��� ≤ sup b∈Σ 1 2 Sd−1 ���∥Y b∥2 − √n∥b∥2 ��� + 2√n rad �Σ 1 2 Sd−1� ≤ C ˆK �� Tr(Σ) + t � ∥Σ∥op � + 2 � n∥Σ∥op. Substituting these into (5.1) and let t = √u, some algebra immediately yields the desired bound. All that remains is to deal with the case when rank(Σ) := r < d. Let the singular value decomposition be Σ = UΣ0U ⊤ for some U ∈ Rd×r with orthonormal columns and positive deﬁnite diagonal Σ0 ∈ Rr×r. Then we construct the isotropic and K-sub-Gaussian Yi = Σ − 1 0 2 U ⊤Xi. Note that E∥(In − UU ⊤)Xi∥2 2 = (In − UU ⊤)UΣ0U ⊤(In − UU ⊤) = 0, hence almost surely we have Xi = UU ⊤Xi = UΣ 1 2 0 Yi. Thus, we let Xi = U0Yi with U0 = UΣ 1 2 0 satisfying rad(U ⊤ 0 Sd−1) ≤ ∥Σ∥1/2 op and ω(U ⊤ 0 Sd−1) ≤ � Tr(Σ), hence we can plug this into the ﬁrst line of (5.1) and run the mechanism again. ■ 5.2 The Proof of Lemma 1 To be self-contained, we give a proof for Lemma 1. Our proof is based on the following result from [17], and similar materials can be found in [2, Sec. 2.2]. Theorem 6. (Adapted from [17, Thms. 1-2]) We let a ∈ R, τ be some random dither and quantize a to Qλ(a + τ), then we deﬁne quantization error as w = Qλ(a + τ) − (a + τ), and quantization 19noise as ξ = Qλ(a + τ) − a. We assume that τ ′ ∼ U [− λ 2, λ 2] is independent of τ. Denote by ı the complex unit, if f(u) = E(exp(ıuτ)) satisﬁes f(2πl λ ) = 0 for any non-zero integer l, then for any a we have w ∼ U [− λ 2 , λ 2]; if g(u) = E(exp(ıuτ))E(exp(ıuτ ′)) satisﬁes g′′(2πl λ ) = 0 for any non-zero integer l, then for any a ∈ R we have E(ξ2) = E(τ + τ ′)2. Now we are ready to prove Lemma 1. Proof of Lemma 1. Observe that supx∈R |Qλ(x) − x| = λ 2, so the boundedness follows from ∥ξ∥∞ ≤ ∥Qλ(X + τ) − (X + τ)∥∞ + ∥τ∥∞ ≤ λ 2 + λ = 3λ 2 . For E[ξ] = 0 and E[ξξ⊤] = λ2 4 Id, it suﬃces to show that they hold when conditioning on X since we have E(·) = EX Eτ|X(·), thus we assume X ∈ Rd is ﬁxed. We let w = Qλ(X + τ) − (X + τ), which gives ξ = τ + w. We now verify that a triangular dither τ = τ1 + τ2 with independent τ1, τ2 ∼ U [− λ 2 , λ 2] satisﬁes both conditions in Theorem 6. First, we calculate that f(u) = E(exp(ıuτ)) = E(exp(ıuτ1) exp(ıuτ2)) = [E(exp(ıuτ1))]2, and further E(exp(ıuτ1)) = � λ/2 −λ/2 λ−1[cos(uτ1) + ı sin(uτ1)]dτ1 = 2 λu sin λu 2 , which vanishes at u = 2πl λ for non-zero integer l. Thus, when given X, Theorem 6 implies that entries of w follow U [− λ 2 , λ 2], which leads to E(ξ|X) = E(τ|X) + E(w|X) = 0. Second, we let τ ′ ∼ U [− λ 2, λ 2] be independent of {τ1, τ2}, then g(u) = f(u)E(exp(ıuτ ′)) = ( 2 λu sin λu 2 )3 satisﬁes g′′(2πl λ ) = 0 for any non-zero integer l. Thus, let ξi be the i-th entry of ξ, when given X Theorem 6 implies that E(ξ2 i |X) = E(τ + τ ′)2 = E(τ)2 + E(τ ′)2 = λ2 4 . For non-diagonal entries, given X, ξi and ξj are independent if i ̸= j, and so E(ξiξj|X) = E(ξi|X)E(ξj|X) = 0. Overall, we have shown that E(ξξ⊤|X) = λ2 4 Id. Finally, we prove ∥ξ∥ψ2 = O(λ) by using ∥X∥ψ2 ≍ supp≥1 [ E|X|p]1/p √p (e.g., [36, Prop. 2.5.2]). Conditioning on X, we have shown that ∥ξ∥∞ ≤ 3λ 2 , E(ξ|X) = 0. Combined with the fact that ξ have independent entries, we have ∥ξ|X∥ψ2 = O(λ) [36, Lem. 3.4.2]. Thus, we have Eξ|X|⟨v, ξ⟩|p ≤ (Cλ√p)p for some absolute constant C and for any v ∈ Sd−1; further averaging over X, we obtain supv∈Sd−1 E|⟨v, ξ⟩|p ≤ (Cλ√p)p, which leads to ∥ξ∥ψ2 = supv∈Sd−1 ∥⟨v, ξ⟩∥ψ2 = O(λ). The proof is complete. ■ 206 Conclusions and Discussions For sub-Gaussian distributions, we proposed in this paper a 2-bit covariance estimator �Σ (3.3) that possesses near optimal operator norm error rate (up to logarithmic factors), as can be clearly seen by comparing our main result (Theorem 5) with Theorem 2. This improves on the estimators in [9, 10], which are essentially sub-optimal over Σ satisfying Tr(Σ) ≪ d∥Σ∥∞. More practically, our second signiﬁcant advantage is that our estimator is free of any tuning parameter, unlike �Σna (1.2) in [10] that requires tuning according to some unknown parameters. As signiﬁcant departures from [9,10] who used uniformly dithered 1-bit quantizer, our quanti- zation procedure is motivated by [2] and based on the triangular dither and a 2-bit quantizer (2.5). This leads to a new 2-bit estimator �Σ2b (2.6) comparable to �Σna in several respects (Section 2), and further, our key estimator �Σ is obtained from �Σ2b by using dithering scales varying across entries and determined by the data (Section 3). Our simulations demonstrate for Gaussian samples the following: (i) �Σ2b numerically outper- forms �Σna quite notably, (ii) our parameter-free �Σ outperforms �Σna, �Σ2b with optimally tuned parameter in the case of Tr(Σ) ≪ d∥Σ∥∞, (iii) the performance of �Σ can be further improved by properly shrinking the dithering scales in (3.1). For (iii), it is remarkable that �Σ(0.5) (deﬁned in (3.6)) oftentimes achieves operator norm errors less than twice of the errors of sample covariance. We close this paper by some discussions. While the numerical improvement of our �Σ2b over �Σna is quite notable over Gaussian samples (Figure 2), this is in general not theoretically provable since it may not be true for certain distributions. As an extreme example,5 suppose that entries of Xi are ±1, then �Σna in (1.2) with τi1, τi2 ∼ U [−1, 1] (i.e., λ = 1) simply reduces to sample covariance, meaning that there is no information loss in the quantization. Thus, our �Σ2b is expected to behave (at least slightly) worse than �Σna. Therefore, �Σna and �Σ2b correspond to two parallel dithered quantization methods that allow for covariance estimation, with the former applying a uniformly dithered sign quantizer twice, while the latter being the 2-bit quantizer in (2.5) associated with a triangular dither; it may be a rushed judgment to declare one superior to the other. Built upon �Σ2b, our major contribution is to devise �Σ that enjoys near optimal operator norm rate and requires no tuning, which is done by using dithering scales in (3.1). It is interesting to note that this development straightforwardly carries over to �Σna. In particular, let τi1, τi2 ∼ U [−1, 1]d and Λ = diag(λ1,min, ..., λd,min) where λj,min is given in (3.1), we consider the following parameter-free estimator �Σ′ = 1 2n n � i=1 ( ˙Xi1 ˙X⊤ i2 + ˙Xi2 ˙X⊤ i1), with    ˙Xi1 = Λ · sgn(Xi + Λτi1) ˙Xi2 = Λ · sgn(Xi + Λτi2) . Then, using techniques provided in the proof of Theorem 5, one can show that �Σ′ possesses a near optimal operator norm error rate comparable to �Σ. Additionally, we expect that properly shrinking 5It is extreme in the sense that there is no quantization need. 21the dithering scales like (3.6) also improves the numerical performance of �Σ′ over Gaussian samples. References [1] F. Chapeau-Blondeau, S. Blanchard, and D. Rousseau, Fisher information and noise- aided power estimation from one-bit quantizers, Digital Signal Processing, 18 (2008), pp. 434– 443. [2] J. Chen, M. K. Ng, and D. Wang, Quantizing heavy-tailed data in statistical estima- tion: (near) minimax rates, covariate quantization, and uniform recovery, arXiv preprint arXiv:2212.14562, (2022). [3] J. Chen, C.-L. Wang, M. K. Ng, and D. Wang, High dimensional statistical estimation under uniformly dithered one-bit quantization, IEEE Transactions on Information Theory, 69 (2023), pp. 5151–5187. [4] J. Chen, Y. Wang, and M. K. Ng, Quantized low-rank multivariate regression with random dithering, arXiv preprint arXiv:2302.11197, (2023). [5] J. Choi, J. Mo, and R. W. Heath, Near maximum-likelihood detector and channel esti- mator for uplink multiuser massive mimo systems with one-bit adcs, IEEE Transactions on Communications, 64 (2016), pp. 2005–2018. [6] A. Danaee, R. C. de Lamare, and V. H. Nascimento, Distributed quantization-aware rls learning with bias compensation and coarsely quantized signals, IEEE Transactions on Signal Processing, 70 (2022), pp. 3441–3455. [7] S. Dirksen, Quantized compressed sensing: a survey, in Compressed Sensing and Its Appli- cations: Third International MATHEON Conference 2017, Springer, 2019, pp. 67–95. [8] S. Dirksen and J. Maly, Supplement to "covariance estimation under one-bit quantization", (2022), https://doi.org/10.1214/22-AOS2239SUPP. [9] S. Dirksen and J. Maly, Tuning-free one-bit covariance estimation using data-driven dither- ing, arXiv preprint arXiv:2307.12613, (2023). [10] S. Dirksen, J. Maly, and H. Rauhut, Covariance estimation under one-bit quantization, The Annals of Statistics, 50 (2022), pp. 3538–3562. [11] S. Dirksen and S. Mendelson, Non-gaussian hyperplane tessellations and robust one-bit compressed sensing, Journal of the European Mathematical Society, 23 (2021), pp. 2913–2947. [12] A. Eamaz, F. Yeganegi, and M. Soltanalian, Modiﬁed arcsine law for one-bit sam- pled stationary signals with time-varying thresholds, in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2021, pp. 5459–5463. 22[13] A. Eamaz, F. Yeganegi, and M. Soltanalian, Covariance recovery for one-bit sampled non-stationary signals with time-varying sampling thresholds, IEEE Transactions on Signal Processing, 70 (2022), pp. 5222–5236. [14] A. Eamaz, F. Yeganegi, and M. Soltanalian, Covariance recovery for one-bit sam- pled stationary signals with time-varying sampling thresholds, Signal Processing, 206 (2023), p. 108899. [15] J. Fang and H. Li, Adaptive distributed estimation of signal power from one-bit quantized data, IEEE Transactions on Aerospace and Electronic Systems, 46 (2010), pp. 1893–1905. [16] D. A. Freedman, Statistical models: theory and practice, cambridge university press, 2009. [17] R. M. Gray and T. G. Stockham, Dithered quantizers, IEEE Transactions on Information Theory, 39 (1993), pp. 805–812. [18] O. A. Hanna, Y. H. Ezzeldin, C. Fragouli, and S. Diggavi, Quantization of distributed data for learning, IEEE Journal on Selected Areas in Information Theory, 2 (2021), pp. 987– 1001. [19] G. Jacovitti and A. Neri, Estimation of the autocorrelation function of complex gaussian stationary processes by amplitude clipped signals, IEEE transactions on information theory, 40 (1994), pp. 239–245. [20] N. Jayant and L. Rabiner, The application of dither to the quantization of speech signals, Bell System Technical Journal, 51 (1972), pp. 1293–1304. [21] H. Jeong, X. Li, Y. Plan, and O. Yilmaz, Sub-gaussian matrices on sets: Optimal tail dependence and applications, Communications on Pure and Applied Mathematics, 75 (2022), pp. 1713–1754. [22] I. T. Jolliffe, Principal component analysis for special types of data, Springer, 2002. [23] H. C. Jung, J. Maly, L. Palzer, and A. Stollenwerk, Quantized compressed sensing by rectiﬁed linear units, IEEE transactions on information theory, 67 (2021), pp. 4125–4149. [24] V. Koltchinskii and K. Lounici, Concentration inequalities and moment bounds for sample covariance operators, Bernoulli, (2017), pp. 110–133. [25] O. Ledoit and M. Wolf, Improved estimation of the covariance matrix of stock returns with an application to portfolio selection, Journal of empirical ﬁnance, 10 (2003), pp. 603–621. [26] J. Limb, Design of dither waveforms for quantized visual signals, The Bell System Technical Journal, 48 (1969), pp. 2555–2582. [27] K. Lounici, High-dimensional covariance matrix estimation with missing observations, Bernoulli, 20 (2014), pp. 1029 – 1058. 23[28] J. Maly, T. Yang, S. Dirksen, H. Rauhut, and G. Caire, New challenges in covariance estimation: multiple structures and coarse quantization, in Compressed Sensing in Information Processing, Springer, 2022, pp. 77–104. [29] L. Roberts, Picture coding using pseudo-random noise, IRE Transactions on Information Theory, 8 (1962), pp. 145–154. [30] L. Schuchman, Dither signals and their eﬀect on quantization noise, IEEE Transactions on Communication Technology, 12 (1964), pp. 162–165. [31] Z. Sun, W. Cui, and Y. Liu, Quantized corrupted sensing with random dithering, IEEE Transactions on Signal Processing, 70 (2022), pp. 600–615. [32] C. Thrampoulidis and A. S. Rawat, The generalized lasso for sub-gaussian measurements with dithered quantization, IEEE Transactions on Information Theory, 66 (2020), pp. 2487– 2500. [33] J. A. Tropp et al., An introduction to matrix concentration inequalities, Foundations and Trends® in Machine Learning, 8 (2015), pp. 1–230. [34] J. H. Van Vleck and D. Middleton, The spectrum of clipped noise, Proceedings of the IEEE, 54 (1966), pp. 2–19. [35] R. Vershynin, Introduction to the non-asymptotic analysis of random matrices, Cambridge University Press, 2012, p. 210–268. [36] R. Vershynin, High-dimensional probability: An introduction with applications in data sci- ence, vol. 47, Cambridge university press, 2018. [37] C. Xu and L. Jacques, Quantized compressive sensing with rip matrices: The beneﬁt of dithering, Information and Inference: A Journal of the IMA, 9 (2020), pp. 543–586. [38] T. Yang, J. Maly, S. Dirksen, and G. Caire, Plug-in channel estimation with dithered quantized signals in spatially non-stationary massive mimo systems, arXiv preprint arXiv:2301.04641, (2023). [39] H. Zhang, J. Li, K. Kara, D. Alistarh, J. Liu, and C. Zhang, Zipml: Training lin- ear models with end-to-end low precision, and a little bit of deep learning, in International Conference on Machine Learning, PMLR, 2017, pp. 4035–4043. 24