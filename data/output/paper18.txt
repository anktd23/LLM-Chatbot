HLD 2023: 1st Workshop on High-dimensional Learning Dynamics On the Equivalence between Implicit and Explicit Neural Networks: A High-dimensional Viewpoint Zenan Ling LINGZENAN@HUST.EDU.CN Zhenyu Liao ZHENYU LIAO@HUST.EDU.CN Robert C. Qiu CAIMING@HUST.EDU.CN Huazhong University of Science and Technology, Wuhan, China Abstract Implicit neural networks have demonstrated remarkable success in various tasks. However, there is a lack of theoretical analysis of the connections and differences between implicit and explicit networks. In this paper, we study high-dimensional implicit neural networks and provide the high dimensional equivalents for the corresponding conjugate kernels and neural tangent kernels. Built upon this, we establish the equivalence between implicit and explicit networks in high dimensions. 1. Introduction Implicit neural networks (NNs) [2] have recently emerged as a new paradigm in neural network design. An implicit NN is equivalent to an infinite-depth weight-shared explicit NN with input- injection. Unlike explicit NNs, implicit NNs generate features by directly solving for the fixed point, rather than through layer-by-layer forward propagation. Moreover, implicit NNs have the remarkable advantage that gradients can be computed analytically only through the fixed point with implicit differentiation. Therefore, training implicit NNs only requires constant memory. Despite the empirical success achieved by implicit NNs [3, 11], our theoretical understanding of these models is still limited. In particular, there is a lack of theoretical analysis of the training dynamics and generalization performance of implicit NNs, and possibly more importantly, whether these properties can be connected to those of explicit NNs. [2] demonstrates that any deep NN can be reformulated as a special implicit NN. However, it remains unknown whether general implicit NNs have advantages over explicit NNs. [6] extends previous neural tangent kernel (NTK) studies to implicit NNs and give the exact expression of the NTK of the ReLU implicit NNs. However, the differences between implicit and explicit NTKs are not analyzed. Moreover, previous works [9, 10] have proved the global convergence of gradient descent for training implicit NNs. However, it is still unclear what distinguishes the training dynamic of implicit NNs and that of explicit NNs. In this paper, we investigate implicit NNs from a high-dimensional view. Specifically, we per- form a fine-grained asymptotic analysis on the eigenspectra of conjugate kernel (CKs) and NTKs of implicit NNs, which play a fundamental role in the convergence and generalization high dimen- sional NNs [8]. By considering input data uniformly drawn from the unit sphere, we derive, with recent advances in random matrix theory, high-dimensional (spectral) equivalents for the CKs and NTKs of implicit NNs, and establish the equivalence between implicit and explicit NNs by match- ing the coefficients of the corresponding asymptotic spectral equivalents. Surprisingly, our results reveal that a single-layer explicit NN with carefully designed activations has the same CK or NTK eigenspectra as a ReLU implicit NN, whose depth is essentially infinite. © Z. Ling, Z. Liao & R.C. Qiu. arXiv:2308.16425v1 [cs.LG] 31 Aug 2023ON THE EQUIVALENCE BETWEEN IMPLICIT AND EXPLICIT NNS 2. Preliminaries 2.1. Implicit and Explicit NNs Implicit NNs. In this paper, we study a typical implicit neural network, the deep equilibrium model (DEQ) [2]. Let X = [x1, · · · , xn] ∈ Rd×n denote the input data. We define a vanilla DEQ with the transform at the l-th layer as h(l) i = � σ2a m Az(l−1) i + � σ2 b m Bxi, z(l) i = ϕ(h(l) i ) (1) where A ∈ Rm×m and B ∈ Rm×d are weight matrices, σa, σb ∈ R are constants, ϕ is an element- wise activation, h(l) i is the pre-activation and z(l) i ∈ Rm is the output feature of the l-th hidden layer corresponding to the input data xi. The output of the last hidden layer is defined by z∗ i ≜ liml→∞ z(l) i and we denote the corresponding pre-activation by h∗ i . Note that z∗ i can be calculated by directly solving for the equilibrium point of the following equation z∗ i = ϕ   � σ2a m Az∗ i + � σ2 b m Bxi   . (2) We are interested in the conjugate kernel and neural tangent kernel (Implicit-CK and Implicit-NTK, for short) of implicit neural networks defined in Eq. (2). Following [6], we denote the corresponding Implicit-CK by G∗ = liml→∞ G(l) where the (i, j)-th entry of G(l) is defined recursively as G(0) ij = x⊤ i xj, Λ(l) ij = � G(l−1) ii G(l−1) ij G(l−1) ji G(l−1) jj � , G(l) ij = σ2 aE(u,v)∼N(0,Λ(l) ij )[ϕ(u)ϕ(v)] + σ2 bx⊤ i xj, ˙G(l) ij = σ2 aE(u,v)∼N(0,Λ(l) ij )[ϕ′(u)ϕ′(v)]. (3) And the Implicit-NTK is defined as K∗ = liml→∞ K(l) whose the (i, j)-th entry is defined as K(l) ij = l+1 � h=1 � G(h−1) ij l+1 � h′=h ˙G(h′) ij � . (4) Explicit Neural Networks. We consider a single-layer fully-connected NN model defined as Y = � 1 pσ(W X) where W ∈ Rp×d is the weight matrix and σ is an element-wise activation function. Let w ∼ N(0, Id), the corresponding Explicit-CK matrix Σ and Explicit-NTK matrix Θ are defined as follows: Σ = Ew[σ(w⊤X)⊤σ(w⊤X)], Θ = Σ + � X⊤X � ⊙ Ew[σ′(w⊤X)⊤σ′(w⊤X)]. (5) 2.2. CKs and NTKs of ReLU Implicit NNs We make the following assumptions on the random initialization, the input data, and activations. 2ON THE EQUIVALENCE BETWEEN IMPLICIT AND EXPLICIT NNS Assumption 1 (i) As n → ∞, d/n → c ∈ (0, ∞). All data points xi, i ∈ [n], are independent and uniformly sampled from Sd−1. (ii) A, B, and W are independent and have i.i.d entries of zero mean, unit variance, and finite fourth kurtosis. Moreover, we require σ2 a + σ2 b = 1. (iii) The activation ϕ of the implicit NN is the normalized ReLU, i.e., ϕ(x) = √ 2 max(x, 0). The activation σ of the explicit NN is a C3 function. Remark 1 (i) Despite derived here for uniform distribution on the unit sphere, we conjecture that our results extend the result to more general distributions by using the technique developed in [5, 7]. (ii) The additional requirement on the variance is to ensure the existence and uniqueness of the fixed point of the NTK and to keep the diagonal entries of the CK matrix at 1, see examples in [6]. (iii) It is possible to extend our results to implicit NNs with general activations by using the technique proposed in [10]. We defer the extension to more general data distributions and activation functions to future work. Under Assumptions 1, the limits of Implicit-CK and Implicit-NTK exist, and one can have precise expressions of G∗ and K∗ as follows [6, 9]. Lemma 1 Let f(x) = √ 1−x2+(π−arccos x)x π . Under Assumptions 1, the fixed point of Implicit-CK G∗ ij is the root of G∗ ij = σ2 af(G∗ ij) + (1 − σ2 a)x⊤ i xj. (6) The limit of Implicit-NTK is K∗ ij = h(G∗ ij) ≜ G∗ ij 1 − ˙G∗ ij where ˙G∗ ij ≜ σ2 aπ−1(π − arccos � G∗ ij � ). (7) 3. Main Results In this section, we prove the high-dimensional equivalents for CKs and NTKs of implicit and explicit NNs. As a result, by matching the coefficients of the asymptotic spectral equivalents, we establish the equivalence between implicit and explicit NNs in high dimensions. 3.1. Asymptotic Approximations CKs. We begin by defining several quantities that are crucial to our results. Note that the unique fixed point of Eq. (6) exists as long as σ2 a < 1. We define the implicit map induced from Eq. (6) as G∗ ij ≜ g(x⊤ i xj). Let ∠∗ = g(0) be the solution of ∠∗ = σ2 af(∠∗) when x⊤ i xj = 0. Using implicit differentiation, one can obtain that g′(0) = 1 − σ2 a 1 − σ2af′(∠∗), g′′(0) = σ2 a(1 − σ2 a)2f′′(∠∗) (1 − σ2af′(∠∗))3 . Now we are ready to present the asymptotic equivalent of the Implicit-CK matrix. Theorem 1 (Asymptotic approximation of Implicit-CKs) Let Assumptions 1 hold. As n, d → ∞, the Implicit-CK matrix G∗ defined in Eq. (6) can be approximated consistently in operator norm, by the matrix G, that is ∥G∗ − G∥2 → 0, where G = α11⊤ + βX⊤X + µIn, with α = g(0) + g′′(0) 2d , β = g′(0), and µ = g(1) − g(0) − g′(0). 3ON THE EQUIVALENCE BETWEEN IMPLICIT AND EXPLICIT NNS Theorem 2 (Asymptotic approximation for Explicit-CKs) Let Assumptions 1 hold. As n, d → ∞, the Explicit-CK matrix Σ defined in Eq. (5) can be approximated consistently in operator norm, by the matrix Σ, that is ∥Σ − Σ∥2 → 0, where Σ = α111⊤ + β1X⊤X + µ1In, with α1 = E[σ(z)]2 + E[σ′′(z)]2 2d , β1 = E[σ′(z)]2, and µ1 = E[σ2(z)] − E[σ(z)]2 − E[σ′(z)]2, for z ∼ N(0, 1). NTKs. For the Implicit-NTK, we define K∗ ij = k(x⊤ i xj), i.e., k(x⊤ i xj) = h(g(x⊤ i xj)), for i, j ∈ [n]. It is easy to check that k(0) = h(∠∗) and k(1) = h(g(1)). Using implicit differentiation again, we have k′(0) = (1 − σ2 a)h′(∠∗) σ2af′(∠∗) − 1 , k′′(0) = (1 − σ2 a)2(h′′(∠∗) − σ2 af′(∠∗)h′′(∠∗) + σ2 ah′(∠∗)f′′(∠∗)) (1 − σ2af′(∠∗))3 . Now we are ready to present the asymptotic equivalent of the Implicit-NTK matrix. Theorem 3 (Asymptotic approximation for Implicit-NTKs) Let Assumptions 1 hold. As n, d → ∞, the Implicit-NTK matrix K∗ defined Eq. (7) in can be approximated consistently in operator norm, by the matrix K, that is ∥K∗ − K∥2 → 0, where K = ˙α11⊤ + ˙βX⊤X + ˙µIn, with ˙α = k(0) + k′′(0) 2d , ˙β = k′(0), and ˙µ = k(1) − k(0) − k′(0). Theorem 4 (Asymptotic approximation for Explicit-NTKs) Let Assumptions 1 hold. As n, d → ∞, the Explicit-NTK matrix Θ defined in Eq. (5) can be approximated consistently in operator norm, by the matrix Θ, that is ∥Θ∗ − Θ∥2 → 0, where Θ = ˙α111⊤ + ˙β1X⊤X + ˙µ1In, with ˙α1 = E[σ(z)]2 + 3E[σ′′(z)]2 2d , ˙β1 = 2E[σ′(z)]2, and ˙µ1 = E[σ2(z)] + E[σ′(z)2] − E[σ(z)]2 − 2E[σ′(z)]2 for z ∼ N(0, 1). Remark 2 (i) Due to the homogeneity of the ReLU function, the Implicit-CK and the Implicit-NTK are essentially inner product kernel random matrices. Consequently, Theorem 1 and 3 can be built upon the results in [4]. We postpone the study on general activations to future work. (ii) The results in Theorem 2 and 4 generalize those of [1, 7] to the cases of “non-centred” activations, i.e., we do not require E[σ(z)] = 0 for z ∼ N(0, 1). 3.2. The Equivalence between Implicit and Explicit NNs In the following corollary, we show a concrete case of a single-layer explicit NN with an quadratic activation, that matches the CK or NTK eigenspectra of a ReLU implicit NN. The idea is to utilize the results of Theorems 1-4 to match the coefficients of the asymptotic equivalents such that α1 = α, β1 = β, µ1 = µ, or ˙α1 = ˙α, ˙β1 = β, ˙µ1 = µ. We implement numerical simulations to verify our theory. The numerical results are shown in Figure 1. 4ON THE EQUIVALENCE BETWEEN IMPLICIT AND EXPLICIT NNS CK NTK (a) Implicit (b) Explicit (c) Quadratic activation Figure 1: We independently generate n = 1 000 data points from the d = 1 200-dimensional unit sphere. We use Gaussian initialization and σ2 a is set as 0.2. Upper: the CK results. Bottom: the NTK results. (a) spectral densities of implicit kernels, (b) spectral densities of explicit kernels, (c) quadratic activations. Corollary 1 We consider a quadratic polynomial activation σ(t) = a2t2 + a1t + a0. Let As- sumptions 1 hold. As n, d → ∞, the Implicit-CK matrix G∗ defined in Eq. (6) can be approximated consistently in operator norm, by the Explicit-CK matrix Σ defined in Eq. (5), i.e., ∥G∗ −Σ∥2 → 0, as long as a2 = ± �µ 2 a1 = ± � β, a0 = ± � α − µ d − a2, and the Implicit-NTK matrix K∗ defined in Eq. (7) can be approximated consistently in operator norm, by the Explicit-NTK matrix Θ defined in Eq. (5), i.e., ∥K∗ − Θ∥2 → 0, as long as a2 = ± � ˙µ 6 , a1 = ± � ˙β 2 , a0 = ± � ˙α − ˙µ d − a2. 4. Conclusion In this paper, we study the CKs and NTKs of high-dimensional ReLU implicit NNs. We prove the asymptotic spectral equivalents for Implicit-CKs and Implicit-NTKs. Moreover, we establish the equivalence between implicit and explicit NNs by matching the coefficients of the asymptotic spectral equivalents. In particular, we show that a single-layer explicit NN with carefully designed activations has the same CK or NTK eigenspectra as a ReLU implicit NN. For future work, it would be interesting to extend our analysis to more general data distributions and activation functions. 5ON THE EQUIVALENCE BETWEEN IMPLICIT AND EXPLICIT NNS Acknowledgements Z. Liao would like to acknowledge the National Natural Science Founda- tion of China (via fund NSFC-62206101) and the Fundamental Research Funds for the Central Universities of China (2021XXJS110) for providing partial support. R. C. Qiu and Z. Liao would like to acknowledge the National Natural Science Foundation of China (via fund NSFC-12141107), the Key Research and Development Program of Hubei (2021BAA037) and of Guangxi (GuiKe- AB21196034). References [1] Hafiz Tiomoko Ali, Zhenyu Liao, and Romain Couillet. Random matrices in service of ml footprint: ternary random features with no performance loss. ICLR, 2022. [2] Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. [3] Shaojie Bai, Vladlen Koltun, and J Zico Kolter. Multiscale deep equilibrium models. Advances in Neural Information Processing Systems, 2020. [4] Noureddine El Karoui. The spectrum of kernel random matrices. The Annuals of Statistics, 2010. [5] Zhou Fan and Zhichao Wang. Spectra of the conjugate kernel and neural tangent kernel for linear-width neural networks. Advances in neural information processing systems, 33:7710– 7721, 2020. [6] Zhili Feng and J Zico Kolter. On the neural tangent kernel of equilibrium models. arxiv, 2020. [7] Lingyu Gu, Yongqi Du, Zhang Yuan, Di Xie, Shiliang Pu, Robert Qiu, and Zhenyu Liao. ” lossless” compression of deep neural networks: A high-dimensional neural tangent kernel approach. Advances in Neural Information Processing Systems, 35:3774–3787, 2022. [8] Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018. [9] Zenan Ling, Xingyu Xie, Qiuhao Wang, Zongpeng Zhang, and Zhouchen Lin. Global con- vergence of over-parameterized deep equilibrium models. In International Conference on Artificial Intelligence and Statistics, pages 767–787. PMLR, 2023. [10] Lan V Truong. Global convergence rate of deep equilibrium models with general activations. arXiv preprint arXiv:2302.05797, 2023. [11] Xingyu Xie, Qiuhao Wang, Zenan Ling, Xia Li, Guangcan Liu, and Zhouchen Lin. Optimiza- tion induced equilibrium networks: An explicit optimization perspective for understanding equilibrium models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022. 6