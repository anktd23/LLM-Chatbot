Mechanism of feature learning in convolutional neural networks Daniel Beaglehole∗,2 Adityanarayanan Radhakrishnan∗,3,4 Parthe Pandit1 Mikhail Belkin1,2 1Halıcıoğlu Data Science Institute, UC San Diego. 2Computer Science and Engineering, UC San Diego. 3Massachusetts Institute of Technology. 4Broad Institute of MIT and Harvard. ∗Equal contribution. Abstract Understanding the mechanism of how convolutional neural networks learn features from image data is a fundamental problem in machine learning and computer vision. In this work, we identify such a mechanism. We posit the Convolutional Neural Feature Ansatz, which states that covariances of filters in any convolutional layer are proportional to the average gradient outer product (AGOP) taken with respect to patches of the input to that layer. We present extensive empirical evidence for our ansatz, including identifying high correlation between covariances of filters and patch-based AGOPs for convolutional layers in standard neural architectures, such as AlexNet, VGG, and ResNets pre-trained on ImageNet. We also provide supporting theoretical evidence. We then demonstrate the generality of our result by using the patch-based AGOP to enable deep feature learning in convolutional kernel machines. We refer to the resulting algorithm as (Deep) ConvRFM and show that our algorithm recovers similar features to deep convolutional networks including the notable emergence of edge detectors. Moreover, we find that Deep ConvRFM overcomes previously identified limitations of convolutional kernels, such as their inability to adapt to local signals in images and, as a result, leads to sizable performance improvement over fixed convolutional kernels. 1 Introduction Neural networks have achieved impressive empirical results across various tasks in natural language pro- cessing [8], computer vision [39], and biology [50]. Yet, our understanding of the mechanisms driving the successes of these models is still emerging. One such mechanism of central importance is that of neural feature learning, which is the ability of networks to automatically learn relevant input transformations from data [37, 43, 55, 56]. An important line of work [5, 14, 23, 32, 35, 43, 52, 55] has demonstrated how feature learning in fully connected neural networks provides an advantage over classical, non-feature-learning models such as kernel machines. Recently, the work [37] identified a connection between a mathematical operator, known as average gradient outer product (AGOP) [17, 21, 47, 48], and feature learning in fully connected networks. This work subsequently demonstrated that the AGOP could be used to enable similar feature learning in kernel machines operating on tabular data. In contrast to the case for fully connected networks, there are few prior works [3, 24] analyzing feature learning in convolutional networks, which have been transformative in computer vision [19, 39]. The work [24] demonstrates an advantage of feature learning in convolutional networks by showing that these models are able to threshold noise and identify signal in image data unlike convolutional kernel methods including Convolutional Neural Tangent Kernels [4]. The work [3] analyzes how deep convolutional networks can correct features in early layers by simultaneous training of all layers. While these prior works identify advantages of feature learning in convolutional networks, they do not identify a general operator that captures such feature learning. The connection between AGOP and feature learning 1 arXiv:2309.00570v1 [stat.ML] 1 Sep 2023in fully connected neural networks [37] suggests that a similar connection should exist for feature learning in convolutional networks. Moreover, such a mechanism could be used to learn analogous features with any machine learning model such as convolutional kernel machines. In this work, we establish a connection between convolutional neural feature learning and the AGOP, which we posit as the Convolutional Neural Feature Ansatz (CNFA). Unlike the fully connected case from [37] where feature learning is characterized by AGOP with respect to network inputs, we demonstrate that con- volutional feature learning is characterized by AGOP with respect to patches of network inputs. We present empirical evidence for the CNFA by demonstrating high average Pearson correlation (in most cases > .9) be- tween AGOP on patches and the covariance of filters across all layers of pre-trained convolutional networks on ImageNet [40] and across all layers of SimpleNet [18] trained on several standard image classification datasets. We additionally prove that the CNFA holds for one step of gradient descent for deep convolu- tional networks. To demonstrate the generality of our identified convolutional feature learning mechanism, we leverage the AGOP on patches to enable feature learning in convolutional kernel machines. We refer to the resulting algorithm as ConvRFM. We demonstrate that ConvRFM captures features similar to those learned by the first layer of convolutional networks. In particular, on various image classification benchmark datasets such as SVHN [33] and CIFAR10 [26], we observe that ConvRFM recovers features corresponding to edge detectors. We further enable deep feature learning with convolutional kernels by developing a layerwise training scheme with ConvRFM, which we refer to as Deep ConvRFM. We demonstrate that Deep ConvRFM learns features similar to those learned by deep convolutional neural networks. Furthermore, we show that Deep ConvRFM overcomes limitations of convolutional kernels identified in [24] and exhibits local feature adaptivity. Lastly, we demonstrate that Deep ConvRFM provides improvement over CNTK and ConvRFM on several standard image classification datasets, indicating a benefit to deep feature learning. Our results advance understanding of how convolutional networks automatically learn features from data and provide a path toward integrating convolutional feature learning into general machine learning models. 2 Convolutional Neural Feature Ansatz (CNFA) Let f : Rc×P ×Q → R denote a convolutional neural network (CNN) operating on P ×Q resolution images with c color channels. The ℓth convolutional layer of a CNN involves applying a function hℓ : Rcℓ−1×Pℓ−1×Qℓ−1 → Rcℓ×Pℓ×Qℓ defined recursively as hℓ(x) = ϕ(� Wℓ ∗hℓ−1(x)) with h1 = x, � Wℓ ∈ Rcℓ×cℓ−1×q×q denoting cℓ filters of size cℓ−1 ×q ×q, ∗ denoting the convolution operation, and ϕ denoting an elementwise activation function. To understand how features emerge in convolutional networks, we abstract a convolutional network to a function of the form f(x) = g(W1x[1, 1], . . . , W1x[i, j], . . . , W1x[P, Q]), i ∈ [P], j ∈ [Q] ; (1) where W1 ∈ Rc1×cq2 is a matrix of c1 stacked filters of size cq2 and x[i, j] ∈ Rcq2 denotes the patch of x centered at coordinate (i, j). This abstraction is helpful since it allows us to consider feature learning in convolutional networks with arbitrary architecture (e.g., pooling layers, batch normalization, etc.) after any given convolutional layer. Up to rotation and reflection by the left singular vectors, the feature extraction properties of W1 are determined by the singular values and right singular vectors of W1. These singular values and vectors can be recovered from the matrix W T 1 W1, which is the empirical (uncentered) covariance of filters in the first layer. This argument extends to analyze features selected at layer ℓ of a CNN by considering a function of the form f(x) = gℓ(Wℓhℓ−1(x)[1, 1], . . . , Wℓhℓ−1(x)[Pℓ−1, Qℓ−1]). We refer to the matrix W T ℓ Wℓ as a Convolutional Neural Feature Matrix (CNFM) and note that this matrix is proportional to the (uncentered) empirical covariance matrix of filters in layer ℓ. We use the form of convolutional networks presented in Eq. (1) to state our Convolutional Neural Feature Ansatz (CNFA). Let Gℓ(x) := gℓ(Wℓhℓ−1(x)[1, 1], . . . , Wℓhℓ−1(x)[Pℓ−1, Qℓ−1]). Then, after training f for at least one epoch of (stochastic) gradient descent on standard loss functions: W ⊤ ℓ Wℓ ∝ n � p=1 � (i,j)∈S ∇hℓ−1(x)[i,j]Gℓ(x) � ∇hℓ−1(x)[i,j]Gℓ(x) �⊤ ; (2) where S = {(i, j)}i∈[Pℓ−1],j∈[Qℓ−1] denotes the set of indices of patches utilized in the convolution operation in layer ℓ. The CNFA (Eq. 2) mathematically implies that the convolutional neural feature matrices are 2AlexNet VGG11 VGG13 VGG16 VGG19 VGG11 BN VGG13 BN VGG16 BN VGG19 BN ResNet18 ResNet34 ResNet50 ResNet101 ResNet152 Pearson Correlation 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 CNFA Verification for Pre-trained Models on ImageNet Initial CNFM and Trained NFM Trained CNFM and AGOP A VGG11 AGOP Initial NFM Trained NFM ResNet18 AlexNet B Figure 1: A. Correlation between initial CNFM and trained CNFM (red) and trained CNFM with AGOP (green) for convolutional layers in VGG, AlexNet, and ResNet on ImageNet (224 × 224 resolution color images). B. Initial CNFM, trained CNFM, and AGOP matrices for the first convolutional layer of ResNet18, VGG11, and AlexNet on ImageNet. proportional to the average gradient outer product (AGOP) with respect to the patches of the input to layer ℓ. The CNFA implies that the structure of covariance matrices of filters in convolutional networks, an object studied in prior work [49], corresponds to AGOP over patches. Intuitively, the CNFA implies that convolutional features are constructed by identifying and amplifying those pixels in any patch that most change the output of the network. We now present extensive empirical evidence corroborating our ansatz. We subsequently present supporting theoretical evidence. 2.1 Empirical evidence for CNFA We now provide empirical evidence for the ansatz by computing the correlation between CNFMs and the AGOP for each convolutional layer in various CNNs. We provide three lines of evidence by computing correlations for the following models: (1) AlexNet [27], all VGGs [46], and all ResNet [19] models pre-trained on ImageNet [40] ; (2) SimpleNet models [18] trained on SVHN [33], GTSRB [20], CIFAR10 [26], CIFAR100, and ImageNet32 [10]; and (3) shallow CNNs across 10 standard computer vision datasets from PyTorch upon varying pooling and patch size of convolution operations. The first set of experiments provides evidence for the ansatz in large-scale state-of-the-art models on ImageNet. The second set provides evidence for the ansatz across standard computer vision datasets. The last set provides evidence for the ansatz holding across architecture choices. CNFA verification for pre-trained state-of-the-art models on ImageNet. We begin by providing evidence for the ansatz on pre-trained state-of-the-art models on ImageNet. In Fig. 1, we present these correlations for AlexNet, all VGG models and all ResNet models pre-trained on ImageNet, which are available for download from the PyTorch library [36].4 As a control, we verify that weights at the end of training are far from initialization (see the red bars in Fig. 1A). Note that despite the complexity involved in training these models (e.g., batch normalization, skip connections, custom optimization procedures, data augmentation) the Pearson correlation between the AGOP and CNFMs are remarkably high (> .9 for each layer of AlexNet and VGG13). In Fig. 1B, we additionally visualize the AGOP and CNFM for the first convolutional layer in AlexNet, VGG11, and ResNet18 to demonstrate the qualitative similarity between these matrices. In addition, in Appendix Fig. 7, we verify that these correlations are lower at initialization than at the end of training indicating that the ansatz is, in fact, a consequence of training. CNFA verification for SimpleNet on CIFAR10, CIFAR100, ImageNet32, SVHN, GTSRB. To verify the ansatz on other datasets, we also trained the SimpleNet model on five datasets including 4We evaluate all correlations between AGOP and CNFMs for all convolutional layers of AlexNet and all VGGs. To simplify computation on ResNets, we evaluate correlations between AGOP and CNFMs for the first layer in each BasicBlock and each Bottleneck, as defined in PyTorch. We note that for ResNet152, this computation involves computing correlation between matrices in 50 Bottleneck blocks. 3Input Image +CNFM +AGOP Layer 2 Layer 4 Layer 1 Layer 3 VGG11 +CNFM +AGOP AlexNet Input Image +CNFM +AGOP VGG11 +CNFM +AGOP AlexNet Figure 2: Comparison of features extracted by CNFMs and AGOPs across layers of VGG11 and AlexNet for two input images. These visualizations provide further supporting evidence that the CNFMs and AGOPs of early layers are performing an operation akin to edge detection. CIFAR10/100, ImageNet32, SVHN, and GTSRB. We note SimpleNet had achieved state-of-the-art results on several of these tasks at the time of its release (e.g., > 95% test accuracy on CIFAR10). We train SimpleNet models using the same optimization procedure provided from [18] (i.e., Adadelta [57] with weight decay and manual learning rate scheduling). We use a small initialization scheme of normally distributed weights with a standard deviation of 10−4 for convolutional layers. We note that we were able to recover high test accuracies across all datasets consistent with the results from [18] (see test accuracies for these trained SimpleNet models in Appendix Fig. 8). As shown in Appendix Fig. 8, we observe consistently high correlation between AGOPs and CNFMs across layers of SimpleNet. CNFA is robust to hyperparameter choices. We lastly study the effect of patch size and architecture choices on the CNFA for networks trained using the Adam optimizer [25]. We generally observe that larger patch sizes slightly reduce the correlation between AGOP and CNFMs, and that max pooling layers (in contrast to no pooling or average pooling) lead to higher correlation (Appendix Fig. 9). Interestingly, these results indicate that the choices used in state-of-the-art CNNs (max pooling layers and patch size of 3) are consistent with those that lead to highest correlation between AGOP and CNFMs. 2.2 Visualizing features captured by CNFM and AGOP We now visualize how the CNFM operates on patches of images to select features and demonstrate that AGOP over patches captures similar features. Both the CNFM and AGOP yield an operator on patches of images. Thus, to visualize how these matrices select features, we expand input images into individual patches, then apply either the CNFM or the AGOP to each patch. We then reduce the expanded image back to its original size by taking the norm over the spatial dimensions of each expanded patch. Formally, the 4value for each coordinate (i, j) ∈ Pℓ−1 × Qℓ−1 is replaced with ∥M 1 2 ℓ hℓ−1(X)[i, j]∥ where Mℓ := W T ℓ Wℓ. Our visualization reflects the magnitude of the patch in the image of the patch transformation. For example, if Mℓ is an edge detector, then ∥M 1 2 ℓ hℓ−1(X)[i, j]∥ will be large, if and only if the patch centered at coordinate (i, j) contains an edge. This visualization technique emerges naturally from the convolution operation in CNNs, where a post- activation hidden unit is generated by applying a filter to each patch independently of the others. Further, this visualization characterizes how a trained CNN extracts features across patches of any image. This is in contrast to visualization techniques based on saliency maps [41, 44, 45, 59], which consider gradients with respect to an entire input image and for a single sample. In addition to the high correlation between AGOP and CNFMs in the previous section, in Fig. 2, we observe that the AGOP and CNFMs transform input images similarly at any given layer of the CNN. For 224 × 224 images from ImageNet, CNFMs and AGOPs extracted from a pre-trained VGG11 model both emphasize objects and their edges in the image. We note these visualizations corroborate hypotheses from prior work that the first layer weights of deep CNNs learn an operator corresponding to edge detection [58]. Moreover, our results imply that the mathematical origin of edge detectors in convolutional neural networks is the average gradient outer product. In the following section, we will corroborate this claim by demonstrating that such edge detectors can be recovered without the use of any neural network through estimating the average gradient outer product of convolutional kernel machines. 2.3 Supporting Theoretical Evidence for CNFA The following theorem (proof in Appendix A) proves the ansatz for general convolutional networks after 1 step of full-batch gradient descent. Theorem 1. Let f denote a function that operates on m patches of size q, i.e., let f(v1, v2, . . . , vm) : Rq × . . . × Rq → R with f(v1, v2, . . . , vm) = g(Wv1, Wv2, . . . , Wvm) where W ∈ Rk×q and g(z1, . . . , zm) : Rk × . . . × Rk → R. Assume g(0) = 0 and ∂g(0) ∂zℓ = ∂g(0) ∂zℓ′ ̸= 0 for all ℓ, ℓ′ ∈ [m]. If W is trained for one step of gradient descent with mean squared loss on data {((v(p) 1 , . . . v(p) m ), yp)}n p=1 from initialization W (0) = 0, then for the point (u1, . . . , um): W (1)T W (1) ∝ m � r=1 ∂f (1)(u1, . . . , um) ∂vr ∂f (1)(u1, . . . , um) ∂vr T ; (3) where f (1)(v1, v2, . . . vm) := g(W (1)v1, W (1)v2, . . . , W (1)vm). We note the assumptions of Theorem 1 hold for several types of convolutional networks. As a simple example, the assumptions hold for convolutional networks with activation function ϕ satisfying ϕ(0) = 0 and ϕ′(0) ̸= 0 (e.g., tanh activation) with remaining layers initialized as constant matrices. Furthermore, we note that while the above theorem is stated for the first layer of a convolutional network, the same proof strategy applies for deeper layers by considering the subnetwork Gℓ(x). 3 CNFA as a general mechanism for convolutional feature learning We now show that the CNFA allows us to introduce a feature learning mechanism in any machine learning model on patches to capture features akin to those of convolutional networks. Given recent work connecting neural networks to kernel machines [22], we focus on convolutional kernels given by the Convolutional Neural Tangent Kernel (CNTK) [4] as our candidate model class. Intuitively, these models can be thought of as combining kernels evaluated across pairs of patches in images. While such models have achieved impressive performance [1, 6, 7, 29, 38, 42], these models do not automatically learn features from data unlike CNNs. Thus, as demonstrated in prior work [24, 52], there are tasks where CNTKs are significantly outperformed by corresponding CNNs. A major consequence of the CNFA is that we can now enable feature learning in CNTKs by leveraging the AGOP over patches. In particular, we can first solve kernel regression with the CNTK and then use 5Algorithm 1 Convolutional Recursive Feature Machine (ConvRFM) Input: X, y, KM, T, q ▷ Train data: (X, y), kernel: KM, iters.: T, and patch size: q Output: α, M ▷ Solution to kernel regression: α, and feature matrix: M M = Icq2 ▷ Initialize M to be the identity matrix of size cq2 × cq2 for t ∈ T do Ktrain = KM(X, X) ▷ KM(X, X)i,j := KM(xi, xj) α = yK−1 train M = 1 n � x∈X � (u,v)∈S(∇x[u,v]f(x))(∇x[u,v]f(x))T ▷ f(x) = αKM(X, x) end for the AGOP of the trained predictor over patches of images to learn features. We call our method the Convolutional Recursive Feature Machine (ConvRFM), as it is the convolutional variant of the original RFM [37]. We will demonstrate that ConvRFM accurately captures first layer feature learning in CNNs and can recover edge detectors as features when trained on standard image classification datasets. To account for deep convolutional feature learning, we extend ConvRFM to Deep ConvRFMs by sequentially learning features in a manner similar to layerwise training in CNNs. We show that Deep ConvRFM: (1) improves performance of CNTKs on local signal adaptivity tasks considered in [24] ; and (2) improves performance of CNTKs on several image classification tasks. 3.1 Convolutional Recursive Feature Machine (ConvRFM) We present the algorithm for ConvRFM in Algorithm 1. The ConvRFM algorithm recursively learns a feature extractor on patches of a given image by implementing the AGOP across patches of training data. Namely, the ConvRFM first builds a predictor with a fixed convolutional kernel. Then, we compute the AGOP of the trained predictor with respect to image patches, which we denote as the feature matrix, M. Lastly, we transform image patches with M and then repeat the previous steps. We provide a concrete example of this algorithm for the convolutional neural network Gausssian process (CNNGP) [9, 28] of a one hidden layer convolutional network with fully connected last layer operating on black and white images below. The CNNGP of a one hidden layer convolutional network with fully connected last layer, activation ϕ, and filter size q is given by K(x, z) = 1 PQ P � i=1 Q � j=1 ˇϕ(x[i, j]T z[i, j], ∥x[i, j]∥, ∥z[i, j]∥) ; where x, z ∈ RP ×Q, x[i, j] ∈ Rq2 denotes the vectorized q × q patch of x centered at coordinate (i, j), and ˇϕ(aT b, ∥a∥, ∥b∥) denotes the dual activation [15] of ϕ. For the case of ReLU activation, this dual activation has a well known form [9] and is given by ˇϕ(aT b, ∥a∥, ∥b∥) = 1 π � aT b � π − arccos � aT b ∥a∥∥b∥ �� + � ∥a∥2∥b∥2 − aT b � . In ConvRFM, we modify the inner product in the kernel above to be a Mahalanobis inner product, con- structing kernels of the form KM(x, z) := 1 PQ P � i=1 Q � j=1 ˇϕ(x[i, j]T Mz[i, j], x[i, j]T Mx[i, j], z[i, j]T Mz[i, j]) ; where M is a learned positive semi-definite matrix. In particular, M is updated as the AGOP of the esti- mator constructed by solving kernel regression with KM. In our experiments, we analyze performance when replacing ˇϕ with the Mahanolobis Laplace kernel used in [37] and with the CNTK of a deep convolutional ReLU network with fully connected last layer. We will make clear our choice of ˇϕ by denoting our method as CNTK-ConvRFM or Laplace-ConvRFM. 6Eigenvectors of CNTK-ConvRFM AGOP (SVHN) A Input Image +AGOP CNTK-ConvRFM +CNFM CNN B C Input Image +AGOP CNTK-ConvRFM SVHN +AGOP Laplace-ConvRFM SVHN Eigenvectors of Laplace-ConvRFM AGOP (SVHN) Figure 3: Features extractors learned by ConvRFM using CNTK (CNTK-ConvRFM) and Laplace kernel (Laplace-ConvRFM), which appear to operate as universal edge detectors. A. Top 8 eigenvectors of CNTK- ConvRFM and Laplace-ConvRFM trained on SVHN. We use 3 × 3 patches for CNTK-ConvRFM and 7 × 7 patches for Laplace-ConvRFM. B. Comparison of patch operators learned by CNTK-ConvRFM (given by the AGOP taken with respect to patches) and CNNs (given by the CNFM). C. Applying patch-based AGOP operators from ConvRFMs trained on SVHN to images from ImageNet. ConvRFM captures first layer features of convolutional neural networks. We now demonstrate that ConvRFM recovers features similar to those learned by first layers of CNNs. In Fig. 3A, we visualize the top eigenvectors of the feature matrix of CNTK-ConvRFM (filter size 3 × 3) and Laplace-ConvRFM (filter size 7 × 7) trained on SVHN. Training details for all methods are presented in Appendix B. We observe that these top eigenvectors resemble edge detectors [16]. In Fig. 3B, we visualize how the feature matrix of the CNTK-ConvRFM and the CNFM of the corresponding finite width CNN trained on SVHN transform SVHN images. Even though both operators arise from vastly different training procedures (solving kernel regression vs. training a CNN), we observe that both operators appear to extract similar features (corresponding to edges of digits) from SVHN images. We provide additional evidence for similarity between ConvRFM and CNN features in Appendix Fig. 10. To demonstrate further evidence of the universality of edge detector features arising from AGOP of CNTK-ConvRFM and Laplace-ConvRFM, we analyze how these AGOPs transform arbitrary images. In particular, in Fig. 3C, we apply these operators extracted from models trained on SVHN to images on ImageNet. We again observe that these operators remarkably extract edges from corresponding ImageNet images, which are of vastly different resolution (224 × 224 instead of 32 × 32) and contain vastly different objects. Such experiments provide conclusive evidence that AGOP with respect to patches of convolutional kernels recovers features akin to edge detectors. We present further experiments demonstrating emergence of edge detectors from convolutional kernels trained on CIFAR10 and GTSRB in Appendix Figs. 11 and 12. In particular, the eigenvectors of the AGOP often resemble Gabor filters with different orientations. In Figure 11, we see that horizontally, vertically, and diagonally aligned eigenvectors identify edges of the same alignment. 3.2 Deep feature learning with Deep ConvRFM ConvRFM is capable of only extracting features by linearly transforming patches of input images, which is analogous to extracting such features using the first layer of a CNN. In contrast, the CNFA implies that deep convolutional networks are capable of learning features in intermediate layers. To enable deep feature learning, we introduce Deep ConvRFM (see Algorithm 2) by sequentially learning features with AGOP in a manner similar to layerwise training in CNNs. In particular, Deep ConvRFM iterates the following steps: 1. Construct a predictor, �f, by training a convolutional kernel machine with kernel KM. 7Layer 1 Layer 2 Layer 3 Input Image +AGOP (Deep ConvRFM) +CNFM (CNN) +CNFM (CNN) +CNFM (CNN) +CNFM (CNN) +AGOP (Deep ConvRFM) +AGOP (Deep ConvRFM) +AGOP (Deep ConvRFM) Input Image Input Image Input Image Figure 4: Visualizations of features for each layer of Deep ConvRFM and the corresponding CNN on SVHN and the noisy digits task from [24]. 2. Update M to be the AGOP with respect to patches of the trained predictor. 3. Transform the data, x, with random features given by ϕ(Wx) where W denotes a set of convolutional filters with weights sampled according to N(0, M) and ϕ is a nonlinearity. Note that while we utilize random features and sample convolutional filters in Deep ConvRFM, we never utilize backpropgation to learn features or train models. Features are learned via the AGOP and models are trained by solving kernel regression, which is a convex optimization problem. For the base kernel for Deep ConvRFM, we utilize the deep CNTK [4] as implemented in the Neural Tangents library [34].5 Deep ConvRFM learns similar features to deep CNNs. We now present evidence that Deep Con- vRFMs learn similar features to those learned by deep CNNs. We analyze features learned by deep ConvRFM and the corresponding CNN on the local signal adaptivity synthetic tasks from [24] and SVHN. For the syn- thetic task from [24], we consider classification of MNIST digits embedded in a larger image of i.i.d. Gaussian noise. Dataset and training details are presented in Appendix B. In Fig. 4, we observe that AGOPs at each layer of Deep ConvRFM and and CNFMs at each layer of the corresponding CNN transform examples from both datasets similarly. 5In order to take gradient with respect to patches using Neural Tangents, we used a workaround that involved expanding images into their patch representations. This workaround unfortunately leads to heavy memory utilization, which limited our analysis of Deep ConvRFMs. Algorithm 2 Deep Convolutional Recursive Feature Machine (Deep ConvRFM) Input: X, y, {Kℓ}ℓ, T, L, q, k ▷ kernels: {Kℓ}ℓ, depth: L, channels: k Output: αL, {Mℓ}L ℓ=1 X1 = X ▷ Initialize embedding for ℓ ∈ L do αℓ, Mℓ = ConvRFM(Xℓ, y, Kℓ, T, q) Sample k filters Wℓ,k′ ∼ N(0, Mℓ) ▷ Wℓ ∈ Rk×q×q Xℓ+1 = ϕ(Wℓ ∗ Xℓ) ▷ ϕ: element-wise non-linearity end for 8A B 50 55 60 65 70 75 80 85 90 95 100 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 Test Accuracy Noise Std. Dev. Black and White Horizontal Bars in Noise Depth 3 - CNTK Depth 3 - ConvRFM Depth 3 - Deep ConvRFM 3-CNN 45 50 55 60 65 70 75 80 85 90 95 0 1 2 3 4 5 6 7 Test Accuracy Noise Std. Dev. MNIST Digits in Noise Depth 3 - CNTK Depth 3 - ConvRFM Depth 3 - Deep ConvRFM 3-CNN Figure 5: Test accuracy of CNTK, ConvRFM, Deep ConvRFM, and the corresponding CNN on local signal adaptivity tasks from [24] as a function of noise level. A. Identifying black and white bars in noisy images. B. MNIST digits placed randomly in noisy background image. Deep ConvRFM overcomes limitations of convolutional kernels. In the work [24] the authors posited local signal adaptivity, the ability to suppress noise and amplify signal in images, as a potential explanation for the superiority of convolutional neural networks over convolutional kernels. As supporting evidence, [24] demonstrated that convolutional networks generalized far better than convolutional kernels on image classification tasks in which images were embedded in a noisy background. We now demonstrate that by incorporating feature learning through patch-AGOPs, Deep ConvRFM exhibits local signal adaptivity on the tasks considered in [24] and thus, similar to CNNs, yield significantly improved performance over convolutional kernels. In particular, we begin by comparing performance of CNTK, Conv RFM, Deep ConvRFM, and corresponding CNNs on the following two image classification tasks from [24]: (1) images of black and white horizontal bars placed in a random position on larger images of Gaussian noise ; (2) MNIST images placed in a random position on larger images of Gausssian noise. The work [24] demonstrated that CNNs, unlike CNTK, could learn to threshold the background noise and amplify the signal in these tasks thus far outperforming CNTKs when the amount of background noise was large. In Fig. 5, we demonstrate that for these tasks CNNs, ConvRFMs, and Deep ConvRFMs all extract local signals and dim background noise through the AGOP, and thus far outperform CNTKs. Moreover, we observe that Deep ConvRFMs can provide up to a 5% improvement in performance over ConvRFM on the synthetic MNIST task, indicating a benefit to deep feature learning. Benefit of deep feature learning on real-world image classification tasks. Lastly, we analyze per- formance of CNTK, ConvRFM, Deep ConvRFM, and the corresponding three convolutional layer CNN on standard image classification datasets available for download from PyTorch. Consistent with our observa- tions for synthetic tasks from [24], we observe in Fig. 6A that ConvRFM and Deep ConvRFM provide an improvement over CNTK across almost all tasks. Moreover, we observe that ConvRFM and Deep Con- vRFM outperform CNTKs consistently when the corresponding CNN outperforms the CNTK. In Fig. 6B, we analyze the impact of deep feature learning by increasing the number of feature learning layers in Deep ConvRFM, i.e., the number of layers for which we utilize the AGOP to learn features. We observe that adding more layers of feature learning leads to consistent performance boost in the local signal adaptivity tasks from [24] and on select datasets such as SVHN and EMNIST [13]. 4 Discussion In this work, we identified a mathematical mechanism of feature learning in deep convolutional networks, which we posited as the Convolutional Neural Feature Ansatz (CNFA). Namely, the ansatz stated that features selected by convolutional networks, given by empirical covariance matrices of filters at any given layer, can be recovered by computing the average gradient outer product (AGOP) of the trained network 93-CNN Deep( ConvRFM ConvRFM CNTK Dataset 84.48±0.33 87.28 85.02 81.38 SVHN 64.90±0.16 67.97 67.97 67.74 CIFAR-10 29.78±0.17 37.64 37.64 37.64 CIFAR-100 93.35±0.34 93.02 93.02 91.76 GTSRB 88.35±0.22 87.60 86.73 86.01 EMNIST A B Test accuracy (%) 0 5 10 15 20 0 1 2 3 % Improvement Number of Feature Learning Layers Performance of Deep RFM SVHN EMNIST Noisy MNIST Digits Horizontal Bars in Noise Figure 6: A. Performance comparison of Deep ConvRFM with the corresponding CNTK and CNN on benchmark image classification datasets from PyTorch. B. Effect of number of feature learning layers on Deep ConvRFM performance. with respect to image patches. We presented empirical and theoretical evidence for the ansatz. Notably, we showed that convolutional filter covariances of neural networks pre-trained on ImageNet (AlexNet, VGG, ResNet) are highly correlated with AGOP with respect to patches (in many cases, Pearson correlation > .9). Since the AGOP with respect to patches can be computed on any function operating on image patches, we could use the AGOP to enable feature learning in any machine learning model operating on image patches. Thus, building on the RFM algorithm for fully connected networks from [37], we integrated the AGOP to enable deep feature learning in convolutional kernel machines, which could not apriori learn features, and referred to the resulting algorithms as ConvRFM and Deep ConvRFM. We demonstrated that ConvRFM and Deep ConvRFM recover features similar to those of deep convolutional neural networks, including evidence that features learned by these models can serve as universal edge detectors, akin to features learned in convolutional networks. Moreover, we demonstrated that ConvRFM and Deep ConvRFM overcome prior limitations of convolutional kernels, including the Convolutional Neural Tangent Kernel (CNTK), such the inability to adapt to localized signals in images [24]. Lastly, we showed a benefit to deep feature learning by demonstrating improvement in performance of Deep ConvRFM over ConvRFM and the CNTK on standard image classification benchmarks. We now conclude with a discussion of implications of our results and future directions. Identifying mechanisms driving success of deep learning. Understanding the mechanisms driving success of neural networks is an important problem for developing effective, interpretable and safe machine learning models. The complexities of training deep neural networks such as custom training procedures and layer structures (batch normalization, dropout, residual connections, etc.) can make it difficult to pinpoint overarching principles leading to effectiveness of these models. The fact that correlation between convolutional neural feature matrices (CNFMs) and AGOPs is high for convolutional networks pre-trained on ImageNet with all of these inherent complexities baked in, provides strong evidence that the connection between AGOP and CNFMs is key to identifying the core principles making these networks successful. Emergence of universal edge detectors with average gradient outer product. Detecting edges in images is a well-studied task in computer vision and classical approaches involved applying fixed convolutional filters to detect edges in images [2, 16, 54]. For example, AlexNet automatically learned filters in its first convolutional layer that were remarkably similar to Gabor filters [30]. Similarly, there was evidence that other convolutional networks pre-trained on ImageNet learned features akin to edge detection in the first layer [58]. Yet, it had been unclear how such filters automatically emerge through training. We demonstrated that the AGOP with respect to patches of a large class of convolutional models (convolutional neural networks and convolutional kernels) trained on various standard image classification tasks consistently recovered edge detectors (see Fig. 2, Fig. 3A, B). We further showed the universality of these edge detector features by demonstrating that features learned by ConvRFM on SVHN automatically identified edges in ImageNet 10images. This strongly suggests that edge detectors emerge from the underlying nature of the task rather than specific properties of architectures. Our findings indicate that understanding connections between AGOP and classical edge detection approaches is a promising direction for understanding emergence of features in the first layer of convolutional neural networks and for identifying simple algorithms to capture deeper convolutional features. Reducing computational complexity of convolutional kernels. In this work, we provided an ap- proach for enabling feature learning in convolutional kernels by iteratively training convolutional kernel ma- chines and computing AGOP of the trained predictor. Given that convolutional kernels are able to achieve impressive accuracy on standard datasets without any feature learning [1, 6, 7, 29, 42], these methods have the potential to provide state-of-the-art results upon incorporating feature learning. Yet, in contrast to the case of classical kernel machines such as those used in [37], evaluating the kernel for an effective CNTK (such as those with Global Average Pooling [4]) can be a far more computationally intensive process than simply training a convolutional neural network. For example, according to Neural Tangents [34], the CNTK of a Myrtle kernel [42] can take anywhere from 300 to 500 GPU hours for CIFAR10. Given that Deep ConvRFM involves constructing a kernel matrix and computing AGOP to capture features at each layer, reducing the evaluation time of convolutional kernels through strategies such as random feature approximations is key to making these approaches scalable. Acknowledgements A.R. is supported by the Eric and Wendy Schmidt Center at the Broad Institute. We acknowledge support from the National Science Foundation (NSF) and the Simons Foundation for the Collaboration on the Theoretical Foundations of Deep Learning6 through awards DMS-2031883 and #814639 as well as the TILOS institute (NSF CCF-2112665). This work used the programs (1) XSEDE (Extreme science and engineering discovery environment) which is supported by NSF grant numbers ACI-1548562, and (2) ACCESS (Advanced cyberinfrastructure coordination ecosystem: services & support) which is supported by NSF grants numbers #2138259, #2138286, #2138307, #2137603, and #2138296. Specifically, we used the resources from SDSC Expanse GPU compute nodes, and NCSA Delta system, via allocations TG-CIS220009. Code Availability All code is available at https://github.com/aradha/convrfm. References [1] B. Adlam, J. Lee, S. Padhy, Z. Nado, and J. Snoek. Kernel regression with infinite-width neural networks on millions of examples. arXiv preprint arXiv:2303.05420, 2023. [2] S. Albawi, T. A. Mohammed, and S. Al-Zawi. Understanding of a convolutional neural network. In International Conference on Engineering and Technology, pages 1–6. IEEE, 2017. [3] Z. Allen-Zhu and Y. Li. Backward feature correction: How deep learning performs deep learning. arXiv preprint arXiv:2001.04413, 2020. [4] S. Arora, S. S. Du, W. Hu, Z. Li, R. Salakhutdinov, and R. Wang. On exact computation with an infinitely wide neural net. In Advances in Neural Information Processing Systems, 2019. [5] J. Ba, M. A. Erdogdu, T. Suzuki, Z. Wang, D. Wu, and G. Yang. High-dimensional asymptotics of feature learning: How one gradient step improves the representation. Advances in Neural Information Processing Systems, 35:37932–37946, 2022. 6https://deepfoundations.ai/ 11[6] A. Bietti. Approximation and learning with deep convolutional models: a kernel perspective. arXiv preprint arXiv:2102.10032, 2021. [7] A. Bietti and J. Mairal. Group invariance, stability to deformations, and complexity of deep convolu- tional representations. The Journal of Machine Learning Research, 20(1):876–924, 2019. [8] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sas- try, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, and D. Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, 2020. [9] Y. Cho and L. Saul. Kernel methods for deep learning. In Advances in Neural Information Processing Systems, 2009. [10] P. Chrabaszcz, I. Loshchilov, and F. Hutter. A downsampled variant of imagenet as an alternative to the cifar datasets. arXiv preprint arXiv:1707.08819, 2017. [11] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition, 2014. [12] A. Coates, H. Lee, and A. Y. Ng. An analysis of single layer networks in unsupervised feature learning. In International Conference on Artificial Intelligence and Statistics, 2011. [13] G. Cohen, S. Afshar, J. Tapson, and A. Van Schaik. Emnist: Extending mnist to handwritten letters. In International Joint Conference on Neural Networks, pages 2921–2926. IEEE, 2017. [14] A. Damian, J. Lee, and M. Soltanolkotabi. Neural networks can learn representations with gradient descent. In Conference on Learning Theory, pages 5413–5452. PMLR, 2022. [15] A. Daniely, R. F. Frostig, and Y. Singer. Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity. In Advances in Neural Information Processing Systems, 2016. [16] R. C. Gonzales and P. Wintz. Digital image processing. Addison-Wesley Longman Publishing Co., Inc., 1987. [17] W. Härdle and T. M. Stoker. Investigating smooth multiple regression by the method of average derivatives. Journal of the American statistical Association, 84(408):986–995, 1989. [18] S. H. Hasanpour, M. Rouhani, M. Fayyaz, and M. Sabokrou. Lets keep it simple, using simple ar- chitectures to outperform deeper and more complex architectures. arXiv preprint arXiv:1608.06037, 2016. [19] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition, 2016. [20] S. Houben, J. Stallkamp, J. Salmen, M. Schlipsing, and C. Igel. Detection of traffic signs in real-world images: The German Traffic Sign Detection Benchmark. In International Joint Conference on Neural Networks, number 1288, 2013. [21] M. Hristache, A. Juditsky, J. Polzehl, and V. Spokoiny. Structure adaptive approach for dimension reduction. Annals of Statistics, pages 1537–1566, 2001. [22] A. Jacot, F. Gabriel, and C. Hongler. Neural Tangent Kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems, 2018. [23] A. Jacot, E. Golikov, C. Hongler, and F. Gabriel. Feature learning in l_2-regularized dnns: Attrac- tion/repulsion and sparsity. Advances in Neural Information Processing Systems, 35:6763–6774, 2022. [24] S. Karp, E. Winston, Y. Li, and A. Singh. Local signal adaptivity: Provable feature learning in neural networks beyond kernels. Advances in Neural Information Processing Systems, 34:24883–24897, 2021. 12[25] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015. [26] A. Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, University of Toronto, 2009. [27] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. Advances in Neural Information Processing Systems, 25, 2012. [28] J. Lee, Y. Bahri, R. Novak, S. Schoenholz, J. Pennington, and J. Sohl-Dickstein. Deep neural networks as Gaussian processes. In International Conference on Learning Representations, 2017. [29] Z. Li, R. Wang, D. Yu, S. S. Du, W. Hu, R. Salakhutdinov, and S. Arora. Enhanced convolutional neural tangent kernels. arXiv preprint arXiv:1911.00809, 2019. [30] S. Luan, C. Chen, B. Zhang, J. Han, and J. Liu. Gabor convolutional networks. IEEE Transactions on Image Processing, 27(9):4357–4366, 2018. [31] S. Maji, E. Rahtu, J. Kannala, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013. [32] A. Mousavi-Hosseini, S. Park, M. Girotti, I. Mitliagkas, and M. A. Erdogdu. Neural networks efficiently learn low-dimensional representations with sgd. arXiv preprint arXiv:2209.14863, 2022. [33] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with unsupervised feature learning. Advances in Neural Information Processing Systems (NIPS), 2011. [34] R. Novak, L. Xiao, J. Hron, J. Lee, A. A. Alemi, J. Sohl-Dickstein, and S. Schoenholz. Neural Tan- gents: Fast and easy infinite neural networks in Python. In International Conference on Learning Representations, 2020. [35] S. Parkinson, G. Ongie, and R. Willett. Linear neural network layers promote learning single-and multiple-index models. arXiv preprint arXiv:2305.15598, 2023. [36] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems, 2019. [37] A. Radhakrishnan, D. Beaglehole, P. Pandit, and M. Belkin. Feature learning in neural networks and kernel machines that recursively learn features. arXiv preprint arXiv:2212.13881, 2022. [38] A. Radhakrishnan, G. Stefanakis, M. Belkin, and C. Uhler. Simple, fast, and flexible framework for matrix completion with infinite width neural networks. Proceedings of the National Academy of Sciences, 119(16):e2115064119, 2022. [39] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, 2021. [40] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and F.-F. Li. ImageNet large scale visual recognition challenge. International Journal of Computer Vision, 2015. [41] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra. Grad-cam: Visual expla- nations from deep networks via gradient-based localization. In Proceedings of the IEEE International Conference on Computer Vision, pages 618–626, 2017. [42] V. Shankar, A. Fang, W. Guo, S. Fridovich-Keil, J. Ragan-Kelley, L. Schmidt, and B. Recht. Neural kernels without tangents. In International Conference on Machine Learning, pages 8614–8623. PMLR, 2020. 13[43] Z. Shi, J. Wei, and Y. Lian. A theoretical analysis on feature learning in neural networks: Emergence from inputs and advantage over fixed features. In International Conference on Learning Representations, 2022. [44] A. Shrikumar, P. Greenside, and A. Kundaje. Learning important features through propagating acti- vation differences. In International Conference on Machine Learning, pages 3145–3153. PMLR, 2017. [45] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013. [46] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. [47] S. Trivedi and J. Wang. The expected jacobian outerproduct: Theory and empirics. arXiv preprint arXiv:2006.03550, 2020. [48] S. Trivedi, J. Wang, S. Kpotufe, and G. Shakhnarovich. A consistent estimator of the expected gradient outerproduct. In UAI, pages 819–828, 2014. [49] A. Trockman, D. Willmott, and J. Z. Kolter. Understanding the covariance structure of convolutional filters. arXiv preprint arXiv:2210.03651, 2022. [50] K. Tunyasuvunakool, J. Adler, Z. Wu, T. Green, M. Zielinski, A. Žídek, A. Bridgland, A. Cowie, C. Meyer, A. Laydon, S. Velankar, G. Kleywegt, A. Bateman, R. Evans, A. Pritzel, M. Figurnov, O. Ronneberger, R. Bates, S. Kohl, and D. Hassabis. Highly accurate protein structure prediction for the human proteome. Nature, 596:1–9, 2021. [51] B. S. Veeling, J. Linmans, J. Winkens, T. Cohen, and M. Welling. Rotation equivariant cnns for digital pathology. In Medical Image Computing and Computer Assisted Intervention, pages 210–218. Springer, 2018. [52] N. Vyas, Y. Bansal, and P. Nakkiran. Limitations of the ntk for understanding generalization in deep learning. arXiv preprint arXiv:2206.10012, 2022. [53] C. Yadav and L. Bottou. Cold case: The lost mnist digits. In Advances in Neural Information Processing Systems 32. Curran Associates, Inc., 2019. [54] R. Yamashita, M. Nishio, R. K. G. Do, and K. Togashi. Convolutional neural networks: an overview and application in radiology. Insights into imaging, 9:611–629, 2018. [55] G. Yang and E. J. Hu. Tensor Programs IV: Feature learning in infinite-width neural networks. In International Conference on Machine Learning, 2021. [56] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How transferable are features in deep neural networks? In Advances in Neural Information Processing Systems, volume 27, 2014. [57] M. D. Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012. [58] M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. In European Con- ference on Computer Vision, pages 818–833. Springer, 2014. [59] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning deep features for discriminative localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2921–2929, 2016. 14A Theoretical Evidence for Deep Convolutional Feature Ansatz Proof of Theorem 1. Gradient descent proceeds as follows: B(1) = B(0) + η n � p=1 m � ℓ=1 ∂g(B(0)v(p) 1 , . . . , B(0)v(p) m ) ∂zℓ (yp − f(v(p) 1 , . . . , v(p) m ))v(p) ℓ T . Since B(0) = 0, g(0) = 0 and ∂g(0) ∂zℓ = G for fixed nonzero G ∈ Rk, the above expression reduces to: B(1) = η n � p=1 m � ℓ=1 Gypv(p) ℓ T . Thus, we conclude that B(1)T B(1) = η2 n � p,p′=1 m � ℓ,ℓ′=1 ypyp′v(p) ℓ GT Gv(p′) ℓ′ T = � η2m2GT G �   n � p,p′=1 m � ℓ,ℓ′=1 ypyp′v(p) ℓ v(p′) ℓ′ T   ∝ n � p,p′=1 m � ℓ,ℓ′=1 ypyp′v(p) ℓ v(p′) ℓ′ T . Now, we finish the proof by showing the right hand side of Eq. (3) is proportional to the same quantity above. First, we have that ∂f (1)(u1, . . . , um) ∂vr = B(1)T ∂g(B(1)u1, . . . , B(1)um) ∂zr . Thus, letting u = (u1, . . . um) and B(1)u = (B(1)u1, . . . , B(1)um), we have that m � r=1 ∂f (1)(u) ∂vr ∂f (1)(u) ∂vr T = m � r=1 B(1)T ∂g(B(1)u) ∂zr ∂g(B(1)u) ∂zr T B(1)T = η2 m � r=1 � n � p=1 m � ℓ=1 ypv(p) ℓ GT ∂g(B(1)u) ∂zr �   n � p′=1 m � ℓ′=1 ∂g(B(1)u) ∂zr T Gy′ pv(p′) ℓ′ T   = η2 n � p,p′=1 m � ℓ,ℓ′=1 ypyp′v(p) ℓ v(p′) ℓ′ T � m � r=1 GT ∂g(B(1)u) ∂zr ∂g(B(1)u) ∂zr T G � ∝ n � p,p′=1 m � ℓ,ℓ′=1 ypyp′v(p) ℓ v(p′) ℓ′ T . B Experimental Details Neural network comparisons. For all neural network experiments, we reported the best test accuracy across all epochs. For the Adam experiments, we trained CNNs without bias, with learning rate 10−4, width 64, without padding and with minibatch size 128. For EMNIST, CIFAR-10/100, SVHN, GTSRB the networks were trained for 500 epochs. For the toy datasets, the networks were trained for 25 epochs. For SGD experiments, the setup was identical except the learning rate was 10−1, and EMNIST, CIFAR-10/100, SVHN, GTSRB were trained for 2000 epochs, and toy datasets for 100 epochs. 15Visualizations. For the visualizations in Figs. 3, 12, the toy tasks were visualized with ∥M 1/2x∥, while CIFAR and SVHN were visualized with ∥Mx∥. Further, for the neural networks in CIFAR and SVHN, the initial weight matrices were subtracted before using the CNFMs. For Figure 4, the visualizations were also done with the full M matrix and subtracting the initial weights. The weight matrices were extracted after 250 epochs. For visualization, the M matrix that gave the best performance of the 5 iterations of ConvRFM was selected, and the CNN neural feature matrices were extracted at the end of training. Deep ConvRFM. For Deep-ConvRFM experiments, we greedily selected the best performing M matrix among 5 rounds of ConvRFM for each depth. Further, we tuned regularization among 10−8, 10−5, 10−3 and divided the train and test kernel matrices by the maximum value of the train kernel matrix. In all of the experiments, we used the same architecture as the CNN. In particular, we sampled 64 filters in each layer and removed bias. Further, to ensure that the ConvRFM did not have access to a kernel of additional depth, we reduced the depth of the kernel by 1 with each layer of Deep ConvRFM. Instead of sampling from M, we generated filters sampled from standard normal distribution and applied M 1/2 to each filter. CNFA verification. For the CNFA verification experiments in Figure 9, we used the uncentered corre- lation (commonly-known as cosine similarity) to measure similarity between the AGOP and CNFM. The correlation was averaged over ten datasets: Fine-Grained Visual Classification of Aircraft [31], PatchCame- lyon [51], CIFAR-10, STL-10 [12], GTSRB, SVHN, Caltech101, DTD [11], QMNIST [53], EMNIST. We used zero padding in all layers, a learning rate of 2 × 10−4, 500 epochs of training with the Adam optimizer, and minibatch size of 128. For datasets with multiple color channels, the RGB color channels were scaled and centered to have means [125.3/255, 123/125, 113.9/225] and standard deviations [63.0/225, 62.1/225, 66.7/225], respectively. Datasets with images larger than 32x32 resolution were resized using PyTorch’s resize transform to 32x32 resolution. All layers were initialized from a standard normal distribution. The first layer was initialized with standard deviation 5 × 10−3, while the remaining layers were sampled with standard deviation 10−2. Convolutional kernel implementation. To implement convolutional kernels, we used the Neural Tan- gents library [34]. Mahalanobis kernels were not implemented directly in this library at the time of pub- lication. To implement ConvRFM, we performed the following procedure: (1) unfold each image into all patches (without any padding), (2) applied M 1/2 to each patch independently, (3) reshaped the images to be 2-dimensional, and (4) set the stride of the first-convolutional layer in the kernel to the patch size, then ran kernel regression. 16ResNet18-Trained ResNet18-Init ResNet34-Trained ResNet34-Init Pearson Correlation 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Comparison of CNFA at Initialization and After Training Figure 7: Comparison of correlation between CNFMs and AGOP for randomly initialized ResNets and pre- trained ResNets on ImageNet. 17Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6 Layer 7 Layer 8 Layer 9 Layer 10 Layer 11 Layer 12 Correlation 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 CNFA Verification for SimpleNet across 5 Datasets Initial CNFM and Trained CNFM Trained CNFM and AGOP A SimpleNet Test Accuracy (%) Dataset 95.13 CIFAR10 76.63 CIFAR100 95.97 SVHN 96.99 GTSRB 51.50 ImageNet32 B SimpleNet Performance Figure 8: A. Correlation between initial CNFM and trained CNFM (red) and trained CNFM and AGOP (green) for each convolutional layer of SimpleNets trained on 5 datasets. B. Performance of SimpleNet on the 5 corresponding datasets. 18Layer 1 Layer 2 Layer 3 Layer 4 Correlation 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 NFM and AGOP after training (CNN-VEC) Patch Size = 3 Patch Size = 5 Patch Size = 7 Layer 1 Layer 2 Layer 3 Layer 4 Correlation 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 NFM at initialization and AGOP (CNN-VEC) Patch Size = 3 Patch Size = 5 Patch Size = 7 Layer 1 Layer 2 Layer 3 Layer 4 Correlation 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 NFM at initialization and AGOP (Avg. Pooling) Patch Size = 3 Patch Size = 5 Patch Size = 7 Layer 1 Layer 2 Layer 3 Layer 4 Correlation 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 NFM and AGOP after training (Avg. Pooling) Patch Size = 3 Patch Size = 5 Patch Size = 7 Layer 1 Layer 2 Layer 3 Layer 4 Correlation 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 NFM at initialization and AGOP (Max Pooling) Patch Size = 3 Patch Size = 5 Patch Size = 7 Layer 1 Layer 2 Layer 3 Layer 4 Correlation 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 NFM and AGOP after training (Max Pooling) Patch Size = 3 Patch Size = 5 Patch Size = 7 A B C D E F Figure 9: Correlation (cosine similarity) of NFM and AGOP across patch sizes and pooling types on ten datasets. (A) CNN-VEC, trained NFM and AGOP, (B) Max pooling, trained NFM and AGOP, (C) Average pooling, trained NFM and AGOP, (D) CNN-VEC, initial NFM and AGOP, (E) Max pooling, initial NFM and AGOP, (F) Average pooling, initial NFM and AGOP. 19A B Input Images +AGOP (ConvRFM) +CNFM (CNN) Input Images +AGOP (ConvRFM) +CNFM (CNN) Input Images +AGOP (ConvRFM) +CNFM (CNN) C Black and White Horizontal Bars MNIST in Noisy Background CIFAR10 Figure 10: Comparison of feature extraction performed by patch-AGOPs and CNFMs from ConvRFM and the corresponding CNN. A. Visualizations for models trained on horizontal bars in noisy backgrounds (the toy task from [24]). B. Visualizations for models trained on MNIST images in noisy backgrounds, which was also the task considered in [24]. C. Visualizations for models trained on CIFAR10. 20Eigenvectors of CNTK-ConvRFM (SVHN) Eigenvectors of Laplace-ConvRFM (SVHN) Eigenvectors of CNTK-ConvRFM (GTSRB) Input Images Figure 11: Top eigenvectors of patch-AGOP from Laplace-ConvRFM and CNTK-ConvRFM trained on standard image classification datasets (SVHN and GTSRB) act as universal edge detectors of different orientations. We visualize images after applying each top eigenvector to image patches of ImageNet images. 21A Eigenvectors of CNTK-ConvRFM AGOP (CIFAR10) Eigenvectors of CNTK-ConvRFM AGOP (GTSRB) Input Image +CNFM (VGG11) (ImageNet) +CNFM (AlexNet) (ImageNet) +AGOP (CNTK-ConvRFM) (SVHN) +AGOP (Laplace-ConvRFM) (SVHN) +AGOP (CNTK-ConvRFM) (GTSRB) +AGOP (CNTK-ConvRFM) (CIFAR10) B Figure 12: A. Visualization of top 8 eigenvectors of patch-AGOP from CNTK-ConvRFM trained on CI- FAR10 and GTSRB. B. Comparison of feature extraction on ImageNet data performed by CNFMs from VGG11, AlexNet pre-trained on ImageNet and patch-AGOP operators of CNTK-ConvRFM trained on SVHN, GTSRB, CIFAR10 and Laplace-ConvRFM trained on SVHN. 22