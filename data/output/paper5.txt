Differentially Private Functional Summaries via the Independent Component Laplace Process Haotian Lin hzl435@psu.edu Department of Statistics, The Pennsylvania State University, University Park, PA 16802, USA Matthew Reimherr mreimherr@psu.edu Department of Statistics, The Pennsylvania State University, University Park, PA 16802, USA and Amazon Science, Seattle, WA 98109, USA Abstract In this work we propose a new mechanism for releasing differentially private functional sum- maries called the Independent Component Laplace Process, or ICLP, mechanism. By treat- ing the functional summaries of interest as truly infinite-dimensional objects and perturbing them with the ICLP noise, this new mechanism relaxes assumptions on data trajectories and preserves higher utility compared to classical finite-dimensional subspace embedding approaches in the literature. We establish the feasibility of the proposed mechanism in multiple function spaces. Several statistical estimation problems are considered, and we demonstrate by slightly over smoothing the summary, the privacy cost will not dominate the statistical error and is asymptotically negligible. Numerical experiments on synthetic and real datasets demonstrate the efficacy of the proposed mechanism. Keywords: Differential Privacy, Functional Data Analysis, Hilbert Space, Reproducing Kernel Hilbert Space, Stochastic Processes, 1. Introduction Data security has garnered critical attention in the last decade as substantial individualized data are collected. The most widely used paradigm in formal data privacy is differential privacy (DP), introduced by Dwork et al. (2006). DP provides a rigorous and interpretable definition for data privacy as it bounds the amount of information that attackers can in- fer from publicly released database queries. Numerous mechanisms have been developed under conventional data settings like scalar or vector-valued data. However, advances in technologies enable us to collect and process densely observed data over some temporal or spatial domains, which are coined functional data to differentiate them from classic mul- tivariate data (Ramsay et al., 2005; Kokoszka and Reimherr, 2017; Ferraty and Romain, 2011). Even though functional data analysis, FDA, has been proven useful in various fields like economics, finance, genetics and etc., and has been researched widely in the statistical community, there are only a few works concerning privacy preservation within the realm of functional data. 1 arXiv:2309.00125v1 [stat.ML] 31 Aug 2023In this paper, we propose an additive noise mechanism for functional summaries, namely infinite dimensional summaries, to achieve ϵ-DP. Additive noise mechanisms are one of the most commonly used mechanisms to achieve DP, which sanitizes statistical summaries by adding calibrated noise from predetermined distributions, e.g. Laplace and Gaussian mech- anism (Dwork et al., 2006, 2014). With functional summaries, classic privacy tools embed the problem into a finite-dimensional subspace by using finite basis expansions to approx- imate summaries and perturb expansion coefficients with i.i.d. noise (Wang et al., 2013; Chandrasekaran et al., 2014; Alda and Rubinstein, 2017; Zhang et al., 2012). However, these mechanisms have potential weaknesses. First, determining the dimension of the sub- space is crucial as it plays a trade-off role between utility and privacy. While data-driven approaches might cause potential privacy leakage, a pre-determined dimension will lack adaption to the data, potentially failing to capture the shape of the functions or injecting excess noise. Second, in multivariate settings, privacy budget allocation over each com- ponent can affect a mechanism’s utility and robustness substantially. Some previous work has shown that capturing the covariance structure in the data can substantially reduce the amount of noise injected (Hardt and Talwar, 2010; Awan and Slavkovi´c, 2021). However, current mechanisms allocate an equal privacy budget to all components, failing to recognize the importance of different functional components, and thus injecting excess noise for “more important” components, degrading utility and robustness. Contribution: To overcome the downsides inherent in finite-dimensional subspaces embedding approaches, we treat the functional summary and privacy noise as truly infinite- dimensional. We propose a mechanism by perturbing functional summaries with a stochastic process called the Independent Component Laplace Process (ICLP), and name this mecha- nism as the ICLP mechanism. It ensures a full path-level release rather than a finite grid of evaluating points. We establish the feasibility of the ICLP mechanism in a general separable Hilbert space, H, by characterizing a subspace of H and show that the feasibility holds if and only if functional summaries reside in this subspace. We also show how the proposed mechanism applies to the space of the continuous functions, even though this space is not a Hilbert space. We provide two strategies based on regularization to restrict the functional summaries to be within a desired subspace and show one can gain privacy for “free” by slightly over smooth the summaries. We demonstrate the proposed mechanism for the mean function protection problem and show it can go beyond classic functional data settings, as it is also applicable to the realm of more classic non-parametric smoothing problems like kernel density estimation. To obtain a privacy-safe regularization parameter in the regular- ization term, we adopt the plug-in approach such that the regularization parameters are tied to the covariance structure of the ICLP noise. This approach not only overcomes the po- tential information leakage in conventional data-driven approaches, but also has theoretical utility guarantees. The proposed mechanism differentiates itself from current mechanisms in the following senses. First, the ICLP mechanism avoids finite-dimensional subspace em- beddings and frees the assumption that every dataset in the database shares the same finite-dimensional subspace. Second, the privacy budgets allocated to each component are not uniform but proportional to the global sensitivity of that component. Compared to uni- form allocation in current mechanisms, like the functional mechanism (Zhang et al., 2012) and Bernstein mechanism (Alda and Rubinstein, 2017), such allocation injects less noise into the more important components, which improves the utility of the released summaries. 2Related Work: In the overlap of functional summaries and differential privacy, the landmark paper is Hall et al. (2013) which provided a framework for achieving (ϵ, δ)-DP on infinite dimensional functional objects, but focussed on a finite grid of evaluation points. The follow-up work in Mirshani et al. (2019) pushed Hall et al. (2013)’s result forward, established (ϵ, δ)-DP over the full functional path for objects in Banach spaces. In more general spaces, Reimherr and Awan (2019) considered elliptical perturbations to achieve (ϵ, δ)-DP in locally convex vector spaces, including all Hilbert spaces, Banach spaces, and Frechet spaces. They also showed the impossibility of achieving ϵ-DP in infinite dimensional objects with elliptical distributions. Turning to ϵ-DP, a series of works have been proposed by resorting to finite-dimensional representations, like polynomial bases, trigonometric bases, or Bernstein polynomial bases to approximate target functional summaries (Wang et al., 2013; Chandrasekaran et al., 2014; Alda and Rubinstein, 2017) and loss functions (Zhang et al., 2012). One then per- turbs the expansion coefficients via the Laplace mechanism independently. In addition to additive noise mechanisms, Awan et al. (2019) extended the exponential mechanism (Mc- Sherry and Talwar, 2007) to any arbitrary Hilbert spaces and showed its application to functional principal component analysis. From the perspective of the robust noise injec- tion, a heterogeneous noise injection scheme (Phan et al., 2019) is proposed by assigning different weighted privacy budgets on each coordinate to further improve the robustness of private summaries. 2. Preliminaries and Notation 2.1 Differential Privacy Let D be the collection of all possible n-unit databases and D be an element of D. Denote the summary of interest as f : D → Ω, where (Ω, F) is a measurable space. We denote the sanitized version of fD by ˜fD, which is a random element of Ω indexed by D. We state the definition of differential privacy in terms of conditional distributions (Wasserman and Zhou, 2010). Definition 1 Let ˜fD be the sanitized version of the functional summary fD. Assume {PD : D ∈ D} is the family of probability measures over Ω induced by { ˜fD : D ∈ D}. We say ˜fD achieves ϵ-DP if for any two adjacent datasets (only different on one record) D, D′, and any measurable set A ∈ F, one has PD(A) ≤ eϵPD′(A). The definition implies the summaries of two adjacent datasets should have almost the same probability distribution. The privacy budget ϵ controls how much privacy will be lost while releasing the result, and a small ϵ implies a higher similarity between PD and PD′ and thus increased privacy. 2.2 Notation We introduce some notation that will be used now throughout the paper. Let H be a real separable Hilbert space with inner product ⟨·, ·⟩H. A random element X ∈ H is said to have 3mean µ ∈ H and (linear) covariance operator C : H → H if E[⟨X, h⟩H] = ⟨µ, h⟩H and Cov(⟨X, h1⟩H, ⟨X, h2⟩H) = ⟨Ch1, h2⟩H, for any h, h1, h2 in H. A linear operator, C, is a covariance operator if and only if it is symmetric, positive semidefinite, and trace class (Bosq, 2000). Denoted {λj, ϕj}j≥1 as the eigenvalues and eigenfunctions of C, then we define two norms associated with C: ∥h∥C = � � � � � j≥1 ⟨h, ϕj⟩2 H λj and ∥h∥1,C = � j≥1 |⟨h, ϕj⟩H| � λj . We also denote the two corresponding subspaces of H as HC = {h ∈ H : ∥h∥C < ∞} and H1,C = {h ∈ H : ∥h∥1,C < ∞}. Note ∥·∥C is the classic Cameron-Martin norm induced by C (Bogachev, 1998) and HC is called the Cameron-Martin space of C. Here ∥·∥1,C is analogous to a weighted ℓ1-norm, and h ∈ H1,C leads to h ∈ HC. Lastly, we will use some common notation when discussing asymptotic results, namely, let an ≍ bn and an ≲ bn(an = O(bn)) denote |an/bn| → c and |an/bn| ≤ c, respectively, for some constant c when n → ∞. 3. The ICLP Mechanism and its Feasibility In this section, we first define the Independent Component Laplace Process, and then formally propose the the ICLP mechanism, where the private summary takes the form of ˜fD = fD + σZ with Z as an ICLP noise. Initially, we assume fD lies in a real separable Hilbert space H. Then we also show that privacy protection can also hold for the space of continuous functions (which is not a Hilbert space) as well under certain assumptions on the covariance operator. The proofs of all the Lemmas and Theorems can be found in Appendix A. 3.1 Independent Component Laplace Process The proposed stochastic process is motivated by Mirshani et al. (2019), who achieved (ϵ, δ)- DP on functional summaries in Banach spaces. Formally, their mechanism can be written as ˜fD = fD + σZ where Z ∼ GP(0, C) is a centered Gaussian process with covariance operator C. There is a dual perspective of this mechanism. By applying the Karhunen- Lo´eve Theorem (Kosambi, 2016), the mechanism is equivalent to ˜fD = fD + σZ = ∞ � j=1 (⟨fD, ϕj⟩ + σ ⟨Z, ϕj⟩) ϕj, (1) where {λj}j≥1 and {ϕj}j≥1 are the eigenvalues and eigenfunctions of C and ⟨Z, ϕj⟩ indepen- dently follows N(0, λj), meaning that the mechanism perturbs each component’s coefficient with independent normal random variables. Unfortunately, the existing Laplace process cannot play a role analogous to the Gaussian process under such an expansion since it is an elliptical distribution and it has been proved that no elliptical distribution can be used to achieve ϵ-DP in infinite-dimensional spaces (Reimherr and Awan, 2019). Motivated by the dual perspective from the Karhunen-Lo´eve expansion and the Laplace mechanism in 4the scalar setting, we consider adding independent Laplace noises with heterogeneous vari- ances in the expansion (1), which is equivalent to perturbing the functional summary with a particular stochastic process. The desired stochastic process is defined as follows. Definition 2 Let X be a random element in H with E ∥X∥2 < ∞. Given a non-negative decreasing real sequence, {λj}j≥1, satisfying � j≥1 λj < ∞, and an orthonormal basis of H, {ϕj}j≥1, we say X is an Independent Component Laplace Process with mean µ if it admits the decomposition X = µ + ∞ � j=1 � λjZjϕj, where Zj are i.i.d. Laplace random variables with zero-mean and variance 1. The collection of square integrable random elements of H is itself a Hilbert space with inner product E⟨X, Y ⟩H. Thus, while the ICLP is given as an infinite sum, it is still well-defined, see Chapter 1 in Bosq (2000). Theorem 3 If the given non-negative decreasing real sequence, {λj}j≥1, satisfies � j≥1 λj < ∞, and the basis {ϕj}j≥1 are orthonormal in H, then the stochastic process defined in Def- inition 2 is well-defined in H. 3.2 Feasibility in Separable Hilbert Spaces To investigate the feasibility of a randomized mechanism, one can start with the equiv- alence/orthogonality of probability measures. As discussed in Awan et al. (2019) and Reimherr and Awan (2019), the probability measures induced by an ϵ-DP mechanism are necessarily equivalent (though this is not sufficient for DP) in a probabilistic sense; oth- erwise, it is impossible to be DP if the measures are orthogonal. More specifically, if the mechanism produces a private summary ˜fD that is probabilistically orthogonal to ˜fD′, i.e. there exists a A ∈ F so that PD(A) = 0 and PD′(A) = 1, then the mechanism cannot be DP since fD and fD′ can be distinguished with probability one on A. In the following, we use this perspective to develop the feasibility of the ICLP mechanism. Denote the probability measure family induced by the ICLP mechanism as {PD : D ∈ D}, in the following theorem, we provide necessary and sufficient conditions for pairwise equivalence in {PD : D ∈ D}. Theorem 4 Let D, D′ ∈ D be two adjacent datasets, ˜fD, ˜fD′ be the private summaries based on the ICLP mechanism, and denote the corresponding probability measures over H as PD and PD′. Then PD and PD′ are equivalent if and only if fD − fD′ ∈ HC = {h ∈ H : ∥h∥C < ∞} . (2) Theorem 4 shows that if the difference of fD and fD′ resides in the Cameron-Martin space of C then the probability family will be pairwise equivalent. An analogous result for the equivalence of elliptical distributions appears in Theorem 2 of Reimherr and Awan (2019) even though the ICLP is not an elliptical distribution. However, it turns out that, unlike el- liptical distributions, pairwise equivalence is not enough for the ICLP mechanism to achieve DP. To see the reason behind this, one needs to consider the density of PD in H. Since there is no common base measure in H that plays the same role as the Lebesgue measure 5in Rd, it is more complicated to consider the density in H. Fortunately, we are adding the same type of noise to functional summaries, and therefore we only need the density as the Radon-Nikodym derivative of PD w.r.t. P0, where P0 is the probability measure induced by σZ. Lemma 5 Let Ph and P0 be the probability measures induced by {h+σZ} and σZ. Suppose h ∈ HC, then the Radon–Nikodym derivative of PD w.r.t. P0 is given by dPh dP0 (z) = exp � − 1 σ � ∥z − h∥1,C − ∥z∥1,C �� , (3) P0 almost everywhere and is unique. Now we are ready to show why Condition (2) is not enough for ϵ-DP. Indeed, the pairwise equivalence only guarantees the density in Equation (3) is well-defined, but does not guar- antee that the density is bounded, which is a requirement for ϵ-DP however. Meanwhile, HC is enough for (ϵ, δ)-DP since it allows densities to be unbounded up to a set with P0 measure less than δ. In the next theorem, we will show the appropriate space that fD −fD′ should reside in is actually a subspace of HC. Theorem 6 Under the same conditions of Theorem 4, let H1,C = {f ∈ H : ∥f∥1,C < ∞} be a subspace of HC and if fD1 − fD2 ∈ HC \ H1,C, then there is no σ ∈ R+ such that the ICLP mechanism, ˜ fD = fD + σZ satisfies ϵ-DP. Indeed, if fD resides in the gap between HC and H1,C, the sensitivity of fD will be infinite and there is no possibility to calibrate the ICLP noise with any σ to achieve ϵ-DP. Now, with the proper space in Theorem 6 and the feasible density in Lemma 5, we can establish the ICLP mechanism formally. Theorem 7 (The ICLP Mechanism) Let fD ∈ H1,C be the functional summary and Z is an ICLP with covariance operator C. Define the global sensitivity (GS) as ∆ = sup D∼D′ ∥fD − fD′∥1,C and σ = ∆ ϵ . (4) Then the sanitized version of fD, ˜fD = fD + σZ, achieves ϵ-DP. 3.3 Extensions To Space of Continuous Functions Theorem 7 implies the ICLP mechanism provides privacy protection for a wide range of infinite-dimensional objects in separable Hilbert spaces. The DP post-processing inequality (Dwork et al., 2014), is an especially important property for functions since practically one may only be interested in a few scalar summaries. However, the post-processing inequality only applies to measurable mappings. If H = L2[0, 1], then this eliminates the possibility 6of releasing point-wise evaluations of the functional summary, since such mappings are not measurable operations in L2[0, 1]. Therefore, in this section, we extend the ICLP mechanism to the space of continuous functions, i.e. C(T) with T a compact set over Rd, where such operations are measurable (and thus protected). We show that the ICLP, under mild conditions, is also in C(T). Theorem 8 Let C : T × T → R be a symmetric, positive definite, bivariate function with compact domain T. If C is α-H¨older continuous in each coordinate, i.e. there exists a positive constant MC, α ∈ (0, 1] s.t. |C (t1, s) − C (t2, s)| ≤ MC |t1 − t2|α then there exists an ICLP, Z, with covariance function C and there exists a modification ˜Z : T × Ω → R of Z that is a continuous process, s.t. 1. ˜Z is sample continuous, i.e. ∀ω ∈ Ω, ˜Zω(t) is continuous w.r.t. t ∈ T; 2. For any t ∈ T, P( ˜Z(t) = Z(t)) = 1. Meaning that there exists a stochastic process in C(T) equally distributed as the ICLP except on a zero-measure set. All of the results of the ICLP mechanism for H in Section 3.2 are now applicable to sum- maries in C(T). Furthermore, the point-wise evaluation is now a measurable operation and thus is protected. We also note that the proof of Theorem 8 is not just a standard theorem from stochastic processes and relies heavily on the structure of the ICLP. 4. Methodology Based on Theorem 6 and 7, to achieve ϵ-DP one would need to guarantee the quantity, fD − fD′ lies in H1,C for any neighboring databases D and D′. This is a challenging task to do directly and it is easier to structure the problem by restricting the individual functional summary residing in H1,C, which automatically leads to fD −fD′ ∈ H1,C. In the rest of this section, we will provide different approaches to construct qualified summaries and derive upper bounds for their global sensitivity. We also apply the ICLP mechanism to achieve privacy protection over several statistical estimation problems. 4.1 Generalized Obtainment of Qualified Summaries IID Laplace: The first strategy utilizes a finite basis expansion (Kokoszka and Reimherr, 2017) and adds IID Laplace noise to each coefficient, which is in the same spirit as all current mechanisms, which rely on finite-dimensional representations. Let {ej}j≥1 be an orthonormal basis in H. Then we approximate the summary using M bases, i.e. ˆfD = M � j=1 fDjej with fDj = ⟨fD, ej⟩. Expanding the functional summaries via a finite basis facilitates dimension reduction so that classic privacy tools can be implemented to sanitize each coefficient. Since the functional summary resides in a finite-dimensional subspace, it naturally satisfies the requirement in 7Theorem 7 (here one takes ϕj = ej and λj = 1) and therefore leads to a finite GS. However, it inherits all the drawbacks from subspace embedding mechanisms. For example, even though the leading components are usually more important for the utility of the estimate, this approach treats all components equally during the privatizing process and thus degrades the utility of the released summary. Additionally, the truncation level M controls the trade- off between variance, bias, and privacy; this approach would force one to either introduce more noise or accept higher bias when more components are required to deal with complex sample trajectories. ℓ1 Regularization: To satisfy fD ∈ H1,C while avoiding a finite-dimensional representa- tion, we consider using a regularization approach with ∥ · ∥1,C as a penalty term. Formally, let L(f, D) : H × D → R be a loss function, consider the following minimization problem, ˆfD = argmin f∈H � L(f, D) + ψ ∥f∥1,Cη � for η ≥ 1, (5) where Cη shares the same eigenfunctions as C while the eigenvalues are raised to λη j and ψ is the regularization parameter. The benefit of using a power of the kernel, Cη, are twofold. First, the space corresponding to ∥ · ∥1,Cη is a subspace of H1,C, guaranteeing that ˆfD ∈ H1,C. Second, it allows more flexibility to control the smoothness of the constructed functional summaries. Late on, we will see even though η = 1 is a natural setting, a slightly over-smoothing summary can be helpful for utility and even make privacy error negligible compared to statistical error. RKHS Regularization: As we will see in Section 4.2, there are some serious drawbacks to using the ∥·∥1,C-norm as the penalty. We, therefore, consider an RKHS approach as our final strategy, which turns out to work quite well in our applications. Formally, for a given η > 1, by the Cauchy-Schwarz inequality, ∥h∥1,C = ∞ � j=1 |hj| � λj = ∞ � j=1 |hj| λ η 2 j λ η−1 j 2 ≤ ∥h∥Cη � trace(Cη−1). Therefore, by taking η > 1 such that Cη−1 is a trace-class operator, we get that ˆfD ∈ H1,C where ˆfD = argmin f∈H � L(f, D) + ψ ∥f∥2 Cη � for η > 1. (6) 4.2 Mean Function Privacy Protection In this part, we consider the problem of protecting the privacy of the mean function sum- mary. Assume X1, X2, · · · , Xn are i.i.d. elements of an arbitrary real separable Hilbert space H with E Xi = µ0 ∈ H. Our goal is to release a differentially private estimator of the true mean function µ0 that satisfies ϵ-DP. When using IID Laplace, one can start with ˆfD = 1 n �n i=1 Xi, which is an unbiased estimator of the mean function µ. While in the other two approaches we use a quadratic loss function in a regularized empirical risk minimization setup with penalty term P(θ) = 8∥θ∥1,Cη or ∥θ∥2 Cη, i.e., ˆµD = argmin θ∈H � 1 n n � i=1 ∥Xi − θ∥2 + ψP(θ) � . (7) In Theorem 9 and Theorem 11, we derive the close form of the estimators and provide their global sensitivity analysis for ℓ1 and RKHS regularization. Theorem 9 (ℓ1 regularization) Let P(θ) = ∥θ∥1,Cηl, then solution of (7) is ˆµl D = ∞ � j=1 sψ,2λ ηl/2 j �� ¯X, ϕj �� ϕj. (8) where sa,b(x) = sgn(|x|) (|x| − a/b)+ is the soft thresholding function with threshold a/b. Furthermore, assume ∥X∥L2 ≤ τ, then for any fixed ψ, there exists an integer Jτ,ψ such that the global sensitivity of ˆµl D satisfies sup D∼D′ ∥ˆµl D − ˆµl D′∥1,C ≤ 2τ n Jτ,ψ � j=1 λ − ηl j 2 , Remark 10 The integer Jτ,ψ := max{j ≥ 1 : τ ≤ ψ/ληl/2 j } indeed can be viewed as a truncation number as the coefficients after Jτ,ϕ will be shrunk to 0, i.e. the summation in (8) is indeed finite. The upper bound for global sensitivity is based on the fact that, in the worst case scenario, the coefficients are not shrunk to zero and thus the soft thresholding adjustments cancel out. Therefore, unfortunately, the ℓ1 regularization doesn’t produce a better sensitivity than the IID Laplace approach while the soft thresholding introduces extra bias into the summary. In the next theorem, we show that the RKHS regularized estimator is indeed infinite- dimensional and has a better sensitivity than the other two. Theorem 11 (RKHS regularization) Let P(θ) = ∥θ∥Cηr , then solution of (7) is ˆµr D = ∞ � j=1 ληr j ληr j + ψ � ¯X, ϕj � ϕj. (9) Furthermore, assume ∥X∥l1 := � j≥1 |⟨X, ϕj⟩| ≤ τ, then the global sensitivity of ˆµr D satisfies sup D∼D′ ∥ˆµr D − ˆµr D′∥1,C ≤ 2τ n sup j    λ ηr− 1 2 j ληr j + ψ.    Remark 12 The coefficients of the RKHS regularized estimator (9) will not be shrunk exactly to zero, and hence one is able to perturb the functional summary with the truly infinite-dimensional ICLP. Besides, other assumptions about the boundness of X can also be used, such as ∥Xi∥ ≤ τ, but they in general produce a substantially larger sensitivity. 9In the next Theorem, we provide a guarantee of the utility of the ICLP mechanism. We demonstrate that with some smoothness assumptions on the true mean function µ0 and non-private estimator ˆµD, the privacy cost, E ∥˜µD − ˆµD∥2 L2, will not dominate the total error, which will be O(n−1) and thus matches the optimal rate without privacy. Therefore, one can restrict the summary into a smoother class such that privacy is gained for “free”. Theorem 13 (Utility Analysis) Assume Xi are i.i.d. observations from X with mean function µ0 and L2-norm bounded by τ, λj ≍ j−2ν. 1. Let ˜µr D be the private summary from RKHS regularization, if ∥µ0∥Cηr < ∞ then E ∥˜µr D − µ0j∥2 L2 = O � ψ− 1 ηr � τ nϵ �2 + 1 n + ψ � . Taking ηr ≥ 1 + (2ν)−1 and � nϵ2 τ 2 �−ηr ≲ ψ ≲ n−1, E ∥˜µr D − ˆµ∥2 L2 = � o(n−1), ηr > 1 + (2ν)−1 O(n−1), ηr = 1 + (2ν)−1 . 2. Let ˜µl D be the private summary from ℓ1 regularization, let Jτ := max{j ≥ 1 : τ ≤ ψ/ληl/2 j }. If ∥µ0∥Cηl < ∞ E ���˜µl D − µ0 ��� 2 L2 = O � J2(r+1) τ � τ nϵ �2 + 1 n + ψ � . Taking ηl ≥ 2(1 + ν−1) and τ − (ηl−1)ν−1 ν+1 (ϵ2n)− ηlν 2(ν+1) ≲ ψ ≲ n−1, E ���˜µl D − ˆµ ��� 2 L2 = � o(n−1), ηl > 2(1 + ν−1) O(n−1), ηl = 2(1 + ν−1) . We note that if ηr ≥ 1+(2ν)−1, the privacy cost will not dominate the mean square error of ˜µr D and if the inequality holds, it is a lower order of n−1 and thus asymptotically negligible. The threshold of ηr depends on the decay rate of the eigenvalues and thus depends on the smoothness of the covariance kernel C. In particular, if C satisfies the Sacks–Ylvisaker conditions (Sacks and Ylvisaker, 1966, 1968, 1970) of order s, then λj ≍ j−2(s+1). For example, setting C equals to the Ornstein–Uhlenbeck covariance function results in ηr = 3/2 and equals to the reproducing kernel of the univariate Sobolev space Wm 2 ([0, 1]) results in ηr = 1 + (2m)−1, see Micchelli and Wahba (1979); Yuan and Cai (2010) for more instances. We also establish a similar utility analysis for ℓ1 regularization, and its mean square error can also reach O(n−1) under similar smoothness assumptions as well. However, RKHS regularization is able to gain “free” privacy with a less smoothness assumption and thus possesses higher utility, which provides a theoretical justification of the superiority of the RKHS method. We will see this reflected later on in Section 6 as well. 104.3 Privacy-Safe Regularization Parameter Selection Determining the regularization parameters in the mechanisms, like the truncation level M (for subspace embedding mechanisms) and ψ (for the ICLP mechanism), is crucial to guarantee a reasonable performance of the private releases. Parameter regularization in statistical modeling has been well studied and Cross Validation (CV), or one of its many variants, is a widely used approach. However, CV focuses on balancing variance and bias in the statistical error, which ignores the trade-off between the privacy cost and the statistical error. To fit CV into the DP framework, Private Cross Validation (PCV) is proposed in Mirshani et al. (2019), which aims to find out the “sweet spot” between privacy cost and statistical error. However, as data-driven approaches, both CV and PCV are not truly privacy-safe since the regularization parameters may contain information about the data. There are some approaches that one can get end-to-end privacy-guaranteed regularization parameters. For example, one can use out-sample public datasets (Zhang et al., 2012) or one can spend extra privacy budget on the tuning process (Chaudhuri et al., 2011; Chaudhuri and Vinterbo, 2013). Since the ICLP mechanism is tied to a kernel, one can obtain privacy-safe regularization parameters by picking kernels whose eigenvalues decay at a polynomial rate and therefore satisfy the conditions in Theorem 13, then the theoretical values for ηl, ηr and ψ in Theorem 13 can be directly used as regularization parameter inputs. We call this approach the plug- in approach. The plug-in approach doesn’t degrade the privacy guarantee, since the plug-in values only rely on sample size, privacy budget, and the noise’s covariance function without further information about the dataset. It also provides theoretical foundations to make sure the overall error will not be dominated by the privacy cost. In practice, the constant for the plug-in value can affect the performance of the ICLP mechanism. In our experiments, we observe that by appropriately normalizing the data trajectories and the trace of the covariance kernel, setting the constant to 1 usually leads to satisfactory performance. In the next section, we will compare the performance of data-driven approaches with plug-in approaches and numerically show that the plug-in approach performs as well as the data- driven approach while being end-to-end privacy safe. 4.4 Beyond Functional Data In this section, we demonstrate how the ICLP mechanism can be applied to more general learning problems where the summary of interest is a function. Kernel Density Estimation: Let D = {x1, · · · , xn} ⊆ T, where T is a compact set over Rd, be an i.i.d. sample from a distribution with density f0. For any given ICLP kernel K, we adopt the RKHS regu- larization by picking the density estimation kernel as Kη with η > 1. For a given d × d symmetric and positive definite bandwidth matrix H, the kernel density estimator under RKHS regularization takes the form of ˆKD(x) = 1 n n � i=1 Kη H (x − xi) = 1 n � det(H) n � i=1 Kη � H− 1 2 (x − xi) � . (10) We now provide the global sensitivity and utility analysis of ˆKD(x) in the following theorem. 11Theorem 14 Suppose Kη(·, ·) is pointwise bounded, then the global sensitivity ∆ of ˆKD(x) in (10) satisfies ∆ = sup D∼D′ ��� ˆKD − ˆKD′ ��� 1,K ≤ 2MK n � det(H) � tr(Kη−1). Furthermore, taking H to be a diagonal matrix with same entry, assume f ′′ 0 is absolutely continuous, � T (f ′′′ 0 (x))2dx < ∞ and � T Kη(x)dx = 1, the risk R satisfies R = E � T � ˜fD(x) − f0(x) �2 dx ≤ O � c1 n2h2d + h4 + c2 nhd � , for some constants c2 and c2. Remark 15 If h is taken to be h ≍ n 1 4+d , then R = O(n− 4 4+d ), which matches the optimal kernel density estimation risk (Wasserman, 2006). The connection between estimating kernel and the noise kernel also appeared in Hall et al. (2013) where they stated that one can achieve (ϵ, δ)-DP by adding a Gaussian process with its covariance function equal to the kernel used in estimation. For privacy-safe bandwidth, h, we can pick h ≍ n 1 4+d to ensure privacy is gained for free. But a private version of “rule of thumb”, see Rao and Scott (1992) and Hall et al. (2013), is also feasible. Regularized Functional Under Empirical Risk Minimization: The functional summaries one desires to release may come from learning algorithms like regularization-based algorithms. In section 4.1, we have proposed using such algorithms to obtain qualified functional summaries. Here, we generalized the approach to broader scenarios like non-parametric regression and classification. Let D = {d1, · · · , dn} be the collection of n samples, where di is a tuple with finite size. Given a loss function L, we consider the following minimization problem, ˆfD = argmin f∈H1,C � 1 n n � i=1 L(di, f) + ψ∥f∥Cη � for some η > 1. (11) When di’s are couples i.e. di = (yi, xi), (11) can be viewed as non-parametric classification (yi’s take discrete value) or regression (yi’s take continuous value) problem. The solution of (11) can be expressed as ˆfD = �n i=1 aiCη(·, di) by the Representer Theorem (Kimeldorf and Wahba, 1971). However, although the theorem provides an elegant solution for (11), it is ill for calculating the global sensitivity as all the elements in the vector (a1, · · · , an) change once we swap one individual in the dataset. In the following theorem, we provide sensitivity analysis for ˆfD under certain regularized conditions. Theorem 16 Suppose ˆfD is the solution of (11) and the loss function L in (11) is an M- admissible loss function (Bousquet and Elisseeff, 2002), then the global sensitivity for ˆfD satisfies ∆ = sup D∼D′ ��� ˆfD − ˆfD′ ��� 1,C ≤ M ψn � sup x Cη(x, x) � tr(Cη−1). 12Remark 17 One can also prove the privacy loss E ∥ ˜fD − ˆfD∥2 L2 is bounded by c1(ψn)−2. We don’t provide utility analysis for this case study as the statistical loss can vary based on different regularization conditions on f0 and is out of the scope of this paper. The application scenario is wide since the upper bound holds for any convex and locally M-admissible loss function and bounded kernel with finite tracer. For example, support vector mechanism with hinge-loss, non-parametric regression with square-loss, and logistic regression with log(1 + x)-loss are potentially applicable learning models. 5. Algorithm and Implementation Based on the definition of the ICLP, the generic implementation of the mechanism can be achieved by Karhunen-Lo´eve expansion. 1. Given any mercer kernel C, obtain its eigenvalues {λj}j≥1 and eigenfunctions{ϕj}j≥1. 2. Generate ICLP noise by Z = � j≥1 � λj 2 Zjϕj where Zj i.i.d. ∼ Lap(1). 3. Calibrate Z to desired privacy level by the global sensitivity ∆ and privacy budget ϵ. However, the summation in generating Z can not be implemented in finite time and usually is terminated at a large integer. Therefore, in practice, we utilize its approximated version Algorithm 1. Algorithm 1: Approximated ICLP mechanism 1 Given the covariance kernel C and K different points (x1, x2, · · · , xK) on the compact domain T , calculate the value of C on the grid expanded by (x1, x2, · · · , xK), i.e. ˆC =    C(x1, x1) · · · C(x1, xK) ... ... ... C(xK, x1) · · · C(xK, xK)    2 Obtain K estimated eigenvalues {ˆλk}K k=1 and eigenfunctions {ˆϕk}K k=1 of ˆC by eigendecomposition. 3 for k in 1,2,· · · , K do 4 Set ˆfDk = ⟨ ˆfD, ˆϕk⟩ and generate Zk from � ˆλk 2 Lap(1) 5 ˜fDk = ˆfDk + σZk where σ = √ 2∆ ϵ 6 Return ˜f = �K k=1 ˜fDkϕj. A natural question about Algorithm 1 is that does all the theoretical analyses still hold if the privacy noise is sampled in a finite approximation manner instead of the “true” infinite sum. The answer is yes as long as the same cutoff K is used both in constructing privacy noise and in expressing the original estimate, which is followed directly from post- processing inequality. However, a key advantage of our theoretical analyses is that the 13privacy guarantees will still hold regardless of what K is used. Another problem regarding the Algorithm 1 is that even though larger K will lead to more accurate estimates of eigenvalues and eigenfunctions, it also increases the computational burden as the algorithm relies on the Karhunen-Lo´eve decomposition. Following, we investigated how different cutoff values, K, will affect computational time by comparing the average computation time for generating 100 ICLPs to 100 Gaussian Processes. We choose the Gaussian Process as the competitor since it is the stochastic process used to achieve (ϵ, δ)-DP for functional data, and sampling Gaussian processes is nothing more than sampling a multivariate Gaussian with covariance ˆC. Theoretically, generating one ICLP and one Gaussian process are both in time complexity O(n3) since both Cholesky and the Eigen decomposition are O(n3). In Table 1, we report the average time to generate 100 ICLPs and 100 GPs under different settings. We found that, generating 100 Gaussian Processes is about 30% to 50% faster than generating 100 ICLPs in practice. Kernel Type K ICLP GP Kernel Type K ICLP GP Exponential 100 0.5676875 0.2734629 Matern(ν = 3 2) 100 0.5710142 0.2706821 200 2.778597 1.683808 200 2.636642 1.617183 500 29.98454 20.90210 500 29.38142 20.56870 Gaussian 100 0.5530248 0.2683786 Matern(ν = 5 2) 100 0.5512221 0.2714773 200 2.617800 1.610193 200 2.618398 1.615180 500 29.26284 20.41770 500 29.29077 20.53389 Table 1: Computation time (in second) for generating 100 ICLPs and GPs under different cutoff value K and covariance kernel C. 6. Numerical Experiments In this section, we numerically evaluate the effectiveness of the ICLP mechanism and some other comparable mechanisms like the IID Laplace and Bernstein mechanism. 6.1 Simulation for Mean Function Protection In this section, we conduct the simulation for the mean function privacy protection problem discussed in Section 4.2. We use the Mat´ern kernel (Cressie and Huang, 1999) as the covariance kernel for the ICLP since its resulting RKHS ties to a particular Sobolev space, allowing us to control the smoothness directly. It takes the form of Cα(s, t) = 1 Γ(ν)2α−1 �√ 2α|t − s| ρ �α Kα �√ 2α|t − s| ρ � where Kα is the modified Bessel function. Denote {λj}j≥1 and {ϕj}j≥1 as its eigenvalues and eigenfunctions respectively. Specifically, we set ρ = 0.1 and α = 1.5 such that λj ≍ j−4. For sample curves, we generate them via Xi(t) = µ0(t) + �100 j=1 j−2Uijϕj(t) where Uij are i.i.d. uniformly from [−2, 2] and t ∈ [0, 1]. We consider four true mean functions scenarios: S-1: µ0(t) = 10t ∗ exp(−t). 14S-2: µ0(t) = 0.3f0.3,0.05(t) + 0.7 ∗ f0.8,0.05(t). S-3: µ0(t) = 0.2 ∗ (f0,0.03(t) + f0.2,0.05(t) + f0.5,0.05(t) − f0.75,0.03(t) + f1,0.03(t)). S-4: µ0(t) = �25 j=1 Rijϕj(t), where Rij i.i.d. ∼ U[−1, 1]. where fa,b is the probability density function of normal distribution with mean a and vari- ance b2. The shape complexity of the curves rises sequentially, for example, S-1 is just a monotonically increasing function, S-2 is a bimodal function, and S-3 and S-4 are many fluc- tuating multimodal functions. For the Bernstein mechanism, the implementation is based on r package diffpriv and we use the sample mean ¯X(t) as the non-private summary by setting the cover size parameter as 20. We measure the performance by mean square er- ror(MSE), i.e. E ∥˜µ − µ0∥2 L2, which it is approximated via Monte-Carlo by generating 1000 private mean functions for each combination of n. 6.1.1 Comparison of PCV and Plug-In In Section 5, we discuss the method for a privacy-safe selection of regularization parameters. Here we demonstrate its effectiveness by comparing it with the PCV method. In ℓ1 and RKHS regularization, we set η and ψplug to be the values in Theorem 13 such that the privacy cost is the same order as the statistical error. For PCV, we obtain ψpcv by 10-fold PCV within the range of [0.1ψplug, 10ψplug]. In the IID Laplace, the plug-in approach is more ambiguous as the plug-in values for truncation level such that ˜µ reaches optimal rate is a collection of integers, i.e. M = {M1, · · · , MK}. We calculate MSE for each M ∈ M and take the smallest MSE as the plug-in result. While for PCV, we consider a wider range of M by adding and subtracting 3 to its maximum and minimum elements. 0 2000 4000 Sample Size n 10 3 10 2 10 1 100 MSE S - 1 0 2000 4000 Sample Size n 10 3 10 2 10 1 100 S - 2 0 2000 4000 Sample Size n 10 3 10 2 10 1 100 S - 3 0 2000 4000 Sample Size n 0.3 1.0 3.0 S - 4 IID Laplace ICLP- 1 ICLP-RKHS pcv plug pcv plug Figure 1: MSE for plug-in and PCV approaches to select regularization parameter ψ under different mechanisms, sample size and true mean functions. The values reported are averages over 100 independent repetitions. One standard error is on the order of graph line width. In Figure 1, we report the MSE for each mechanism for different sample size n with ϵ = 1. For the rather simple µ0 in S-1, the MSE curves of the plug-in almost line up with the PCV ones for IID Laplace, ℓ1, and RKHS regularization. In S-2 and S-3, the ICLP mechanism still has consistent curves between the plug-in and PCV while the plug-in MSE curve of the IID Laplace has a step-down pattern. This pattern is due to the fact that the maximum number in M is determined by the sample size, so with complex curves and small sample 15sizes, the IID Laplace does not have enough components to estimate the mean of the curve well, but as the sample size increases, more available components make the estimate better. In S-4, PCV does better than the plug-in approach for all mechanisms, which is expected since more components are required in IID Laplace and less penalty is required in RKHS regularization and PCV always did so. Since it has been shown that selecting regularization parameters via the plug-in approach has a reasonable and consistent performance to PCV, we use the plug-in approach in the following simulations to be fully privacy safe. 6.1.2 Comparison of Different Mechanisms Under the same settings, we compare the performance of different mechanisms under dif- ferent sample sizes n via MSE. The results are reported in Figure 2. 100 300 1000 3000 Sample Size n 10 3 10 2 10 1 100 101 MSE S - 1 100 300 1000 3000 Sample Size n 10 3 10 2 10 1 100 101 S - 2 100 300 1000 3000 Sample Size n 10 3 10 2 10 1 100 101 S - 3 IID Laplace Bernstein ICLP- 1 ICLP-RKHS 100 300 1000 3000 Sample Size n 0.01 0.10 1.00 S - 4 Figure 2: MSE for different mechanisms under different sample sizes n and true mean functions. The values reported are averages over 100 independent repetitions. One standard error is on the order of graph line width. It is known that the optimal non-private MSE under L2 norm is of order n−1, and it should be a straight line with slope −1 after taking log10. From Figure 2, it can be observed that the ICLP mechanism with the RKHS regularization approach always achieves the best performance under all scenarios. Its MSE curves also decrease in the same pattern as the desired optimal non-private MSE in S-1 to S-3 while in S-4, the MSE decreases slower than expected with small n as the plug-in value will over smooth summaries, but return parallel with the straight line as n increases. On the other hand, the ℓ1 regularization is almost the worse one as expected. The IID Laplace and Bernstein are quite close to the RKHS regularization in S-1, showing their effectiveness in simpler curve scenarios, but they fail to mimic the behaviors of the RKHS regularization where curves’ shapes are more complex i.e. S-2 to S-4. Although the IID Laplace can narrow the gap from the RKHS regularization, this only happens as n increases, and thus its effectiveness can be restrictive when limited samples are available. 6.2 Simulation for Density Kernel Estimator Protection To demonstrate the wide range of application scenarios of the ICLP mechanism, we conduct a simulation on private kernel density estimates. We consider the setting under R and R2 with samples generated from two mixture Gaussian distributions. 161. R setting: xi i.i.d. ∼ 2 � i=1 piN(µi, 0.1; 0, 1), where N(µ, σ; a, b) is a truncated normal distribution over [a, b] and p1 = 0.6, µ1 = 0.3, µ2 = 0.7. 2. R2 setting: xi i.i.d. ∼ 2 � i=1 piN � µi, � 1 0.5 0.5 1 � ; � 5 −5 � , � 5 −5 �� where N(µ, Σ; a, b) is a multivariate truncated normal distribution over [a1, b1] × [a2, b2]. Specifically, we take p1 = 0.6, µ1 = (−3, −3), µ2 = (3, 2). We compare the ICLP mechanism (RKHS regularization), IID Laplace, and the Bernstein mechanism. For the ICLP mechanism and the Bernstein mechanism, we pick multiple smoothing parameters η and lattice number K to demonstrate how they affect private curves and surfaces; while for IID Laplace, we select the truncated number that provides the best fit under PCV criteria. We generate 2000 samples under each scenario and use the Gaussian kernel in R and exponential kernel in R2. We use h ≍ n1/(4+d) where d = 1, 2 to ensure we gain privacy for free and being privacy safe. The results are reported in Figure 3 and Figure 4. First, for the univariate setting, we can see the ICLP mechanism performs similarly to the IID Laplace; a higher η produces less variability in the curves but tends to be over- smooth. The Bernstein mechanism needs over 30 lattice points in the interval to catch the shape of the bimodal curve but results in producing a messy tail at both ends. A lower lattice number produces better tails but fails to catch the bimodal pattern. For R2 we can see by slightly over smoothing, the ICLP mechanism releases private KDEs very close to non-private ones. A smaller η (Figure 4a) is more precise at peaks but will be ”noisy” around lower density regions; while a larger η (Figure 4c) produces smooth lower density regions but causes underestimating at peaks. Figure 4b shows that there is a clear ”sweet point” to tradeoff smoothness and underestimation. The IID Laplace performs similarly to the underestimating ICLP case, but the peaks of private KDE don’t fully line up with the non-private one. The Bernstein mechanism, on the other hand, fails to produce similar surfaces to the non-private estimator even though we increase the number of lattice points. 7. Real Applications This section presents two real data applications of the proposed methods to study releasing functional summaries for different types of functional datasets. 7.1 Application to Medical and Energy Usage Data In this section, we aim to release a private mean function that satisfies ϵ-DP for the fol- lowing two different type datasets: First, we consider medical data of Brain scans Diffusion 170.0 0.2 0.4 0.6 0.8 1.0 0.0 0.5 1.0 1.5 2.0 (a) 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.5 1.0 1.5 2.0 (b) 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.5 1.0 1.5 2.0 (c) 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.5 1.0 1.5 2.0 (d) x 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.5 1.0 1.5 2.0 (e) x 0.0 0.2 0.4 0.6 0.8 1.0 −0.5 0.0 0.5 1.0 1.5 2.0 2.5 (f) Figure 3: Non-private (Blue) and 10 random realization of private (Red) KDE curves. (a) ICLP mechanism with η = 1.25 (b) ICLP mechanism with η = 1.5 (c) IID Laplace and (d)-(f) Bernstein mechanism with K = 10, 30, 50. (a) (b) (c) (d) (e) (f) Figure 4: 3D plot of non-private (Blue) and private (Red) KDE over R2. (a)-(c) ICLP mechanism with η = 1.01, 1.05, 1.2 (d) IID Laplace and (e)-(f) Bernstein mechanism with K = 5, 10. Tensor Imaging (DTI) dataset available in the r package refund. The DTI dataset pro- vides fractional anisotropy (FA) tract profiles for the corpus callosum (CCA) of the right corticospinal tract (RCST) for patients with Multiple Sclerosis and for controls. Specifi- 18cally, we study the CCA data, with 382 patients measured at 93 equally spaced locations of the CCA. Second, we study the Electricity demand in the Adelaide dataset available in the r package fds. The dataset consists of half-hourly electricity demands from Sunday to Saturday in Adelaide between 6/7/1997 and 31/3/2007. Our analysis focuses on Monday specifically, meaning the dataset consists of 508 days measured at 48 equally spaced time points. Producing privacy-enhanced versions of summaries for such type of energy data is meaningful for protecting public energy institutions such as the power grid from hackers’ attacks. To measure the performance of each mechanism, since the true mean function is not available under such settings, we use the expected distance between the release summary, ˜µ, and the non-private sample means, ˆµ, i.e. E ∥˜µ − ˆµ∥2 L2. We consider Mat´ern kernel with α = 1.5 and 2.5 and ρ = 0.1. The expectation is approximated by Monte-Carlo with 1000 generated ˜µ. The results are reported in Table 2 and each data point is an average of 100 replicate experiments. We also visualized private mean estimates for each mechanism in Figure 5. From Figure 2, it can be observed that the expected distance decreases in a similar pattern as the privacy budget increases for both data sets. We can see that the expected distance of the IID Laplace soon stops changing, which shows that most of the errors of the IID Laplace are concentrated on statistical errors. This shows that in order to avoid adding too much noise to the later components, the IID Laplace has to compromise on using fewer leading components, which leads to higher statistical errors. This can also be seen in Figure 5, where the IID Laplace (green one) can only estimate an approximate shape, but fail to get a better shape estimate locally. ℓ1 regularization and Bernstein mechanism also have similar performance patterns and have much worse results for smaller budgets. Finally, the RKHS regularization performs the best among all approaches as its released summaries (blue ones) can estimate the shapes precisely and also have much smaller expected distances than the non-private mean. Eletricity (C1.5) IID Laplace Bernstein l1 RKHS non−private Eletricity (C2.5) IID Laplace Bernstein l1 RKHS non−private DTI (C1.5) IID Laplace Bernstein l1 RKHS non−private DTI (C2.5) IID Laplace Bernstein l1 RKHS non−private Figure 5: Non-private sample mean and private means for different mechanisms with Mat´ern kernel C 3 2 and C 5 2 with ϵ = 1. The curves in light grey indicate the original samples. 19Eletricity Demand Kernel ϵ I.I.D. Laplace Bernstein ℓ1 RKHS C 3 2 1/8 0.28280.1981 2.21101.5374 3.91102.9464 1.37261.2935 1/4 0.17310.0419 0.61400.3859 1.03910.8066 0.34590.2783 1/2 0.14530.0098 0.21170.1276 0.34150.1897 0.08950.0686 1 0.13830.0025 0.11060.0352 0.16490.0728 0.02410.0171 2 0.13660.0007 0.08580.0151 0.12040.0238 0.00800.0044 4 0.13620.0002 0.07930.0074 0.10890.0105 0.00390.0011 C 5 2 1/8 0.28050.1645 2.22791.5880 4.16143.5764 1.42611.3547 1/4 0.17030.0376 0.60530.4227 1.11680.8931 0.34680.2963 1/2 0.14270.0094 0.20920.0954 0.34970.2276 0.09200.0692 1 0.13600.0026 0.11090.0374 0.16390.0655 0.02510.0168 2 0.13420.0006 0.08570.0167 0.11640.0233 0.00890.0049 4 0.13380.0002 0.07940.0072 0.10440.0090 0.00480.0012 DTI(cca) Kernel ϵ I.I.D. Laplace Bernstein ℓ1 RKHS C 3 2 1/8 0.63050.2878 6.24734.2907 4.42453.9617 3.19962.6794 1/4 0.44180.0663 1.61221.0886 1.26911.1059 0.80390.6994 1/2 0.39520.0165 0.49290.3046 0.47010.2522 0.20400.1627 1 0.38360.0045 0.20800.0824 0.27470.0756 0.05550.0369 2 0.38090.0010 0.13610.0285 0.22650.0265 0.01860.0106 4 0.38010.0003 0.11870.0133 0.21430.0133 0.00940.0028 C 5 2 1/8 0.63030.2867 6.21554.8556 4.74534.0892 3.08412.7095 1/4 0.44370.0762 1.62751.1514 1.30490.9318 0.78200.7225 1/2 0.39740.0158 0.49070.3274 0.47930.2431 0.20280.1790 1 0.38590.0040 0.20730.0890 0.27480.0804 0.05810.0395 2 0.38300.0010 0.13660.0287 0.22310.0325 0.02160.0107 4 0.38230.0003 0.11870.0137 0.21020.0126 0.01260.0029 Table 2: Expected distance between release mean function estimate and sample mean for Electricity demand and DTI(cca) data sets. The numbers in the subscript indicate the standard error (×10−3). 7.2 Application to Human Mortality Data Publishing the entire age-at-death distribution in a given country/region usually provides more comprehensive information about human lifespan and health status than publishing crude mortality rates, and a privacy-enhanced version of this distributional type summary ensures that an attacker cannot infer information about individuals/groups in a particular age range. Following, we study how to release privacy-safe mortality distributions across different regions. The mortality data for each region are collected from the United Nation World Population Prospects 2019 Databases (https://population.un.org/wpp/Download), and the data table record the number of deaths for each region and age. We estimate the probability density function for each region and private the estimates via the proposed method and its competitors. The privacy budget is set to be 1. Again, 20we measure the performance between via E ∥ ˜f − ˆf∥2 L2 where ˜f is the private KDE and ˆf is non-private one, and the expectation is approximated via Monte Carlo by 200 private KDEs. The results are reported in Table 3. Again, we visualized the private KDEs for each region and mechanism in Appendix B. From the table, we see the ICLP mechanism (RKHS approach) with η = 1.01 has smaller errors in developing regions while IID Laplace is better in developed regions. This is reasonable as developed regions usually have better medical conditions so that the mortality age concentrates between 70 and 80 and their densities are unimodal, and thus a few leading components/basis are sufficient to represent the density function in these regions. Under this scenario, it is understandable that the IID Laplace is better since the noise needed are added to a few components. On the other hand, the situations are opposite in developing regions, where the infant mortality rates are higher. Therefore, these density functions show a multi-modal pattern and require more components/basis to get a close approximation, and the ICLP mechanism assigns less randomness to the summaries thanks to its heterogeneous variance noise injection procedure. Mechanisms Region RKHS (η = 1.01) RKHS (η = 1.05) I.I.D. Laplace Bernstein (K = 10) Bernstein (K = 20) Eastern Africa 1.68510.507 5.2497.549 5.2527.881 3.82312.243 2.40011.896 Middle Africa 1.1220.451 5.2810.410 4.8702.152 3.9690.313 2.0900.327 Northern Africa 2.3980.680 8.8730.656 3.9911.807 11.3150.546 5.7100.646 Southern Africa 2.4871.491 6.4270.919 2.9901.809 3.9640.666 2.3950.763 Western Africa 1.58813.699 5.7759.216 5.75211.482 4.34715.649 2.60915.493 Central Asia 5.8042.715 11.8291.547 7.3193.739 13.1631.346 8.1161.808 Eastern Asia 3.5514.414 13.4466.872 3.9006.958 15.87814.110 8.1629.175 Southern Asia 2.0402.267 8.6193.737 2.9682.962 10.0448.099 4.9624.888 South-Eastern Asia 2.2591.871 9.4983.565 2.5662.323 10.4288.328 5.1574.778 Western Asia 2.1950.585 8.6890.624 3.7682.005 9.3800.510 4.6400.578 Eastern Europe 3.9903.658 13.5014.924 4.8175.762 15.5429.356 8.4386.414 Northern Europe 7.3181.626 19.0871.187 7.1741.493 22.8461.218 13.3391.304 Southern Europe 7.4570.977 21.0760.931 7.1371.083 27.5000.852 15.8730.927 Western Europe 6.8192.184 19.9592.782 6.4453.417 25.6065.375 14.7603.965 Caribbean 6.4532.890 11.2472.022 5.9813.634 10.7141.580 7.2312.296 Central America 1.6360.548 7.7770.685 1.1460.537 5.6370.480 2.6160.448 South America 2.2442.498 9.4614.096 2.8863.276 9.2946.744 4.7004.333 Northern America 2.2051.734 10.6052.792 1.5372.297 10.0285.546 4.7463.153 Australia/New Zealand 23.52511.299 29.4955.040 22.7828.189 28.3395.053 23.9837.582 Table 3: Expected L2 distance between release KDEs and non-private KDEs for each region with ϵ = 1. The numbers in the subscript indicate the standard error (×10−3). 8. Conclusion In this paper, we proposed a new mechanism, the ICLP mechanism, to achieve ϵ-DP for infinite-dimensional objects. It provides a wide range of output privacy protections with more flexible data assumptions and a more robust noise injection process than current mechanisms. Theorem 6 and 8 establish its feasibility in separable Hilbert spaces and 21spaces of continuous functions. Several approaches are proposed to construct qualified summaries compatible with the ICLP mechanism., along with plug-in parameters selection to guarantee end-to-end protection. In the example of mean function privacy protection, we also show that slightly over-smoothing the summary promotes the trade-off between utility and privacy, and can match the known optimal rate for mean function estimation. There are some limitations of the proposed mechanism and interesting future works. As we show in Section 5, the implementation of the ICLP mechanism relies on the Karhunen- Lo´eve expansion and thus will be computationally expensive. Therefore, a computation approach that doesn’t rely on the Karhunen-Lo´eve expansion is an important future di- rection. Additionally, even though various experiment results show that by appropriately processing the sample trajectories and the ICLP covariance kernel, omitting the constant for plug-in values has led to satisfactory performance, we believe a more careful investigation of the constant can further enhance performance. Acknowledgments and Disclosure of Funding This work was partially supported by the National Science Foundation, NSF SES-1853209. A. Appendix: Main Proofs A.1 Proof of Lemma 5 Proof To show the explicit form of Randon-Nikodym derivative of PD w.r.t. P0, we define an isometry between H and l2 to avoid considering probability measures over H. Given an orthonormal basis {ϕj}∞ j=1 of H, one can define a mapping T : H → l2 by T (f) = {⟨f, ϕj⟩}∞ j=1, and its inverse is T −1({⟨f, ϕj⟩}) = �∞ j=1⟨f, ϕj⟩ϕj. This mapping is an isometry between H and l2 and we can consider the probability measure over l2 rather than over H. For a Laplace r.v.s. X ∼ Lap(µ, b) over R, it induces a probability measure over (R, B), where B is the borel set over R, as γµ,b(dx) = 1 2bexp � −|x − µ| b � dx. Let {(λj, ϕj)}j∈N be the eigencomponents (eigenfunctions and eigenvalues) of C and let λj = 2b2 j. By Existence of Product Measures Theorem (Tao, 2011) and isomorphism mapping T , PD◦T is a unique probability measure defined as γ(fD, C) := �∞ j=1 γfDj,bj over (R∞, B∞) := (�∞ j=1 Rj, �∞ j=1 Bj). We further restrict γ(fD, C) on (l2, σ(l2)) and keep denoting it by γ(fD, C). We now start to prove Lemma 1, by showing the form of Radom-Nikodym derivative of γ(h, C) w.r.t. γ(0, C) is the same as derivative of {PD : D ∈ D} to P0 and takes the form of dPh dP0 (z) = exp � − 1 σ � ∥z − h∥1,C − ∥z∥1,C �� , (12) 22First, we need to show the r.h.s. of the above Equation is well-defined when h ∈ HC. Define HM(z) = M � j=1 |zj − hj| − |zj| bj and H(z) = lim M→∞ HM(z). We need to show there exists a set A with P0(A) = 1 such that H(z) exists and is finite on A. Suppose zj ∼ Lap(0, bj), with some calculation, one has Var (HM(z)) = M � j=1 Var �|zj − hj| − |zj| bj � = M � j=1 � 1 − exp � −hj bj �� = M � j=1 h2 j b2 j , where the last equality is by Taylor expansion. By Fatou’s Lemma and condition h ∈ HC, Var(H(z)) < ∞. The set A is Ω and therefore with P0-measure 1. Therefore, once h ∈ HC, then r.h.s. of Equation exists and is well-defined. Next, we aim to prove that dPh dP0 (z) is the Radon-Nikodym derivative of h + σZ w.r.t. σZ. Let g(x) = exp � − √ σ2 � ∥x − h∥1,C − ∥x∥1,C �� and dP ∗ h(x) = g(x)dP0(x). Therefore, we only need to show that Ph and P ∗ h are the same measure. We finish this proof by showing they have the same moment generating function. MGFPh(t) = EPh exp {⟨X, t⟩H} = ∞ � j=1 � R exp {xjtj} dγhj,bj(xj) = ∞ � j=1 exp {hjtj} 1 − (bjtj)2 . where the second inequality comes from the result that Ph is product measure of γhj,bj. For the moment generate function of P ∗ h, MGFP ∗ h (t) = EP ∗ h exp {⟨X, t⟩H} = EQ g(X)exp {⟨X, t⟩H} = ∞ � j=1 � R exp � − 1 σ �|xj − hj| − |xj| bj � + xjtj � dγ0,bj(xj) = ∞ � j=1 � R exp {xjtj} dγhj,bj(xj) = ∞ � j=1 exp {hjtj} 1 − (bjtj)2 = MGFPh(h). 23Therefore, Ph and P ∗ h are the same measures. A.2 Proof of Theorem 3 Proof To prove the existence of the ICLP in H, we only need to prove E⟨X, X⟩H < ∞, then the proof can be done by Fubini’s theorem. Notice E⟨X, X⟩H = E � j≥1 � k≥1 � λj � λkZjZk⟨ϕj, ϕk⟩H = E ∞ � j=1 λjZ2 j Since �∞ j=1 E λjZ2 j = �∞ j=1 λj < ∞, by Fubini’s theorem, E⟨X, X⟩H < ∞, which proves the existence of X. A.3 Proof of Theorem 4 Proof By the fact that T is an isomorphism mapping between H and l2, to prove PD and PD′ are equivalent, it’s sufficient to prove γ(fD, C) and γ(fD′, C) are equivalent. We now formally prove γ(h, C) and γ(0, C) iff h ∈ HC. For “if” part, for two infinite product measures, we can apply Kakutani’s theorem (Kakutani, 1948). Then the two measure are equivalent if ∞ � j=1 log � R � γj(hj, bj) γj(0, bj) γj(0, bj)(dx). A few calculation leads to the target space is H∗ C =   h ∈ H : � j≥1 �|hj| 2bj − log(1 + |hj| 2bj ) � < ∞    , and we now prove that, H∗ C = HC. We only need to prove that for a non-negative sequence {aj}j≥1, the series � j≥1[an − log(1 + an)] converges if and only if � j≥1 a2 n converges. Let f(x) be the Taylor expansion of x − log(1 + x) and g(x) = x2. Besides, note that limn→∞ f(an) = 0 if and only if limn→∞ an = 0. Thus, • If � j≥1[an − log(1 + an)] < ∞, then limn→∞ f(an) = 0 and so limn→∞ an = 0. Then, lim n→∞ f(an) g(an) = lim n→∞ an − log(1 + an) a2n = 1 2 therefore by limit comparison test � j≥1 a2 n converges too. • If � j≥1 a2 n < ∞, by the same statement as above also holds and therefore � j≥1[an − log(1 + an)] < ∞. For the “only if” part, the argument is the same as proof of Theorem 2 in Reimherr and Awan (2019). 24A.4 Proof of Theorem 6 Proof We prove the theorem via contradiction. Assume if fD ∈ HC\H1,C, for any given fixed ϵ, ∃ σ ∈ R such that mechanism fD + σZ still satisfy ϵ-DP, then by post-process property of differential privacy, we know that for any transformation G : H → H, G(fD) is also ϵ-DP. Now, ∀ J ∈ N, consider G to be a projection mapping into first J components, i.e. GJ(fD) = �J j=1⟨fD, ϕj⟩ϕj. Therefore, by assumption, ∀ J ∈ N, GJ(fD) is ϵ-DP, i.e. exp   − √ 2 σ J � j=1 � |⟨fD − z, ϕj⟩| √ 2bj − |⟨z, ϕj⟩| √ 2bj �   ≤ exp {ϵ} , except for z ∈ A where A is zero-measure set. Define Bj = {zj : |zj| > |fj|} and SJ = � z ∈ l2 : zj ∈ Bj, ∀1 ≤ j ≤ J and zj ∈ R, ∀j > J � . Then ∀ z ∈ SJ, exp   − √ 2 σ J � j=1 � |⟨fD − z, ϕj⟩| √ 2bj − |⟨z, ϕj⟩| √ 2bj �   = exp    √ 2 σ J � j=1 � |⟨fD, ϕj⟩| √ 2bj �   ≤ exp {ϵ} . However, since h ∈ HK\H1,C, one can always find an J s.t. exp � 1 σ �J j=1 |⟨fD,ϕj⟩| bj � > exp {ϵ} and therefore contradiction holds and no such σ ∈ R+ exists. The remaining thing will be to prove SJ is not a zero-measure set. By Existence of Product Measure in Tao (2011), γ0,C(SJ) = J � j=1 γ0, √ 2bj(Bj) where r.h.s. greater than 0 by definition of Bj. A.5 Proof of Theorem 7 Proof By Lemma 1, the density of ˜fD w.r.t. to σZ is dPD dP0 (z) = exp � − 1 σ � ∥z − fD∥1,C − ∥z∥1,C �� . We aim to show that for any measurable subset A ⊆ H, one has PD(A) ≤ eϵPD′(A), which is equivalent to show PD(A) = � A dPD(x) = � A dPD dPD′ (x)dPD′(x) ≤ eϵ � A dPD′(x). 25Then dPD dPD′ (x) = dPD dP0 (x)/dPD′ dP0 (x) = exp � − 1 σ � ∥x − fD∥1,C − ∥x − fD′∥1,C �� ≤ exp � 1 σ ∥fD′ − fD∥1,C � . Recall the global sensitivity for the ICLP mechanism is ∆ = sup D∼D′ ∥fD − fD′∥1,C, therefore, ∀x ∈ H, dPD dPD′ (x) ≤ eϵ and PD(A) = � A dPD dPD′ (x)dPD′(x) ≤ eϵ � A dPD′(x) holds. A.6 Proof of Theorem 8 Proof Z(t) − Z(s) = � λ1/2 j Zj(ϕj(t) − ϕj(s)) So E[exp{t(Z(t) − Z(s))}] = ∞ � j=1 1 1 − t2λj(ϕj(t) − ϕj(s))2 = exp   − ∞ � j=1 log � 1 − t2λj 2 (ϕj(t) − ϕj(s))2 �   with t satisfies 0 ≤ t2λj(ϕj(t) − ϕj(s))2 < 1, ∀ j. Notice λj(ϕj(t) − ϕj(s))2 = λj⟨Ct − Cs, ϕj⟩2 C ≤ ⟨Ct − Cs, Ct − Cs⟩C = C(t, t) − 2C(t, s) + C(s, s) ≤ 2MC|t − s|α, MC be the H¨older-continuous constant, and this leads to 0 ≤ t ≤ ( 1 MC ) 1 2 |t − s|− α 2 . Now notice that for 0 ≤ x < 1 and f(x) = − log(1 − x) we have f′(x) = 1/(1 − x) and so − log(1 − x) = f(x) = f(0) + xf′(ζ) = x 1 − ζ ≤ x 1 − x, 26for some ζ ∈ (0, x). So we have then − log(1 − t2λj 2 (ϕj(t) − ϕj(s))2) ≤ t2λj 2 (ϕj(t) − ϕj(s))2 1 − t2λj 2 (ϕj(t) − ϕj(s))2 ≤ t2λj 2 (ϕj(t) − ϕj(s))2 max k � (1 − t2λk 2 (ϕk(t) − ϕk(s))2)−1 � . Again, choose t such that (1 − t2λk 2 (ϕk(t) − ϕk(s))2)−1 ≤ M0 < ∞ so that we get E[exp{t(Z(t) − Z(s))}] ≤ exp � M0 t2 2 � λj(ϕj(t) − ϕj(s))2 � = exp �M0t2 2 (C(t, t) − 2C(t, s) + C(s, s)) � ≤ exp � M0MCt2|t − s|α� . Therefore, by Chernoff bound, we got P (|X(s) − X(t)| ≥ a) ≤ E[exp{t(Z(t) − Z(s))}] exp{ta} ≤ exp � M0MCt2|t − s|α − ta � . The minimizer of r.h.s. with respect to t is t0 = a 2M0MC|t−s|α , with restriction of t, we get a ≤ 2M0|t − s| 1 2 α. We consider the following two cases: Case 1 : Suppose a ≤ 2M0|t − s| 1 2 α, then the minimizer is t0 = a 2M0MC|t−s|α , then P (|X(s) − X(t)| ≥ a) ≤ exp � − ˜ M1a2|t − s|−α� , for some generic constant ˜ M1 taking a(x) = C|x|β with β ≥ 1 2α, then ∞ � n=1 a(2−n) = ∞ � n=1 2−nβ and ∞ � n=1 2n exp{− ˜ M2|2n|α−2β}, The first series converges if β < 1. However, to make the second one converges, we need α > 2β which leads to α > α, contradiction. Case 2 : Suppose a > 2M0|t − s| 1 2 α, then the minimizer is t0 = ( 1 MC ) 1 2 |t − s|− 1 2 α, then r.h.s. = exp � M0 − � 1 MC � 1 2 |t − s|− 1 2 αa � . Therefore, for we pick function a(x) = 2M0|x|β > 2M0|x| 1 2 α, with β ∈ (0, 1 2α), then �∞ j=1 a(2−n) < ∞ and ∞ � j=1 2nb(2−n) = ∞ � j=1 2n exp � M0 − � 1 MC � 1 2 |2−n|− 1 2 αa(2−n) � = ∞ � j=1 2n exp � M0 − � 1 MC � 1 2 |2n| 1 2 α−β � 27by the range of β, we have �∞ j=1 2nb(2−n) < ∞. Then the proof is completed by Kolo- mogorov theorem. A.7 Proof of Theorem 9 Proof To obtain the close form of ℓ1-regularized estimator, we expand Xi − θ by the eigenfunctions ϕj, i.e. 1 n n � i=1 ∥Xi − θ∥2 + ψ∥θ∥1,Cηl = 1 n n � i=1 ������ ∞ � j=1 ⟨Xi − θ, ϕj⟩ϕj ������ 2 + ψ∥θ∥1,Cηl = 1 n n � i=1 ∞ � j=1 ⟨Xi − θ, ϕj⟩2 + ψ∥θ∥1,Cηl = ∞ � j=1 � 1 n n � i=1 (Xij − θj)2 + ψ |θj| ληl/2 j � (13) Solving the minimization problem with in the bracket, then for each j, ˆθj = sgn( ¯Xj − ψ 2ληl/2 j ) � ¯Xj − ψ 2ληl/2 j �+ . Then ˆµl D = ∞ � j=1 ˆθj = ∞ � j=1 sψ,2λ ηl/2 j �� ¯X, ϕj �� ϕj.ϕj For the global sensitivity, sup D∼D′ ∥ˆµl D − ˆµl D′∥1,C = sup D∼D′ Jτ � j=1 |sψ,λ ηl/2 j �� ¯XD, ϕj �� − sψ,λ ηl/2 j �� ¯XD′, ϕj �� | λ ηl 2 j ≤ sup D∼D′ Jτ � j=1 | � ¯XD − ¯XD′, ϕj � | λ ηl j2 ≤ 2τ n Jτ � j=1 1 λ ηl j2 , where the first inequality is based on the fact that, in the worst case, the j-th coefficients based on D and D′ will not be shrunk to 0 simultaneously and thus should have the same sensitivity without soft-threshold function. A.8 Proof of Theorem 11 Proof Recall the object function F(θ) = 1 n n � i=1 ∥Xi − θ∥2 + ψ∥θ∥2 Cηr . 28and after dropping everything not involving θ, we have F(θ) = −2⟨ ¯X, θ⟩H + ⟨θ, θ⟩H + ψ⟨θ, θ⟩Cηr = −2⟨ ¯X, Cηrθ⟩Cηr + ⟨θ, Cηrθ⟩Cηr + ψ⟨θ, θ⟩Cηr . The second equality is based on Hilbert space’s own dual, i.e ⟨·, ·⟩H = ⟨·, C(·)⟩HC. Thus the minimizer of the F(θ) is ˆµr D = (Cηr + ψI)−1 Cηr( ¯X) = ∞ � j=1 ληr j ληr j + ψ � ¯X, ϕj � H ϕj where the second equality follow by expansion ˆµr D under the eigenfunction ϕj. For the global sensitivity, the upper bound for supD∼D′ ∥ˆµr D − ˆµr D′∥1,C is sup D∼D′ ∥ˆµD − ˆµD′∥1,C = sup D∼D′ ∞ � j=1 λ ηr− 1 2 j ληr j + ψ ��� ¯X − ¯X′, ϕj ��� ≤ sup D∼D′ sup j    λ ηr− 1 2 j ληr j + ψ    ∞ � j=1 ��� ¯X − ¯X′, ϕj ��� ≤ sup D∼D′    1 n sup j    λ ηr− 1 2 j ληr j + ψ    ∞ � j=1 ��� � Xn − X ′ n, ϕj ����    ≤ 2τ n sup j    λ ηr− 1 2 j ληr j + ψ    . A.9 Proof of Theorem 13 Proof For RKHS regularization: Recall that the form of ˆµr D and its global sensitivity, for privacy cost: E ∥˜µ − ˆµ∥2 L2 = 2∆2 ϵ2 ∞ � j=1 λj ≲  sup j λ ηr− 1 2 j ληr j + ψ   2 1 (nϵ)2 . Consider f(x) = xηr−0.5 xηr+ψ , and observe that f(x) ≲ ψ− 1 2ηr , then, E ∥˜µ − ˆµ∥2 L2 ≲ n−2ψ− 1 η2 . 29For Statistical Error: The n−1 part comes from variance while for bias, ∥ E ˆµ − µ0∥2 L2 = ∞ � j=1 � ψ ληr j + ψ �2 ⟨µ0, ϕj⟩2 ≤ ψ∥µ0∥2 Cηr ≲ ψ, where the last inequality is by assuming ∥µ0∥Cηr < ∞. Combining privacy cost and statis- tical error, one get the desired results. For ℓ1 regularization: Consider privacy cost, let Jτ := max{j ≥ 1 : τ ≤ ψ/2ληl/2 j }, then E ∥˜µ − ˆµ∥2 L2 = 4τ 2 n2ϵ2 ∆2 Jτ � j=1 λj ≲ (ϵn)−2   Jτ � j=1 jν   2 Jτ � j=1 j−2ν ≲ (ϵn)−2 Jmax{3,2(ν+1)} τ . As we assume the noise kernel with finite trace, then ν > 1 2 and E ∥˜µ − ˆµ∥2 L2 ≲ (ϵn)−2 J2(ν+1) τ . Next, we turn to Statistical Error. Define µ0,ψ = �∞ j=1 f ψ,2λ ηl2 j (⟨µ0, ϕj⟩) ϕj, by triangular inequality E ∥ˆµ − µ0∥2 L2 ≤ E ∥ˆµ − µ0,ψ∥2 L2 + ∥µ0,ψ − µ0∥2 L2 . For the bias term, let A = � j : |µj| ≥ ψ 2λ ηl/2 j � , then ∥µ0,ψ − µ0∥2 L2 = � A � µ0j − f ψ,2λ ηl2 j (µ0j) �2 + � Ac � µ0j − f ψ,2λ ηl2 j (µ0j) �2 . Starting with the summation over A, since λ − ηl2 j 2 < |µ0j| ψ we have � A � µ0j − f ψ,2λ ηl2 j (µ0j) �2 = � A ψ2 4ληl j ≤ ψ � A |µ0j| 2λ ηl j2 ≤ ψ 2 ∥µ0∥1,Cηl . Turning to summation over Ac, since |µ0j| ≤ ψ 2λ ηl2 j , � Ac � µ0j − f ψ,2λ ηl2 j (µ0j) �2 = � Ac µ2 0j ≤ ψ � Ac |µ0j| 2λ ηl j2 ≤ ψ 2 ∥µ0∥1,Cηl . Therefore, the overall bias is bounded by ∥µ0,ψ − µ0∥2 L2 ≤ ψ 2 ∥µ0∥1,Cηl . 30Now consider variance term E ∥ˆµ − µ0,ψ∥2 L2, ∥ˆµ − µ0∥2 L2 = ∞ � j=1 � f ψ,2λ ηl2 j � ¯Xj � − f ψ,2λ ηl2 j (µ0j) �2 . Similar to bias part, the summation can be decomposed to sum of four disjoint pieces A0,0 = {| ¯Xj| ≤ ψ/2λ ηl 2 j , |µj| ≤ ψ/2λ ηl 2 j }, A0,1 = {| ¯Xj| ≤ ψ/2λ ηl 2 j , |µj| > ψ/2λ ηl 2 j }, A1,0 = {| ¯Xj| > ψ/2λ ηl 2 j , |µj| ≤ ψ/2λ ηl 2 j }, A1,1 = {| ¯Xj| > ψ/2λ ηl 2 j , |µj| > ψ/2λ ηl 2 j }. When j ∈ A0,0, the summation is zero. Consider j ∈ A0,1, since | ¯Xj| ≤ ψ 2λ ηl2 j , � f ψ,2λ ηl2 j � ¯Xj � − f ψ,2λ ηl2 j (µ0j) �2 = � f ψ,2λ ηl2 j (µ0j) �2 =  µ0j − sgn(µ0j) ψ 2λ ηl 2 j   2 ≤ � µ0j − ¯Xj �2 . By symmetry, we get the same bound over A1,0. So lastly we consider summation over A1,1 For j ∈ A1,1 we have � f ψ,2λ ηl2 j � ¯Xj � − f ψ,2λ ηl2 j (µ0j) �2 =  µ0j − ¯Xj − � sgn(µ0j) − sgn( ¯Xj) � ψ 2λ ηl 2 j   2 . If both µ0j and ¯Xj have the same sign, then this is just (µ0j − ¯Xj)2. If they have opposite signs, then we have ������ � sgn(µ0j) − sgn( ¯Xj) � ψ 2λ ηl 2 j ������ ≤ ��µ0j − ¯Xj �� . Therefore, � f ψ,2λ ηl2 j � ¯Xj � − f ψ,2λ ηl2 j (µ0j) �2 ≤ 4 � µ0j − ¯Xj �2 , for j ∈ A1,1. Finally, the overall variance term is bounded by E ∥ˆµ − µ0∥2 L2 ≤ 4 E �� ¯X − µ0 ��2 L2 ≤ 4 n E ∥X1∥2 L2 ≲ n−1. 31A.10 Proof of Theorem 14 Proof Recall that the exact form of the kernel density estimator is ˆKD(x) = 1 n � det(H) n � i=1 Kη � H− 1 2 (x − xi) � . Then by the definition of global sensitivity, ∆ = sup D∼D′ ��� ˆKD − ˆKD′ ��� 1,K ≤ 1 n � det(H) ���Kη(H− 1 2 xn) − Kη(H− 1 2 x ′ n) ��� Kη � tr(Kη−1) ≤ 1 n � det(H) � tr(Kη−1) � 2 � Kη(0) − Kη(H− 1 2 (xn − x ′ n)) � ≤ 2MK n � det(H) � tr(Kη−1). The first inequality is based on the Cauchy–Schwarz inequality, which is also used in deriving the RKHS regularization approach. The last inequality holds by the assumption that Kη(·, ·) is pointwise bounded. Turning to the utility, taking H to be a diagonal matrix with same entry, then assump- tions stated in the Theorem 14 and by the Theorem 6.28 in Wasserman (2006), the risk R satisfies R = E � T � ˜fD(x) − f0(x) �2 dx ≤ 2 ∗ � E � T � ˜fD(x) − ˆfD(x) �2 dx + E � T � ˆfD(x) − f0(x) �2 dx � ≤ O � c1 n2h2d + h4 + c2 nhd � . for some constants c1 and c2. A.11 Proof of Theorem 16 Proof Recall while deriving the RKHS regularization approach, for a given η > 1 s.t. tr(Cη−1) is finite, we have ∥h∥1,C ≤ ∥h∥Cη � trace(Cη−1), substituting h by ˆfD − ˆfD′ leads to ∥ ˆfD − ˆfD′∥1,C ≤ ��� ˆfD − ˆfD′ ��� Cη � trace(Cη−1), meaning that we need to found the upper bound for ∥ ˆfD − ˆfD′∥Cη. First, let t ∈ [0, 1], δD′,D = ˆfD′ − ˆfD and LD(f) = 1 n �n i=1 Ldi,f. Notice that ˆfD′ and ˆfD are the minimizers of (7), we have LD � ˆfD � + ψ ��� ˆfD ��� 2 Cη ≤ LD � ˆfD + tδD′,D � + ψ ��� ˆfD + tδD′,D ��� 2 Cη , 32and LD � ˆfD′ � + ψ ��� ˆfD′ ��� 2 Cη ≤ LD � ˆfD′ − tδD′,D � + ψ ��� ˆfD′ − tδD′,D ��� 2 Cη . Combining the two inequalities above, LD � ˆfD � −LD � ˆfD + tδD′,D � + LD � ˆfD′ � − LD � ˆfD′ − tδD′,D � ≤ ψ ���� ˆfD + tδD′,D ��� 2 Cη − ��� ˆfD ��� 2 Cη + ��� ˆfD′ − tδD′,D ��� 2 Cη − ��� ˆfD′ ��� 2 Cη � Then using the same proof techniques in Section 4.3 of Hall et al. (2013), we have ∥ ˆfD − ˆfD′∥Cη ≤ M ψn � sup x Cη(x, x), which completes the proof. B. Appendix: Additional Results for Numerical Experiments B.1 Results for Different ICLP Covariance Kernel We also conduct the simulations under different ICLP covariance kernels. We set α = 5/2, such that the corresponding RKHS of this kernel is tied to W 3 2 ([0, 1]) and thus λj ≍ j−6, i.e. ν = 3. We repeat the comparison between plug-in and PCV and the experiments that compare different mechanisms under different n. The results are reported in Figure 6. From the figure, it can be observed that the results based on C 5 2 are almost the same as the results based on C 3 2 . B.2 Visualization of Age-at-Death KDE We present the visualization of the comparison between non-private KDE and private KDEs for different mechanisms in Figure 7. 330 2000 4000 Sample Size n 10 3 10 2 10 1 100 MSE S - 1 0 2000 4000 Sample Size n 10 3 10 2 10 1 100 S - 2 0 2000 4000 Sample Size n 10 3 10 2 10 1 100 S - 3 0 2000 4000 Sample Size n 0.3 1.0 3.0 S - 4 IID Laplace ICLP- 1 ICLP-RKHS pcv plug pcv plug 100 300 1000 3000 Sample Size n 10 3 10 2 10 1 100 101 MSE S - 1 100 300 1000 3000 Sample Size n 10 3 10 2 10 1 100 101 S - 2 100 300 1000 3000 Sample Size n 10 3 10 2 10 1 100 101 S - 3 IID Laplace Bernstein ICLP- 1 ICLP-RKHS 100 300 1000 3000 Sample Size n 0.01 0.10 1.00 S - 4 Figure 6: Plugin and PCV comparison (Top) and MSE for different mechanism (Bottom) for Mat´ern Kernel with α = 5 2 and ρ = 0.1. References F. Alda and B. I. Rubinstein. The bernstein mechanism: Function release under differential privacy. In Thirty-First AAAI Conference on Artificial Intelligence, 2017. J. Awan and A. Slavkovi´c. Structure and sensitivity in differential privacy: Comparing k-norm mechanisms. Journal of the American Statistical Association, 116(534):935–954, 2021. J. Awan, A. Kenney, M. Reimherr, and A. Slavkovi´c. Benefits and pitfalls of the exponen- tial mechanism with applications to hilbert spaces and functional pca. In International Conference on Machine Learning, pages 374–384. PMLR, 2019. V. I. Bogachev. Gaussian measures. Number 62. American Mathematical Soc., 1998. D. Bosq. Linear processes in function spaces: theory and applications, volume 149. Springer Science & Business Media, 2000. O. Bousquet and A. Elisseeff. Stability and generalization. The Journal of Machine Learning Research, 2:499–526, 2002. K. Chandrasekaran, J. Thaler, J. Ullman, and A. Wan. Faster private release of marginals on small databases. In Proceedings of the 5th conference on Innovations in theoretical computer science, pages 387–402, 2014. 340 20 40 60 80 100 0.000 0.010 Eastern Africa 0 20 40 60 80 100 0.000 0.010 0.020 Middle Africa 0 20 40 60 80 100 0.000 0.010 0.020 Northern Africa 0 20 40 60 80 100 0.000 0.010 Southern Africa 0 20 40 60 80 100 0.000 0.010 0.020 Western Africa 0 20 40 60 80 100 0.000 0.010 0.020 Central Asia 0 20 40 60 80 100 0.000 0.015 Eastern Asia 0 20 40 60 80 100 0.005 0.015 Southern Asia 0 20 40 60 80 100 0.005 0.015 South−Eastern Asia 0 20 40 60 80 100 0.000 0.010 0.020 Western Asia 0 20 40 60 80 100 0.000 0.015 0.030 Eastern Europe 0 20 40 60 80 100 0.000 0.015 0.030 Northern Europe 0 20 40 60 80 100 0.00 0.02 Southern Europe 0 20 40 60 80 100 0.00 0.02 Western Europe 0 20 40 60 80 100 0.000 0.010 0.020 Caribbean 0 20 40 60 80 100 0.000 0.010 0.020 Central America 0 20 40 60 80 100 0.000 0.010 0.020 South America 0 20 40 60 80 100 0.000 0.015 Northen America non−private KDE RKHS(1.01) RKHS(1.05) IID Laplace Bernstein Figure 7: Non-private and private kernel density estimates of age-at-death density in dif- ferent regions under different mechanisms with ϵ = 1. 35K. Chaudhuri and S. A. Vinterbo. A stability-based validation procedure for differentially private machine learning. Advances in Neural Information Processing Systems, 26:2652– 2660, 2013. K. Chaudhuri, C. Monteleoni, and A. D. Sarwate. Differentially private empirical risk minimization. Journal of Machine Learning Research, 12(3), 2011. N. Cressie and H.-C. Huang. Classes of nonseparable, spatio-temporal stationary covariance functions. Journal of the American Statistical association, 94(448):1330–1339, 1999. C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis. In Theory of cryptography conference, pages 265–284. Springer, 2006. C. Dwork, A. Roth, et al. The algorithmic foundations of differential privacy. Found. Trends Theor. Comput. Sci., 9(3-4):211–407, 2014. F. Ferraty and Y. Romain. The Oxford handbook of functional data analaysis. Oxford University Press, 2011. R. Hall, A. Rinaldo, and L. Wasserman. Differential privacy for functions and functional data. The Journal of Machine Learning Research, 14(1):703–727, 2013. M. Hardt and K. Talwar. On the geometry of differential privacy. In Proceedings of the forty-second ACM symposium on Theory of computing, pages 705–714, 2010. S. Kakutani. On equivalence of infinite product measures. Annals of Mathematics, pages 214–224, 1948. G. Kimeldorf and G. Wahba. Some results on tchebycheffian spline functions. Journal of mathematical analysis and applications, 33(1):82–95, 1971. P. Kokoszka and M. Reimherr. Introduction to functional data analysis. Chapman and Hall/CRC, 2017. D. Kosambi. Statistics in function space. In DD Kosambi, pages 115–123. Springer, 2016. F. McSherry and K. Talwar. Mechanism design via differential privacy. In 48th Annual IEEE Symposium on Foundations of Computer Science (FOCS’07), pages 94–103. IEEE, 2007. C. A. Micchelli and G. Wahba. Design problems for optimal surface interpolation. Technical report, WISCONSIN UNIV-MADISON DEPT OF STATISTICS, 1979. A. Mirshani, M. Reimherr, and A. Slavkovi´c. Formal privacy for functional data with gaussian perturbations. In International Conference on Machine Learning, pages 4595– 4604. PMLR, 2019. N. Phan, M. Vu, Y. Liu, R. Jin, D. Dou, X. Wu, and M. T. Thai. Heterogeneous gaussian mechanism: Preserving differential privacy in deep learning with provable robustness. arXiv preprint arXiv:1906.01444, 2019. 36J. Ramsay, J. Ramsay, B. Silverman, et al. Functional Data Analysis. Springer Science & Business Media, 2005. J. Rao and A. Scott. A simple method for the analysis of clustered binary data. Biometrics, pages 577–585, 1992. M. Reimherr and J. Awan. Elliptical perturbations for differential privacy. arXiv preprint arXiv:1905.09420, 2019. J. Sacks and D. Ylvisaker. Designs for regression problems with correlated errors. The Annals of Mathematical Statistics, 37(1):66–89, 1966. J. Sacks and D. Ylvisaker. Designs for regression problems with correlated errors: many parameters. The Annals of Mathematical Statistics, 39(1):49–69, 1968. J. Sacks and D. Ylvisaker. Designs for regression problems with correlated errors iii. The Annals of Mathematical Statistics, 41(6):2057–2074, 1970. T. Tao. An introduction to measure theory, volume 126. American Mathematical Society Providence, RI, 2011. Z. Wang, K. Fan, J. Zhang, and L. Wang. Efficient algorithm for privately releasing smooth queries. In NIPS, pages 782–790. Citeseer, 2013. L. Wasserman. All of nonparametric statistics. Springer Science & Business Media, 2006. L. Wasserman and S. Zhou. A statistical framework for differential privacy. Journal of the American Statistical Association, 105(489):375–389, 2010. M. Yuan and T. T. Cai. A reproducing kernel hilbert space approach to functional linear regression. The Annals of Statistics, 38(6):3412–3444, 2010. J. Zhang, Z. Zhang, X. Xiao, Y. Yang, and M. Winslett. Functional mechanism: regression analysis under differential privacy. arXiv preprint arXiv:1208.0219, 2012. 37