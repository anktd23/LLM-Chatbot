Interactive and Concentrated Differential Privacy for Bandits Achraf Azize and Debabrota Basu Équipe Scool, Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189- CRIStAL F-59000 Lille, France {achraf.azize,debabrota.basu}@inria.fr Abstract Bandits play a crucial role in interactive learning schemes and modern recom- mender systems. However, these systems often rely on sensitive user data, making privacy a critical concern. This paper investigates privacy in bandits with a trusted centralized decision-maker through the lens of interactive Differential Privacy (DP). While bandits under pure ϵ-global DP have been well-studied, we contribute to the understanding of bandits under zero Concentrated DP (zCDP). We provide minimax and problem-dependent lower bounds on regret for finite-armed and linear bandits, which quantify the cost of ρ-global zCDP in these settings. These lower bounds reveal two hardness regimes based on the privacy budget ρ and suggest that ρ-global zCDP incurs less regret than pure ϵ-global DP. We propose two ρ- global zCDP bandit algorithms, AdaC-UCB and AdaC-GOPE, for finite-armed and linear bandits respectively. Both algorithms use a common recipe of Gaussian mechanism and adaptive episodes. We analyze the regret of these algorithms to show that AdaC-UCB achieves the problem-dependent regret lower bound up to multiplicative constants, while AdaC-GOPE achieves the minimax regret lower bound up to poly-logarithmic factors. Finally, we provide experimental validation of our theoretical results under different settings. 1 Introduction Multi-armed bandit (in brief, bandits) (Lattimore and Szepesvári, 2020) is the archetypal setting of reinforcement learning consisting of K actions corresponding to K unknown reward distributions {νa}a∈[K]. We call {νa}a∈[K] ≜ ν an environment or a bandit instance. For T time steps, a bandit algorithm (or policy) π chooses an action (or arm) at ∈ [K] and receives a reward rt from the reward distribution νat. The goal of the policy is to maximise the cumulative reward �T t=1 rt or equivalently minimise the regret, i.e. the cumulative reward that π cannot achieve since it does not know the optimal reward distribution a priori. Bandits are increasingly used in a wide range of sequential decision-making tasks under uncertainty, such as recommender systems (Silva et al., 2022), strategic pricing (Bergemann and Välimäki, 1996), clinical trials (Thompson, 1933) to name a few. These applications often involve individuals’ sensitive data, such as personal preferences, financial situation, and health conditions, and thus, naturally, invoke data privacy concerns in bandits. Example 1 (DoctorBandit). Let us consider a bandit algorithm recommending one of K medicines with distributions of outcomes {νa}a∈[K]. Specifically, on the t-th day, a new patient ut arrives, and medicine at ∈ [K] is recommended to her by a policy π. To recommend a medicine at, the policy might either consider the specific medical conditions (or context) ct of patient ut, or ignore it. Then, the patient’s reaction to the medicine is observed. If the medicine cures the patient, the observed reward rt = 1, otherwise rt = 0. This observed reward can reveal sensitive information about the health condition of patient ut. Thus, the goal of a privacy-preserving bandit algorithm is 16th European Workshop on Reinforcement Learning (EWRL 2023). arXiv:2309.00557v1 [stat.ML] 1 Sep 2023Algorithm 1 Sequential interaction between a policy and users 1: Input: A policy π = {πt}T t=1 and Users {ut}T t=1 represented by the table d ≜ {x1, . . . , xT } ∈ (RK)T 2: Output: A sequence of actions a1, . . . , aT 3: for t = 1, . . . , T do 4: π recommends action at ∼ πt(. | a1, r1, . . . , at−1, rt−1) 5: ut sends the sensitive reward rt ≜ xt,at to π 6: end for to recommend a sequence of medicines (actions) that cures the maximum number of patients while protecting the privacy of these patients. We present this interactive process in Algorithm 1. Differential privacy for bandits. Motivated by such data-sensitive scenarios, privacy issues are widely studied for bandits for different settings, such as stochastic bandits (Mishra and Thakurta, 2015; Tossou and Dimitrakakis, 2016; Sajed and Sheffet, 2019; Azize and Basu, 2022; Hu and Hegde, 2022), adversarial bandits (Tossou and Dimitrakakis, 2017), and linear contextual bandits (Shariff and Sheffet, 2018; Neel and Roth, 2018; Hanna et al., 2022). All these works adhere to Differential Privacy (DP) Dwork et al. (2014) as the framework to ensure the data privacy of users, which is presently the gold-standard of privacy-preserving data analysis. DP dictates that an algorithm’s output has a limited dependency on the presence of any single user. Also, multiple formulations of DP, namely local and global, are extended to bandits Basu et al. (2019). Here, we focus on the global DP formulation, where users trust the centralised decision-maker, i.e. the policy, and provide it access to the raw sensitive rewards. The goal of the policy is to reveal the sequence of actions while protecting the privacy of the users and achieving minimal regret. The existing works on global DP preserving bandits consider pure ϵ-DP and assume that the action sequence is published non-interactively in one-shot. In this paper, we extend the study of privacy in bandits to the settings, where an adversary interacts with a policy at each step (Vadhan and Wang, 2021), and the algorithm aims to achieve popular relaxations of pure DP, e.g. zero Concentrated DP (zCDP) (Dwork and Rothblum, 2016). Interactive DP. A bandit algorithm induces an interactive process (Algorithm 1). At each step of this interaction, an adversary can manipulate the arm suggested by the algorithm and return it a reward from another arm. This situation is invoked in non-compliant bandits, where the user deploys an arm other than the recommended one, and in poisoning attacks, where a manipulated version of reward is sent to the policy either to leak information or to destroy its performance. This motivates us to define Interactive DP for bandits. A bandit policy π satisfying Interactive DP protects all possible outcomes corresponding to a user by making the view of the adversary indistinguishable when interacting with the policy on neighbouring reward datasets. Our effort resonates with the recent works in DP (Vadhan and Wang, 2021; Vadhan and Zhang, 2022; Lyu, 2022), where an analyst interacts with an offline dataset through an adaptive sequence of queries. The goal is to preserve privacy while responding to these adaptive queries. Our work extends the study of Interactive DP to the online setting, where a bandit algorithm generates its data by sequentially interacting with the environment (Section 2). Relaxations of pure DP for bandits. Pure DP is widely studied for different settings of bandits. Recently, lower bounds on regret for finite-armed and linear bandits preserving pure global DP, and algorithm design techniques to match the lower bounds are proposed (Azize and Basu, 2022). This still leaves open the question that what will be the minimal cost of preserving relaxations of pure DP in bandits as stated in (Shariff and Sheffet, 2018; Azize and Basu, 2022). Additionally, pure DP is often achieved by using Laplace noise to perturb the statistics computed on history. While in practice, Gaussian noise is widely used to perturb statistics computed on the dataset that leads to preserving relaxations of pure DP, namely (ϵ, δ)-DP (Dwork et al., 2014), Rényi DP (RDP) (Mironov, 2017), and zero Concentrated DP (zCDP) (Dwork and Rothblum, 2016), but not pure DP. Our goal is to provide a complete picture of regret’s lower and upper bounds for a relaxation of pure DP. In private bandits, proving regret lower bounds often rely on coupling arguments where group privacy is a central property (Azize and Basu, 2022). Since zCDP scales well under group privacy, we adopt zCDP as the relaxation of pure DP. In this work, we investigate zCDP in two settings of bandits: stochastic bandits with finitely many arms, and stochastic linear bandits with (fixed) finitely many arms. To our knowledge, we are the first to study the complexity of zCDP for bandits with global DP. The central questions that we aim to address are: 1. What is the minimal cost to pay in terms of regret to achieve ρ-global zCDP for bandits? 2. How to design bandit algorithms that can achieve these regret lower bounds order-optimally? 2Table 1: The complexity of bandits with ρ-global zCDP. Each lower bound is the maximum of the classical non-private bound and the corresponding bound in the third column. Setting Type Regret Lower Bound due to ρ zCDP 1 Regret Upper Bound Finite-armed Minimax ρ−1/2K (Thm 2, a) O �� KT log(T) � + O � ρ−1/2K � log(T) � (Thm 5, a) Problem Dependent 2 ρ−1/2 � a:∆a>0 � ∆ata−1� � log(T) (Thm 2, b) O �� a log T ∆a � + O � ρ−1/2K � log(T) � (Thm 5, b) Linear Minimax ρ−1/2d (Thm 3) O �� dT log(KT) � + O � ρ−1/2d log 3 2 (KT) � (Thm 7) Our contributions. Answering these questions leads us to: 1. Hardness as regret lower bounds: First, addressing the open problem of (Shariff and Sheffet, 2018; Azize and Basu, 2022), we prove minimax and problem-dependent lower bounds for finite-armed bandits, and minimax lower bound for linear bandits with ρ-global zCDP that quantify the cost to ensure ρ-global zCDP in these settings (Section 3). The minimax lower bounds show the existence of two privacy regimes depending on the privacy budget ρ and the horizon T. Specifically, for ρ = Ω(T −1), an optimal algorithm does not have to pay any cost to ensure privacy in both settings. In the problem-dependent analysis, the additional regret due to ρ-global zCDP in finite-armed bandits appears as a lower order term, i.e. Ω �� (log T)/ρ � , with respect to the non-private lower bound Ω(log T). In contrast, the regret due to ϵ-global DP, Ω ((log T)/ϵ), is not a lower order term. 2. Order-optimal algorithm design: We propose two algorithms, AdaC-UCB and AdaC-GOPE, that preserves ρ-global zCDP for finite-armed and linear bandits, respectively (Section 4). Both algorithms share the same algorithmic blueprint. First, they add a calibrated Gaussian noise to reward statistics. Second, they run in adaptive episodes, with the number of episodes being logarithmic in T. We analyse the regret of both algorithms and show that they match the lower bounds up to multiplicative factors. AdaC-UCB achieves the problem-dependent lower up to multiplicative constants, while both AdaC-UCB and AdaC-GOPE match the corresponding minimax lower bounds up to poly-logarithmic factors. We summarise all the lower and upper bounds in Table 1. In Section 5, we numerically validate their performances in different settings. 3. Technical tools: We propose a novel technique to generate lower bounds for bandits with ρ-zCDP using coupling arguments. We adapt this technique to the sequential bandit setup and use it to derive regret lower bounds with a generic proof. We also discuss in depth the effect of partial information (bandit feedback) on the definition of DP for bandits (Appendix A). We also prove a lower bound on the cost of reward poisoning against an Interactive DP bandit algorithm (Theorem 8). This opens up a direction to bridge privacy defences and attacks for bandits. 2 Bandits with Interactive DP: The formulation First, we formalise Interactive DP for bandits with a centralised decision-maker. We adopt the Interactive DP definition as studied in (Vadhan and Zhang, 2022; Lyu, 2022), where a mechanism M is viewed as a party in an interactive protocol, interacting with a possibly adversarial analyst or users. We represent each user ut by the vector xt ≜ (xt,1, . . . , xt,K) ∈ RK, where xt,a represents the potential reward observed if action a was recommended to user ut. Due to the bandit feedback, if at step t action qt ∈ [K] was queried from the environment, only the reward rt = xt,qt is observed at step t. Thus, the set of users {ut}T t=1 is represented by the table of potential rewards d ≜ {x1, . . . , xT } ∈ (RK)T . We view the policy π as an interactive mechanism, taking as input the table of potential rewards d while interacting with a possibly adversarial analyst B. The interactive protocol is described in Definition 1. Definition 1 (The bandit-adversary interactive protocol). A policy π = {πt}T t=1, with its input d the table of potential rewards, and the adversary B = {Bt}T t=1 follow the interactive process: For t = 1, . . . , T: 1. The bandit algorithm selects an action ot ∼ π(d; q1, q2, . . . , qt−1), 2. The adversary returns a query action qt = Bt(o1, o2, . . . , ot). 3. The bandit algorithm observes the reward corresponding to qt for user ut, i.e. xt,qt. 1Here, we only express the private part of the lower bound 2For Bernoulli bandits, ta = ∆a and the lower bound reduces to Kρ−1/2� log(T). 3Here, we denote π(d; q1, q2, . . . , qt−1) ≜ πt(· | q1, x1,q1, . . . , qt−1, xt−1,qt−1). At each step t, the policy π recommends an action ot. The adversary B observes the recommended action ot, and chooses (adversarially) a query action qt, based on the history of recommended actions (os)t−1 s=1. At step t + 1, the policy recommends the next action ot+1 based only on its private input d containing all the sensitive rewards information about the users, and the adversarially chosen query actions (qs)t s=1. Following the Interactive DP framework, the policy π is a differentially private interactive mechanism if the view of adversary B, i.e. View(B ↔ π(d)) ≜ (o1, . . . , oT ), is indistinguishable when the interaction is run on two neighbouring tables of rewards d and d′. They represent two sets of users differing by only one individual, i.e. one row. Formally, it implies that the Hamming distance between the two tables d and d′ is one, dHam(d, d′) ≜ �T t=1 1 {xt ̸= x′ t}. We denote them by d ∼ d′. Definition 2 (Interactive DP policy). A policy π = {πt}T t=1 is said to be a. Interactive (ϵ, δ)-DP policy for a given δ ∈ [0, 1) if, for every pair of neighboring table of potential rewards datasets d, d′ ∈ X, every adversary B ∈ B, and every subset of possible views S ⊆ [K]T , Pr[View(B ↔ π(d)) ∈ S] ≤ exp(ϵ) · Pr[View(B ↔ π(d′)) ∈ S] + δ. b. Interactive (α, ϵ)-RDP policy for an α > 1 if, for every adversary B ∈ B, supd∼d′ Dα(View(B ↔ π(d))∥View(B ↔ π(d′))) ≤ ϵ. c. Interactive (ξ, ρ)-zCDP policy if, for every α ∈ (1, ∞), and every adversary B ∈ B, sup d∼d′ Dα(View(B ↔ π(d))∥View(B ↔ π(d′))) ≤ ξ + ρα. (1) Here, Dα(P∥Q) ≜ 1 α−1 log EQ �� dP dQ �α� denotes the Rényi divergence of order α between P and Q. We define ϵ-pure global DP as (ϵ, 0)-DP and ρ-global zCDP to be (0, ρ)-zCDP. Implications of ensuring Interactive DP for bandits: We elaborate on three interesting implications of the interactive definition of privacy in bandits, compared to the non-interactive definition adopted in the literature. We recall the non-interactive definition in detail in Appendix A. 1. Interactive adversarial hypothesis testing: Interactive DP (Def. 2) defends against an online adversary, who can manipulate the actions recommended by the policy. The adversary participates in the interaction between the algorithm and environment and can run an interactive (or sequential) hypothesis testing (Wald, 1992) to distinguish between two neighbouring datasets based on its view of the interaction. In contrast, the adversary in the non-interactive definition only observes the sequence of actions a = (at)T t=1 that the policy π recommends without any interference. Based on the sequence a, the adversary runs a one-shot hypothesis testing to distinguish between two neighbouring datasets (Kairouz et al., 2015). Hence, Interactive DP allows us to defend against a stronger and more realistic adversary. 2. Protecting non-compliant users: Interactive DP (Def. 2) protects the privacy of the users even if they are non-compliant (Kallus, 2018; Stirn and Jebara, 2018), i.e. the users decide to ignore the recommendations of the policy and choose a different arm. Specifically, the policy recommends an action ot at step t, but the adversary could choose another query action qt, different than ot. The reward rt = dt,qt and the decision of the policy ot+1 in the next step depend on the query action qt and not ot. In contrast, the non-interactive definition only protects the privacy of compliant users. 3. Defending against online poisoning attacks: In the Interactive DP definition, the policy rec- ommends action ot at step t and expects to receive the corresponding reward dt,ot. However, an adversary may intentionally query a different action qt, resulting in the observed reward dt,qt. This can be viewed as poisoning the reward from dt,ot to dt,qt (Liu and Shroff, 2019). The interactive definition inherently provides robustness against online reward poisoning. In Appendix A.4, we show that if a policy π is consistent and ρ-global zCDP, an online oracle attacker has to incur Ω( � (log T)/ρ) cost to make the policy choose a non-optimal target arm linearly. Thus, for a smaller privacy budget ρ, i.e. high privacy, an attacker has to poison further more to succeed. Remark: Handling bandit feedback. Ensuring privacy in bandit settings requires careful consid- eration of how the bandit feedback, or partial information, is handled. When a policy selects arms at, it only observes the reward rt = dt,at associated with arm at, while the other rewards dt,a for a ̸= at remain unobserved. A fundamental question in defining privacy for bandits is whether the private input dataset of the policy π should be considered as the set of observed rewards {rt}T t=1 (referred to as View DP), or the entire table of potential rewards d (referred to as Table DP). In 4App. A, we compare both definitions and demonstrate that Table DP is a stronger notion, as being Table DP implies being View DP. This intuition stems from the fact that Table DP protects users ut by safeguarding all their potential responses. We also establish that, for pure DP, the two notions are equivalent. However, for approximate DP and its variations, transitioning from View DP to Table DP causes a significant loss in the privacy budget due to group privacy considerations. For a detailed comparison and discussion, we refer to App. A. In Definition 2, we adopt the Table DP framework. Goal: Regret minimisation. Hereafter, we adopt ρ-global zCDP (Eq. (1)) as the privacy definition. The goal is to design a ρ-global zCDP policy that minimises regret. To define regret, we adhere to the classic interaction of bandits as explained in Algorithm 1. Specifically, the adversary is no longer part of the interaction. The policy π directly interacts with the set of users {u1, . . . , uT } as in Algorithm 1, and the goal is to maximize the expected cumulative reward, or equivalently minimize the expected regret. We study two settings: finite-armed stochastic bandits and stochastic linear bandits. Now, we formally define regrets for them. Finite-armed stochastic bandits. The environment ν ≜ (νa : a ∈ [K]) consists of K arms (or reward distributions) with finite means (µa)a∈[K]. For any horizon T, regret is defined as RegT (π, ν) ≜ Tµ⋆ − E � T � t=1 rt � = K � a=1 ∆aE [Na(T)] , (2) where µ⋆ ≜ maxa∈[K] µa is the mean of the optimal arm a⋆, ∆a ≜ µ⋆ − µa is the sub-optimality gap of the arm a, and Na(T) ≜ �T t=1 1 {at = a} is the number of times the arm a is played till T in the interaction of Algorithm 1. The expectation is taken both on the randomness of the environment ν and the policy π, using the canonical bandit model (Chapter 4.6 of (Lattimore and Szepesvári, 2020)). Stochastic linear bandits. We consider that a fixed set of actions A ⊂ Rd is available at each round, such that |A| = K. The rewards are generated by a linear structural equation. Specifically, at step t, the observed reward is rt ≜ ⟨θ⋆, at⟩ + ηt, where θ⋆ ∈ Rd is the unknown parameter, and ηt is a conditionally 1-subgaussian noise, i.e. E [exp (ληt) | a1, η1, . . . , at−1] ≤ exp � λ2/2 � almost surely for all λ ∈ R. For any horizon T > 0, the regret of a policy π is RegT (π, A, θ⋆) ≜ Eθ⋆ � T � t=1 ∆At � , (3) where suboptimality gap ∆a ≜ maxa′∈A ⟨a′ − a, θ⋆⟩. Eθ⋆[·] is the expectation with respect to the measure of outcomes induced by the interaction of π and the linear bandit environment (A, θ⋆). Remark. There are two interaction protocols: The bandit-adversary interactive protocol of Defini- tion 1 and the sequential interaction between a policy and users of Algorithm 1. The bandit-adversary interactive protocol is used to analyse the privacy of the policy. Specifically, we want to design a policy for which the view of an adversary is “similar" when only one user changes in the interaction of Definition 1. On the other hand, to analyse the accuracy of the policy, we adhere to the "classic" sequential interaction between a policy and users. In this interaction (Algorithm 1), the policy recommends at each time-step an action at and observes the reward rt corresponding to the user ut in the table d, i.e. rt = xt,at. There is no adversary in this interaction, and the goal of the policy is to maximize the expected cumulative reward or equivalently minimize the expected regret, when interacting with users, without the presence of the adversary. In brief, we want to design a policy that verifies the “adversarial" privacy constraint and minimizes the classic “expected" regret. 3 Lower bounds on regret of bandits with ρ-global zCDP In this section, we quantify the cost of ρ-global zCDP for bandits by providing regret lower bounds for any ρ-global zCDP policy. These lower bounds on regret provide valuable insight into the inherent hardness of the problem and establish a target for optimal algorithm design. We first derive a ρ-global zCDP version of the KL-decomposition Lemma using a sequential coupling argument. The regret lower bounds are then retrieved by plugging the KL upper bound in classic regret lower bound proofs. A summary of the lower bounds is in Table 1, while the proof details are deferred to Appendix C. KL decomposition lemma. To proceed with the lower bounds, first, we are interested to control the Kullback-Leibler (KL) divergence between marginal distributions induced by a ρ-zCDP mechanism 5when the datasets are generated using two different distributions. In particular, if P1 and P2 are two data-generating distributions over X n, we define the marginals over the output of mechanism M as Mν(A) ≜ � d∈X n M (A | d) dPν (d) , (4) when the inputs are generated from P1 and P2, i.e. for ν ∈ {1, 2} and A ∈ F. Theorem 1 (KL decomposition for ρ-zCDP). Let P1 and P2 be two product distributions over X n, i.e. P1 = �n i=1 p1,i and P2 = �n i=1 p2,i, where pν,i for ν ∈ {1, 2}, i ∈ [1, n] are distributions over X. Let ti ≜ TV (p1,i ∥ p2,i). If M is ρ-zCDP, then KL (M1 ∥ M2) ≤ ρ � n � i=1 ti �2 + ρ n � i=1 ti(1 − ti) (5) This is a centralised ρ-zCDP version of the KL-decomposition lemma under local DP (Duchi et al., 2013, Theorem 1), and a ρ-zCDP version of the Sequential Karwa-Vadhan lemma (Azize and Basu, 2022). In Appendix B, we elaborate on the new proof technique, which can be of parallel interest. Leveraging this decomposition, now, we derive two flavours of regret lower bounds, namely minimax and problem-dependent. The minimax lower bound expresses the best regret achievable by a policy on the corresponding worst-case environment. The problem-dependent lower bound controls the regret of a ‘reasonable’ (consistent) policy for a specific environment that the policy interacts with. Lower bounds on regret for finite-armed bandits Theorem 2 (Minimax and problem-dependent lower bounds for finite-armed bandits). (a) Minimax. Let Πρ be the set of ρ-zCDP policies. For any K > 1, T ≥ K − 1, and 0 < ρ ≤ 1, Regminimax T,ρ ≜ inf π∈Πρ sup ν∈EK RegT (π, ν) ≥ max � 1 27 � � T(K − 1) �� � without ρ-global zCDP , 1 44 K − 1 √ρ � �� � with ρ-global zCDP � . (b) Problem-dependent. Let E = M1 × · · · × MK be a class of environments with K arms, where Ma is a set of reward distributions with finite means. Let π be a consistent policy3 over E satisfying ρ-global zCDP . Then, for all ν = (Pa)K a=1 ∈ E, (i.e. Pa ∈ Ma ), it holds that lim inf T →∞ RegT (π, ν) � log(T) ≥ � a:∆a>0 ∆a √ρ tinf (Pa, µ∗, Ma). where tinf (P, µ∗, M) ≜ infP ′∈M {TV (P ∥ P ′) : µ (P ′) > µ∗} Comments on the minimax bound. The minimax regret lower bound suggests the existence of two hardness regimes depending on ρ and T. When ρ < (27/44)(K − 1)/T, i.e. the high-privacy regime, the lower bound becomes Ω � K/√ρ � , and ρ-global zCDP bandits incur more regret than non-private ones. When ρ > (27/44)(K − 1)/T, i.e. in the low-privacy regime, the lower bound retrieves the non-private lower bound, i.e. Ω( √ KT), and thus, we can achieve privacy for free. Comments on the problem-dependent bound. The problem-dependent lower bound shows that the price of privacy is a lower order term Ω �� log(T)/ρ � . For a fixed privacy budget ρ and asymptotically in T, this is negligible compared to the non-private problem-dependent regret lower bound of Ω (� a log(T)/∆a). In contrast, for pure ϵ-global DP, the price of privacy in the problem- dependent regret is Ω (log(T)/ϵ), which is not a second-order term (Azize and Basu, 2022). Thus, in a problem-dependent perspective, privacy is ‘free’ only for ρ-global zCDP, but not for ϵ-global DP. Lower bound on regret for linear bandits Theorem 3 (Minimax Lower Bounds for Linear Bandits). Let A = [−1, 1]d and Θ = Rd. Then, for any ρ-global zCDP policy, we have that Regminimax T (A, Θ) ≥ max          exp(−2) 8 d √ T � �� � without ρ-global zCDP , exp(−2.25) 4 d √ρ � �� � with ρ-global zCDP          . 3A policy π is called consistent over a class of environments E, if ∀ν ∈ E and p > 0, limT →∞ RT (π,ν) T p = 0. 6Algorithm 2 AdaC-UCB. Changes due to privacy are in blue. 1: Input: Privacy budget ρ, an environment ν with K arms, optimism parameter β > 3 2: Output: Actions satisfying ρ-global zCDP 3: Initialisation: Choose each arm once and let t = K 4: for ℓ = 1, 2, . . . do 5: Let tℓ = t + 1 6: Compute aℓ = argmaxa Iρ a(tℓ − 1, β) (Eq. (6)) 7: Choose arm aℓ until round t such that Naℓ(t) = 2Naℓ(tℓ − 1) 8: end for Two privacy regimes. Similar to the finite-arm case, the minimax regret lower bound for linear bandits suggests the existence of two hardness regimes for ρ ≥ 4 exp(−0.5) T and ρ ≤ 4 exp(−0.5) T . A generic proof technique. In order to prove the lower bounds, we deploy the KL upper bound of Theorem 1 in the classic proof scheme of regret lower bounds (Lattimore and Szepesvári, 2020). The high-level idea of proving bandit lower bounds is selecting two hard environments, which are hard to statistically distinguish but are conflicting, i.e. actions that may be optimal in one is sub-optimal in other. The KL upper bound of Theorem 1 allows us to quantify the extra-hardness to statistically distinguish environments due to the additional ‘blurriness’ created by the ρ-zCDP constraint. 4 Algorithm design: AdaC-UCB and AdaC-GOPE In this section, we propose AdaC-UCB and AdaC-GOPE, two algorithms that satisfy ρ-global zCDP for finite-armed and linear bandits respectively. The two algorithms share a similar blueprint: the Gaussian mechanism and adaptive episodes. For each setting, we present the algorithm, provide a privacy and a regret analysis, and compare the regret upper bounds to the regret lower bounds. 4.1 Stochastic finite-armed bandits Now, we study the setting of finite-armed bandits under ρ-zCDP as detailed in Section 2. Algorithm. AdaC-UCB is an extension of the generic algorithmic wrapper proposed by Azize and Basu (2022) for bandits with ρ-global zCDP. Following (Azize and Basu, 2022), AdaC-UCB relies on three ingredients: arm-dependent doubling, forgetting, and adding calibrated Gaussian noise. First, the algorithm runs in episodes. The same arm is played for a whole episode, and double the number of times it was last played. Second, at the beginning of a new episode, the index of arm a, as defined in Eq. (6), is computed only using samples from the last episode, where arm a was played, while forgetting all the other samples. In a given episode, the arm with the highest index is played for all the steps. Due to these two ingredients, namely doubling and forgetting, each empirical mean computed in the index of Eq. (6) only needs to be ρ-zCDP for the algorithm to be ρ-global zCDP, avoiding the need of composition theorems. We formalise this intuition in Lemma 7 of Appendix D. For AdaC-UCB, we use the private index to select the arms (Line 6 of Algorithm 2) as Iρ a(tℓ − 1, β) ≜ ˆµℓ a + N � 0, σ2 a,ℓ � + Ba(tℓ − 1, β). (6) Here, ˆµℓ a is the empirical mean of rewards collected in the last episode in which arm a was played, σ2 a,ℓ ≜ 1 2ρ×( 1 2 Na(tℓ−1)) 2 is the variance of the Gaussian noise. Finally, the exploration bonus is defined as Ba(tℓ − 1, β) ≜ �� 1 2× 1 2 Na(tℓ−1) + 1 ρ×( 1 2 Na(tℓ−1)) 2 � β log(tℓ). The term in blue rectifies the non-private confidence bound of UCB for the added Gaussian noise. Theorem 4 (Privacy of AdaC-UCB). For rewards in [0, 1], AdaC-UCB satisfies ρ-global zCDP. Proof sketch. The main idea is that a change in one user only affects the empirical mean calculated in one episode, which is made private using the Gaussian Mechanism and Lemma 7. Since the actions are computed only using the private empirical means, AdaC-UCB is ρ-global zCDP thanks to the post-processing lemma. We refer to Appendix D for the complete proof. Theorem 5 (Regret analysis of AdaC-UCB). For rewards in [0, 1] and β > 3, AdaC-UCB yields (a) a problem-dependent regret upper bound � a:∆a>0 � 8β ∆a log(T) + 8 � β ρ � log(T) + 2β β−3 � , and 7Algorithm 3 AdaC-GOPE. Changes due to privacy are in blue. 1: Input: Privacy budget ρ, A ⊂ Rd and δ 2: Output: Actions satisfying ρ-global zCDP 3: Initialisation: Set ℓ = 1, t1 = 1 and A1 = A 4: for ℓ = 1, 2, . . . do 5: βℓ ← 2−ℓ 6: Step 1: Find the G-optimal design πℓ for Aℓ: max π∈P(Aℓ) |Supp(π)|≤d(d+1)/2 log det V (π). (7) 7: Step 2: Sℓ ← Supp (πℓ) 8: Choose each action a ∈ Sℓ for Tℓ(a) ≜ ⌈cℓπℓ(a)⌉ times where cℓ is defined by Eq (8). 9: Observe rewards {rt} tℓ+� a Tℓ(a) t=tℓ 10: Tℓ ← � a∈Sℓ Tℓ(a) and tℓ+1 ← tℓ + Tℓ + 1 11: Step 3: Estimate the parameter as ˆθℓ = V −1 ℓ �tℓ+1−1 t=tℓ atrt with Vℓ = � a∈Sℓ Tℓ(a)aa⊤ 12: Step 4: Make the parameter estimate private ˜θℓ = ˆθℓ +V − 1 ℓ 2 Nℓ, where Nℓ ∼ N � 0, 2d ρcℓ Id � . 13: Step 4: Eliminate low rewarding arms: Aℓ+1 = � a ∈ Aℓ : maxb∈Aℓ � ˜θℓ, b − a � ≤ 2βℓ � . 14: end for (b) a minimax regret upper bound O �� KT log(T) � + O � Kρ−1/2� log(T) � . Order-optimality of AdaC-UCB. The problem-dependent regret upper bound of AdaC-UCB matches the problem-dependent regret lower bound of Theorem 2 up to multiplicative constants for Bernoulli Bandits. On the other hand, the minimax regret upper bound of AdaC-UCB achieves the minimax regret lower bound of Theorem 2 up to an extra √log T term, which is usually the extra cost to pay in minimax regret for the UCB algorithm. Discussion on related bounds. Under a distributed setting and for (α, ϵ)-RDP, Chowdhury and Zhou (2022) propose a variant of Successive Elimination (SE) with Skellam noise, which achieves a O(K � log(T)/ϵ) private regret. However, for non-private bandits, optimism-based strategies achieve optimality and have better performance than SE. This is shown by Azize and Basu (2022) while comparing their adaptive mechanism, AdaP-UCB, with DP-SE in the case of ϵ-pure DP. Similar reasoning follows here. Second, Skellam Noise is less practical to sample from than Gaussian Noise. 4.2 Stochastic Linear Bandits Here, we study ρ-global zCDP for stochastic linear bandits with a finite number of arms, as in Sec. 2. Algorithm. We propose AdaC-GOPE (Algorithm 3), which is a ρ-global zCDP extension of the G-Optimal design-based Phased Elimination (GOPE) algorithm (Lattimore and Szepesvári, 2020, Algorithm 12). AdaC-GOPE is a phased elimination algorithm. At the end of each episode ℓ, AdaC-GOPE eliminates the arms that are likely to be sub-optimal, i.e. the ones with an empirical gap exceeding the current threshold (βℓ = 2−ℓ). The elimination criterion only depends on the samples collected in the current episode. In addition, the actions to be played during an episode are chosen based on the solution of an optimal design problem (Equation (7)) that helps to exploit the structure of arms and to minimise the number of samples needed to eliminate a suboptimal arm. In particular, if πℓ is the G-optimal solution for Aℓ at phase ℓ, then each action a ∈ Aℓ is played Tℓ(a) ≜ cℓπℓ(a) times, where for δ′ ≜ δ Kℓ(ℓ+1), cℓ ≜ 8d β2 ℓ log � 4 δ′ � + 2d βℓ �2 ρ � d + 2 � d log � 2 δ′ � + 2 log � 2 δ′ ��1/2 . (8) The term in blue is the additional length of the episode to compensate for the noisy statistics used to ensure privacy. The samples collected in the current episode do not influence which actions are played in it. This decoupling allows: (a) the use of the tighter confidence bounds available in the fixed design setting (Appendix F.2), and (b) avoiding privacy composition theorems and using, therefore, Lemma 7 to make the algorithm private. Note that AdaC-GOPE can be seen as a generalisation of DP-SE (Sajed and Sheffet, 2019) to the linear bandit setting. 810−1 101 103 Privacy budget ρ 0 100 200 300 Regpriv − Regnon−priv T = 105 T = 106 T = 107 0.0 0.2 0.4 0.6 0.8 1.0 Step t ×107 0.0 0.2 0.4 0.6 0.8 1.0 PoP ρ =0.1 ρ = 0.5 ρ = 1 (a) Finite-armed Bandits 10−1 101 103 Privacy budget ρ 0 500 1000 Regpriv − Regnon−priv T = 104 T = 105 T = 106 0.0 0.5 1.0 Step t ×107 0.0 0.1 0.2 0.3 PoP ρ = 0.01 ρ = 0.1 ρ = 1 (b) Linear Bandits Figure 1: For each bandit setting, the left figure represents the evolution of the difference between the private and non-private regret with respect to the privacy budget ρ. The right figure represents the evolution of the price of privacy (PoP) with respect to the time step. Assumption 1 (Boundedness). We assume that: (1) actions are bounded: ∥a∥2 ≤ 1 for all a in A, (2) rewards are bounded: |rt| ≤ 1, and (3) the unknown parameter is bounded: ∥θ⋆∥2 ≤ 1. Theorem 6 (Privacy of AdaC-GOPE). Under Assumption 1, AdaC-GOPE satisfies ρ-global zCDP. Proof sketch. Similar to Theorem 4, a change in one user only affects the estimate ˆθℓ in one episode. Thanks to Lemma 7, it is enough that each ˆθℓ is ρ-zCDP with respect to the sequence of rewards collected in the corresponding episode. Since the actions only depend on the estimates { ˆθℓ}ℓ, the algorithm is ρ-global zCDP by the post-processing lemma. We refer to Appendix D for the proof. Theorem 7 (Regret analysis of AdaC-GOPE). Under Assumption 1 and for δ ∈ (0, 1), with probability at least 1 − δ, the regret RT of AdaC-GOPE (Algorithm 3) is upper-bounded by A � dT log � K log(T ) δ � + Bd √ρ � log � K log(T ) δ � log(T), where A and B are universal constants. If δ = 1 T , then E(RT ) ≤ O �� dT log(KT) � + O � d √ρ(log(KT)) 3 2 � . Order-optimality of AdaC-GOPE. The minimax regret upper bound of AdaC-GOPE matches with the minimax regret lower bound of Theorem 3 up to an extra (log KT) 3 2 factor. Related algorithms and bounds. Hanna et al. (2022) and Li et al. (2022) study private variants of the GOPE algorithm for pure ϵ-global DP and (ϵ, δ)-DP, respectively. However, both algorithms differ in how they privatize the estimated parameter ˆθ compared to AdaC-GOPE. They add noise to each sum of rewards �tℓ+1−1 t=tℓ rt (Line 11, Alg. 3), whereas we add noise in ˆθl (Line 12, Alg. 3). As a result, though we achieve linear dependence on the dimension d as suggested by the lower bound, others do not (d2 for (Hanna et al., 2022) and d3/2 for (Li et al., 2022)). In Appendix F, we analyse in detail the impact of adding noise at different steps of GOPE, both theoretically and experimentally. 5 Experimental analysis Now, we empirically verify whether AdaC-UCB and AdaC-GOPE can achieve privacy for free. Experimental setup. For finite-armed bandits, we test AdaC-UCB with β = 1 and compare it to its non-private counterpart, i.e. a UCB algorithm with adaptive episodes and forgetting. We test the algorithms for Bernoulli bandits with 5-arms and means {0.75, 0.625, 0.5, 0.375, 0.25} (as in (Sajed and Sheffet, 2019)). For linear bandits, we implement AdaC-GOPE and compare it to GOPE. We set the failure probability to δ = 0.001 and the noise to be ρt = N(0, 1). We use the Frank-Wolfe algorithm to solve the G-optimal design problem (Lattimore and Szepesvári, 2020). We chose K = 10 actions randomly on the unit tri-dimensional sphere (d = 3). The true parameter θ⋆ is also chosen randomly on the tri-dimensional sphere. For both settings, we run the private and non-private algorithms 100 times for a horizon T = 107, and compare the average regret between the private and non-private algorithms in Figure 1. Results and analysis. We reach two conclusions from the results of both settings. 1. Free-privacy in low-privacy regime. For a fixed horizon T, the difference between the private and non-private regret, Regpriv − Regnon−priv, converges to zero as the privacy budget ρ → ∞. Thus, our algorithms achieve the same regret as their non-private counterparts in the low-privacy regime. 2. Asymptotic no price of privacy. For a fixed privacy budget ρ, the Price of Privacy (PoP), i.e. PoP ≜ Regpriv−Regnon−priv Regnon−priv converges to zero as the horizon T increases. This observation resonates with both the theoretical regret upper bounds of the algorithms and the hardness suggested by the lower bounds, where cost due to privacy appears as lower-order terms. 96 Conclusion and future works We study bandits with interactive ρ-global zCDP. First, we demonstrate the benefits of adopting the Interactive DP definition for bandits. Then, we prove the minimax and problem-dependent regret lower bounds for finite-armed and linear bandits, showing that the additional regret due to ρ-global zCDP is less compared to pure ϵ-global DP. The minimax bound additionally shows the existence of two hardness regimes and privacy can be achieved for free in the low-privacy regime. We propose AdaC-UCB and AdaC-GOPE, which satisfy ρ-global zCDP using a generic algorithmic blueprint, and match the regret lower bounds up to constants and poly-logarithmic factors respectively. A possible future direction is to derive regret lower bounds for bandits with (ϵ, δ)-DP. Both pure ϵ-DP and ρ-zCDP enjoy a (‘tight’) group privacy property that gives meaningful lower bounds for bandits, when applied with coupling arguments. These arguments fail to adapt to (ϵ, δ)-DP. An interesting technical challenge would be to adapt, for bandits, the fingerprinting lemma, which is a technique used for proving (ϵ, δ)-DP lower bounds (Bun et al., 2014; Kamath et al., 2022). For the algorithm design, it would be also interesting to see how to close the multiplicative gaps. Acknowledgments and Disclosure of Funding This work is supported by the AI_PhD@Lille grant. D. Basu acknowledges the Inria-Kyoto University Associate Team “RELIANT” for supporting the project, and the ANR JCJC for the REPUBLIC project (ANR-22-CE23-0003-01). We also thank Philippe Preux for his support. References Azize, A. and Basu, D. (2022). When privacy meets partial information: A refined analysis of differentially private bandits. Advances in Neural Information Processing Systems, 35:32199– 32210. Basu, D., Dimitrakakis, C., and Tossou, A. (2019). Differential privacy for multi-armed bandits: What is it and what is its cost? arXiv preprint arXiv:1905.12298. Bergemann, D. and Välimäki, J. (1996). Learning and strategic pricing. Econometrica: Journal of the Econometric Society, pages 1125–1149. Bun, M. and Steinke, T. (2016). Concentrated differential privacy: Simplifications, extensions, and lower bounds. In Theory of Cryptography, pages 635–658, Berlin, Heidelberg. Springer Berlin Heidelberg. Bun, M., Ullman, J., and Vadhan, S. (2014). Fingerprinting codes and the price of approximate differential privacy. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing, pages 1–10. Chowdhury, S. R. and Zhou, X. (2022). Distributed differential privacy in multi-armed bandits. arXiv preprint arXiv:2206.05772. Duchi, J. C., Jordan, M. I., and Wainwright, M. J. (2013). Local privacy and statistical minimax rates. In Proc. of IEEE Foundations of Computer Science (FOCS). Dwork, C., Roth, A., et al. (2014). The algorithmic foundations of differential privacy. Foundations and Trends® in Theoretical Computer Science, 9(3–4):211–407. Dwork, C. and Rothblum, G. N. (2016). Concentrated differential privacy. ArXiv, abs/1603.01887. Hanna, O. A., Girgis, A. M., Fragouli, C., and Diggavi, S. (2022). Differentially private stochastic linear bandits:(almost) for free. arXiv preprint arXiv:2207.03445. Hu, B. and Hegde, N. (2022). Near-optimal thompson sampling-based algorithms for differentially private stochastic bandits. In Uncertainty in Artificial Intelligence, pages 844–852. PMLR. Jun, K.-S., Li, L., Ma, Y., and Zhu, J. (2018). Adversarial attacks on stochastic bandits. Advances in Neural Information Processing Systems, 31. 10Kairouz, P., Oh, S., and Viswanath, P. (2015). The composition theorem for differential privacy. In International conference on machine learning, pages 1376–1385. PMLR. Kallus, N. (2018). Instrument-armed bandits. In Algorithmic Learning Theory, pages 529–546. PMLR. Kamath, G., Mouzakis, A., and Singhal, V. (2022). New lower bounds for private estimation and a generalized fingerprinting lemma. arXiv preprint arXiv:2205.08532. Lalanne, C., Garivier, A., and Gribonval, R. (2022). On the statistical complexity of estimation and testing under privacy constraints. arXiv preprint arXiv:2210.02215. Lattimore, T. and Szepesvári, C. (2020). Bandit algorithms. Cambridge University Press. Li, F., Zhou, X., and Ji, B. (2022). Differentially private linear bandits with partial distributed feedback. In 2022 20th International Symposium on Modeling and Optimization in Mobile, Ad hoc, and Wireless Networks (WiOpt), pages 41–48. IEEE. Liu, F. and Shroff, N. (2019). Data poisoning attacks on stochastic bandits. In International Conference on Machine Learning, pages 4042–4050. PMLR. Lyu, X. (2022). Composition theorems for interactive differential privacy. In Advances in Neural Information Processing Systems. Mironov, I. (2017). Rényi differential privacy. In Proceedings of 30th IEEE Computer Security Foundations Symposium (CSF), pages 263–275. Mishra, N. and Thakurta, A. (2015). (Nearly) optimal differentially private stochastic multi-arm bandits. In UAI. Neel, S. and Roth, A. (2018). Mitigating bias in adaptive data gathering via differential privacy. In International Conference on Machine Learning, pages 3720–3729. PMLR. Sajed, T. and Sheffet, O. (2019). An optimal private stochastic-MAB algorithm based on an optimal private stopping rule. Shariff, R. and Sheffet, O. (2018). Differentially private contextual linear bandits. In Advances in Neural Information Processing Systems, pages 4296–4306. Silva, N., Werneck, H., Silva, T., Pereira, A. C., and Rocha, L. (2022). Multi-armed bandits in recommendation systems: A survey of the state-of-the-art and future directions. Expert Systems with Applications, 197:116669. Stirn, A. and Jebara, T. (2018). Thompson sampling for noncompliant bandits. arXiv preprint arXiv:1812.00856. Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3-4):285–294. Tossou, A. C. and Dimitrakakis, C. (2016). Algorithms for differentially private multi-armed bandits. In Thirtieth AAAI Conference on Artificial Intelligence. Tossou, A. C. and Dimitrakakis, C. (2017). Achieving privacy in the adversarial multi-armed bandit. In Thirty-First AAAI Conference on Artificial Intelligence. Vadhan, S. and Wang, T. (2021). Concurrent composition of differential privacy. In Theory of Cryptography: 19th International Conference, TCC 2021, Raleigh, NC, USA, November 8–11, 2021, Proceedings, Part II 19, pages 582–604. Springer. Vadhan, S. and Zhang, W. (2022). Concurrent composition theorems for all standard variants of differential privacy. arXiv preprint arXiv:2207.08335. Wald, A. (1992). Sequential tests of statistical hypotheses. Springer. 11Appendix Table of Contents A Privacy definitions for bandits 13 A.1 Non-interactive DP for bandits . . . . . . . . . . . . . . . . . . . . . . . . . . 13 A.2 Interactive DP for bandits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 A.3 Consequences of the Interactive DP definition . . . . . . . . . . . . . . . . . . 17 A.4 Poisoning attacks against Interactive DP . . . . . . . . . . . . . . . . . . . . . 19 B Lower bounds via couplings for concentrated DP 20 B.1 From the KL to a transport problem . . . . . . . . . . . . . . . . . . . . . . . . 20 B.2 Proxy solution to the transport Problem . . . . . . . . . . . . . . . . . . . . . . 21 C Regret lower bounds for bandits under ρ-global zCDP 23 C.1 Stochastic finite-armed bandits: Minimax lower bound . . . . . . . . . . . . . . 23 C.2 Stochastic finite-armed bandits: Problem-dependent lower bound . . . . . . . . 24 C.3 Stochastic linear bandits: Minimax lower bound . . . . . . . . . . . . . . . . . 25 D Privacy proofs 30 D.1 The privacy lemma of non-overlapping sequences . . . . . . . . . . . . . . . . 30 D.2 Generic privacy proof of AdaC-UCB and AdaC-GOPE . . . . . . . . . . . . . 31 E Stochastic bandits with global zCDP 35 E.1 Concentration inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 E.2 Regret analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 E.3 Extensions to (ϵ, δ)-global DP and (α, ϵ)-global RDP . . . . . . . . . . . . . . 37 F Linear Bandits with global zCDP 38 F.1 Basic definitions of optimal design . . . . . . . . . . . . . . . . . . . . . . . . 38 F.2 Concentration inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 F.3 Regret analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 F.4 Extensions to (ϵ, δ)-global DP and (α, ϵ)-global RDP . . . . . . . . . . . . . . 42 F.5 Adding noise at different steps of AdaC-GOPE . . . . . . . . . . . . . . . . . . 43 G Existing technical results and definitions 45 H Extended experimental analysis 46 12A Privacy definitions for bandits In this section, we discuss different ways to adopt Differential Privacy (DP) in bandits. The main ingredients to specify in order to have a complete definition are (1) the mechanism in question, (2) the input dataset, (3) the neighbouring relationship between the input datasets, and (4) the output of the mechanism. For all the adaptations of DP for bandits studied in this section, the output of the mechanism is the same, i.e. a sequence of actions in [K]T . The mechanism in question is always induced by the policy. For completeness, we recall the definition of the policy in Definition 3. The main differences in the adaptations of DP originate from two sources: 1. Considering the policy as an interactive or non-interactive mechanism. 2. Considering the input of the mechanism to be the sequence of observed rewards, i.e. r = {r1, . . . , rT } ∈ RT , that we call View DP. Alternatively, considering the input of the mechanism to be the full table of potential rewards, i.e. d = {d1, . . . , dT } ∈ (RK)T , that we call Table DP. Before starting, we recall the definition of the policy. Let T ∈ N be the horizon. For each t ∈ [T], let Ωt = ([K] × R)t and Ft = B(Ωt) with B being the Borel set. Definition 3. A policy π is a sequence of rules (πt)T t=1 , where each πt is a probability kernel from the histories (Ωt−1, Ft−1) to arms ([K], 2[K]). Since [K] is discrete, we adopt the convention that for i ∈ [K], πt(i | a1, r1, . . . , at−1, rt−1) = πt({i} | a1, r1, . . . , at−1, rt−1) In the following, we first consider the non-interactive definition of privacy in bandits, which is the definition usually adopted in the private bandit literature. There, we mainly discuss the relationship between View and Table DP for different variants of DP. Then, we formalize the interactive definition of privacy in bandits. We state several consequences of the interactive definition. Also, we discuss its relation to the non-interactive definition, as well as to poisoning attacks. A.1 Non-interactive DP for bandits In this section, we define View and Table DP. Then, we discuss their relations. We prove that Table DP always implies View DP, with the same privacy parameters. However, the converse may not be always true. The equivalence can only be shown for pure ϵ-DP. For other variants of DP, there could be a huge increase in the privacy budget. To commence, we recall the defintions of variants of DP for a non-interactive mechansim M. Definition 4 (Variants of Approximate DP (ADP) for non-interactive mechanisms). A non-interactive mechanism M, that assigns to each dataset d a probability distribution Md on some measurable space (X, F), satisfies 1. (ϵ, δ)-DP ( Dwork et al. (2014)) for a given δ ∈ [0, 1) if sup A∈F,d∼d′ Md(A) − eϵMd′(A) ≤ δ. 2. (α, ϵ)-Rényi DP (RDP) ( Mironov (2017)) for an α > 1 if sup d∼d′ Dα(Md∥Md′) ≤ ϵ. 3. (ξ, ρ)-zero Concentrated DP (zCDP) ( Bun and Steinke (2016)) if, for all α ∈ (1, ∞), sup d∼d′ Dα(Md∥Md′) ≤ ξ + ρα. Here, two datasets d and d′ are said to be neighbouring (denoted by d ∼ d′) if their Hamming distance is one. Dα(P∥Q) ≜ 1 α−1 log EQ �� dP dQ �α� denotes the Rényi divergence of order α between P and Q. We define ϵ-pure DP as (ϵ, 0)-DP and ρ-zCDP to be (0, ρ)-zCDP. 13A.1.1 View DP This is the definition usually adopted in the literature of private bandits (Mishra and Thakurta, 2015; Tossou and Dimitrakakis, 2016; Sajed and Sheffet, 2019; Azize and Basu, 2022). We formalise it by stating its main ingredients, and coin the term "View DP". Input. The input considered is only the sequence of observed rewards r = (r1, . . . , rT ) and the neighbouring is a change in one reward in this sequence. Mechanism. The induced mechanism from the interaction of π and a list of rewards r ≜ (rt)t∈[T ] ∈ RT is Vπ such that Vπ : RT → P([K]T ) r → Vπ r The mechanism Vπ, when applied to a sequence of observed rewards r, outputs (in one shot) a sequence of actions aT ≜ (a1, . . . , aT ) ∈ [K]T , with probability Vπ r (aT ) = �T t=1 πt(at−1|a1, r1, . . . at, rt−1). Neighbouring input. The Hamming distance between two lists of rewards r, r′ ∈ RT is the number of different elements in r and r′, i.e. dHam(r, r′) ≜ T � t=1 1 {rt ̸= r′ t} Privacy definition. A policy π is • (ϵ, δ)-view DP if Vπ is (ϵ, δ)-DP • (α, ϵ)-view RDP if Vπ is (α, ϵ)-RDP • (ξ, ρ)-view zCDP if Vπ is (ξ, ρ)-zCDP A.1.2 Table DP To formalise the intuition of Figure 2, Table DP protects the patients by considering the input of the mechanism as the table which represents all the possible outcomes of the bandit interaction. Again in the following, we explain the mechanism to be made DP, its inputs, outputs and the neighbouring relation between its input. Interaction protocol. Let ν ≜ {Pa : a ∈ [K]} a bandit instance with K arms. The policy π interacts with the environment ν up to a given time horizon T to produce a history HT ≜ {(at, rt)}T t=1. The iterative steps of this interaction process yielding HT verify two conditions: 1. the conditional probability of choosing an action at = a at time t is dictated only by the policy πt(a|Ht−1), 2. the conditional distribution of reward rt given (Ht−1, at) is Pat. Thus, the policy can be seen as a set of adaptively chosen queries, applied to an adaptively-gathered data set of rewards. Conceived this way, it is hard to decouple inputs from outputs to extend DP correctly and protect the privacy of users. Input. To overcome this problem, we will adhere to the random table model of bandits (Lattimore and Szepesvári, 2020). Each user ut is represented by the row vector xt ≜ (xt,1, . . . , xt,K) ∈ RK, where xt,a represents the potential reward observed, if action a was recommended to user ut. Due to the bandit feedback, only rt = xt,at is observed at step t. One can verify that defined this way, the induced history HT ≜ {(at, rt)}T t=1 from the interaction between π and ν still verifies the two conditions 1. and 2. as defined above. The distribution of HT depends both on the stochasticity of the environment ν and the randomness of the policy π and is denoted by Pν,π. 14Using the random table model, the input corresponding to the set of users {u1, . . . , uT } is the fixed dataset d = {x1, . . . , xT } ∈ (RK)T . This way, the bandit interaction can be seen as applying a set of adaptively chosen queries on a fixed dataset. Mechanism. The induced mechanism from the interaction of the policy π and a table of rewards d ≜ {(xt,i)i∈[K]}t∈[T ] ∈ (RK)T is Mπ such that Mπ : (RK)T → P([K]T ) d → Mπ d The mechanism Mπ when applied to a dataset d outputs (in one shot) a sequence of actions aT ≜ (a1, . . . , aT ) ∈ [K]T with probability Mπ d(aT ) = �T t=1 πt(at|a1, x1,a1, . . . at−1, xt−1,at−1). Neighbouring Input. A change in one user reflects as a change in one row in the table d, so we define dHam(d, d′) ≜ �T t=1 1 {xt ̸= x′ t} = �T t=1 1 � ∃i ∈ [K], xt,i ̸= x′ t,i � . Privacy definition. A policy π is • (ϵ, δ)-Table DP if Mπ is (ϵ, δ)-DP • (α, ϵ)-Table RDP if Mπ is (α, ϵ)-RDP • (ξ, ρ)-Table zCDP if Mπ is (ξ, ρ)-zCDP Figure 2: An example of the interaction of a policy with two sets of patients, that differ in one user only. Each row represents the potential reactions of the patient to each medicine, but only one reaction is observed by the policy, i.e. the framed one. A change in one patient reflects as a change in one row in this table of potential rewards. A.1.3 Relation Between Table ADP and View ADP Definitions Table DP is a stronger notion of privacy than View DP. Lemma 1. For a fixed policy π, we have that Mπ is ADP ⇒ Vπ is ADP. Proof. Suppose Mπ is ADP. Let r, r′ ∈ RT be two lists of rewards such that dHam(r, r′) = 1. Define d such that dt,i = rt for all i ∈ [K] and all t ∈ [T], i.e. d is the table of rewards where r is concatenated colon-wise K times. We define d′ similarly with respect to r′. For d, d′ defined this way, we have that dHam(d, d′) = 1 , Vπ r = Mπ d and Vπ r′ = Mπ d′. This means that • If Mπ is (ϵ, δ)-DP, then ∀A ∈ P([K]T ) Vπ r (A) − eϵVπ r′(A) = Mπ d(A) − eϵMπ d′(A) ≤ δ and Vπ is (ϵ, δ)-DP. 15• If Mπ is (α, ϵ)-RDP, then Dα(Vπ r ∥Vπ r′) = Dα(Mπ d∥Mπ d′) ≤ ϵ and Vπ is (α, ϵ)-RDP • If Mπ is (ξ, ρ)-zCDP, then Dα(Vπ r ∥Vπ r′) = Dα(Mπ d∥Mπ d′) ≤ ξ + ρα and Vπ is (ξ, ρ)-zCDP. For pure ϵ-DP, View DP and Table DP are equivalent. Lemma 2. For a fixed policy π, we have that Mπ is ϵ-DP ⇔ Vπ ϵ-DP. Proof. (Proving ⇒) Mπ is ϵ-DP → Vπ ϵ-DP is true by Lemma 1, because an ϵ-DP mechanism is also (ϵ, δ)-DP for δ = 0. (Proving ⇐) Suppose Vπ is ϵ-DP. We want to show that Mπ is ϵ-DP too. Let d, d′ ∈ (RK)T such that dHam(d, d′) = 1. For aT ∈ [K]T , let daT ≜ (d1,a1, d2,a2, . . . , dT,aT ) ∈ RT be the trajectory of reward induced by aT in d. Since dHam(d, d′) = 1, we have that ∀aT ∈ [K]T dHam(daT , d′ aT ) ≤ 1. Let aT ∈ [K]T . We have that Mπ d(aT ) = Vπ daT (aT ) ≤ eϵVπ d′ aT (aT ) = eϵMπ d′(aT ) where the inequality is because Vπ is ϵ-DP and daT ∼ d′ aT . Thus, Mπ is ϵ-DP. Remark 1. The crux of the reciprocal proof comes from the fact that to prove ϵ-DP, you only need to check the atomic events aT . In that case, we can link Mπ and Vπ easily. This is not the case for approximate DP. For example, for (ϵ, δ), there is a huge loss in parameters. Lemma 3. For a fixed policy π, we have that Vπ is (ϵ, δ)-DP ⇒ Mπ is (ϵ, KT δ)-DP. Proof. Suppose Vπ is (ϵ, δ)-DP. Let d, d′ ∈ (RK)T such that dHam(d, d′) = 1. We have that, for every a ∈ [K]T , dHam(daT , d′ aT ) ≤ 1. Let E ⊂ [K]T be an event, i.e a set of sequences. We have that Mπ d(E) = � aT ∈E Mπ d(aT ) = � aT ∈E Vπ daT (aT ) ≤ (a) � aT ∈E (eϵVπ d′ aT (aT ) + δ) ≤ (b) eϵMπ d′(E) + KT δ , where (a) holds true because Vπ is (ϵ, δ)-DP, and (b) is true because card(E) ≤ KT . This means that Mπ is (ϵ, KT δ)-DP All in all, Table DP is the notion of privacy that we adhere to in this paper, since it protects all the potential responses of an individual rather than just the observed one. 16A.2 Interactive DP for bandits The classic non-interactive definition of DP (Definition 4) considers only mechanisms M that release answers in one shot. However, data analysts often interact with a database in an adaptive fashion. This motivates the study of interactive mechanisms to capture full-featured privacy-preserving data analytics. Here, we adopt the Interactive DP definition as expressed in (Vadhan and Zhang, 2022). The mechanism M is viewed as a party in an interactive protocol, interacting with a possibly adversarial analyst. We recall the complete definition here. Definition 5 (Interactive protocol). An interactive protocol (A, B) is any pair of functions on tuples of binary strings. The interaction between A with input xA and B with input xB is the following random process (denoted (A(xA), B(xB))): 1. Uniformly choose random coins rA and rB for A and B, respectively. 2. Repeat the following for i = 0, 1, . . .. (a) If i is even, let mi = A(xA, m1, m3, . . . , mi−1; rA). (b) If i is odd, let mi = B(xB, m0, m2, . . . , mi−1; rB). (c) If mi = halt, then exit loop. The view of a party in an interactive protocol captures everything the party “observes” dur- ing the execution. If (A, B) is an interactive protocol, A’s view of the interaction is the tuple ViewA(A(xA) ← B(xB)) = (rA, xA, m1, m3, . . .) consisting of all the messages received by A in the execution of the protocol together with the private input xA and random coins rA. B’s view of (A(xA; rA), B(xB; rB)) is defined symmetrically. In the setting of DP, Party A is the mechanism, where the input xA is the dataset. Party B is the adversary that does not have an input xB. Since we only care about the view of the adversary, we will drop the subscript and denote the view of the adversary as View(B ↔ M(x)). With this notation, interactive differential privacy is defined by asking for the views of an adversary on any pair of neighbouring datasets View(B ↔ M(x)) and View(B ↔ M(x′)) satisfying the same closeness notion as in non-interactive differential privacy. Definition 6 (Variants of Approximate DP (ADP) for Interactive mechanisms). A mechanism M is said to be an 1. (ϵ, δ)-DP interactive mechanism for a given δ ∈ [0, 1) if, for every pair of neighboring datasets d, d′ ∈ X, every adversary B ∈ B, and every subset of possible views S ⊆ Range(View), we have Pr[View(B ↔ M(x)) ∈ S] ≤ exp(ϵ) · Pr[View(B ↔ M(x′)) ∈ S] + δ. 2. (α, ϵ)-RDP interactive mechanism for an α > 1 if, for every adversary B ∈ B sup d∼d′ Dα(View(B ↔ M(d))∥View(B ↔ M(d′))) ≤ ϵ. 3. (ξ, ρ)-zCDP interactive mechanism if, for every α ∈ (1, ∞), and every adversary B ∈ B sup d∼d′ Dα(View(B ↔ M(d))∥View(B ↔ M(d′))) ≤ ξ + ρα. The interactive protocol Definition 5 is adapted to bandits in Definition 1. Similarly, the interactive definitions of Definition 6 are formalised for bandits in Definition 2. A.3 Consequences of the Interactive DP definition Here, we state different corollaries and lemmas, obtained as consequences of Interactive DP. First, we recall that to check the interactive DP condition, it is enough to only consider deterministic adversaries (Lemma 2.2 in Vadhan and Wang (2021)). Second, it is easy to see that interactive DP implies non-interactive DP. Lemma 4. If π is Interactive b-ADP, π is b-global ADP. Proof. This is direct by taking the identity-adversary Bid defined by Bid t (o1, . . . , ot) = ot. 17We also provide the following lemma, that relates the interactive and non-interactive definitions, using an interactive post-processing. Lemma 5 (Relation between interactive and non-interactive DP for bandits). π is Interactive b-ADP if and only if, for every deterministic adversary B, πB is b-Table ADP, where πB = {πB t }T t=1 and πB t (a | a1, r1 . . . , at−1, rt−1) ≜ πt(a | B(a1), r1, B(a1, a2), r2 . . . , B(a1, . . . , at−1), rt−1) (9) Remark 2. We use b-ADP as a shorthand for properties that are true for the three variants of DP. Here, b is the budget, namely b = (ϵ, δ), (α, ϵ), (ξ, ρ). Proof. This is direct by observing that for every deterministic adversary B, the view of adversary B reduces to View(B ↔ M(d)) = MπB. This means that any interactive policy could be simulated by interactive post-processing of a mech- anism verifying non-interactive DP. If a policy is "closed" under interactive post-processing, both interactive and non-interactive DP definitions are equivalent. Finally, we provide a "group privacy" property, verified by any Interactive DP policy. Corollary 1. If π is a ρ-global zCDP policy then, for any sequence of actions (a1, . . . , aT ) and any two sequence of rewards r ≜ {r1, . . . , rT } and r’ ≜ {r′ 1, . . . , r′ T }, we have that T � t=1 KL � πt(. | a1, r1, . . . , at−1, rt−1) �� πt(. | a1, r′ 1, . . . , at−1, r′ t−1) � ≤ ρdHam(r, r’)2 Proof. Let a ≜ (a1, . . . , aT ) be a fixed sequence of actions. Let r ≜ {r1, . . . , rT } and r’ ≜ {r′ 1, . . . , r′ T } be two sequences of rewards. Step 1: The constant adversary. We consider the constant adversary Ba defined as Ba(o1, . . . , ot) ≜ at i.e. Ba is the adversary that always queries at step t the action at, independently of the actions recommended by the policy. Let πa ≜ πBa as defined in Eq. (9). Since π is ρ-global zCDP, using Lemma 5, then Mπa is ρ-zCDP. And Lemma 1 gives that Vπa is ρ-zCDP. Step 2: Group privacy of zCDP. Using the group privacy property of ρ-zCDP i.e. Theorem 10 with α = 1, we get that KL (Vπa r ∥ Vπa r’ )) ≤ ρ dHam(r, r’)2. (10) Step 3: Decomposing the view of the constant adversary. On the other hand, we have that Vπa r (o1, . . . , oT ) = T � t=1 πt(ot | a1, r1, . . . , at−1, rt−1). In other words Vπa r = �T t=1 πt(. | a1, r1, . . . , at−1, rt−1). Similarly, Vπa r’ = �T t=1 πt(. | a1, r′ 1, . . . , at−1, r′ t−1). Hence, we get KL (Vπa r ∥ Vπa r’ )) = T � t=1 KL � πt(. | a1, r1, . . . , at−1, rt−1) �� πt(. | a1, r′ 1, . . . , at−1, r′ t−1) � (11) Plugging Equaion (11) in Inequality (10) concludes the proof. 18A.4 Poisoning attacks against Interactive DP We recall the setting of poisoning attacks for bandits (Jun et al., 2018; Liu and Shroff, 2019). A poisoning attacker B sits between a policy π and the real environment ν. When the policy pulls the action at, the environment generates the real reward r0 t ∼ νat and the attacker decides on an attack αt. The reward observed by the policy π is then rt = r0 t − αt. The goal of the attacker B is to manipulate π to choose a sub-optimal target arm (call it K without loss of generality) while spending a minimum cumulative attack cost �T t=1 |αt| in expectation. The attack is successful if the number of pulls of the target arms NK(T) = T − o(T). The Oracle attack (Jun et al., 2018) is a trivial attack when the attacker knows the real means of the environment ν. The attack proceeds by attacking any round t where a non-target arm at ̸= K is pulled by π. The Oracle attacker pulls down the reward of the corresponding arm by αt = ∆γ at = max {µat − µK + γ, 0} for a small parameter γ > 0. The Oracle attack transforms the original bandit problem into one where all non-target arms have an expected reward of less than µk. Theorem 8 (Defense against Oracle attack). If π is a consistent ρ-global zCDP policy, the Oracle attacker needs Ω �� log(T ) ρ � expected cumulative cost to succeed. Proof. The oracle attack targets the arm K and makes it appear optimal for the policy π. Since π is a consistent policy, π will linearly pull the ‘optimal arm’ in the transformed bandit, which is arm K. Thus, the Oracle attacker can succeed. On the other hand, the Oracle attack, defined by αt = ∆γ at1 {at ̸= K} = max {µat − µK + γ, 0} 1 {at ̸= K} has an expected cumulative cost of E � T � t=1 |αt| � = K−1 � a=1 E[Na(T)]∆γ at Since π is a consistent ρ-global zCDP policy, the problem-dependent regret lower bound (Theorem 2) gives that for a ̸= K, E[Na(T)] = Ω �� log(T ) ρ � which concludes the proof. 19B Lower bounds via couplings for concentrated DP In this section, we are interested in controlling the distance (the Kullback-Leibler, i.e. KL) between marginal distributions induced by a differentially private mechanism, when the datasets are generated using two different distributions. This type of information-theoretic bounds is generally the main step for many standard methods for obtaining minimax lower bounds. Our main theorem in this section relates the effect of Concentrated DP on this information-theoretic quantity. In particular, if P1 and P2 are two data-generating distributions over X n, we are interested in the marginals over the output of the mechanism M when the inputs are generated from P1 and P2, i.e. for ν ∈ {1, 2} and A ∈ F Mν(A) ≜ � d∈X n M (A | d) dPν (d) (12) In the following, we will provide general results to bound the KL divergence between the distributions M1 and M2 defined in (12), when the mechanism M is ρ-zCDP. The upper bound depends on the privacy budget ρ and the per-step total variation distance between the data-generating distributions P1 and P2. We recall the definition of an f-divergence. Definition 7 (f-divergence). Let f : (0, ∞) → R be a convex function with f(1) = 0. Let P and Q be two probability distributions on a measurable space (X, F). If P ≪ Q then the f-divergence is defined as Df(P∥Q) ≜ EQ � f �dP dQ �� where dP dQ is a Radon-Nikodym derivative and f(0) ≜ f(0+). B.1 From the KL to a transport problem Let P1 and P2 two distributions over X n. Define C as a coupling of (P1, P2), i.e. the marginals of C are P1 and P2. We denote by Π(P1, P2) the set of all the couplings between P1 and P2. Let M1 and M2 be defined as in (12). We recall the definition of an f-divergence. Theorem 9. We have that Df(M1∥M2) ≤ inf C∈Π(P1,P2) E(d,d′)∼C[Df(Md∥Md′)]. (13) Proof. Let C be a coupling of P1 and P2. We provide a visual proof of the theorem. First, we recall Theorem 15. If PX PY |X −→ PY and PX QY |X −→ QY , then Df (PY ∥QY ) ≤ EX∼PX � Df � PY |X∥QY |X �� . The idea is to use Theorem 15, where the input is a pair of datasets (d, d′) sampled from the coupling C, the first channel applies the private mechanism to the first dataset, the second channel applies the mechanism to the second dataset. In other words, • X = (d, d′) a pair of datasets in X n 20• the input distribution is PX = C the coupling distribution. • the first channel is the mechanism applied to the first dataset PY |X = M(Y | d). • the second channel is the mechanism applied to the second dataset QY |X = M(Y | d′). • Y is the output of the mechanism Using this notation, we have that • PY = M1 • QY = M2 • Df � PY |X∥QY |X � = Df(Md∥Md′). Using Theorem 15, we have that Df(M1∥M2) ≤ E(d,d′)∼C[Df(Md∥Md′)]. which is true for every coupling C. Taking the infimum over the couplings concludes the proof. We will use the group privacy to upper bound the RHS of Equation 13. Theorem 10 (Group Privacy for ρ-zCDP, Proposition 27, Bun and Steinke (2016)). If M is ρ-CDP, then ∀d, d′ ∈ X n, ∀α ≥ 1, Dα(Md∥Md′) ≤ ρdHam(d, d′)2α. Combining the last two theorems gives the following corollary. Corollary 2. If M is ρ-CDP, then KL (M1 ∥ M2) ≤ ρ inf C∈Π(P1,P2) E(d,d′)∼C[dHam(d, d′)2]. Proof. Let M be ρ-CDP. Applying Theorem 9, with f(x) = x log(x) gives that KL (M1 ∥ M2) ≤ ρ inf C∈Π(P1,P2) E(d,d′)∼C[KL (Md ∥ Md′)]. Applying Theorem 10 with α = 1 gives that KL (Md ∥ Md′) ≤ ρdHam(d, d′)2 Combining both inequalities gives the final bound. B.2 Proxy solution to the transport Problem Deriving the sharpest upper bound for the KL would require solving the transport problem inf C∈Π(P1,P2) E(d,d′)∼C[dHam(d, d′)2]. As a proxy, we will use maximal couplings. Proposition 1. Let P1 and P2 be two probability distributions that share the same σ-algebra. There exists a coupling c∞(P1, P2) ∈ Π(P1, P2) called a maximal coupling, such that E(X1,X2)∼c∞(P1,P2) [1 {X1 ̸= X2}] = TV (P1 ∥ P2) Using maximal coupling for data-generating distributions that are product distributions yields the following bound. 21Theorem 1 (KL decomposition for ρ-zCDP). Let P1 and P2 be two product distributions over X n, i.e. P1 = �n i=1 p1,i and P2 = �n i=1 p2,i, where pν,i for ν ∈ {1, 2}, i ∈ [1, n] are distributions over X. Let ti ≜ TV (p1,i ∥ p2,i). If M is ρ-zCDP, then KL (M1 ∥ M2) ≤ ρ � n � i=1 ti �2 + ρ n � i=1 ti(1 − ti) Proof. Let ci ∞ be a maximal coupling between p1,i and p2,i for all i ∈ [1, n]. We define the coupling C∞ ≜ �n i=1 ci ∞. Then C∞ is a coupling of P1 and P2. Since dHam(d, d′) = �n i=1 1 {di ̸= d′ i} we get that, for (d, d′) ∼ C∞, dHam(d, d′) ∼ n � i=1 Bernoulli(ti), where ti ≜ TV (p1,i ∥ p2,i). This further yields E(d,d′)∼C∞[dHam(d, d′)2] = � n � i=1 ti �2 + n � i=1 ti(1 − ti). Corollary 2 concludes the proof. Comments on the bound of Theorem 1. This is a centralised ρ-zCDP version of the KL- decomposition lemma under local DP (Duchi et al., 2013, Theorem 1), and a ρ-zCDP version of the Sequential Karwa-Vadhan lemma (Azize and Basu, 2022). We also refer to (Lalanne et al., 2022) that uses similar coupling ideas to derive ρ-zCDP variants of LeCam and Fano inequalities. 22C Regret lower bounds for bandits under ρ-global zCDP In this section, we will use the result of Theorem 1 in classic regret lower bounds for bandits to generate multiple lower bounds, namely minimax and problem dependent for stochastic and minimax for linear bandits. C.1 Stochastic finite-armed bandits: Minimax lower bound Theorem 2 (Part a: Minimax lower bound for finite-armed bandits). Let Πρ be the set of ρ-zCDP policies. For any K > 1, T ≥ K − 1, and 0 < ρ ≤ 1, Regminimax T,ρ ≜ inf π∈Πρ sup ν∈EK RegT (π, ν) ≥ max � 1 27 � � T(K − 1) �� � without ρ-global zCDP , 1 44 K − 1 √ρ � �� � with ρ-global zCDP � . Proof. The non-private part of the lower bound is due to Theorem 15.2 in Lattimore and Szepesvári (2020). To prove the private part of the lower bound, we plug our KL decomposition theorem into the proofs of regret lower bounds for bandits. Step 1: Choosing the ‘hard-to-distinguish’ environments. First, we fix a ρ-zCDP policy π . Let ∆ be a constant (to be specified later), and ν be a Gaussian bandit instance with unit variance and mean vector µ = (∆, 0, 0, ..., 0). To choose the second bandit instance, let a ≜ arg mini∈[2,K] Eν,π[Ni(T)] be the least played arm in expectation other than the optimal arm 1. The second environment ν′ is then chosen to be a Gaussian bandit instance with unit variance and mean vector µ′ = (∆, 0, 0, . . . 0, 2∆, 0 . . . , 0), where µ′ j = µj for every j except for µ′ a = 2∆. The first arm is optimal in ν and the arm i is optimal in ν′. Since T = Eνπ [N1(T)] + � i>1 Eνπ [Ni(T)] ≥ (K − 1)Eνπ [Na(T)], we observe that na ≜ Eνπ [Na(T)] ≤ T K − 1 Step 2: From lower bounding regret to upper bounding KL-divergence. Now by the classic regret decomposition and Markov inequality (Lemma 11), we get4 RegT (π, ν) = (T − Eνπ [N1(T)]) ∆ ≥ Mνπ (N1(T) ≤ T/2) T∆ 2 , and RegT (π, ν′) = ∆Eν′π [N1(T)] + � a/∈{1,i} 2∆Eν′π [Na(T)] ≥ Mν′π (N1(T) > T/2) T∆ 2 . Let us define the event A ≜ {N1(T) ≤ T/2} = {(a1, a2, . . . , aT ) : card({j : aj = 1}) ≤ T/2}. By applying the Bretagnolle–Huber inequality, we have: RegT (π, ν) + RegT (π, ν′) ≥ T∆ 2 (Mνπ(A) + Mν′π(Ac)) ≥ T∆ 4 exp(−KL (Mνπ ∥ Mν′π)) Step 3: KL-divergence decomposition with ρ-global zCDP. Now, we apply Theorem 1 along with an oracle argument similar to (Shariff and Sheffet, 2018). Since ν and ν′ only differ in the distribution of arm a, the oracle coupling induces a maximal coupling only on the samples coming from arm a. Specifically, we build the following oracle coupling O. When π samples an action i ̸= a, the oracle O provides the same sample twice, i.e. ri ∼ νi and r′ i = ri. Otherwise, for the samples coming from 4In all regret lower bound proofs, we are under the probability space over sequence of actions, produced when π interacts with ν for T time-steps. We do this to use the KL-divergence decomposition of Mνπ 23arm a, the oracle provides, in expectation, na fresh iid samples from the maximal coupling between νa and ν′ a. Using Theorem 1 with the oracle coupling O, n = na and ti = ta ≜ TV (νa ∥ ν′ a), we get that KL (Mνπ ∥ Mν′π) ≤ ρ(n2 at2 a + nata(1 − ta)) ≤ ρ(n2 at2 a + nata) . The last inequality is due to the fact that 1 − ta ≤ 1. Finally, using Pinsker’s Inequality (Lemma 13), we obtain ta = TV (νa ∥ ν′ a) ≤ � 1 2KL (N(0, 1) ∥ N(2∆, 1)) = ∆ Step 4: Choosing the worst ∆. Plugging back in the regret expression, we find RegT (π, ν) + RegT (π, ν′) ≥ T∆ 4 exp � −ρ � n2 a∆2 + na∆ �� ≥ T∆ 4 exp � −ρ � na∆ + 1 2 �2� ≥ T∆ 4 exp � −ρ � T K − 1∆ + 1 2 �2� By optimising for ∆, we choose ∆ = K−1 T � 1 √ρ − 1 2 � > 0, since ρ ≤ 1. This gives that RegT (π, ν) + RegT (π, ν′) ≥ K − 1 4 � 1 √ρ − 1 2 � exp (−1) ≥ K − 1 8√ρ exp (−1) We conclude the proof by lower bounding 1 8 exp(−1) ≥ 1 22, and using 2 max(a, b) ≥ a + b. C.2 Stochastic finite-armed bandits: Problem-dependent lower bound Theorem 2 (Part b: Problem-dependent lower bounds for finite-armed bandits). Let E = M1 × · · · × MK be a class of environments with K arms, where Ma is a set of reward distributions with finite means. Let π be a consistent policy5 over E satisfying ρ-global zCDP . Then, for all ν = (Pa)K a=1 ∈ E, (i.e. Pa ∈ Ma ), it holds that lim inf T →∞ RegT (π, ν) � log(T) ≥ � a:∆a>0 ∆a √ρ tinf (Pa, µ∗, Ma). where tinf (P, µ∗, M) ≜ infP ′∈M {TV (P ∥ P ′) : µ (P ′) > µ∗} Proof. Let π be a consistent policy satisfying ρ-global zCDP. Let µa be the mean of the a-th arm in ν, ta = tinf (Pa, µ∗, Ma). Fix a suboptimal arm a, and let β > 0 be an arbitrary constant. Step 1: Choosing the ‘hard-to-distinguish’ environment. Let ν′ ≜ � P ′ j �K j=1 ∈ E be a bandit with P ′ j = Pj for j ̸= a and P ′ a ∈ Ma be such that TV (Pa ∥ P ′ a) ≤ ta + β and µ (P ′ a) > µ∗, which exists by the definition of ta. Let µ′ ∈ RK be the vector of means of distributions of ν′. 5A policy π is called consistent over a class of environments E, if ∀ν ∈ E and p > 0, limT →∞ RT (π,ν) T p = 0. 24Step 2: From lower bounding regret to upper bounding KL-divergence. For simplicity of notations, we use RegT = RegT (π, ν), Reg′ T = RegT (π, ν), and A = {(a1, a2, . . . , aT ) : card({j : aj = 1}) ≤ T/2}. Then, by regret decomposition and Markov Inequality 11, we obtain RegT + Reg′ T ≥ T 2 (Mνπ(A)∆a + Mν′π (Ac) (µ′ a − µ∗)) (14) ≥ T 2 min {∆a, µ′ a − µ∗} (Mνπ(A) + Mν′π (Ac)) ≥ T 4 min {∆a, µ′ a − µ∗} exp(−KL (Mνπ ∥ Mν′π)) Step 3: KL-divergence decomposition with ρ-global zCDP. Similar to Step 3 in the previous minimax proof, we build the oracle coupling O that provides a maximal coupling only on the samples coming from arm a. Using Theorem 1 with the oracle coupling O, n = na and ti = TV (νa ∥ ν′ a) = ta + β, we get that KL (Mνπ ∥ Mν′π) ≤ ρ[n2 a(ta + β))2 + na(ta + β))] where na ≜ Eνπ [Na(T)] . Step 4: Rearranging and taking the limit inferior. Thus, we get RegT + Reg′ T ≥ T 4 min {∆a, µ′ a − µ∗} exp � −ρ � n2 a (ta + β)2 + na (ta + β) �� Solving for na gives that na ≥ � 4c(T) + 1 − 1 2(ta + β) where c(T) ≜ 1 ρ log � T min{∆a,µ′ a−µ∗} 4(RegT +Reg′ T) � . Now, taking the limit on both sides leads to lim inf T →∞ Eνπ [Na(T)] � log(T) ≥ 1 (ta + β) lim inf T →∞ � c(T) log(T) = 1 (ta + β) � � � �1 ρ � 1 − lim sup T →∞ log � RegT + Reg′ T � log(T) � = 1 (ta + β) �1 ρ The last equality follows from the definition of consistency, which says that for any p > 0, there exists a constant Cp such that for sufficiently large T, RegT + Reg′ T ≤ CpT p. This property implies that lim sup T →∞ log � RegT + Reg′ T � log(T) ≤ lim sup T →∞ p log(T) + log (Cp) log(T) = p, which gives the result since p > 0 was an arbitrary constant. We arrive at the claimed result by taking the limit as β tends to zero. C.3 Stochastic linear bandits: Minimax lower bound First, we give a specific coupling lemma for the linear case and plug it in the minimax lower bound proofs. 25Let ν = {Pa, a ∈ [K]} and ν′ = {P ′ a, a ∈ [K]} be two bandit instances. When the policy π interacts with the bandit instance ν, it induces a marginal distribution mν,π over the sequence of actions, i.e. mν,π(a1, . . . , aT ) ≜ � r1,...,rT T � t=1 πt(at | a1, r1, . . . , at−1, rt−1)pat(rt) drt. We define mν′,π similarly. Lemma 6. If π is ρ-global zCDP, then KL (mν,π ∥ mν′,π) ≤ ρ � Eν,π � T � t=1 tat ��2 + ρ Eν,π � T � t=1 tat(1 − tat) � + ρ Vν,π � T � t=1 tat � (15) where tat ≜ TV � Pat �� P ′ at � and Eν,π and Vν,π are the expectation and variance under mν,π respectively. Proof. We adapt the proofs of Appendix B to the bandit case, by creating a coupled bandit instance. Let ν = {Pa : a ∈ [K]} and ν′ = {P ′ a : a ∈ [K]} be two bandit instances. Define ca as the maximal coupling between Pa and P ′ a. Let π = {πt}T t=1 be a ρ-global zCDP policy. Here, we build a coupled environment γ of ν and ν′. The policy π interacts with the coupled environment γ up to a given time horizon T to produce a history {(At, Rt, R′ t)}T t=1. The iterative steps of this interaction process are: 1. the probability of choosing an action At = a at time t is dictated only by the policy πt and A1, R1, A2, R2, . . . , At−1, Rt−1, i.e. ignores {R′ s}t−1 s=1. 2. the distribution of rewards (Rt, R′ t) is cAt and is conditionally independent of the previous observed history {(As, Rs, R′ s)}t−1 t=1. This interaction is similar to the interaction process of policy π with the first bandit instance ν, with the addition of sampling an extra R′ t from the coupling of Pat and P ′ at. The distribution of the history induced by the interaction of π and the coupled environment can be defined as pγπ(a1, r1, r′ 1 . . . , aT , rT , r′ T ) ≜ T � t=1 πt(at | a1, r1, . . . , at−1, rt−1)cat(rt, r′ t) To simplify the notation, let a ≜ (a1, . . . , aT ), r ≜ (r1, . . . , rT ) and r’ ≜ (r′ 1, . . . , r′ T ). Also, let ca(r, r’) ≜ �T t=1 cat(rt, r′ t) and π(a | r) ≜ �T t=1 πt(at | a1, r1, . . . , at−1, rt−1). We put h ≜ (a, r, r’). With the new notation pγπ(a, r, r’) ≜ π(a | r)ca(r, r’) Similarly, we define qγπ(a, r, r’) ≜ π(a | r’)ca(r, r’) It follows that mν,π is the marginal of pγπ when integrated over (r, r’), and mν′,π is the marginal of qγπ when integrated over (r, r’), i.e. mν,π(a) = � r,r’ pγπ(a, r, r’) dr dr’ and mν′,π(a) = � r,r’ qγπ(a, r, r’) dr dr’ By the data-processing inequality, we get that KL (mν,π ∥ mν′,π) ≤ KL (pγπ ∥ qγπ) (16) 26In the following, upper case variables refer to random variables. We have that KL (pγπ ∥ qγπ) (a) = EH≜(A,R,R’)∼pγπ � log � π(A | R)cA(R, R’) π(A | R’)cA(R, R’) �� (b) = T � t=1 EH∼pγπ � log �πt(At | A1, R1, . . . At−1, Rt−1) πt(At | A1, R′ 1, . . . At−1, R′ t−1) �� (c) = T � t=1 EH∼pγπ � EH∼pγπ � log �πt(At | A1, R1, . . . At−1, Rt−1) πt(At | A1, R′ 1, . . . At−1, R′ t−1) � �� A1, R1, . . . At−1, Rt−1 �� (d) = T � t=1 EH∼pγπ � EAt∼πt(.|A1,R1,...At−1,Rt−1) � log �πt(At | A1, R1, . . . At−1, Rt−1) πt(At | A1, R′ 1, . . . At−1, R′ t−1) ��� (e) = T � t=1 EH∼pγπ � KL � πt(. | A1, R1, . . . At−1, Rt−1) �� πt(. | A1, R′ 1, . . . At−1, R′ t−1) �� , where we obtain (a): by definition of pγπ, qγπ and the KL divergence (b): by definition of π(A | R) and π(A | R’) (c): using the towering property of the expectation (d): using that, conditioned on the history (A1, R1, . . . At−1, Rt−1), the distribution of At is πt(. | A1, R1, . . . At−1, Rt−1). (e): by definition of the KL divergence On the other hand, Corollary 1, we have that T � t=1 KL � πt(. | A1, R1, . . . At−1, Rt−1) �� πt(. | A1, R′ 1, . . . At−1, R′ t−1) � ≤ ρd2 Ham(R, R′) which means that KL (pγπ ∥ qγπ) ≤ EH∼pγπ � ρd2 Ham(R, R′) � (a) = EH∼pγπ � EH∼pγπ � ρd2 Ham(R, R′) �� A �� (b) = ρ EH∼pγπ � EH∼pγπ � dHam(R, R′) �� A �2 + ρ V � dHam(R, R′) �� A �� (c) = ρ Eν,π   � T � t=1 tat �2  + ρ Eν,π � T � t=1 tat(1 − tat) � (d) = ρ � Eν,π � T � t=1 tat ��2 + ρ Eν,π � T � t=1 tat(1 − tat) � + ρ Vν,π � T � t=1 tat � , where we obtain (a): using the towering property of the expectation (b) and (d): by definition of the variance (c): using that dHam(R, R′) = �T t=1 1 {Rt ̸= R′ t} where 1 {Rt ̸= R′ t} | At ∼ Bernoulli(tat) by the definition of the maximal coupling and the sum is iid given A. Finally, plugging the upper bound in Inequality (16) concludes the proof. 27Theorem 3 (Minimax lower bounds for linear bandits). Let A = [−1, 1]d and Θ = Rd. Then, for any ρ-global zCDP policy, we have that Regminimax T (A, Θ) ≥ max          exp(−2) 8 d √ T � �� � without ρ-global zCDP , exp(−2.25) 4 d √ρ � �� � with ρ-global zCDP          . Proof. For the non-private lower bound, Theorem 24.1 of (Lattimore and Szepesvári, 2020) gives that, Regminimax T (A, Θ) ≥ exp(−2)d 8 √ T. Now, we focus on proving the ρ-global zCDP part of the lower bound. Let Θ = � − 1 T √ρ, 1 T √ρ �d . For θ, θ′ ∈ Θ, let ν and ν′ be the bandit instances corresponding resp. to θ and θ′. We denote Mθ = Mν,π and Mθ′ = Mν′,π. Let Eθ and Eθ′ the expectations under Mθ and Mθ′ respectively. Step 1: From lower bounding regret to upper bounding KL-divergence We begin with RegT (A, θ) = Eθ � T � t=1 d � i=1 (sign (θi) − Ati) θi � ≥ 1 T√ρ d � i=1 Eθ � T � t=1 I {sign (Ati) ̸= sign (θi)} � ≥ 1 √ρ d � i=1 Mθ � T � t=1 I {sign (Ati) ̸= sign (θi)} ≥ T/2 � In this derivation, the first equality holds because the optimal action satisfies a∗ i = sign (θi) for i ∈ [d]. The first inequality follows from an observation that (sign (θi) − Ati) θi ≥ |θi| I {sign (Ati) ̸= sign (θi)}. The last inequality is a direct application of Markov’s inequality 11. For i ∈ [d] and θ ∈ Θ, we define pθ,i ≜ Mθ � T � t=1 I {sign (Ati) ̸= sign (θi)} ≥ T/2 � . Now, let i ∈ [d] and θ ∈ Θ be fixed. Also, let θ′ j = θj for j ̸= i and θ′ i = −θi. Then, by the Bretagnolle-Huber inequality, pθ,i + pθ′,i ≥ 1 2 exp (−KL (Mθ ∥ Mθ′)) . Step 2: KL-divergence decomposition with ρ-global zCDP. Define pt ≜ TV (N (⟨At, θ⟩ , 1) ∥ N (⟨At, θ′⟩ , 1)). From Lemma 6, we obtain that KL (Mθ ∥ Mθ′) ≤ ρ � Eνπ � T � t=1 pt ��2 + ρ � Eνπ � T � t=1 pt �� + ρ Vν,π � T � t=1 pt � On the other hand, using Pinsker’s inequality (Lemma 13), we have that T � t=1 pt ≤ T � t=1 � 1 2KL (N (⟨At, θ⟩ , 1) ∥ N (⟨At, θ′⟩ , 1)) 28≤ T � t=1 � 1 4 � ⟨At, θ − θ′⟩2� ≤ 1 2 � T � t=1 |⟨At, θ − θ′⟩| � ≤ 1 2 � T � t=1 |At,i| (2 |θi|) � ≤ 1 2 � T × 2 1 T√ρ � = 1 √ρ. The last inequality holds true because At ∈ [−1, 1]d and θ, θ′ ∈ � − 1 T √ρ, 1 T √ρ �d . This gives that Eνπ � T � t=1 pt � ≤ 1 √ρ and Vνπ � T � t=1 pt � ≤ 1 4ρ Plugging back in the KL decomposition, we get that, KL (Mθ ∥ Mθ′) ≤ ρ � 1 √ρ �2 + ρ � 1 √ρ � + ρ � 1 4ρ � = 1 + √ρ + 1 4 ≤ 9 4 where the last inequality is due to ρ ≤ 1. Step 3: Choosing the ‘hard-to-distinguish’ θ. Now, we have that pθ,i + pθ′,i ≥ 1 2 exp (−9/4) Now, we apply an ‘averaging hammer’ over all θ ∈ Θ, such that |Θ| = 2d, to obtain � θ∈Θ 1 |Θ| d � i=1 pθ,i = 1 |Θ| d � i=1 � θ∈Θ pθ,i ≥ d 4 exp(−9 4). This implies that there exists a θ ∈ Θ such that �d i=1 pθ,i ≥ d exp(− 9 4)/4. Step 4: Plugging back θ in the regret decomposition. With this choice of θ, we conclude that RegT (A, θ) ≥ 1 √ρ d � i=1 pθ,i ≥ exp(− 9 4) 4 d √ρ 29D Privacy proofs In this section, we give a complete proof of the privacy of both AdaC-UCB and AdaC-GOPE. Both algorithms share the same blueprint. We first formalise the intuition behind the blueprint in Lemma 7, then give a generic proof of privacy and specify the minor differences to complete the proofs in the last section. D.1 The privacy lemma of non-overlapping sequences Remark 3. The Privacy Lemma shows that when the mechanism M is applied to non-overlapping subsets of the input dataset, there is no need to use the composition theorems. Plus, there is no additional cost in the privacy budget. Lemma 7 (Privacy Lemma). Let M be a mechanism that takes a set as input. Let ℓ < T and t1, . . . tℓ, tℓ+1 be in [1, T] such that 1 = t1 < · · · < tℓ < tℓ+1 − 1 = T. Let’s define the following mechanism G : {x1, . . . , xT } → ℓ � i=1 M{xti,...,xti+1−1} (17) In other words, G is the mechanism we get by applying M to the partition of the input dataset {x1, . . . , xT } according to t1 < · · · < tℓ < tℓ+1, i.e.     x1 x2 ... xT     G→    o1 ... oℓ    where oi ∼ M{xti,...,xti+1−1}. We have that (a) If M is (ϵ, δ)-DP then G is (ϵ, δ)-DP (b) If M is (α, ϵ)-RDP then G is (α, ϵ)-RDP (c) If M is (ξ, ρ)-zCDP then G is (ξ, ρ)-zCDP Proof. Let x ≜ {x1, . . . , xT } and x′ ≜ {x′ 1, . . . , x′ T } be two neighboring datasets. This implies that ∃j ∈ [1, T] such that xj ̸= x′ j and ∀t ̸= j, xt = x′ t. Let ℓ′ be such that tℓ′ ≤ j ≤ tℓ′+1 − 1. (a) Suppose M is (ϵ, δ)-DP. For every output event E = E1 × · · · × Eℓ, we have that Gx(E) = ℓ � i=1 M{xti,...,xti+1−1}(Ei) = M{xtℓ′ ,...,xtℓ′+1−1}(Eℓ′) ℓ � i=1,i̸=ℓ′ M{xti,...,xti+1−1}(Ei) ≤ � eϵM{x′ tℓ′ ,...,x′ tℓ′+1−1}(Eℓ′) + δ � ℓ � i=1,i̸=ℓ′ M{xti,...,xti+1−1}(Ei) = eϵGx′(E) + δ × ℓ � i=1,i̸=ℓ′ M{xti,...,xti+1−1}(Ei) 30≤ eϵGx′(E) + δ since �ℓ i=1,i̸=ℓ′ M{xti,...,xti+1−1}(Ei) ≤ 1 Which gives that G is (ϵ, δ)-DP. (b) M is (α, ϵ)-RDP. We have that Dα(Gx∥Gx′) = 1 α − 1 log �� o=(o1,...,oℓ) Gx′(o) � Gx(o) Gx′(o) �α� Since Gx(o) = ℓ � i=1 M{xti,...,xti+1−1}(oi) and Gx′(o) = ℓ � i=1 M{x′ ti,...,x′ ti+1−1}(oi) we get Gx(o) Gx′(o) = M{xtℓ′ ,...,xtj ,...,xtℓ′+1−1}(oi) M{xtℓ′ ,...,x′ tj ,...,xtℓ′+1−1}(oi) Thus, Dα(Gx∥Gx′) = Dα(M{xtℓ′ ,...,xtj ,...,xtℓ′+1−1}∥M{xtℓ′ ,...,x′ tj ,...,xtℓ′+1−1}) ≤ ϵ Which gives that G is (α, ϵ)-RDP. (c) M is (ξ, ρ)-zCDP. Similarly, we have that Dα(Gx∥Gx′) = Dα(M{xtℓ′ ,...,xtj ,...,xtℓ′+1−1}∥M{xtℓ′ ,...,x′ tj ,...,xtℓ′+1−1}) ≤ ξ + ρα . Thus, G is (ξ, ρ)-zCDP. For each of the three algorithms proposed, the final actions can be seen as a post-processing of some private quantity of interest (empirical means for AdaC-UCB or the parameter ˆθ for linear and contextual bandits). However, we cannot directly conclude the privacy of the proposed algorithms using just a post-processing argument and Lemma 7. This is because the steps corresponding to the start of an episode in the algorithms t1 < · · · < tℓ < tℓ+1 are adaptive and depend on the dataset itself, while for Lemma 7, those have been fixed before. To deal with the adaptive episode, we propose a generic privacy proof. D.2 Generic privacy proof of AdaC-UCB and AdaC-GOPE In this section, we give one generic proof that works for the two proposed algorithms. First, we give a summary of the intuition of the proof for dealing with adaptive episodes. By fixing two neighbouring tables of rewards d and d′ that only differ at some user uj, and a deterministic adversary B, we have that • the view of the adversary B from the beginning of the interaction until step j will be the same • the adaptive episodes generated by the policy in the first j steps will be the same, which means that step j will fall in the same episode in the view of B when interacting with π(d) or π(d′) • for these fixed similar episodes, we use the privacy Lemma 7 31• the view of B from step j + 1 until T will be private by post-processing Let d = {d1, . . . , dT } and d′ = {d′ 1, . . . , d′ T } two neighbouring reward tables in (RK)T . Let j ∈ [1, T] such that, for all t ̸= j, dt = d′ t. Let B be a deterministic adversary. We want to show that Dα(View(B ↔ π(d))∥View(B ↔ π(d′))) ≤ αρ. Step 1. Sequential decomposition of the view of the adversary B We observe that due to the sequential nature of the interaction, the view of B can be decomposed to a part that depends on d<j ≜ {d1, . . . , dj−1}, which is identical for both d and d′ and a second conditional part on the history. First, let us denote View(B ↔ π(d)) ≜ PB,π d . We have that, for every sequence of actions o ≜ (o1, . . . , oT ) ∈ [K]T PB,π d (o) = T � t=1 πt � ot | B(o1), d1,B(o1), . . . , B(o1, . . . , ot−1), dt−1,B(o1,...,ot−1) � ≜ PB,π d<j (o≤j)PB,π d (o>j | o≤j) where • o≤j ≜ (o1, . . . , oj) and o>j ≜ (oj+1, . . . , oT ) • PB,π d<j (o≤j) ≜ �j t=1 πt � ot | B(o1), d1,B(o1), . . . , B(o1, . . . , ot−1), dt−1,B(o1,...,ot−1) � • PB,π d (o>j | o≤j) ≜ �T t=j+1 πt � ot | B(o1), d1,B(o1), . . . , B(o1, . . . , ot−1), dt−1,B(o1,...,ot−1) � Similarly PB,π d′ (o) = PB,π d<j (o≤j)PB,π d′ (o>j | o≤j) since d′ <j = d<j. Step 2. Decomposing the Rényi divergence. We have that e(α−1)Dα(PB,π d ∥PB,π d′ ) = � o∈[K]T PB,π d′ (o) � PB,π d (o) PB,π d′ (o) �α = � o∈[K]T PB,π d′ (o) � PB,π d (o>j | o≤j) PB,π d′ (o>j | o≤j) �α = � o≤j∈[K]j PB,π d<j (o≤j) � o>j∈[K]T −j PB,π d′ (o>j | o≤j) � PB,π d (o>j | o≤j) PB,π d′ (o>j | o≤j) �α = � o≤j∈[K]j PB,π d<j (o≤j)e(α−1)Dα(PB,π d (.|o≤j)∥PB,π d′ (.|o≤j)) = Eo≤j∼PB,π d<j � e(α−1)Dα(PB,π d (.|o≤j)∥PB,π d′ (.|o≤j))� Step 3. The adaptive episodes are the same, before step j. Let ℓ such that tℓ ≤ j < tℓ+1 in the view of B when interacting with d. Let us call it ψπ d (j) ≜ ℓ. Similarly, let ℓ′ such that tℓ′ ≤ j < tℓ′+1 in the view of B when interacting with d. Let us call it ψπ d′(j) ≜ ℓ′. Since ψπ d (j) only depends on d<j, which is identical for d and d′, we have that ψπ d (j) = ψπ d′(j) with probability 1. 32We call ξj the last time-step of the episode ψπ d (j), i.e ξj ≜ tψπ d (j)+1 − 1. Step 4. Private sufficient statistics. Fix o≤j. Let rs ≜ ds,B(o1,...,os), for s ∈ [1, j], be the reward corresponding to the action chosen by B in the table d. Similarly, r′ s ≜ d′ s,B(o1,...,os) for d′. Let us define Lj ≜ G{r1,...,rξj } and L′ j ≜ G{r′ 1,...,r′ ξj }, where G is defined as in Eq. 17, using the same episodes for d and d′. The underlying mechanism M, used to define G, will be specified for each algorithm in Section D.2.1. In addition, the specified mechanism M will verify ρ-zCDP with respect to its set input. Using the structure of the policy π, there exists a randomised mapping fdξj +1,...,dT such that PB,π d (. | o≤j) = fdξj +1,...,dT (Lj) and PB,π d′ (. | o≤j) = fdξj +1,...,dT (L′ j). In other words, the view of the adversary B from step ξj + 1 until T only depends on the sufficient statistics Lj and the new inputs dξj+1, . . . , dT , which are the same for d and d′. For example, the sufficient statistics are the private mean estimate of the active arm in each episode for AdaC-UCB and the noisy parameter estimate ˆθ for AdaC-GOPE. Step 5. Concluding with Lemma 7 and post-processing. Using Lemma 7, we have that Dα(Lj, L′ j) ≤ αρ Using the post-processing property of Dα (Lemma 10), we get that Dα(PB,π d (. | o≤j)∥PB,π d′ (. | o≤j)) = Dα(fdξj +1,...,dT (Lj)∥fdξj +1,...,dT (L′ j)) ≤ Dα(Lj, L′ j) ≤ αρ Finally, we conclude by taking the expectation with respect to o≤j ∼ PB,π d<j e(α−1)Dα(PB,π d ∥PB,π d′ ) = Eo≤j∼PB,π d<j � e(α−1)Dα(PB,π d (.|o≤j)∥PB,π d′ (.|o≤j))� ≤ e(α−1)αρ Thus, we conclude Dα(PB,π d ∥PB,π d′ ) ≤ αρ Remark 4. The same proof could be adapted to (α, ϵ)-RDP, by just showing that the Rényi divergence is smaller than ϵ rather than αρ. For (ϵ, δ)-DP, the same proof follows by changing Dα to the Hockey- Stick Divergence i.e Dfϵ(P, Q) = E[fϵ( dP dQ)] where fϵ = max(t − eϵ, 0). Otherwise, just rewriting the proof using the probability of events is straightforward too. D.2.1 Instantiating the specifics of privacy proof for each algorithm In this section, we instantiate Step 4 of the generic proof for each algorithm, by specifying the mechanism G and M in the proof and showing that they are ρ-zCDP. • For AdaC-UCB, the mechanism M is the private empirical mean statistic, i.e M{r1,...,rt} ≜ 1 t �t s=1 rs + N � 0, 1 2ρt2 � . Since rewards are in [0, 1], by the Gaussian Mechanism (i.e. Theorem 14) M is ρ-DP. • For AdaC-GOPE, the mechanism M is a private estimate of the linear parameter θ, i.e M{rtℓ,...,rtℓ+1−1} ≜ V −1 ℓ ��tℓ+1−1 t=tℓ asrs � + V − 1 ℓ 2 Nℓ where Vℓ = � a∈Sℓ Tℓ(a)aa⊤, Nℓ ∼ N � 0, 2 ρg2 ℓId � and gℓ = maxb∈Aℓ ∥b∥V −1 ℓ . 33To show that M is ρ-zCDP, we rewrite ˆθℓ = V −1 ℓ ��tℓ+1−1 t=tℓ asrs � = V − 1 ℓ 2 ϕℓ where ϕℓ ≜ V − 1 ℓ 2 ��tℓ+1−1 t=tℓ asrs � . Let {rs}tℓ+1−1 s=tℓ and {r′ s}tℓ+1−1 s=tℓ two neighbouring sequence of rewards that differ at only step j ∈ [tℓ, tℓ+1 − 1]. We have that ∥ϕℓ − ϕ′ ℓ∥2 = ∥V − 1 ℓ 2 [aj(rs − r′ s)] ∥2 ≤ 2∥V − 1 ℓ 2 aj∥2 ≤ 2gℓ since rj, r′ j ∈ [−1, 1]. Using the Gaussian Mechanism (i.e. Theorem 14), this means that ϕℓ + Nℓ is ρ-zCDP and M is too by post-processing. 34E Stochastic bandits with global zCDP E.1 Concentration inequalities Lemma 8. Assume that (Xi)1≤i≤n are iid random variables in [0, 1], with E(Xi) = µ. Then, for any δ ≥ 0, P � ˆµn + Zn − �� 1 2n + 1 ρn2 � log �1 δ � ≥ µ � ≤ δ, (18) and P � ˆµn + Zn + �� 1 2n + 1 ρn2 � log �1 δ � ≤ µ � ≤ δ, (19) where ˆµn = 1 n �n t=1 Xt and Zn ∼ N � 0, 1 2ρn2 � . Proof. Let Y = (ˆµn + Zn − µ). Using Properties 2 and 3 of Lemma 15, we get that Y is � 1 4n + 1 2ρn2 -subgaussian. We conclude using the concentration on subgaussian random variables, i.e. Lemma 14. E.2 Regret analysis Theorem 5 (Part a: Problem-dependent regret). For rewards in [0, 1] and β > 3, AdaC-UCB yields a regret upper bound of � a:∆a>0 � 8β ∆a log(T) + 8 � β ρ � log(T) + 2β β − 3 � . Proof. By the generic regret decomposition of Theorem 11 in Azize and Basu (2022), for every suboptimal arm a, we have that E[Na(T)] ≤ 2ℓ+1 + P � Gc a,ℓ,T � T + β β − 3, where Ga,ℓ,T = � ˆµa,2ℓ + Zℓ + �� 1 2 × 2ℓ + 1 ρ × (2ℓ)2 � β log(T) < µ1 � . such that Zℓ ∼ N � 0, 1 2ρ×(2ℓ)2 � Step 1: Choosing an ℓ. Now, we observe that P(Gc a,ℓ,T ) = P � ˆµa,2ℓ + Zℓ + �� 1 2 × 2ℓ + 1 ρ × (2ℓ)2 � β log(T) ≥ µ1 � = P � ˆµa,2ℓ + Zℓ − �� 1 2 × 2ℓ + 1 ρ × (2ℓ)2 � β log(T) ≥ µa + ϵ � for ϵ = � ∆a − 2 �� 1 2×2ℓ + 1 ρ×(2ℓ)2 � β log(T) � . The idea is to choose ℓ big enough so that ϵ ≥ 0. Let us consider the contrary, i.e. ϵ < 0 ⇒ 2ℓ < 2β log(T) ∆2a � 1 + ∆a � 1 ρβ log(T) � 35⇒ 2ℓ < 2β ∆2a log(T) + 2 � β ρ∆2a � log(T) (20) Thus, by choosing ℓ = � 1 log(2) log � 2β ∆2a log(T) + 2 � β ρ∆2a � log(T) �� we ensure ϵ > 0. This also implies that P(Gc a,ℓ,T ) ≤ P � ˆµa,2ℓ + Zℓ − �� 1 2 × 2ℓ + 1 ρ × (2ℓ)2 � β log(T) ≥ µa � ≤ 1 T β The last inequality is due to Equation 18 of Lemma 8. Step 2: The regret bound. Combining Steps 1 and 2, we get that E[Na(T)] ≤ β β − 3 + 2ℓ+1 + T × 1 T β ≤ 8β ∆2a log(T) + 8 � β ρ∆2a � log(T) + 2β β − 3. (21) Plugging this upper bound back in the definition of problem-dependent regret RegT (AdaC-UCB, ν) ≤ � a:∆a>0 � 8β ∆a log(T) + 8 � β ρ � log(T) + 2β β − 3 � . Theorem 5 (Part b: Minimax regret). For rewards in [0, 1] and β > 3, AdaC-UCB yields a regret upper bound of O �� KT log(T) � + O � K �1 ρ log(T) � . Proof. Let ∆ be a value to be tuned later. We observe that RegT (AdaP-UCB, ν) = � a ∆aE[Na(T)] = � a:∆a≤∆ ∆aE[Na(T) + � a:∆a>∆ ∆aE[Na(T)] ≤ T∆ + � a:∆a>∆ ∆a � 8β ∆2a log(T) + 8 � β ρ∆2a � log(T) + 2β β − 3 � (Eq. 21) ≤ T∆ + 8βK log(T) ∆ + 8K � β log(T) ρ + 3β β − 3 � a ∆a ≤ 4 � 2βKT log(T) + 8K � β log(T) ρ + 3β β − 3 � a ∆a . Here, the last step is tuning ∆ = � 8βK log(T ) T . 36E.3 Extensions to (ϵ, δ)-global DP and (α, ϵ)-global RDP In this section, we specify the modifications required to make AdaC-UCB (ϵ, δ)-global DP and (α, ϵ)-global RDP. Also, we give the corresponding regret upper bounds. The difference comes from the different calibrations of the Gaussian Mechanism (Thm 14). Adapting the analysis from ρ-zCDP reduces to changing the 1 2ρ factor to 2 ϵ2 log( 1.25 δ ) for (ϵ, δ)-DP and to α 2ϵ for (α, ϵ)-RDP, i.e. varying the constant b in Theorem 14. (ϵ, δ)-global DP. The private index to select the arms (Line 6 of Algorithm 2) becomes Iρ a(tℓ − 1, β) ≜ ˆµℓ a + N � 0, σ2 a,ℓ � + Ba(tℓ − 1, β). where σ2 a,ℓ ≜ 2 log( 1.25 δ ) ϵ2×( 1 2 Na(tℓ−1)) 2 , and the exploration bonus is Ba(tℓ − 1, β) ≜ � � � � � 1 2 × 1 2Na(tℓ − 1) + 4 log( 1.25 δ ) ϵ2 × � 1 2Na(tℓ − 1) �2 � β log(tℓ) . Thus, the regret upper bounds become: Problem-dependent: � a:∆a>0 � 8β ∆a log(T) + 8 � 4 βϵ2 log �1.25 δ �� log(T) + 2β β − 3 � . Problem-independent: O �� KT log(T) � + O  K � log � 1 δ � ϵ � log(T)   . (α, ϵ)-global RDP. The private index to select the arms (Line 6 of Algorithm 2) becomes Iρ a(tℓ − 1, β) ≜ ˆµℓ a + N � 0, σ2 a,ℓ � + Ba(tℓ − 1, β). where σ2 a,ℓ ≜ α 2ϵ×( 1 2 Na(tℓ−1)) 2 , and the exploration bonus is Ba(tℓ − 1, β) ≜ � � � � � 1 2 × 1 2Na(tℓ − 1) + α ϵ × � 1 2Na(tℓ − 1) �2 � β log(tℓ) . The regret upper bounds become: Problem-dependent: � a:∆a>0 � 8β ∆a log(T) + 8 � βα ϵ � log(T) + 2β β − 3 � . Problem-independent: O �� KT log(T) � + O � K �α ϵ log(T) � . 37F Linear Bandits with global zCDP F.1 Basic definitions of optimal design Definition 8 (Optimal design). Let A ⊂ Rd and π : A → [0, 1] be a distribution on A so that � a∈A π(a) = 1. Let V (π) ∈ Rd×d and f(π), g(π) ∈ R be given by V (π) = � a∈A π(a)aaT , f(π) = log det V (π), g(π) = max a∈A ∥a∥V (π)−1 • π is called a design • The set Supp (π) ≜ {a ∈ π : π(a) ̸= 0} is called the core set of A • A design that maximises f is known as a D-optimal design • A design that minimises g is known as G-optimal design Theorem 11 (Kiefer–Wolfowitz theorem). Assume that A is compact and span(A) = Rd. The following are equivalent: • π⋆ is a minimiser of g. • π⋆ is a maximiser of f. • g(π⋆) = d Furthermore, there exists a minimiser π⋆ of g such that |Supp (π)| ≤ d(d+1) 2 F.2 Concentration inequalities Let a1, . . . , at be deterministically chosen without the knowledge of r1, . . . , rt. Let π be an optimal design for A. Let Vt ≜ �t s=1 asaT s = � a∈A Na(t)aaT be the design matrix, ˆθt = V −1 t �t s=1 asrs be the least square estimate and ˜θt = ˆθt + V − 1 t 2 Nt where Nt ∼ N � 0, 2 ρg2 t Id � , where gt ≜ maxb∈A ∥b∥V −1 t . Theorem 12. Let δ ∈ [0, 1] and βt ≜ gt � 2 log � 4 δ � + g2 t � 2 ρ � d + 2 � d log � 2 δ � + 2 log � 2 δ �� . For every a ∈ A, we have that P ���� � ˜θt − θ⋆, a ���� ≥ βt � ≤ δ. Proof. For every a ∈ A � ˜θt − θ⋆, a � = � ˆθt − θ⋆, a � + aT V − 1 t 2 Nt = � ˆθt − θ⋆, a � + Zt where Zt ≜ aT V − 1 t 2 Nt. Step 1: Concentration of the least square estimate. Using Eq.(20.2) from Chapter 20 of Lattimore and Szepesvári (2020), we have that P ���� � ˆθt − θ⋆, a ���� ≥ gt � 2 log �4 δ �� ≤ δ 2 Step 2: Concentration of the injected Gaussian noise. On the other hand, using Cauchy-Schwartz, we have that |Zt| = ���aT V − 1 t 2 Nt ��� ≤ ∥V − 1 t 2 a∥.∥Nt∥ ≤ gt∥Nt∥ 38using that ∥V − 1 t 2 a∥ = ∥a∥V −1 t ≤ gt. Here, Nt = � 2 ρgtN(0, Id). Thus, using Lemma 16, we get P   |Zt| ≥ g2 t � � � �2 ρ � d + 2 � d log �2 δ � + 2 log �2 δ ��   ≤ δ 2 Steps 1 and 2 together conclude the proof. Corollary 3. Let β be a confidence level. If each action a ∈ A is chosen for Na(t) ≜ ctπ(a) where ct ≜   8d β2 log �4 δ � + 2d β � � � �2 ρ � d + 2 � d log �2 δ � + 2 log �2 δ ��  then, for t = � a∈Supp(π) Na(t), we get that P ���� � ˜θt − θ⋆, a ���� ≥ β � ≤ δ . Proof. We have that Vt = � a∈Supp(π) Na(t)aaT ≥ ctV (π) This means g2 t = max b∈A ∥b∥2 V −1 t ≤ 1 ct max b∈A ∥b∥2 V (π)−1 = g(π) ct = d ct , where the last equality is because π is an optimal design for A. Recall that βt ≜ gt � 2 log � 4 δ � + g2 t � 2 ρ � d + 2 � d log � 2 δ � + 2 log � 2 δ �� . Thus, βt ≤ � d ct � 2 log �4 δ � + d ct � � � �2 ρ � d + 2 � d log �2 δ � + 2 log �2 δ �� ≤ � 2d log � 4 δ � � 8d β2 log � 4 δ � + d � 2 ρ � d + 2 � d log � 2 δ � + 2 log � 2 δ �� 2d β � 2 ρ � d + 2 � d log � 2 δ � + 2 log � 2 δ �� = β 2 + β 2 = β The final inequality is due to ct ≥ 8d β2 log � 4 δ � , and ct ≥ 2d β � 2 ρ � d + 2 � d log � 2 δ � + 2 log � 2 δ �� . We conclude the proof using Theorem 12. F.3 Regret analysis Theorem 13. Under Assumption 1 and for δ ∈ (0, 1), with probability at least 1 − δ, the regret RT of AdaC-GOPE (Algorithm 3) is upper-bounded by A � dT log �K log(T) δ � + Bd √ρ � log �K log(T) δ � log(T) 39where A and B are universal constants. If δ = 1 T , then E(RT ) ≤ O �� dT log(KT) � + O �� 1 ρd(log(KT)) 3 2 � Proof. Step 1: Defining the good event E. Let E ≜ ∞ � ℓ=1 � a∈Aℓ ���� � ˜θℓ − θ∗, a ���� ≤ βℓ � . Using Corollary 3, we get that P(¬E) ≤ ∞ � ℓ=1 � a∈Aℓ P ���� � ˜θℓ − θ∗, a ���� > βℓ � ≤ ∞ � ℓ=1 � a∈Aℓ δ kℓ(ℓ + 1) ≤ δ Step 2: Good properties under E. We have that under E • The optimal arm a⋆ ∈ arg maxa∈A ⟨θ∗, a⟩ is never eliminated. Proof. for every episode ℓ and b ∈ Aℓ, we have that under E, � ˜θℓ, b − a⋆� = � ˜θℓ − θ⋆, b − a⋆� + ⟨θ⋆, b − a⋆⟩ ≤ � ˜θℓ − θ⋆, b − a⋆� ≤ ��� � ˜θℓ − θ∗, a⋆���� + ��� � ˜θℓ − θ∗, b ���� ≤ 2βℓ where the first inequality is because ⟨θ⋆, b − a⋆⟩ ≤ 0 by definition of the optimal arm a⋆. This means that a⋆ is never eliminated. • Each sub-optimal arm a will be removed after ℓa rounds where ℓa ≜ min{ℓ : 4βℓ < ∆a}. Proof. We have that under E, � ˜θℓa, a⋆ − a � ≥ ⟨θ⋆, a⋆⟩ − βℓa − ⟨θ⋆, a⟩ − βℓa = ∆a − 2βℓa > 2βℓa which means that a get eliminated at the round ℓa. • for a ∈ Aℓ+1, we have that ∆a ≤ 4βℓ. Proof. If ∆a > 4βℓ, then by the definition of ℓa, ℓ ≥ ℓa and arm a is already eliminated, i.e. a /∈ Aℓ+1 Step 3: Regret decomposition under E. Fix ∆ to be optimised later. Under E, each sub-optimal action a such that ∆a > ∆ will only be played for the first ℓ∆ rounds where ℓ∆ ≜ min{ℓ : 4βℓ < ∆} = � log2 � 4 ∆ �� We have that RT = � a∈A ∆aNa(T) 40= � a:∆a>∆ ∆aNa(T) + � a:∆a≤∆ ∆aNa(T) = ℓ∆∧ℓ(T ) � ℓ=1 � a∈Aℓ ∆aTℓ(a) + T∆ ≤ ℓ∆∧ℓ(T ) � ℓ=1 4βℓ−1Tℓ + T∆ where the last inequality is thanks to the third bullet point in Step 2, i.e. ∆a ≤ 4βℓ−1 for a ∈ Aℓ. Also ℓ(T) is the total number of episodes played until timestep T. Step 4: Upper-bounding Tℓ and ℓ(T) under E. We have that Tℓ = � a∈Sℓ Tℓ(a) = � a∈Sℓ   8dπℓ(a) β2 ℓ log �4kℓ(ℓ + 1) δ � + 2dπℓ(a) βℓ � � � �2 ρ � d + 2 � d log �2kℓ(ℓ + 1) δ � + 2 log �2kℓ(ℓ + 1) δ ��  ≤ d(d + 1) 2 + 8d β2 ℓ log �4kℓ(ℓ + 1) δ � + 2d βℓ � � � �2 ρ � d + 2 � d log �2kℓ(ℓ + 1) δ � + 2 log �2kℓ(ℓ + 1) δ �� . since βℓ+1 = 1 2βℓ and �ℓ(T ) ℓ=1 Tℓ = T, there exists a constant C such that ℓ(T) ≤ C log(T). In other words, the length of the episodes is at least doubling so their number is logarithmic. Which means that, for ℓ ≤ ℓ(T), there exists a constant C′ such that log �4kℓ(ℓ + 1) δ � ≤ C′ log �k log(T) δ � hence Tℓ ≤ d(d + 1) 2 + 8d β2 ℓ C′ log �k log(T) δ � + 4d βℓ � 1 ρC′ log �k log(T) δ � Step 5: Upper-bounding regret under E. Under E ℓ∆∧ℓ(T ) � ℓ=1 4βℓ−1Tℓ ≤ ℓ∆∧ℓ(T ) � ℓ=1 8βℓ � d(d + 1) 2 + 8d β2 ℓ C′ log �k log(T) δ � + 4d βℓ � 1 ρC′ log �k log(T) δ �� ≤ 4d(d + 1) + 64dC′ log �k log(T) δ � � ℓ∆ � ℓ=1 2ℓ � + 32d � 1 ρC′ log �k log(T) δ � ℓ(T) ≤ 4d(d + 1) + 16dC′ log �k log(T) δ � �16 ∆ � + 32d � 1 ρC′ log �k log(T) δ � ℓ(T) ≤ 4d(d + 1) + C1d log �k log(T) δ � 1 ∆ + C2d � 1 ρ log �k log(T) δ � log(T) All in all, we have that RT ≤ 4d(d + 1) + C2d � 1 ρ log �k log(T) δ � log(T) + C1d log �k log(T) δ � 1 ∆ + T∆ 41Step 6: Optimizing for ∆. Taking ∆ = � C1d T log � k log(T ) δ � , we get that RT ≤ A � dT log �k log(T) δ � + Bd � 1 ρ log �k log(T) δ � log(T) Step 7: Upper-bounding the expected regret. For δ = 1 T , we get that E(RT ) ≤ (1 − δ)RT (δ) + δT ≤ RT (δ) + 1 ≤ C′ 1 � dT log(kT) + C′ 2 �1 ρd log(kT) 3 2 F.4 Extensions to (ϵ, δ)-global DP and (α, ϵ)-global RDP In this section, we specify the modifications required to make AdaC-GOPE (ϵ, δ)-global DP and (α, ϵ)-global RDP, and provide the corresponding regret upper bounds. The difference comes from the different calibrations of the Gaussian Mechanism (Thm 14). Adapting the analysis from ρ-zCDP reduces to changing the 1 2ρ factor to 2 ϵ2 log( 1.25 δ ) for (ϵ, δ)-DP and to α 2ϵ for (α, ϵ)-RDP, i.e. varying the constant b in Theorem 14. (ϵ, δ)-global DP. The number of times each action a is played at episode ℓ for AdaC-GOPE is Tℓ(a) ≜ cℓπℓ(a) times, where for δ′ ≜ δ Kℓ(ℓ+1), cℓ ≜ 8d β2 ℓ log � 4 δ′ � + 2d βℓ � 8 ϵ2 log(1.25 δ ) � d + 2 � d log � 2 δ′ � + 2 log � 2 δ′ ��1/2 The added Gaussian noise in Step 4 of AdaC-GOPE becomes Nℓ ∼ N � 0, 8d ϵ2cℓ log( 1.25 δ )Id � . Thus, the regret upper-bound becomes O �� dT log(KT) � + O �� 1 ϵ2 log �1.25 δ � d(log(KT)) 3 2 � (α, ϵ)-global RDP. The number of times each action a is played at episode ℓ for AdaC-GOPE is Tℓ(a) ≜ cℓπℓ(a) times, where for δ′ ≜ δ Kℓ(ℓ+1), cℓ ≜ 8d β2 ℓ log � 4 δ′ � + 2d βℓ � 2α ϵ � d + 2 � d log � 2 δ′ � + 2 log � 2 δ′ ��1/2 The added Gaussian noise in Step 4 of AdaC-GOPE becomes Nℓ ∼ N � 0, 2dα ϵcℓ Id � . Thus, the regret upper-bound becomes O �� dT log(KT) � + O ��α ϵ d(log(KT)) 3 2 � . 42F.5 Adding noise at different steps of AdaC-GOPE In order to make the GOPE algorithm differentially private, the main task is to derive a private estimate of the linear parameter θ at each phase ℓ, i.e. ˆθℓ. If the estimate is private with respect to the samples used to compute it, i.e. ˆθℓ = V −1 ℓ ��tℓ+1−1 t=tℓ asrs � w.r.t {rs}tℓ+1−1 s=tℓ , then due to forgetting and post-processing, the algorithm turns private too. We discuss three different ways to make the empirical estimate ˆθℓ private. 1. Adding noise in the end. A first attempt would be to analyse the L2 sensitivity of ˆθℓ directly, and adding Gaussian noise calibrated by the L2 sensitivity of ˆθℓ. Let {rs}tℓ+1−1 s=tℓ and {r′ s}tℓ+1−1 s=tℓ two neighbouring sequence of rewards that differ at only step j ∈ [tℓ, tℓ+1 − 1]. Then, we have that ∥ˆθℓ − ˆθ′ℓ∥2 = ∥V −1 ℓ [aj(rs − r′ s)] ∥2 ≤ 2∥V −1 ℓ aj∥2 since rj, r′ j ∈ [−1, 1]. However, it is hard to control the quantity ∥V −1 ℓ aj∥2 without additional assumptions. The G-optimal design permits only to control another related quantity, i.e. ∥aj∥V −1 ℓ = ∥V − 1 ℓ 2 aj∥2. Thus, it is better to add noise at a step before if one does not want to add further assumption. 2. Adding noise in the beginning. Since ˆθℓ = V −1 ℓ ��tℓ+1−1 t=tℓ asrs � , another way to make ˆθℓ private is by adding noise directly to the sum of observed rewards. Specifically, one can rewrite the sum tℓ+1−1 � t=tℓ asrs = � a∈Sℓ a � at=a,t∈[tℓ,tℓ+1−1] rt . Since rewards are in [−1, 1], the L2 sensitivity of � at=a,t∈[tℓ,tℓ+1−1] rt is 2. Thus, by Theorem 14, this means that the noisy sum of rewards � at=a,t∈[tℓ,tℓ+1−1] rt + N � 0, 2 ρ � is ρ-zCDP. Hence, by post-processing lemma, the corresponding noisy estimate ˆθℓ + V −1 ℓ �� a∈Sℓ aN � 0, 2 ρ �� is a ρ-zCDP estimate of ˆθℓ. This is exactly how both Hanna et al. (2022) and Li et al. (2022) derive a private version of GOPE for different privacy definitions, i.e. pure ϵ-DP for Hanna et al. (2022) and (ϵ, δ)-DP for Li et al. (2022), respectively. The drawback of this approach is that the variance of the noise depends on the size of the support Sℓ of the G-optimal design. To deal with this, both Hanna et al. (2022) and Li et al. (2022) solve a variant of the G-optimal design to get a solution where |Sℓ| ≤ 4d log log d + 16 rather than the full d(d + 1)/2 support of AdaC-GOPE’s optimal design. And still, the dependence on d in the private part of the regret achieved by both these algorithms are d2 in (Hanna et al., 2022, Eq (18)), and d 3 2 in (Li et al., 2022, Eq (56)), respectively. Thus, both of these existing algorithms do not achieve to the linear dependence on d in the regret term due to privacy, as suggested by the minimax lower bound. 3. Adding noise at an intermediate level. In contrast, AdaC-GOPE adds noise to the statistic ϕℓ = V − 1 2 ℓ �tℓ+1−1 � t=tℓ asrs � . ϕℓ is an intermediate quantity between the sum of rewards �tℓ+1−1 t=tℓ asrs, and the parameter ˆθℓ, whose L2 sensitivity can be controlled directly using the G-optimal Design. Due to this subtle observation, the private estimation ˜θℓ of AdaC-GOPE is independent of the size of the support Sℓ. 43Hence, the regret term of AdaC-GOPE due to privacy enjoys a linear dependence on d, as suggested by the minimax lower bound. Conclusion. In brief, to achieve the same DP guarantee with the same budget, one may arrive at it by adding noise at different steps, and the resulting algorithms may have different utilities. In general, adding noise at an intermediate level of computation (not directly to the input, i.e. local and not output perturbation) generally gives the best results. Remark 5. We also compare the empirical performance of AdaC-GOPE with a variant where the noise is added to the sum statistic i.e. ˜θℓ ≜ ˆθℓ+V −1 ℓ �� a∈Sℓ aN � 0, 2 ρ �� . The results are presented in Appendix H validating that AdaC-GOPE yields the lowest regret with respect to the other noise perturbation strategy. 44G Existing technical results and definitions In this section, we summarise the existing technical results and definitions required to establish our proofs. Lemma 9 (Post-processing Lemma (Proposition 2.1, (Dwork et al., 2014))). If a randomised algo- rithm A satisfies (ϵ, δ)-Differential Privacy and f is an arbitrary randomised mapping defined on A’s output, then f ◦ A satisfies (ϵ, δ)-DP. Theorem 14 (The Gaussian Mechanism ( Dwork et al. (2014), Mironov (2017), Bun and Steinke (2016))). Let f : X → Rd be a mechanism with L2 sensitivity s(f) ≜ max d∼d′∥f(d) − f(d′)∥2. Let g ≜ f + Z, such that Z ∼ N(0, b × s(f)2Id). Here, N(µ, Σ) denotes the Gaussian distribution with mean µ and co-variance matrix Σ, and ∥ · ∥2 denotes the L2 norm on Rd. Then, for b = 2 ϵ2 log( 1.25 δ ), α 2ϵ, 1 2ρ, g satisfies (ϵ, δ)-DP, (α, ϵ)-RDP and ρ-zCDP respectively. Lemma 10 (Post-processing property of Renyi Divergence, Lemma 2.2 Bun and Steinke (2016)). Let P and Q be distributions on Ω and let f : Ω → Θ be a function. Let f(P) and f(Q) denote the distributions on Θ induced by applying f to P and Q respectively. Then Dα(f(P)∥f(Q)) ≤ Dα(P∥Q). Lemma 11 (Markov’s Inequality). For any random variable X and ε > 0, P(|X| ≥ ε) ≤ E[|X|] ε . Definition 9 (Consistent Policies). A policy π is called consistent over a class of bandits E if for all ν ∈ E and p > 0, it holds that lim T →∞ RegT (π, ν) T p = 0. The class of consistent policies over E is denoted by Πcons (E). Lemma 12 (Bretagnolle-Huber inequality). Let P and Q be probability measures on the same measurable space (Ω, F), and let A ∈ F be an arbitrary event. Then, P(A) + Q (Ac) ≥ 1 2 exp(−D(P, Q)), where Ac = Ω\A is the complement of A. Lemma 13 (Pinsker’s Inequality). For two probability measures P and Q on the same probability space (Ω, F), we have KL (P ∥ Q) ≥ 2(TV (P ∥ Q))2. Definition 10 (Sub-Gaussianity). A random variable X is σ-subgaussian if for all λ ∈ R, it holds that E[exp(λX)] ≤ exp(λ2σ2/2) Lemma 14 (Concentration of Sub-Gaussian random variables). If X is σ-sub-Gaussian, then for any ϵ ≥ 0, P (X ≥ ϵ) ≤ exp � − ϵ2 2σ2 � Lemma 15 (Properties of Sub-Gaussian Random Variables). Suppose that X1 and X2 are indepen- dent and σ1 and σ2-sub-Gaussian, respectively, then 1. cX is |c| σ-sub-Gaussian for all c ∈ R. 2. X1 + X2 is � σ2 1 + σ2 2-sub-Gaussian. 3. If X has mean zero and X ∈ [a, b] almost surely, then X is b−a 2 -sub-Gaussian. Lemma 16 (Concentration of the χ2-Distribution, Claim 17 of Shariff and Sheffet (2018)). If X ∼ N(0, Id) and δ ∈ (0, 1), then P � ∥X∥2 ≥ d + 2 � d log �1 δ � + 2 log �1 δ �� ≤ δ Theorem 15 (Conditioning Increases f-divergence). Let PX PY |X −→ PY and PX QY |X −→ QY . Then, Df (PY ∥QY ) ≤ EX∼PX � Df � PY |X∥QY |X �� . 45H Extended experimental analysis In this section, we add an experimental comparison between AdaC-GOPE and a variant of AdaC-GOPE where the way of making the estimate ˆθℓ private is different (Section F.5). In AdaR- GOPE-Var, Step 4 changes to ˜θAdaR-GOPE-Var ℓ = ˆθℓ + V −1 ℓ � � a∈Sℓ aN � 0, 2 ρ �� . We compare AdaC-GOPE and AdaR-GOPE-Var in the same experimental setup and instances as in Section 5, for different privacy budgets ρ and report the results in Figure 3. 0.0 0.5 1.0 Step t ×106 101 103 105 Regret AdaC-GOPE AdaC-GOPE-Var (a) ρ = 0.001 0.0 0.5 1.0 Step t ×106 101 103 105 Regret AdaC-GOPE AdaC-GOPE-Var (b) ρ = 0.01 0.0 0.5 1.0 Step t ×106 101 103 Regret AdaC-GOPE AdaC-GOPE-Var (c) ρ = 0.1 0.0 0.5 1.0 Step t ×106 101 103 Regret AdaC-GOPE AdaC-GOPE-Var (d) ρ = 1 Figure 3: Evolution of the regret over time for AdaC-GOPE and Adar-GOPE-Var for different values of the privacy budget ρ As suggested by the regret analysis, AdaC-GOPE achieves less regret, especially in the high privacy regime where the private part of the regret has more impact. 46