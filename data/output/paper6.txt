Astronomy & Astrophysics manuscript no. main ©ESO 2023 September 4, 2023 Bayesian deep learning for cosmic volumes with modified gravity J.E. García-Farieta,1, 2 ⋆, Héctor J Hortúa3, 4 ⋆⋆ and Francisco-Shu Kitaura1, 2 1 Instituto de Astrofísica de Canarias, s/n, E-38205, La Laguna, Tenerife, Spain e-mail: jorge.farieta@iac.es 2 Departamento de Astrofísica, Universidad de La Laguna, E-38206, La Laguna, Tenerife, Spain 3 Grupo Signos, Departamento de Matemáticas, Universidad El Bosque, Bogotá, Colombia 4 Maestría en Ciencia de Datos, Universidad Escuela Colombiana de Ingeniería Julio Garavito, Bogotá, Colombia Received September 15, 1996; accepted March 16, 1997 ABSTRACT Context. The new generation of galaxy surveys will provide unprecedented data allowing us to test gravity deviations at cosmological scales at a much higher precision than achievable previously. A robust cosmological analysis of the large-scale structure demands exploiting the nonlinear information encoded in the cosmic web. Machine Learning techniques provide such tools, however, do not provide a priori assessment of uncertainties. Aims. This study aims at extracting cosmological parameters from modified gravity (MG) simulations through deep neural networks endowed with uncertainty estimations. Methods. We implement Bayesian neural networks (BNNs) with an enriched approximate posterior distribution considering two cases: one with a single Bayesian last layer (BLL), and another one with Bayesian layers at all levels (FullB). We train both BNNs with real-space density fields and power-spectra from a suite of 2000 dark matter only particle mesh N-body simulations including modified gravity models relying on MG-PICOLA covering 256 h−1 Mpc side cubical volumes with 1283 particles. Results. BNNs excel in accurately predicting parameters for Ωm and σ8 and their respective correlation with the MG parameter. Furthermore, we find out that BNNs yield well-calibrated uncertainty estimates overcoming the over- and under-estimation issues in traditional neural networks. We observe that the presence of MG parameter leads to a significant degeneracy with σ8 being one of the possible explanations of the poor MG predictions. Ignoring MG, we obtain a deviation of the relative errors in Ωm and σ8 by at least 30%. Moreover, we report consistent results from the density field and power spectra analysis, and comparable results between BLL and FullB experiments which permits us to save computing time by a factor of two. This work contributes in setting the path to extract cosmological parameters from complete small cosmic volumes towards the highly nonlinear regime. Key words. cosmology: – large-scale structure of Universe - cosmological parameters; methods: data analysis - statistical - numerical 1. Introduction Cosmic acceleration is one of the most critical concerns in modern cosmology. In the context of the concordance model ΛCDM (Λ-Cold Dark Matter), this acceleration is attributed to the existence of a fluid with negative pressure that is represented by the cosmological constant, Λ, in general relativity (GR) equations. However, the existence of such fluid introduces some conceptual and theoretical issues that have not been fully ad- dressed, either observational or theoretical. Alternative theories, such as modified gravity models, have attracted attention as a natural explanation for cosmic acceleration without invoking a cosmological constant (see, e.g., Nojiri et al. 2017, for a recent review). Among the plethora of alternative models, some parametrizations of f(R) gravity have gained popularity due to their ability to reproduce the standard model’s predictions accu- rately. Indeed, both cosmological scenarios, ΛCDM and f(R), are highly successful in providing an accurate description of the Universe on large scales, from cosmic microwave background (CMB) observations to the data of galaxy clustering (Berti et al. 2015). Unlike the standard scenario, the f(R) models do not require a cosmological constant but instead modify the behavior of ⋆ jorge.farieta@iac.es ⋆⋆ hhortuao@unbosque.edu.co gravity itself. The modification of Einstein’s general relativity involves the addition of a scalar field that emulates cosmic acceleration. This feature of f(R) models has made them perfect templates for tracking departures from standard gravity. Consequently, a crucial task within the scope of precision cosmology is to quantify the potential variations of gravity using appropriate techniques that are sensitive to modified gravity effects. Some of the approaches to achieve this aim include utilizing clustering anisotropies (Jennings et al. 2012; García-Farieta et al. 2019; Hernández-Aguayo et al. 2019; García-Farieta et al. 2020), tracer bias and sample selection (García-Farieta et al. 2021), cosmic voids (Voivodic et al. 2017; Perico et al. 2019; Contarini et al. 2021), halo mass functions (Hagstotz et al. 2019; Gupta et al. 2022) and peculiar velocities (Johnson et al. 2016; Ivarsen et al. 2016; Lyall et al. 2023). Matter distribution is a rich source of cosmological in- formation that has been exploited for many years through various techniques. One of the most used techniques to extract information from the large-scale structure (LSS) data, relies on the two-point statistics as described by the two-point cor- relation function or its equivalent in Fourier space, the matter power spectrum. Despite its success in capturing all possible cosmological information contained in a density field, it fails to capture features affected by the non-Gaussian nature of density perturbations, and its accuracy and precision cannot be relied Article number, page 1 of 13 arXiv:2309.00612v1 [astro-ph.CO] 1 Sep 2023A&A proofs: manuscript no. main upon for probing small angular scales. Since the estimators up to second order do not contain all cosmological information, other techniques beyond the two-point statistics have been studied to extract the additional information such are N-point correlation functions (Peebles 2001; Takada & Jain 2003; Zhang et al. 2022; Brown et al. 2022; Veropalumbo et al. 2022; Philcox et al. 2022), Minkowski functionals (Kratochvil et al. 2012; Hikage et al. 2003; Fang et al. 2017), peak count statistics (Kacprzak et al. 2016; Peel et al. 2017; Fluri et al. 2018; Harnois-Déraps et al. 2021), density split statistics (Paillas et al. 2021), cosmic shear (Kilbinger 2015; Van Waerbeke et al. 2001), cosmic voids Cai et al. (2015); Bos et al. (2012); Hamaus et al. (2016); Lavaux & Wandelt (2010), and tomo- graphic analysis based on the Alcock-Paczynski test (Park & Kim 2010; Zhang et al. 2019; Li et al. 2015; Luo et al. 2019; Dong et al. 2023). For a overview of contemporary cosmological probes, refer to Weinberg et al. (2013) and Moresco et al. (2022). Recently, Deep Neural Networks (DNNs) have been pro- posed as a new alternative for not only recollecting the three- dimensional (3D) density field information without specifying beforehand the summary statistic such as the power spectrum but also, for managing the demanding computational needs in astrophysics (Dvorkin et al. 2022). The CNN algorithms have been explored as a valuable tool in modified gravity scenar- ios, mainly with applications in weak lensing maps as: emu- lator building (Tamosiunas et al. 2021) as well as to investi- gate observational degeneracies between modified gravity mod- els and massive neutrinos (Merten et al. 2019; Peel et al. 2019), CMB patch maps analysis (Hortúa et al. 2020b), N-body sim- ulations (Lazanu 2021; de Oliveira et al. 2020; Kodi Ramanah et al. 2020), but also Bayesian Neural Networks (BNNs) have been employed to identify and classify power spectra that devi- ate from ΛCDM such modified gravity models (Mancarella et al. 2022). Despite their capability for extracting information from complex data, standard DNNs still suffer from overfit- ting/memorizing noisy labels during the training phase, and their point estimations are not always reliable. Bayesian Neural Net- works (BNNs) are extensions from those DNNs that provide probabilistic properties on their outcomes and yield predictive uncertainties. These BNNs employ Variational Inference (VI) to infer posterior distributions for the weights suitable to cap- ture uncertainties related to the network outputs (Graves 2011; Gunapati et al. 2022). Although VI speeds the computation of the posterior distribution when analytic approaches are consid- ered, these assumptions can also introduce a bias (Charnock et al. 2022) that yields overconfident uncertainty predictions and significant deviations from the true posterior. In Hortúa et al. (2020a) and Hortua (2021), the authors added normaliz- ing flows on top of BNNs to give the joint parameter distribu- tion more flexibility. However, that approach is not implemented into the Bayesian framework, still preserving the bias. In a re- cent work (Hortúa et al. 2023), the authors improved the previ- ous methodology by applying multiplicative normalizing flows, resulting in accurate uncertainty estimates. In this paper, we fol- low the same approach by building BNNs models adapted to both 3D-density field and its power spectra to constrain mod- ified gravity from cosmological simulations. We show that in the solely case of non-Gaussian signals it is possible to improve the posterior distributions and that when the additional informa- tion from the power spectrum is considered, they yield more sig- nificant performance improvements without underestimating the posterior distributions. This paper is organized as follows. Sec- tion 2 offers a summary of structure formation in modified grav- ity cosmologies and the reference simulations created for train- ing and testing the BNNs. In section 3 we briefly introduce the BNN concept and section 4 shows the architectures and config- uration used in the paper. The results are presented in section 5 and an extended discussion of the findings is presented in section 6. Conclusions are given in section 7. 2. Large-scale structure in modified gravity In this section, we present the gravity model which coincides with ΛCDM in the limiting case of a vanishing f(R) parameter introduced below. 2.1. Structure formation and background In f(R) cosmologies, the dynamics of matter is determined by the modified Einstein field equations. The most straightforward modification of GR that circumvents Λ emerge by including a function of the curvature scalar in the Einstein-Hilbert action. In this modification, the equations of motion are enriched with a term that depends on the curvature scalar and that creates the same effect as dark energy (for a review on different MG mod- els see e.g. Tsujikawa et al. 2008; De Felice & Tsujikawa 2010). For consistency across various cosmological scales, Hu & Saw- icki (2007, hereafter HS) proposed a viable f(R) function that is able to satisfy tight constraints at solar system scales, as well as accurately describe the dynamics of the ΛCDM background. In these models the modified Einstein-Hilbert action is given by S EH = � d4x √−g �R + f(R) 16πG � , (1) where g is the metric tensor, G the Newton’s gravitational con- stant, R the curvature scalar and f(R) a scalar function that con- strains the additional degree of freedom. In the HS model such function takes the form f(R) = −m2 c1 � −R/m2�n c2 �−R/m2�n + 1 , (2) where n, c1, c2 are model parameters, and m2 ≡ ΩmH2 0, with Ωm being the present fractional matter density and H0 the Hubble parameter at present time. For n = 1, which is the f(R) model we will consider in this paper the function can be written as follows f(R) ≈ −2Λ + | fR0|R2 0 R . (3) Here fR0 represents the dimensionless scalar field at present time, meaning the only additional degree of freedom that stems from the derivative of f(R) with respect to the curvature scalar, fR. The modified Einstein field equations and analogous Friedmann equations that describe the HS model’s background can be ob- tained from minimizing the action (for a thorough derivation see Song et al. 2007). To further understand the formation and evo- lution of large-scale structures in MG, it is crucial to describe the matter perturbations, δm, around the background (see Song et al. 2007). The MG effects are captured by the growth of den- sity perturbations in the matter dominated era when mildly non- linear regime is important (Laszlo & Bean 2008). In particular, when considering linear perturbations, the equations of the evo- lution of matter overdensities in Fourier space are as follows Article number, page 2 of 13Author: García-Farieta, Hortúa & Kitaura (Tsujikawa 2008): ¨δm + � 2H + ˙F 2F � ˙δm − ρm 2F δm = 1 2F �� −6H2 + k2 a2 � δF + 3Hδ ˙F + 3δ ¨F � , δ ¨F + 3Hδ ˙F + �k2 a2 + F 3FR − 4H2 − 2 ˙H � δF = 1 3δρm + ˙F˙δm , (4) with H being the Hubble parameter, k the comoving wavenum- ber of the perturbations, a the scale factor, ρm the matter den- sity field and F ≡ ∂ f/∂R. The solution to system of Eqs. (4) provides a detail description of δm, which includes most of the cosmological information, since it is a direct result of the grav- itational interaction of cosmic structures. In fact, to get insights into the underlying cosmic parameters, the density field is the primary source to be investigated using summary statistics. The Eqs. (4) make evident the connection between the density field and the scalaron of MG. Consequently, any departure from the GR would be measurable through the density field, either with the structure growth rate or its tracer distribution. A particular feature of the f(R) models is the so-called chameleon mecha- nism. This mechanism reconciles the departures of GR with the bounds imposed by local tests of gravity. It endows the mass of the scalar field with the ability to depend on the local matter density. More precisely, the signatures of MG can be detected in regions of lower matter density where the scalar field becomes lighter, leading to potential observable effects that deviate from standard gravity. The 3D matter power spectrum, denoted as P(k), is the pri- mary statistical tool employed to extract cosmological insights from the density field. It characterizes the overdensities as a function of scale and is estimated through the following average over Fourier space: (2π)3P(k)δ3 D �k − k′� = �δ(k)δ �k′�� , (5) where δ3 D is the three-dimensional Dirac-delta function. This function contains all information from the statistics of the den- sity field in the linear regime, and a decreasing fraction of the to- tal information on smaller scales, if initial density fields followed Gaussian statistics. In this work we use the Pylians31 library to estimate the overdensity field as well as the power spectrum. 2.2. Modified gravity simulations The simulations were created with the COmoving Lagrangian Acceleration (COLA) algorithm (Tassev et al. 2013; Koda et al. 2016), which is based on a Particle-Mesh code that solves the equations of motion following the Lagrangian Perturbation The- ory (LPT) trajectories of the particles. This algorithm speed up the computation of the gravitational force using a very few timesteps and still get correct results on the largest scales. In particular we used MG-PICOLA2 (Winther et al. 2017), a modified version of L-PICOLA (Howlett et al. 2015) that has been exten- sively tested against full N-body simulations and that extends the gravity solvers to MG models, including the HS parametrization. We run a set of 2500 MG simulations varying four cosmological 1 https://pylians3.readthedocs.io/en/master/index.html 2 The code can be found at https://github.com/HAWinther/ MG-PICOLA-PUBLIC Table 1. The summary of the set-up of the MG simulations. Left: cos- mology parameters. Right: set-up parameters used for MG-PICOLA code. Cosmologies Simulation setup Ωm [0.1, 0.5] Boxsize 256 h−1 Mpc h [0.5, 0.9] Np 1283 σ8 [0.6, 1.0] Grid force 1283 0.1 log10 | fR0| [0.4, 0.6] IC 2LPT zini = 49 Ωb 0.0489 Steps 100 ns 0.9665 kNy 1.58 Fig. 1. Diagram illustrating the multidimensional parameter space vari- ations. Each line represents a data point’s parameter values, with four parameters {Ωm, h, σ8, 0.1 log10 |fR0|} visualized along separate axes. Fig. 2. The projected overdensity field at redshift z = 0 derived from an arbitrary chosen simulation within the ensemble of 2500 MG simu- lations. The normalized density field was calculated using a CIC mass assignment scheme. parameters Θ = {Ωm, h, σ8, fR0}, where h is the reduced Hub- ble parameter, σ8 the r.m.s. density fluctuation within a top-hat sphere of 8 h−1 Mpc radius and fR0 the amplitude of the modi- fied gravity function in the HS model. The remaining cosmolog- ical parameters are set to Ωb = 0.048206 and ns = 0.96, which correspond to the values reported by Planck Collaboration et al. Article number, page 3 of 13A&A proofs: manuscript no. main Fig. 3. Projected density field of dark matter in a region of 256 × 256 × 20 (h−1Mpc)3 from 100 out of 2500 simulations of MG arbitrarily chosen. The snapshots are taken at z = 0 and the legend displays the set of cosmological parameters to be {Ωm, h, σ8, fR0}. The cuts in the density field highlight the broad coverage of the parameter space of the MG simulations. Different features can be observed by naked-eye, such as variations in the filamentary structure of the cosmic web. (2020). The parameter space is sampled with random numbers uniformly distributed within the specified ranges for each param- eter (see Table 1). Since the typical values of the modified gravity parameter goes as powers of ten, | fR0| ∼ 10n with n ∈ [−4, −6], we choose to sample a fraction of its logarithm in order to cover the range of powers equally, i.e., �fR0 = 0.1 log10 | fR0|. Figure 1 illustrates the parameter space variations of the 2500 MG cos- mologies, each one is represented by a gray line. The values of the cosmological parameters, Θ, are distributed along the differ- ent vertical axis. Each simulation follows the dynamics of 1283 particles in a small box of comoving side-length 256 h−1 Mpc using 100 timesteps from an initial redshift zi = 49 up to red- shift z = 0. This simulation resolution allows us to reasonably Article number, page 4 of 13Author: García-Farieta, Hortúa & Kitaura investigate the impacts of MG on large scales, in particular for the fR0 values considered in this work. However, it is not as ef- fective at very small scales, where higher resolution is required. In fact, MG solvers have undergone extensive testing using low- resolution simulations (see, for example, Puchwein et al. (2013); Li et al. (2012); Hernández-Aguayo et al. (2022)). These tests show the enhancement of the power spectrum in simulations of 256 h−1 Mpc, where MG effects begins to be appreciable. The setup of the MG simulations used in this work is summarized in Table 1. In a recent research, a similar setup was employed with light-weight deterministic CNN to estimate a subset of parame- ters of a flat ΛCDM cosmology (Pan et al. 2020), however in this work we choose a time-step larger by a factor of 2.5. The initial conditions for the MG simulations were created with 2LPTic (Crocce et al. 2006, 2012) based on a ΛCDM template at zi, moreover, a distinct random seed was assigned to each simula- tion to generate varying distributions of large-scale power. This approach allows our neural network to effectively capture the in- herent cosmic variance. We calculate the overdensity field, δm, for each snapshot at redshift z = 0, employing the cloud-in-cell (CIC) mass assig- ment scheme (Hockney & Eastwood 1981) on a regular grid con- sisting of N3 = 1283 voxels. The training set comprises 80% of the data, which corresponds to 2000 boxes containing the over- density fields, while the remaining 20% of the data was used for testing. Fig. 2 displays the 3D overdensity field plus the unity, δm + 1 = ρm/¯ρm, projected along each plane of the box. The displayed data corresponds to an arbitrarily chosen combination of parameters within the MG simulation suite at redshift z = 0. Similarly, Fig. 3 displays the 2D density field of dark matter in a region of 256×256×20 (h−1Mpc)3 from 100 out of 2500 simula- tions of MG arbitrarily chosen, with the cosmological parameter combination as indicated by the labels. The cuts in the density field provides a visual means to discern distinct features of the cosmic web, observable to the naked eye. These features include variations in the filamentary structure of the cosmic web, which become evident in the zones of under- and over-densities. Addi- tionally, we output the matter power spectrum of all realizations by directly computing the modulus of each Fourier mode from the particle distribution, |δm(k)|2. The Fig. 4 shows the different matter power spectrum for the entire MG simulation suit. The variations in the shape of the spectrum correspond to the joint effect of cosmological parameters that were varied as shown in the label. We consider the effective range of the power spectrum up to the Nyquist frequency, kNy, which in our simulations cor- responds to k ≈ 1.58 Mpc/h. The full datasets used in this pa- per, 3D overdensity fields as well as power spectra, are publicly available in Zenodo3. 3. Bayesian Neural Networks The primary goal of Bayesian Neural Networks (BNNs) is to estimate the posterior distribution p(w|D), which represents the probability distribution of the weights w of the network given the observed data D = (X, Y) (Abdar et al. 2021; Gal 2016; Graves 2011). The posterior distribution, denoted as p(w|D), can be de- rived using Bayes’ law: p(w|D) ∼ p(D|w)p(w). This expres- sion involves a likelihood function, p(D|w), which represents the probability of the observed data D given the weights w, as well as a prior distribution on the weights, denoted as p(w). After the computation of the posterior, the probability distribution of a 3 Data will be available upon publication. Please contact the authors for earlier access Fig. 4. The matter power spectrum at z = 0 of the MG simulation suit. The variations in the spectrum correspond to changes in each of the four parameters that were varied, {Ωm, h, σ8, |0.1 log fR0|}. The correspond- ing range of each of parameter is shown in the label. new test example x∗ can be determined by p(y∗|x∗, D) = � w p(y∗|x∗, w)p(w|D)dw , (6) being p(y∗|x∗, w) the predictive distribution corresponding to the set of weights. In the context of neural networks, it is important to note that the direct computation of the posterior is not fea- sible (Gal 2016). To circumvent this limitation, variational in- ference (VI) techniques approximating the posterior distribution have been introduced (Graves 2011). VI considers a family of simple distributions, denoted as q(w|θ), which is characterized by a parameter θ. The objective of VI is to identify a distribution q(w|θ∗) that minimizes the Kullback-Leibler divergence between q(w|θ) and p(w|D), where θ∗ represents the optimal parameter values, being KL[·∥·] the Kullback-Leibler divergence. This min- imization is equivalent to maximizing the evidence lower bound (ELBO) (Gal 2016), ELBO(θ) = Eq(w|θ) � log p(Y|X, w)� − KL �q(w|θ) ���p(w)� , (7) where Eq(w|θ)[log p(Y|X, w)] is the expected log-likelihood with respect to the variational posterior and KL[q(w|θ)||p(w)] is the divergence of the variational posterior from the prior. It can be observed from Eq. (7) that the Kullback-Leibler (KL) divergence serves as a regularizer, compelling the variational posterior to shift towards the modes of the prior. A frequently employed op- tion for the variational posterior entails utilizing a product of in- dependent Gaussian distributions, specifically mean-field Gaus- sian distributions, with each parameter w being associated with its own distribution (Abdar et al. 2021) q(w|θ) = � i j N(w; µi j, σ2 i j) , (8) where i and j are the indices of the neurons from the previous- and the current layers, respectively. Applying the reparametriza- tion trick we obtain wi j = µi j + σi j ∗ ϵi j, where ϵi j is drawn from the normal distribution. Moreover, if the prior is a composition of independent Gaussian distributions, the KL-divergence between Article number, page 5 of 13A&A proofs: manuscript no. main the prior and the variational posterior can be calculated analyt- ically. This characteristic enhances the computing efficiency of this approach. 3.1. Multiplicative normalizing flows Gaussian mean-field distributions described in Eq. (8) are the most commonly utilized family for the variational posterior in BNNs. Unfortunately, this distribution lacks the capacity to ade- quately represent the intricate nature of the true posterior. Hence, it is anticipated that enhancing the complexity of the variational posterior will yield substantial improvements in performance. This is attributed to the capability of sampling from a more re- liable distribution, which closely approximates the true poste- rior distribution. Indeed, the process of improving the variational posterior demands efficient computational methods while ensur- ing its numerical feasibility. Multiplicative Normalizing Flows (MNFs) have been proposed to efficiently adapt the posterior dis- tributions through the utilization of auxiliary random variables and the normalizing flows (Louizos & Welling 2017). Mixture normalizing flows (MNFs) suggest that the variational posterior can be mathematically represented as an infinite mixture of dis- tributions q(w|θ) = � q(w|z, θ)q(z|θ)dz , (9) with θ the learnable posterior parameter, and z ∼ q(z|θ) ≡ q(z)4 the vector with the same dimension of the input layer, which plays the role of an auxiliary latent variable. Also, allowing for local reparametrizations, the variational posterior for fully con- nected layers becomes w ∼ q(w|z) = � ij N(w; ziµij, σ2 ij) . (10) Where we can increase the flexibility of the variational pos- terior by enhancing the complexity of q(z). This can be done using normalizing flows since the dimensionality of z is much lower compared to the weights. Starting from samples z0 ∼ q(z0) from fully factorized Gaussians (see Eq. (8)), a rich distribu- tion q(zK) can be obtained by applying successively invertible fk-transformations zK = NF(z0) = fK ◦ · · · ◦ f1(z0) , (11) log q(zK) = log q(z0) − K � k=1 log �����det ∂fk ∂zk−1 ����� . (12) To handle the intractability of the posterior, Louizos & Welling (2017) suggest to use again Bayes law q(zK)q(w|zK) = q(w)q(zK|w) and introduce a new auxiliary distribution r(zK|w, ϕ) parameterized by ϕ, with the purpose of approximating the pos- terior distribution of the original variational parameters q(zK|w) to further lower the bound of the KL divergence term. Accord- ingly, the KL-divergence term can be rewritten as follows − KL �q(w) ���p(w)� ≥ Eq(w,zK) � − KL �q(w|zK) ���p(w)� + log q(zK) + log r(zK|w, ϕ) � . (13) 4 For the sake of clarity in notation, the parameter θ will no longer be considered in the subsequent discussion. The first term can be analytically computed since it will be the KL-divergence between two Gaussian distributions, while the second term is computed via the normalizing flow generated by fK (see Eq. (12)). Furthermore, the auxiliary posterior term is parameterized by inverse normalizing flows as follows (Touati et al. 2018) z0 = NF−1(zK) = g−1 1 ◦ · · · ◦ g−1 K (zK) , (14) and log r(zK|w, ϕ) = log r(z0|w, ϕ) + K � k=1 log ������det ∂g−1 k ∂zk ������ , (15) where one can parameterize g−1 K as another normalizing flow. A flexible parametrization of the auxiliary posterior can be given by z0 ∼ r(zK|w, ϕ) = � i N(z0; ˜µi(w, ϕ), ˜σ2 i (w, ϕ)) , (16) where the parameterization of the mean ˜µ, and the variance ˜σ2 is carried out by the masked RealNVP (Dinh et al. 2017) as the choice of normalizing flows. 3.2. Multiplicative normalizing flows in voxel-grid representation In this section we present our first result, where we have gener- alized Eq. (10) to 3D convolutional layers where cosmological simulated data are structured. To this end, we started with the extension of sampling from the variational posterior as w ∼ q(w|z) = Dd � i Dh � j Dw � k Df � l N(w; zlµi jkl, σ2 i jkl) , (17) where Dh, Dw, Dd are three spatial dimensions of the boxes, Algorithm 1 Forward propagation for each Convolutional 3D layer. Mw, Σw are the means and variances of each layer, H is the input layer, and NF(·) is the masked RealNVP normalizing flow. ⊙ corresponds to element-wise multiplication. H ← Input conv3D-layer (minibatch) z0 ∼ q(z0) zT f = NF(z0) Mh = H ∗ (Mw ⊙ reshape(zT f , [1, 1, 1, Df ])) Vh = H2 ∗Σw E ∼ N(0, 1) return Mh + √Vh ⊙ E and Df is the number of filters for each kernel. The objective is to address the challenge of enhancing the adaptability of the approximate posterior distribution for the weight coming from a 3D-convolutional layer. Algorithm 1 outlines the procedure to forward propagation for each 3D-convolutional layer. Similar to the fully connected case, the auxiliary parameter affects only the mean with the purpose of avoiding large variance and we kept a linear mapping to parametrize the inverse normalizing flows instead of applying tanh feature transformations. Article number, page 6 of 13Author: García-Farieta, Hortúa & Kitaura Table 2. Configuration of the (Se)-ResNet backbone used for all exper- iments presented in this paper. (Se)-ResNet-18 backbone Layer Name Input Shape Output Shape Batch Norm (Nbatch, 128,128,128,1) (Nbatch, 128,28,128,1) 3D Convolutional (Nbatch, 128,128,128,1) (Nbatch, 64,64,64,16) Batch Norm+ReLU (Nbatch, 64,64,64,16) (Nbatch, 64,64,64,16) Max Pooling 3D (Nbatch, 64,64,64,16) (Nbatch, 32,32,32,16) Batch Norm+ReLU (Nbatch, 32,32,32,16 ) (Nbatch, 32,32,32,16 ) Resblock 1 � (Nbatch, 32, 32, 32, 16) (Nbatch, 16, 16, 16, 32) � (Nbatch, 16,16,16,32) Batch Norm+ReLU (Nbatch, 16,16,16,32) (Nbatch, 16,16,16,32 ) Resblock 2 � (Nbatch, 16, 16, 16, 32) (Nbatch, 8, 8, 8, 64) � (Nbatch, 8,8,8,64) Batch Norm+ReLU (Nbatch, 8,8,8,64) (Nbatch, 8,8,8,64 ) Resblock 3 � (Nbatch, 8, 8, 8, 64) (Nbatch, 4, 4, 4, 128) � (Nbatch, 4,4,4,128) Batch Norm+ReLU (Nbatch, 4,4,4,128) (Nbatch, 4,4,4,128) Resblock 4 � (Nbatch, 4, 4, 4, 128) (Nbatch, 2, 2, 2, 256) � (Nbatch, 2,2,2,256) Batch Norm+ReLU (Nbatch, 2,2,2,256 ) (Nbatch, 2,2,2,256) Global Avg Pooling (Nbatch, 2,2,2,256) (Nbatch, 256) 4. The Bayesian architecture set-up We will examine four distinct architectures of Bayesian Neu- ral Networks (BNNs) as outlined in Section 3. Two of these architectures include Bayesian layers located only on the top of the network, the so-called Bayesian Last Layer (denoted as BLL), while the remaining have Bayesian layers at all their levels (FullB). The pipelines utilized in our study were de- veloped using TensorFlow v:2.12 and TensorFlow-probability v:0.19 (Abadi et al. 2015). The architecture used for all net- works has ResNet-18 as the backbone, which is depicted in a schematic manner in Table 2. The Resblock nature depends on whether we build either ResNet or SeResNet topology. The lat- ter is a variant of ResNet that employs squeeze-and-excitation blocks that adaptively recalibrate channel-wise feature responses by explicitly modeling interdependencies between channels (Hu et al. 2019). Fig. 5 depicts each Resblock and how the skip con- nections are defined. These architectures were designed using the GIT repository classification-models-3D. ResNet18 contains 2510149 trainable parameters while SeResNet has 3069270 but, these numbers are duplicates when we move towards a fully Bayesian scheme because two parameters need to be optimized (the mean and standard deviation) for each network parameter. In this study, 50 layers were employed for the masked RealNVP normalizing flow. The development of these convolutional lay- ers was made using also the repositories TF-MNF-VBNN and MNF-VBNN (Louizos & Welling 2017). Finally, all networks end with a multivariate Gaussian distribution layer, consisting of fourteen trainable parameters. These parameters include four means, denoted as µ, which correspond to the cosmological pa- rameters, as well as ten elements representing the covariance ma- trix Σ. The loss function to be optimized is given by the ELBO, Eq. (7), where the second term is associated with the negative SeResNet18 ResNet18 Fig. 5. Resblock schema depending on the architecture used. Top: Res- block when SeResNet18 is employed. The orange dashed rectangle shows the skip SE-connection schema used in the SeResNet18 resblock. Bottom: Resblock when ResNet is employed. log-likelihood (NLL) −NLL ∼ 1 2 log |Σ| + 1 2(y − µ)⊤ (Σ)−1 (y − µ) , (18) averaged over the mini-batch. The optimizer used was Adam with first and second moments exponential decay rates of 0.9 and 0.999, respectively (Kingma & Ba 2014). The learning rate starts from 5×10−4 and it is reduced by a factor of 0.9 in case any improvement has not been observed after 8 epochs. Furthermore, we have applied a warm-up period for which the model turns on progressively the KL term in Eq. (7). This is achieved by intro- ducing a β variable in the ELBO, i.e., β · KL �q(w|θ) ���p(w)�, so, this parameter starts being equal to 0 and grows linearly to 1 dur- ing 12 epochs (Sønderby et al. 2016). BNNs were trained with 8 batches and early stopping callback was presented to avoid over- fitting. The infrastructure put in place by Google Cloud Platform (GCP) uses a nvidia-tesla-t4 of 16 GB GDDR6 in a N1 machine series shared-core. 4.1. Quantifying the performance The metrics employed for determining the network performance were the Mean Square Error (MSE), ELBO, and the coefficient of determination r2. Moreover, we quantify the quality of the un- certainty estimates through reliability metrics. Following Laves et al. (2020) and Guo et al. (2017), we can define a perfect cali- bration of regression uncertainty as E ˆσ2 � abs��||y − µ||2 ��� ˆσ2 = α2� − α2�� ∀ � α2 ∈ R ��� α2 ≥ 0 � , (19) being abs[.] the absolute value function. So, the predicted uncer- tainty ˆσ2 is partitioned into K bins with equal width, to variance Article number, page 7 of 13A&A proofs: manuscript no. main per bin is defined as var(Bk) := 1 ���Bk ��� � i∈Bm 1 N N � n=1 �µi,n − yi �2 , (20) with N stochastic forward passes. In addition, the uncertainty per bin is defined as uncert(Bk) := 1 |Bk| � i∈Bk ˆσ2 i , (21) allow us to compute the expected uncertainty calibration error (UCE) in order to quantify the miscalibration UCE := K � k=1 |Bk| m ���var(Bk) − uncert(Bk) ��� , (22) with the number of inputs m and set of indices Bk of inputs, for which the uncertainty falls into the bin k. 5. Results In this section, we present the results of several experiments de- veloped to quantify the performance of Bayesian deep learning neural networks for constraining the cosmological parameters in modified gravity scenarios. 5.1. Parameter estimation from the overdensity field under voxel-grid representation Using the configuration described in Sec. 4, we designed four experiments inspired by two successful deep learning architec- tures, ResNet18 and Se-ResNet18. The former is a residual net- work commonly known due to its efficiency in several computer vision tasks, while the latter has been chosen because of its abil- ity to improve the interdependencies between the channels of convolutional feature layers (Hu et al. 2019). Furthermore, the modification of the models was also based on the insertion of a set of Bayesian layers at either the top of the model (BLL), or in the entire architecture (FullB). The motivation for exploring both possibilities comes from the fact that intuitively, adding a Bayesian layer at the end of the network (BLL), can be viewed as Bayesian linear regression with a learnable projected feature space, allowing for a successful balance between scalability and the degree of model-agnosticism (Fiedler & Lucia 2023; Watson et al. 2021). On the contrary, although fully Bayesian networks (FullB) would demand high computational resources, it has been reported that their Bayesian hidden layers are susceptible to out- of-distribution (OOD) examples that might improve predictive uncertainty estimates (Henning et al. 2021). The results of the experiments performed in this work are summarized in Table 3. Here we can observe the performance of each architecture on the test set. In the top part of the table, the results of SeRes- Net18 topology are shown, while in the bottom part, the results of ResNet18 are presented. The left columns of the table corre- spond to the FullB scheme, while the left one to the Bayesian Last Layer, BLL. Comparing all approaches, we observe that FullB-SeResNet18 slightly outperforms the rest of the models in terms of accuracy (described by r2) and uncertainty quality pro- vided by UCE. However, significant differences were not found in the reported metrics for ResNet and its SeResNet counterpart, except for the inference time where the BLL models clearly out- perform the FullB ones. This brings the idea that FullBs yield small improvements in the computation of uncertainties at the expense of duplicating the inference time. In addition, both ar- chitectures estimate σ8 more efficiently than for any other pa- rameter, especially in contrast to h or 0.1 log10 | fR0|, although the FullBs respond slightly better to MG effects. Fig. 6 displays the scatter relationship between the predicted and ground truth val- ues of each cosmological parameter using FullB-SeResNet18. It also shows the degeneracy directions that arise from obser- vations defined as Ωmh2 and σ8Ω0.25 m . The diagonal gray lines correspond to the ideal case of a perfect parameter prediction. Each data point represents the mean value of the predicted dis- tributions, and the error bars stand for the heteroscedastic un- certainty associated with epistemic plus aleatoric uncertainty at 1-σ confidence level. As we observe, BNNs learn how to accu- rately predict the value for Ωm and σ8, but they fail in capturing information related to the MG effects and the Hubble parame- ter. Even though parameter estimation derives from all features of the fully nonlinear 3D overdensity field, the horizontal scatter pattern that exhibits the Hubble and MG parameters implies that essential underlying connections are not effectively captured. A similar result for the Hubble parameter using DCNNs in ΛCDM can be found in Villaescusa-Navarro et al. (2020). 5.2. Parameter estimation coming from the matter power spectrum In this section, we show the results of using the power spec- trum to extract the cosmological parameters in MG scenarios. Following the same methodology as described in the voxel- grid representation, we implement two BNN models that provide distributed predictions for the cosmological parameters. Table 4 schematically presents the architecture used for this purpose. This represents a Fully Connected Network (FCN) with 60000 trainable parameters, and it was derived from KerasTuner5 as a framework to make scalable hyperparameter optimization. We work with a Bayesian Last Layer model (BLL-FCN) along with a Full Bayesian topology where all dense layers are probabilis- tic (FullB-FCN). Here, the power spectrum computed from the N-body simulations is kept until k ≈ 1.58h−1 Mpc, obtaining ar- rays of 85 dimensions. The results of this approach are shown in Table 5. In contrast to the voxel-grid representation where the Full Bayesian approach outperforms most of the models, here we clearly observe that BLL approach works better than the fully Bayesian one. These results show a similar performance compared to the 3D overdensity field. We expected this behavior since most of the voxel-grid information should be encoded into the two-point correlator. Notice also that some parameters such as σ8 or the derived parameters provide higher accuracy when they are predicted with the voxel-grid approach supporting the fact that 3D-convolutional layer extracts further information be- yond the linear part. The interplay between the fR0 parameter and the shape of the power spectrum is essential for testing and constraining gravity theories. The immediate effect of fR0 on the power spectrum is to modulate its amplitude, most notably at small scales. Furthermore, this parameter of the HS model ex- hibits a substantial degeneracy with σ8, which produces a sim- ilar effect on the power amplitude, but not in a scale-dependent manner as MG does. The strongest deviations of the power spec- trum from the ΛCDM model are observed for high values of fR0, in our case ∼ 10−4 (see Fig. 4). Because of this degeneracy, it is probable that some of the MG information is being encoded in the σ8 parameter rather than the fR0 parameter. This hypothesis, 5 https://keras.io/keras_tuner/ Article number, page 8 of 13Author: García-Farieta, Hortúa & Kitaura Table 3. Metrics for the test set for all BNNs architectures. Top: SeResNet18, bottom: ResNet18. High UCE values indicate miscalibration. Bold text is the minimum (maximum) value, ↓ (↑) as indicated in the metric name, among the different parameters. Metrics FullB-SeResNet18 BLL-SeResNet18 Ωm h σ8 0.1 log10 |fR0| Ωmh2 σ8Ω0.25 m Ωm h σ8 0.1 log10 | fR0| Ωmh2 σ8Ω0.25 m MSE ↓ 0.001 0.01 0.0007 0.003 0.0009 0.0008 0.003 0.013 0.0012 0.0035 0.0009 0.0013 r2 ↑ 0.86 0.15 0.94 0.04 0.85 0.93 0.80 0.03 0.90 0.008 0.85 0.89 UCE ↓ 0.07 0.07 0.08 0.02 0.08 0.12 0.03 0.3 0.08 0.05 0.022 0.15 AV-MSE ↓ 0.0043 0.0051 NLL ↓ -99.21 -3.38 Inf.Time [ms] 397 290 Metrics FullB-ResNet18 BLL-ResNet18 Ωm h σ8 0.1 log10 |fR0| Ωmh2 σ8Ω0.25 m Ωm h σ8 0.1 log10 | fR0| Ωmh2 σ8Ω0.25 m MSE ↓ 0.001 0.01 0.0007 0.003 0.001 0.0008 0.0025 0.012 0.0015 0.003 0.0015 0.001 r2 ↑ 0.86 0.15 0.95 0.04 0.83 0.93 0.82 0.10 0.89 0.05 0.75 0.92 UCE ↓ 0.07 0.09 0.09 0.01 0.08 0.20 0.014 0.078 0.09 0.07 0.024 0.14 AV-MSE ↓ 0.0043 0.0048 NLL ↓ -95.12 -3.34 Inf.Time [ms] 345 262 Fig. 6. True vs Predicted values provided by the FullB model, for Ωm, σ8, and some derivative parameters. Points are the mean of the predicted distributions, and error bars stand for the heteroscedastic uncertainty associated with epistemic and aleatoric uncertainty at 1σ. Table 4. Configuration of the fully connected neural network used for constraining parameters from the power spectrum. Fully connected neural network Layer Name Input Shape Output Shape Dense Layer (Nbatch, 85) (Nbatch, 64) ReLU (Nbatch, 64) (Nbatch, 64) Dense Layer (Nbatch, 64) (Nbatch, 64) ReLU+Batch Norm (Nbatch, 64) (Nbatch, 64) Dense Layer (Nbatch, 64 ) (Nbatch, 64) ReLU (Nbatch, 64) (Nbatch, 64) Dense Layer (Nbatch, 64 ) (Nbatch, 14) Multivariate normal (Nbatch, 14) (Nbatch, 4 ) however, would require additional tests of the BNN with a re- duced parameter space in addition to isolating the impact of the sole case of a zero fR0, which we leave for future work. 5.3. Comparison among approaches based on marginalized parameter constraints Finally, we choose one example from the test set to compare the constrain contours predicted by the best models presented in the paper so far. Fig. 7 compares the parameter constraints at 68% and 95% confidence levels predicted for the FullB-SeResNet18 and FullB-FCN models. The true values of the example are re- ported in Table 6 as well as represented by dashed lines in the triangular plot. Notice that both models yield decent predictions for the marginal distribution, but they differ in the correlation among cosmological parameters, as σ8 and fR0 where this be- Article number, page 9 of 13A&A proofs: manuscript no. main Table 5. Metrics for the power spectra test set with Fully-Connected Networks (FCN). High UCE values indicate miscalibration. MSE and NLL are computed only over the cosmological parameters. Bold text is the minimum (maximum) value, ↓ (↑) as indicated in the metric name, among the different parameters. Metrics FullB-FCN BLL-FCN Ωm h σ8 0.1 log10 | fR0| Ωmh2 σ8Ω0.25 m Ωm h σ8 0.1 log10 | fR0| Ωmh2 σ8Ω0.25 m MSE ↓ 0.0023 0.012 0.0007 0.003 0.0013 0.0011 0.0023 0.011 0.00078 0.0030 0.0012 0.0012 r2 ↑ 0.83 0.11 0.94 0.06 0.77 0.90 0.83 0.16 0.94 0.073 0.80 0.89 UCE ↓ 0.026 0.12 0.022 0.022 0.026 0.092 0.023 0.15 0.017 0.023 0.016 0.10 AV-MSE ↓ 0.0045 0.0043 NLL ↓ 64.86 1.80 Inf.Time [ms] 3.01 2.21 Table 6. Parameters in the 95% intervals taken from the parameter con- straint contours from one example of MG simulations test set predicted by the FullB-SeResnet18 and FullB-FCN. Parameter SeResNet18 FCN Target Ωm 0.36+0.13 −0.13 0.37+0.12 −0.12 0.3865 h 0.69+0.22 −0.21 0.72+0.23 −0.23 0.6274 σ8 0.664+0.081 −0.082 0.667+0.060 −0.060 0.6822 0.1 log10 |fR0| 0.51+0.11 −0.11 0.51+0.13 −0.14 0.5557 σ8Ω0.25 m 0.512+0.081 −0.082 0.519+0.059 −0.059 0.5379 Ωmh2 0.167+0.085 −0.079 0.190+0.096 −0.091 0.1521 Table 7. Relative error comparison among different CNN approaches for MG and ΛCDM simulations. The relative error has been defined as δy ≡ ∆y/y, where y stands for Ωm, σ8 and ∆y is the uncertainty. Method δΩm δσ8 Reference CNN 0.0048 0.0053 Pan et al. (2020) CNN 0.0280 0.0120 Ravanbakhsh et al. (2017) VBNNs 0.2128 0.0545 Hortúa et al. (2023) FlipoutBNN 0.2444 0.0844 Hortúa et al. (2023) SeResNet 0.3611 0.1220 This work FCN 0.3243 0.0900 This work havior is more notorious. It implies clearly that 3D-convolutions extract further information beyond the linear regime that allows to constrain more tightly the parameter estimation. 6. Summary and discussion We consider a wide range of MG simulations, varying their cos- mological parameters, encompassing cosmologies with large de- viations from the standard GR to parameters closest to those that mimic the dynamics of a Universe based on GR. The overdensity field of each snapshot was computed using the CIC mass assign- ment and subsequently, we obtained its power spectrum. To con- strain the main set of cosmological parameters, we introduced a novel architecture of a BNN and designed several experiments to test its ability to predict MG cosmologies. The experiments consist of building two Bayesian networks based on stochastic layers located at either the top or at all levels of the architecture. This approach is motivated by the question of whether BNNs provide better accuracy and robustness performance when we work with full or partial network configurations. Starting from the 3D overdensity field, we found that although the FullB pre- dicts slightly better the cosmological parameters than the BLL, the latter is accurate enough to retrieve cosmological informa- tion from the density field, especially for Ωm and σ8. Similarly, we tested BNNs using the two-point statistics described by the power spectrum for reasonable scales limited by the Nyquist fre- quency. The results of this experiment show that the information learned by the networks can predict the parameters with sim- ilar accuracy to the 3D field. Both configurations of the BNN architectures fall short of capturing both the Hubble parameter and the MG effects. This underscores the necessity of improving the training dataset in terms of resolution and scale for the 3D density setup. Despite the slight constraints for some cosmolog- ical parameters, the methodology can be relevant in applications where it is combined with classical inference methods (Hortúa et al. 2020b). The multiplicative normalizing flows technique in BNNs employed in this paper has proved to bring well predic- tions and accurate uncertainty estimates thanks to the ability to transform the approximate posterior into a more expressive dis- tribution consistent with the data complexity. This is a significant improvement compared to standard VI where the posterior is re- stricted only to a Gaussian configuration. Nevertheless, the effect of assuming a Gaussian prior distribution of the weights under this approach is still unknown (Fortuin et al. 2022). In future work, we will explore Multiplicative normalizing flows with dif- ferent prior distributions over the weights and analyze how the prior influences the uncertainty calibration and performance. The finding that the MG parameter is poorly predicted when using the information provided by the density field demonstrates, on the one hand, the effectiveness of the chameleon screening mechanism in mimicking the ΛCDM model, as well as the need for further analysis with other datasets more sensitive to the ef- fects of MG. It should be noted that in our study we have con- sidered parameters that produce the same effect, i.e., that are de- generate. Therefore, it is not straightforward to attribute a single characteristic of the overdensity field exclusively to a single pa- rameter, as in the case of fR0 and σ8. The proposed architectures are sufficiently general from a statistical standpoint to estimate posterior distributions. However, this study has revealed that the available information is inadequate to predict all parameters solely from a single source. This underscores the significance of resolving degeneracies between cosmological parameters by in- corporating supplementary data or diverse features present in the cosmological simulations. Such an approach enables the BNNs to gain a richer learning phase and parse out the signals of each cosmology. This task will be the focus of a forthcoming paper, where we plan to evaluate the BNNs robustness using simula- tions of higher resolution and more intricate datasets in redshift space, incorporating velocity information alongside particle po- sitions. In Table 7, we also present a comparison of the relative errors for the two best-estimated parameters using CNN and N-body simulations from the literature. We observed significant discrep- ancies in the relative errors of σ8 and Ωm, approximately 90% when Bayesian inference is not employed (see Ravanbakhsh et al. 2017; Pan et al. 2020). This outcome arises from using solely ΛCDM simulations in both training and test datasets, in Article number, page 10 of 13Author: García-Farieta, Hortúa & Kitaura 0.2 0.3 0.4 0.5 0.6 m 0.1 0.2 0.3 mh2 0.4 0.5 0.6 8 0.25 m 0.3 0.4 0.5 0.6 0.7 0.1log10|fR0| 0.6 0.7 0.8 8 0.4 0.6 0.8 1.0 h 0.4 0.6 0.8 1.0 h 0.6 0.7 0.8 8 0.3 0.4 0.5 0.6 0.7 0.1log10|fR0| 0.4 0.5 0.6 8 0.25 m 0.1 0.2 0.3 mh2 FullB-SeResNet18 FullB-FCN Fig. 7. 68% and 95% parameter constraint contours from one example of the test dataset using FullB-SeResNet and Full-FCN. The diagonal plots are the marginalized parameter constraints, and dashed lines stand for the true values reported in Table 6. We derive these posterior distributions using GetDist (Lewis 2019). contrast to our estimates that encompass an additional parameter accounting for MG and include a calibration procedure of the uncertainties. Furthermore, when contrasting the performance of BLL architectures on MG and ΛCDM simulations, such as QUIJOTE (see e.g. Hortúa et al. 2023), we find a deviation of the relative errors close to 30% when modified gravity effects are not considered. This result clarifies that when utilizing FullB- SeResNet18, the errorbars for Ωm are 1.3 times larger and for σ8 are 2.1 times larger in comparison to FlipoutBNN. In the con- text of BNNs, when separately considering the two cosmologi- cal models — MG and ΛCDM — we assess the performance in terms of the MSE metric, comparing it to the results presented by Hortúa et al. (2023), who employed a similar architecture. Specifically, using FullBs in both cosmological models, we ob- serve an improvement by a factor 13 in the MSE of the MG pre- dictions over the ΛCDM ones. The r2 metric was used to com- pare the confidence range of the individual parameters. In terms of this metric, we report that σ8 has a larger deviation (r2 = 0.95 in MG and r2 =0.99 in ΛCDM), which accounts for 4.2% of the expected uncertainty. The marginal difference in the coefficient Article number, page 11 of 13A&A proofs: manuscript no. main of determination for predicting Ωm is only 0.01 when comparing the results of the model trained with MG against the one trained with ΛCDM. In both cases, it is noteworthy that a high r2 value does not necessarily confer complete certainty regarding indi- vidual parameter estimates, particularly when parameter degen- eracy is taken into account. Furthermore, one interesting possi- bility to refine the constraints on f(R) gravity is given by training a specialized network that distinguishes entirely between ΛCDM and f(R), offering the potential to detect a non-zero fR0. Further investigations, including high-resolution simulations as well as extensions beyond ΛCDM, promise to further enhance the ca- pabilities of the BNNs approach. The techniques explored in this work still need further developments to be applied to ob- servational data. The bias and peculiar motions of the tracers and the systematics coming from galaxy surveys still need to be taken into account. They could be nonetheless, potentially, im- plemented in Bayesian inference algorithms of the large-scale structure (such as, e.g., Kitaura et al. 2021). Alongside this pa- per, we make available the scripts which can be accessed at the github repository https://github.com/JavierOrjuela/ Bayesian-Neural-Net-with-MNFs-for-f-R-. 7. Conclusions One of the intriguing possibilities for explaining the observed accelerated expansion of the Universe is the modification of gen- eral relativity on large scales. Matter distribution analysis via N-body simulations offers a perfect scenario for tracking de- partures from standard gravity. Among different parametriza- tions, f(R) has emerged as an interesting model due to its abil- ity to reproduce the standard model’s predictions accurately. In this manuscript, we analyzed the possibility of using Bayesian Deep Learning methods for constraining cosmological param- eters from modified gravity simulations. Below, we summarize the main take-aways from this study: 1. BNNs can predict with higher accuracy cosmological param- eters, especially for Ωm and σ8 from the overdensity field. However, based on the assumption of simulating boxes with 256 h−1 Mpc to acquire MG effects on large scales, BNNs were unable to effectively extract MG patterns from the overdensity field to yield accurate f(R)-parameter estima- tion. However when comparing parameter estimation with ΛCDM-only simulations, we find that there is a consider- able underprediction of the uncertainties of σ8 when possible modified gravity effects are not taken into account. In addi- tion, special attention should be paid to parameter degenera- cies that may be present not only in two-point statistics but in more features of the density field. We conclude that higher resolution and further intricate datasets in redshift space, in- corporating velocity information alongside particle positions can be approaches that should be addressed to improve the network predictions. 2. It is observed that cosmological parameters can be recov- ered directly from the simulations using convolutional-based models with the potential of extracting patterns without spec- ifying any N-point statistics beforehand. This is supported by the fact that networks trained with overdensity fields and power spectra predicted decent predictions but with distinc- tive correlations among the parameters. 3D-convolutions ex- tracted supplementary information beyond the linear regime that allowed them to constrain tightly the parameter estima- tion. 3. We generalized the Multiplicative normalizing flows for BNNs to the 3D convolutional level, allowing us to work with fully transformed stochastic neural networks. As a proof of concept, we ran several experiments in order to verify that this approach not only achieved the performance reached by the deterministic models but also yielded well-calibrated un- certainty estimates. 4. We probed the impact of the parameter estimation based on the Bayesian Last Layer (BLL) and fully Bayesian ap- proaches. The results showed that fullBs provide slightly higher quality predictions along with accurate uncertainty estimates. Nevertheless, this improvement is not significant enough to prefer this approach with respect to the BLL, where the latter has the advantage of being relatively model- agnostic, easily scalable, and 2×inference time faster. Acknowledgements. This paper is based on work supported by the Google Cloud Research Credits program with the award GCP19980904. HH acknowledges support from créditos educación de doctorados nacionales y en el exterior- Colciencias and the grant provided by the Google Cloud Research Credits pro- gram. JEGF is supported by the Spanish Ministry of Universities, through a María Zam- brano grant (program 2021-2023) at Universidad de La Laguna with reference UP2021-022, funded within the European Union-Next Generation EU. FSK and JEGF acknowledge the IAC facilities and the Spanish Ministry of Science and Innovation (MICINMINECO) under project PID2020-120612GB-I00. We also thank the personnel of the Servicios Informáticos Comunes (SIC) of the IAC. References Abadi, M., Agarwal, A., Barham, P., et al. 2015, TensorFlow: Large-Scale Ma- chine Learning on Heterogeneous Systems, software available from tensor- flow.org Abdar, M., Pourpanah, F., Hussain, S., et al. 2021, Information Fusion, 76, 243 Berti, E., Barausse, E., Cardoso, V., et al. 2015, Classical and Quantum Gravity, 32, 243001 Bos, E. G. P., van de Weygaert, R., Dolag, K., & Pettorino, V. 2012, MNRAS, 426, 440 Brown, Z., Mishtaku, G., & Demina, R. 2022, A&A, 667, A129 Cai, Y.-C., Padilla, N., & Li, B. 2015, MNRAS, 451, 1036 Charnock, T., Perreault-Levasseur, L., & Lanusse, F. 2022, Bayesian Neural Net- works (WORLD SCIENTIFIC), 663–713 Contarini, S., Marulli, F., Moscardini, L., et al. 2021, MNRAS, 504, 5021 Crocce, M., Pueblas, S., & Scoccimarro, R. 2006, MNRAS, 373, 369 Crocce, M., Pueblas, S., & Scoccimarro, R. 2012, 2LPTIC: 2nd-order La- grangian Perturbation Theory Initial Conditions, Astrophysics Source Code Library, record ascl:1201.005 De Felice, A. & Tsujikawa, S. 2010, Living Reviews in Relativity, 13, 3 de Oliveira, R. A., Li, Y., Villaescusa-Navarro, F., Ho, S., & Spergel, D. N. 2020, Fast and Accurate Non-Linear Predictions of Universes with Deep Learning Dinh, L., Sohl-Dickstein, J., & Bengio, S. 2017, in International Conference on Learning Representations Dong, F., Park, C., Hong, S. E., et al. 2023, ApJ, 953, 98 Dvorkin, C., Mishra-Sharma, S., Nord, B., et al. 2022, arXiv e-prints, arXiv:2203.08056 Fang, W., Li, B., & Zhao, G.-B. 2017, Phys. Rev. Lett., 118, 181301 Fiedler, F. & Lucia, S. 2023, Improved uncertainty quantification for neural net- works with Bayesian last layer Fluri, J., Kacprzak, T., Sgier, R., Refregier, A., & Amara, A. 2018, J. Cosmology Astropart. Phys., 2018, 051 Fortuin, V., Garriga-Alonso, A., Ober, S. W., et al. 2022, in International Confer- ence on Learning Representations Gal, Y. 2016, PhD thesis, University of Cambridge García-Farieta, J. E., Hellwing, W. A., Gupta, S., & Bilicki, M. 2021, Phys. Rev. D, 103, 103524 García-Farieta, J. E., Marulli, F., Moscardini, L., Veropalumbo, A., & Casas- Miranda, R. A. 2020, MNRAS, 494, 1658 García-Farieta, J. E., Marulli, F., Veropalumbo, A., et al. 2019, MNRAS, 488, 1987 Graves, A., ed. 2011, Practical Variational Inference for Neural Networks, ed. A. Graves, Vol. 24 (Curran Associates, Inc.) Gunapati, G., Jain, A., Srijith, P. K., & Desai, S. 2022, Publications of the Astro- nomical Society of Australia, 39, e001 Guo, C., Pleiss, G., Sun, Y., & Weinberger, K. Q. 2017, in Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML 17 (JMLR.org), 1321–1330 Article number, page 12 of 13Author: García-Farieta, Hortúa & Kitaura Gupta, S., Hellwing, W. A., Bilicki, M., & García-Farieta, J. E. 2022, Phys. Rev. D, 105, 043538 Hagstotz, S., Costanzi, M., Baldi, M., & Weller, J. 2019, MNRAS, 486, 3927 Hamaus, N., Pisani, A., Sutter, P. M., et al. 2016, Phys. Rev. Lett., 117, 091302 Harnois-Déraps, J., Martinet, N., Castro, T., et al. 2021, MNRAS, 506, 1623 Henning, C., D’Angelo, F., & Grewe, B. F. 2021, Are Bayesian neural networks intrinsically good at out-of-distribution detection? Hernández-Aguayo, C., Hou, J., Li, B., Baugh, C. M., & Sánchez, A. G. 2019, MNRAS, 485, 2194 Hernández-Aguayo, C., Ruan, C.-Z., Li, B., et al. 2022, J. Cosmology Astropart. Phys., 2022, 048 Hikage, C., Schmalzing, J., Buchert, T., et al. 2003, PASJ, 55, 911 Hockney, R. W. & Eastwood, J. W. 1981, Computer Simulation Using Particles (crc Press) Hortua, H. J. 2021, arXiv e-prints, arXiv:2112.11865 Hortúa, H. J., García, L., & Castaneda, L. 2023, Front. Astron. Space Sci., 10 Hortúa, H. J., Malagò, L., & Volpi, R. 2020a, Machine Learning: Science and Technology, 1, 035014 Hortúa, H. J., Volpi, R., Marinelli, D., & Malagò, L. 2020b, Physical Review D, 102 Howlett, C., Manera, M., & Percival, W. J. 2015, Astronomy and Computing, 12, 109 Hu, J., Shen, L., Albanie, S., Sun, G., & Wu, E. 2019, Squeeze-and-Excitation Networks Hu, W. & Sawicki, I. 2007, Phys. Rev. D, 76, 064004 Ivarsen, M. F., Bull, P., Llinares, C., & Mota, D. 2016, A&A, 595, A40 Jennings, E., Baugh, C. M., Li, B., Zhao, G.-B., & Koyama, K. 2012, MNRAS, 425, 2128 Johnson, A., Blake, C., Dossett, J., et al. 2016, MNRAS, 458, 2725 Kacprzak, T., Kirk, D., Friedrich, O., et al. 2016, MNRAS, 463, 3653 Kilbinger, M. 2015, Reports on Progress in Physics, 78, 086901 Kingma, D. P. & Ba, J. 2014, arXiv e-prints, arXiv:1412.6980 Kitaura, F.-S., Ata, M., Rodríguez-Torres, S. A., et al. 2021, MNRAS, 502, 3456 Koda, J., Blake, C., Beutler, F., Kazin, E., & Marin, F. 2016, MNRAS, 459, 2118 Kodi Ramanah, D., Charnock, T., Villaescusa-Navarro, F., & Wandelt, B. D. 2020, Monthly Notices of the Royal Astronomical Society, 495, 4227–4236 Kratochvil, J. M., Lim, E. A., Wang, S., et al. 2012, Phys. Rev. D, 85, 103513 Laszlo, I. & Bean, R. 2008, Phys. Rev. D, 77, 024048 Lavaux, G. & Wandelt, B. D. 2010, MNRAS, 403, 1392 Laves, M.-H., Ihler, S., Fast, J. F., Kahrs, L. A., & Ortmaier, T. 2020, in Medical Imaging with Deep Learning Lazanu, A. 2021, Journal of Cosmology and Astroparticle Physics, 2021, 039 Lewis, A. 2019, arXiv e-prints, arXiv:1910.13970 Li, B., Zhao, G.-B., Teyssier, R., & Koyama, K. 2012, J. Cosmology Astropart. Phys., 2012, 051 Li, X.-D., Park, C., Sabiu, C. G., & Kim, J. 2015, MNRAS, 450, 807 Louizos, C. & Welling, M. 2017, in Proceedings of the 34th International Confer- ence on Machine Learning - Volume 70, ICML’17 (JMLR.org), 2218–2227 Luo, X., Wu, Z., Li, M., et al. 2019, ApJ, 887, 125 Lyall, S., Blake, C., Turner, R., Ruggeri, R., & Winther, H. 2023, MNRAS, 518, 5929 Mancarella, M., Kennedy, J., Bose, B., & Lombriser, L. 2022, Phys. Rev. D, 105, 023531 Merten, J., Giocoli, C., Baldi, M., et al. 2019, MNRAS, 487, 104 Moresco, M., Amati, L., Amendola, L., et al. 2022, Living Reviews in Relativity, 25, 6 Nojiri, S., Odintsov, S. D., & Oikonomou, V. K. 2017, Phys. Rep., 692, 1 Paillas, E., Cai, Y.-C., Padilla, N., & Sánchez, A. G. 2021, MNRAS, 505, 5731 Pan, S., Liu, M., Forero-Romero, J., et al. 2020, Science China Physics, Mechan- ics, and Astronomy, 63, 110412 Park, C. & Kim, Y.-R. 2010, ApJ, 715, L185 Peebles, P. J. E. 2001, in Astronomical Society of the Pacific Conference Series, Vol. 252, Historical Development of Modern Cosmology, ed. V. J. Martínez, V. Trimble, & M. J. Pons-Bordería, 201 Peel, A., Lalande, F., Starck, J.-L., et al. 2019, Phys. Rev. D, 100, 023508 Peel, A., Lin, C.-A., Lanusse, F., et al. 2017, A&A, 599, A79 Perico, E. L. D., Voivodic, R., Lima, M., & Mota, D. F. 2019, A&A, 632, A52 Philcox, O. H. E., Slepian, Z., Hou, J., et al. 2022, MNRAS, 509, 2457 Planck Collaboration, Aghanim, N., Akrami, Y., et al. 2020, A&A, 641, A6 Puchwein, E., Baldi, M., & Springel, V. 2013, MNRAS, 436, 348 Ravanbakhsh, S., Oliva, J., Fromenteau, S., et al. 2017, arXiv e-prints, arXiv:1711.02033 Sønderby, C. K., Raiko, T., Maaløe, L., Sønderby, S. K., & Winther, O. 2016, in Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS’16 (Red Hook, NY, USA: Curran Associates Inc.), 3745–3753 Song, Y.-S., Hu, W., & Sawicki, I. 2007, Phys. Rev. D, 75, 044004 Takada, M. & Jain, B. 2003, MNRAS, 340, 580 Tamosiunas, A., Winther, H. A., Koyama, K., et al. 2021, MNRAS, 506, 3049 Tassev, S., Zaldarriaga, M., & Eisenstein, D. J. 2013, J. Cosmology Astropart. Phys., 2013, 036 Touati, A., Satija, H., Romoff, J., Pineau, J., & Vincent, P. 2018, arXiv e-prints, arXiv:1806.02315 Tsujikawa, S. 2008, Phys. Rev. D, 77, 023507 Tsujikawa, S., Uddin, K., Mizuno, S., Tavakol, R., & Yokoyama, J. 2008, Phys. Rev. D, 77, 103009 Van Waerbeke, L., Mellier, Y., Radovich, M., et al. 2001, A&A, 374, 757 Veropalumbo, A., Binetti, A., Branchini, E., et al. 2022, J. Cosmology Astropart. Phys., 2022, 033 Villaescusa-Navarro, F., Hahn, C., Massara, E., et al. 2020, ApJS, 250, 2 Voivodic, R., Lima, M., Llinares, C., & Mota, D. F. 2017, Phys. Rev. D, 95, 024018 Watson, J., Andreas Lin, J., Klink, P., Pajarinen, J., & Peters, J. 2021, in Proceed- ings of Machine Learning Research, Vol. 130, Proceedings of The 24th Inter- national Conference on Artificial Intelligence and Statistics, ed. A. Banerjee & K. Fukumizu (PMLR), 1198–1206 Weinberg, D. H., Mortonson, M. J., Eisenstein, D. J., et al. 2013, Phys. Rep., 530, 87 Winther, H. A., Koyama, K., Manera, M., Wright, B. S., & Zhao, G.-B. 2017, J. Cosmology Astropart. Phys., 2017, 006 Zhang, H., Samushia, L., Brooks, D., et al. 2022, MNRAS, 515, 6133 Zhang, Z., Gu, G., Wang, X., et al. 2019, ApJ, 878, 137 Article number, page 13 of 13