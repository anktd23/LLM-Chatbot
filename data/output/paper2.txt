INTERPRETATION OF HIGH-DIMENSIONAL LINEAR REGRESSION: EFFECTS OF NULLSPACE AND REGULARIZATION DEMONSTRATED ON BATTERY DATA Joachim Schaeffer Technical University of Darmstadt Control and Cyber-Physical Systems Laboratory Karolinenpl. 5 Darmstadt, 64289, Germany Eric Lenz Technical University of Darmstadt Control and Cyber-Physical Systems Laboratory Karolinenpl. 5 Darmstadt, 64289, Germany William C. Chueh Stanford University 450 Jane Stanford Way Stanford, 94305, CA, USA Martin Z. Bazant Massachusetts Institute of Technology 77 Massachusetts Avenue Cambridge, 02139, MA, USA Rolf Findeisen Technical University of Darmstadt Control and Cyber-Physical Systems Laboratory Karolinenpl. 5 Darmstadt, 64289, Germany Richard D. Braatz Massachusetts Institute of Technology 77 Massachusetts Avenue Cambridge, 02139, MA, USA braatz@mit.edu ABSTRACT High-dimensional linear regression is important in many scientific fields. This article considers discrete measured data of underlying smooth latent processes, as is often obtained from chemical or biological systems. Interpretation in high dimensions is challenging because the nullspace and its interplay with regularization shapes regression coefficients. The data’s nullspace contains all coefficients that satisfy Xw = 0, thus allowing very different coefficients to yield identical predic- tions. We developed an optimization formulation to compare regression coefficients and coefficients obtained by physical engineering knowledge to understand which part of the coefficient differences are close to the nullspace. This nullspace method is tested on a synthetic example and lithium-ion battery data. The case studies show that regularization and z-scoring are design choices that, if chosen corresponding to prior physical knowledge, lead to interpretable regression results. Otherwise, the combination of the nullspace and regularization hinders interpretability and can make it impossible to obtain regression coefficients close to the true coefficients when there is a true underlying linear model. Furthermore, we demonstrate that regression methods that do not produce coefficients orthog- onal to the nullspace, such as fused lasso, can improve interpretability. In conclusion, the insights gained from the nullspace perspective help to make informed design choices for building regression models on high-dimensional data and reasoning about potential underlying linear models, which are important for system optimization and improving scientific understanding. Keywords Interpretable Machine Learning · Linear Regression · High Dimensions · Nullspace · Functional Data · Regression Coefficients · Lithium-Ion Batteries arXiv:2309.00564v1 [stat.ML] 1 Sep 2023Nullspace Perspective on Regression Coefficients 1 Introduction Many important regression problems have the dimensionality of the data p much larger than the sample size n [1]–[4]. Consequently, p ≫ n and the matrix X ∈ Rn×p of predictors is “wide”. This case arises, for example, in most spectroscopies, lithium-ion batteries [5], [6], brain imaging, and computational biology [1], [3], [7]. Classical literature on linear regression [8]–[10] focuses mainly on the case where p < n and mostly assumes full column rank; however, many linear regression methods work well with wide predictor matrices. While Ordinary Least Squares (OLS) is not defined for wide data matrices because X⊤X is singular, the related minimum norm solution [11] can be used instead. Ridge Regression (RR) and other shrinkage-based regression methods (e.g., Least Absolute Shrinkage and Selection Operator (lasso), Elastic Net (EN)) do not suffer from this problem due to the penalty term that is added to the main diagonal of X⊤X. The fused lasso, a generalization of the lasso, adds an L1-norm penalty of adjacent regression coefficient differences to the objective function [12]. This additional penalty encourages piecewise constant regression coefficients, i.e., sparsity in regression coefficient differences. Thus it is required that the predictors can be ordered in some meaningful way. Latent variable methods such as Partial Least Squares (PLS) and Principal Component Regression (PCR) are popular choices for high-dimensional regression in the chemometrics community. A key question is how to interpret high-dimensional linear regression results and the corresponding regression coefficients. In particular, how to reason about an underlying (linear) model for scientific insights and system optimization? Technically, regression coefficients for a linear model can be analyzed and compared to engineering or scientific expectations in terms of shape (e.g., peaks, plateaus, slopes), which is often done implicitly by engineers when looking at regression coefficients. However, as shown in this article, such an interpretation can lead to misleading conclusions. This article develops a method, based on the nullspace of the predictor matrix N(X), for comparing coefficients obtained by different methods with each other for the case of high-dimensional data that was generated by a smooth latent process, also called functional data [13].1 We use the fact that N(X) consists of all solutions to Xw = 0 and thus the predictions do not change when adding a vector of the nullspace to the regression coefficients X(β + w) = Xβ. The nullspace and its interplay with regularization significantly influence the shape of the regression coefficients. Therefore, an understanding of the effect of the nullspace is needed for interpretation and scientific understanding. Our objective is to support such an understanding with this article. The next section briefly introduces the key linear regression methods used in this article. Then the nullspace approach is derived. Subsequently, case studies are presented on fully synthetic data, lithium-ion battery data with two different synthetic linear responses, and the measured nonlinear cycle life response [15]. The conclusion section summarizes the key learnings. All code and data used in this article are open-source and open-access, allowing the reproduction of results. 2 Motivation and Linear Regression Linear, static models, assuming mean-centered data, have the general form y = Xβ∗ + ϵ (1) where the input data matrix X ∈ Rn×p, n is the number of observations, p is the number of predictors, and we assume that p ≫ n. Our work is motivated by measurements of chemical or biochemical systems, i.e., discrete, noisy measurements of an assumed smooth underlying process. Consequently, we assume a latent model structure, i.e., X (independently of y) can be approximated in a lower dimensional space, and X is not sparse. Most of the analysis in this article is technically not limited to this assumption. However, the nullspace perspective is motivated by a latent model structure and the high multicollinearity of columns that arises from functional data. The coefficients β ∈ Rp contain the relation between X and y. The errors ϵ ∈ Rp are assumed to be homoscedastic, to have zero means, and to be uncorrelated. Linear regression denotes statistical methods to determine ˆβ from data X and y minimizing the error ˆϵ concerning a defined measure of the error, y = ˆy + ˆϵ = X ˆβ + ˆϵ. (2) The objective of regression methods is to find a ˆβ that yields predictions that are reasonably close to the predictions of β∗ when applied to independent data, i.e., were not available during training. When a true underlying linear model 1Measured data from chemical and other systems often exhibit a certain degree of smoothness and can be considered to originate from discretized functions (similar to the assumptions made in [14]). The term smoothness, as used in this article, refers to data in which neighboring values are linked to each other to some extent, are not too different from one another, and there exists an underlying function that is differentiable once or multiple times. 2Nullspace Perspective on Regression Coefficients exists, interpretation and scientific insights would be supported by achieving a different goal, which is to reconstruct the true coefficients, i.e., ˆβ = β∗, where β∗ are the true coefficients of the model. As shown in [6], often columns in high-dimensional functional data are correlated, and regularized regression will find a solution that is optimal for its objective function; however, the resulting regression coefficients can be visually very different from β∗ due to the interplay of the regularization and the nullspace, N(X). Furthermore, in practice, β∗ is not known and the true underlying system might be nonlinear, requiring a thorough understanding of the interplay of regularization and the nullspace to draw reasonable conclusions about the underlying model. Generally, the regression coefficients associated with ˆβModel are random variables because X and y are realizations from a system that contains randomness (e.g., measurement errors, random system processes, etc.). One approach to model the regression coefficients probabilistically is Bayesian linear regression which places a prior on the regression coefficients and yields their posterior distribution, conditioned on data, which can then be analyzed (e.g., see [16] for more information on Bayesian linear regression for high-dimensional data). While probabilistic modeling of regression coefficients is important, we focus on analyzing linear regression methods that do not model regression coefficients probabilistically because chemical engineers commonly use non-probabilistic models. We use βModel to denote that it is a realization of the random variable by the “Model” and specific training data. From here, we drop the “hat” notation because it is clear from the model name in the superscript that the coefficients were obtained by regression from data. OLS regression estimates with the closed-form solution βOLS = (X⊤X)−1X⊤y for the case p < n have low bias and are optimal under the assumption of the Gauss-Markov theorem. However, the regression coefficients β have a very large variance if the condition number of X⊤X is large, as is the case for many real-world data analytics problems, resulting in low prediction accuracy on unseen data. RR addresses this problem by adding the squared L2-norm of the weights as a penalty to the least-squares objective [17]: min β ∥y − Xβ∥2 2 + λ∥β∥2 2, (3) yielding the closed-form solution βRR = (X⊤X + λI)−1X⊤y. The regularization penalty adds to the main diagonal of X⊤X and ensures that the resulting matrix is also invertible in the case p > n. RR improves the model’s generalization by introducing a bias that reduces variance in the estimated parameters [18]. For p < n and λ → 0, RR converges to OLS. In the more general case, without making assumptions about the dimensionality and rank of the real matrix X, Singular Value Decomposition (SVD), X = UΣV⊤, can be used to show that β0 = lim λ→0 βλ = X†y. (4) The full derivation and further information can be found in the Supplementary Information (SI), Sec. S1 and [4]. For the case p > n, the Moore-Penrose-Inverse X† can be written as β0 = X⊤(XX⊤) −1y. (5) This expression is known as the minimum norm solution (e.g., [11]), β0 = arg min β � ∥β∥2 2 �� ∥y − Xβ∥2 2 = 0 � . (6) For any ˜β that fulfills X ˜β = y (i.e., regression coefficients that fit the data X and y perfectly including the noise), (5) can be used to show that X( ˜β − β0) = 0, and that ( ˜β − β0)⊤β0 = ( ˜β − β0)⊤X⊤(XX⊤)−1y = (X( ˜β − β0))⊤(XX⊤)−1y = 0 (7) and consequently ( ˜β − β0) ⊥ β0 which is equivalent to N(X) ⊥ β0 [19]. Thus there exists a set of regression coefficients S that all fulfill X ˜β = y with ˜β ∈ S. Most regularized regression methods solve an optimization of the form min β ∥y − Xβ∥2 2 + F(β). (8) Regularized methods trade the perfect fit to the training data against the objective of keeping regression coefficients small. This trade-off is seen in the objective used to define the regularization methods. The orthogonality of the regression coefficients to N(X) does not hold for arbitrary regularization terms F(β). Orthogonality holds for RR, 3Nullspace Perspective on Regression Coefficients because the regularization term in (3) is always smaller for coefficients orthogonal to N(X). The PCR coefficients are orthogonal to the N(X) because all eigenvectors of X⊤X that correspond to nonzero eigenvalues are orthogonal to each other and to the nullspace. Similarly, the PLS coefficients are also orthogonal to the nullspace by construction. The pathological case of y being the nullvector must be excluded and is not relevant. Proofs for orthogonality between regression coefficients and nullspace for RR, PCR, and PLS are included in the SI, Sec. S2. However, regression coefficients obtained by the lasso and EN are not orthogonal to N(X) because of the L1-norm. For more information on regularized high-dimensional regression, see [6], [20]. Depending on the function F : Rp → R, regression coefficients obtain different shapes. Usually, methods such as RR, PCR, and PLS yield solutions that are not sparse, which can make interpretation difficult. An alternative method is the lasso, however, sparsity is often not a reasonable assumption for functional data. A generalization of the lasso is min β 1 2∥y − Xβ∥2 2 + λ∥Dβ∥1 (9) where choosing D as the identity matrix recovers the lasso. For D1 =   1 −1 0 · · · 0 0 1 −1 ... ... ... ... ... ... 0 0 · · · 0 1 −1   , (10) the resulting model is called the one-dimensional fused lasso which penalizes the L1-norm of the regression coefficients as well as their differences, but thus requires predictors that can be ordered [12], a characteristic of functional data [13]. The choice of D can incorporate expectations about the underlying model structure [21], and can thus yield models that should be interesting for many chemical engineering problems for its flexibility to incorporate assumptions, and its potential to yield easier-to-interpret regression coefficients. However, the authors are not aware that the fused lasso is currently being applied in the chemical engineering community, despite the popularity of the fused lasso in the statistics community. In the next section, we derive the nullspace method to compare the regression coefficients of different regularized models thoroughly. 3 Nullspace Method The addition of a vector v ∈ N(X), i.e., a vector in the nullspace, to any β yields coefficients with unchanged predictions. The vectors in the nullspace affect only the regularization term in the objective function. We are interested in a method for understanding the effects of the nullspace when comparing different coefficients and how such a comparison can be used to reason about underlying relationships. Consider a regularized regression model called A, ˆyA = XβA, (11) where βA is associated with method A. For any vector v in the nullspace, this equation implies that ˆyA = X(βA + v). (12) We want to compare the regression coefficients βA with other coefficients βB. The coefficients βB can either be another estimator obtained by another regression method or instead be chosen for engineering or scientific reasons (e.g., constant regression coefficients). Thus we propose finding coefficients v∗ ∈ N(X) that are closest to the difference between the coefficients under comparison β∆ = βA − βB. This approach can be formalized by min v ∥β∆ + v∥2 2 (13a) subject to Xv = 0, (13b) This optimization is a convex quadratic program with linear constraints. The solution is the projection of β∆ onto the nullspace, v∗ = (X⊤(XX⊤)−1X − I)β∆, (14) where XX⊤ is assumed to be invertible. The derivation is included in the SI, Sec. S3. The expression can be simplified by inserting the singular value decomposition X = UΣV⊤, v∗ = (VΣ⊤(ΣΣ⊤)−1ΣV⊤ − I)β∆, (15) which can be used to improve the numerical efficiency. Simplifying (15) leads to v∗ = � V � In 0 0 0 � V⊤ − I � β∆. (16) 4Nullspace Perspective on Regression Coefficients The property that V is an orthogonal matrix leads to v∗ = −V � 0 0 0 Ip−n � V⊤β∆. (17) The projection onto the nullspace can be a hard requirement that might yield a vector v∗ that is dominated by noise and difficult to interpret, in particular, if XX⊤ is ill-conditioned as is often the case for many real-world chemical engineering problems. Furthermore, regularization shapes regression coefficients by trading their variance against a bias towards zero to improve generalization. However, regularized regression coefficients usually differ from the true coefficients (if they exist), and their difference is not expected to lie exactly within the nullspace but might be close to it, motivating the relaxed optimization. We propose to reformulate the optimization in (13) to allow deviations from the nullspace, min v ∥β∆ + v∥2 2 + γ∥Xv∥2 2, (18) where γ is a nonnegative scalar. Setting the derivative of (18) with respect to v to zero gives vγ = −(γX⊤X + I)−1β∆. (19) For γ = 0, the nullspace is not considered and β∆ = v0. For γ → ∞, the optimization converges to (17), as seen by lim γ→∞ −(γX⊤X + I)−1β∆ = lim γ→∞ −V(γΣ⊤Σ + I)−1V⊤β∆ = −V � 0 0 0 Ip−n � V⊤β∆. (20) Analyzing the nullspace, i.e., comparing the coefficients βA and βA + vγ with βB, allows to identify which differences can be removed with a vector that is close to the nullspace and which differences would require significant deviations from the nullspace and are thus mainly responsible for the differences of the associated predictions. We propose to select γ, i.e., the penalization strength for deviations from the nullspace, based on a change in prediction accuracy to make it easier to interpret the result. That is, we define γ based on the Normalized-Root-Mean-Square Error (NRMSE) defined by s = max i {yi} − min i {yi} (21) L(ˆy, y) = 1 s√n∥ˆy − y∥2, (22) leading to the heuristic: max γ γ (23) subject to ��L(X(βA + vγ), y) − L(X(βA), y) ��≤ c vγ = −(γX⊤X + I)−1β∆ where c defines the maximum loss function change introduced by the nullspace approach that is considered acceptable. The optimization (23) is not convex for most practical examples but is easy to solve because it only has one degree of freedom, γ.2 4 Case Studies This section demonstrates the nullspace method on several example cases to derive insights for interpretation of regression coefficients. The data X and y are generated synthetically for the first example. The second and third examples are on data from lithium-ion batteries [15], where we use constructed response variables by assuming different linear relationships to showcase the differences between regression coefficients and true coefficients. The last example uses the measured cycle life response where the true relationship between X and y is unknown. 2For example, the optimization can be solved by plotting the left-hand side of the inequality with respect to γ. 5Nullspace Perspective on Regression Coefficients 4.1 Synthetic Parabolic Data The parabolic example is inspired by measurements of some quantity over a continuous domain (e.g., time, concentration, voltage) to keep the data and relationships simple. The data are drawn from xi = aid ⊙ d, i ∈ {1, 2, 3, · · · , 50}, (24) d = [1.0, 1.01, 1.02, · · · , 3.0], (25) X∗ = [x1, x2, · · · xn]⊤ , where d is the vector of discretizations on the underlying domain, with a constant spacing of 0.01 and a length of p = 201, and ⊙ is the element-wise product. The parameters ai ∼ N(µ, σ2) with µ = 0.3 and σ = 0.3. Consequently, X ∈ R50×201. We define the response as y∗ = X∗β∗, with β∗ = 1 pI. (26) The true coefficients are thus equal. Subsequently, we add white Gaussian noise to the data and response xi = x∗ i + ϵxi (27) y = y∗ + ϵy (28) yielding the matrices X and y for use in regression. The added noise ϵxi ∈ R201 is chosen such that the average Signal- to-Noise Ratio (SNR) of each sample (xi) is 50 and the SNR of y is 50 as well. Figure 1a shows the mean-centered 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 3.00 d 4 2 0 2 4 6 X a) Data Training 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 3.00 d 0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008 Coefficients b) Nullspace Perspective PLS 1 , NRMSE: 0.105% PLS 1 +v , 10.0, NRMSE : 0.104% * , NRMSE: 0.105% Figure 1: a) Mean centered parabolic data, with white Gaussian noise corresponding to an SNR of 50 added to X and y prior to mean centering, b) true coefficients in black and regression coefficients in green and nullspace-modified PLS coefficients in magenta. data, where each line corresponds to a matrix row. The 201 individual data points of each row are connected with a line, which is a reasonable visualization because of the underlying functional structure. We picked a PLS model with one component to learn the relationship between X and y. The PLS method is popular among chemical engineers, and its regularization parameter, the number of components, is discrete and simple to choose. Figure 1b shows that the true coefficients and the PLS coefficients have very different shapes. However, their predictions and prediction accuracies are almost identical (cf. SI, Sec. S4). The noise leads to a prediction error even when the true coefficients are used (i.e., 0.105% NRMSE). Using the proposed nullspace method with hand-selected γ = 10 to compare the true coefficients with the PLS coefficients shows that the resulting vector v10 is very close to the nullspace, i.e., does not significantly change the prediction accuracy and yields the adjusted coefficients βPLS 1 + v10 that are very similar to the true coefficients β∗. While βPLS 1 is orthogonal to the nullspace, βPLS 1 + v10 is not orthogonal to the nullspace. Due to the simple underlying structure of the data and the model, the PLS coefficients yield a similar prediction accuracy. However, the PLS coefficients have a smaller L2-norm, i.e., ∥βPLS 1 ∥2 2 < ∥β∗∥2 2, due to the implicit regularization of PLS. Assume that the coefficients are expected to be piecewise constant for physical reasons. We can then reformulate the regression as a generalized lasso problem with the matrix D in (9) set to D1. Figure 2 shows the regression coefficients 6Nullspace Perspective on Regression Coefficients 1.75 2.00 2.25 2.50 2.75 3.00 d Data 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 3.00 d 0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008 Coefficients Nullspace Perspective RR CV1SE, NRMSE: 0.117% RR CV1SE+v , 0.1, NRMSE : 0.116% FL CV1SE, NRMSE: 0.104% Figure 2: Ridge coefficients in green and fused lasso coefficients in black based on CV and the one-standard-error rule. Nullspace- modified ridge coefficients in magenta. associated with ridge regression and the fused lasso. The regularization parameter was chosen by Cross-Validation (CV) and the one-standard-error rule [3]. Figure 2 looks remarkably similar to Fig. 1. The fused lasso coefficients are nearly identical to the true coefficients, and the ridge coefficients are similar to the PLS coefficients with one component but slightly noisier. From the data alone, it is not possible to state whether y was constructed from constant or parabolic coefficients. Furthermore, regression coefficients obtained from methods that are orthogonal to the nullspace can yield coefficients that appear to disagree with prior knowledge at first sight. As this example shows, methods that are not orthogonal to the nullspace such as the fused lasso can be advantageous for interpretation and conclusions if selected based on prior knowledge. 4.2 Lithium-Ion Battery Data As a real-world measurement data example, we consider a Lithium Iron Phosphate (LFP) battery dataset, which contains cycling data for 124 batteries [15]. Each battery has a fixed charging and discharging protocol. The charging protocols vary widely between the cells, whereas the discharge is constant and identical for all cells. The objective of the original paper was the prediction of the cycle life, i.e., the number of cycles until the battery’s capacity drops below 80% of its nominal capacity. Features based on the difference between the discharge capacity of voltage curves for two cycles, subsequently called ∆Qa−b, were shown to linearly correlate well with the logarithm of the cycle life. For this case study, we use the cycle pair a = 100 and b = 10, as done in [15]. Furthermore, we denote by a tilde (∆ �Q100−10) that the columns are mean centered. The dimensionality ∆Q100−10 ∈ R41×1000 due to the high resolution of the discharge capacity over the voltage domain. More information about the data set and reasoning about the modeling objective can be found in [15]. Figure 3a shows the LFP data set, partitioned into training, primary, and secondary test data as suggested in [15]. Figure 3b shows the mean subtracted training data. The data of the shortest-lived battery is clearly separated from the remainder of the data set. However, we keep this battery in the data set, as its influence on the training is benign. Figure 3c shows the z-scored training data (i.e., standardized data, yielding unit variance columns). The unit (Ah) is lost by z-scoring the data. Usually, z-scoring is recommended for data with features that have different units and thus might vary by orders of magnitude. However, for functional high-dimensional data, the unit of all columns is the same. Nevertheless, the measured values can vary by order of magnitude. Figure 3c shows that the noise in the voltage region 3.2–3.5 V is amplified by rescaling because of a lower signal-to-noise ratio in this voltage region. A more detailed analysis of the SNR can be found in the SI, Sec. S5. However, whether z-scoring is useful does not only depend on the data matrix X but also on its underlying relationship with y, which we explore next on synthetic responses y before moving to the cycle life response. 7Nullspace Perspective on Regression Coefficients 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 Voltage (V) 0.14 0.12 0.10 0.08 0.06 0.04 0.02 0.00 Q100 10 (Ah) a) Data Training Test Test 2 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 Voltage (V) 0.10 0.08 0.06 0.04 0.02 0.00 0.02 0.04 Q100 10 (Ah) b) Mean Centered Training Data Training 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 Voltage (V) 5 4 3 2 1 0 1 2 QSTD 100 10 c) Z-Scored Training Data Training Figure 3: a) LFP Discharge capacity difference between cycle 100 and 10, data split into training, primary and secondary test set, b) mean centered training data, c) z-scored training data. 4.2.1 Synthetic Response Constant Coefficients. The response for this example is the sample mean defined in (26), with p = 1000 to match the dimensionality of the LFP data set with added white Gaussian noise corresponding to an SNR of 50. Figure 4a shows 3.0 3.2 3.4 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 Voltage (V) 0.0000 0.0002 0.0004 0.0006 0.0008 0.0010 0.0012 0.0014 Coefficients a) Nullspace Perspective PLS 2 , NRMSE: 0.159% PLS 2 +v , 1.45e+03, NRMSE : 0.149% * , NRMSE: 0.127% 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 d 5 4 3 2 1 0 1 2 Q100 10 Data Train 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 Voltage (V) 0.0 0.5 1.0 1.5 2.0 Coefficients 1e 5 b) Nullspace Perspective, Z-Scored Data PLS 4 , NRMSE: 0.108% PLS 4 +v , 2.7e-04, NRMSE : 0.118% * , NRMSE: 0.127% Figure 4: a) True coefficients in black, PLS coefficients based on CV and the one-standard-error deviation rule in green, nullspace- modified coefficients in magenta, b) nullspace perspective similar to a, with PLS coefficients estimated with the one-standard-error rule corresponding to z-scored data. the nullspace perspective for the constant coefficient response with the data on the original scale, and Fig. 4b based on z-scored data (i.e., columns of X are scaled to have a unit standard deviation). The number of PLS components is determined by CV and the one-standard-error rule. The PLS model associated with the z-scored data needs more components. The nullspace penalization parameter γ was chosen in both cases to yield c = 0.01% NRMSE prediction error change. Figure 4a shows the differences between the true coefficients and the PLS model’s regression coefficients in the section from 2.0–3.1 V are relatively close to one another; however, some differences remain. The differences in the voltage region from 3.2 to 3.5 V only have a minor effect on the prediction results. Most of the difference between the regression coefficients in this area is associated with the nullspace, indicated by the large difference between the nullspace-modified PLS coefficients in magenta and the original PLS coefficients in green. Thus, the differences in the region 3.2 to 3.5 V do not change the prediction results on the training data significantly and arise due to the interplay of the regularization objective with the nullspace. When the data are z-scored, most of the visible differences between the PLS coefficients in green and the true coefficients in black are contained in the enlarged nullspace (Fig. 4b). The modified coefficients match the true coefficients very well. The prediction error difference between the PLS coefficients in green and the nullspace-modified coefficients in magenta is 0.01% NRMSE. The remaining differences between the true coefficients in black and the modified coefficient in magenta are barely visible but are responsible for another 0.01% NRMSE prediction error change, highlighting the effect of the nullspace. Comparing Figs. 4a and 4b shows that, in case the true coefficients are constant (i.e., all columns are equally important), z-scoring can help regression to yield coefficients that are more similar to the true coefficients. 8Nullspace Perspective on Regression Coefficients Column Mean Coefficients. The true coefficient vector β∗ for the next synthetic example is the column mean of the data X prior to column centering β∗ j = 1 n n � i=1 xi,j, (29) β∗ = �β∗ 1, β∗ 2, · · · , β∗ p �⊤. The PLS model with 6 components associated with the z-scored data picks up a high amount of noise in the voltage regions from 3.3 to 3.5 V (Fig. 5b). In contrast, the PLS model with 3 components associated with the data on the original scale converges well to the true coefficients over the entire voltage region (Fig. 5a). The small differences are very closely associated with the nullspace. Here, z-scoring amplifies and feeds noise into the model, manifesting as the spiky regression coefficients, with the most extreme spikes in the voltage regions from 3.2 to 3.5 V (Fig. 5b). Still, the PLS model associated with the z-scored data has approximately the same prediction accuracy as the PLS model associated with the original data. 3.0 3.2 3.4 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 Voltage (V) 0.04 0.03 0.02 0.01 0.00 Coefficients a) Nullspace Perspective PLS 3 , NRMSE: 0.101% PLS 3 +v , 8.4, NRMSE : 0.106% * , NRMSE: 0.112% 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 d 5 4 3 2 1 0 1 2 Q100 10 Data Train 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 Voltage (V) 0.0008 0.0006 0.0004 0.0002 0.0000 Coefficients b) Nullspace Perspective, Z-Scored Data PLS 6 , NRMSE: 0.101% PLS 6 +v , 4.5e-04, NRMSE : 0.106% * , NRMSE: 0.112% Figure 5: a) True coefficients in black, PLS coefficients based on CV and the one-standard-error rule in green, nullspace-modified coefficients in magenta, b) nullspace perspective similar to a, with PLS coefficients estimated with the one-standard-error rule corresponding to z-scored data. Suppose there was some prior evidence or physical intuition that the true coefficients are constant or at least of similar magnitude. Then, z-scoring feeds the assumption that all the columns’ importance is in the same order of magnitude to the model. However, if the coefficients are expected to vary by an order of magnitude (e.g., as is the case for the true coefficients being the column mean of the data), then not z-scoring the data accounts for the assumption that the scale of the columns is correlated with the assumed underlying true coefficients. The two examples show that the potential effects of z-scoring on the regression coefficients should be considered carefully for functional data. When data are z-scored, the model can become better at learning the underlying relationship, but noise may be amplified, depending on the noise structure. 4.2.2 Measured Cycle Life Response The measured response associated with the LFP battery data is the cycle life. We train the models by using the logarithm of the cycle life and use the same training, primary test, and secondary test set as suggested in [15]. We determine the regularization parameter based on the minimum CV error and do not employ the one-standard-error rule.3 The PLS coefficients with five components have a similar shape as the fused lasso coefficients (cf. Figs. 6ab). However, the PLS coefficients have high-frequency perturbations, in particular, in the voltage range from 2.9–3.2 V, which is likely due to noise, making the PLS coefficients harder to interpret. The fused lasso coefficients (Fig. 6b) clearly indicate three regions of importance, enabling a physical interpretation. The range around 2.0–2.1 V is associated with the capacity change of the cell between cycles 10 and 100. Around 2.4 V, a different pattern can be seen in the data (Fig. 3), corresponding to the negative peak in the regression coefficients, which may correspond to LFP cathode degradation associated with iron anti-site defects, as the free energy of reaction (overpotential times charge) exceeds 3The standard deviation of the CV error is large due to the long-living cells that heavily influence the prediction performance, which would lead to overly conservative regularization estimates. 9Nullspace Perspective on Regression Coefficients 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 Voltage (V) 3 2 1 0 1 Coefficients a) PLS 5 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 Voltage (V) b) FL D1 Figure 6: Cross-validated regression coefficients, original data (cf. Fig. 3b): a) PLS coefficients, b) fused lasso coefficients. their formation energy ∼0.55 eV [22]. This interpretation is also consistent with experiments showing that chemical reduction of LFP by citric acid is able to heal iron anti-site defects with a similar free energy of reaction of 0.58 eV [23]. The voltage range around 2.9–3.3 V contains most of the regression coefficient peaks. The two dominant plateaus of the Open-Circuit Voltage (OCV), which result from the single broad plateau of LFP superimposed with two more narrow plateaus of graphite, are located here, and most of the cell’s capacity is discharged in this voltage range. These voltage plateaus correspond to phase transformations of the porous electrodes [24], specifically between the low and high-density stable phases of LFP, as well as between stages 1, 2, and 3 of lithiated graphite [25]. The fused-lasso coefficients showcase three distinct negative and positive peaks, corresponding to changes in the rate-dependent tilt of the voltage plateaus, which may result from changes in particle-size-dependent nucleation barriers and population dynamics of reaction-controlled phase transformations [25], [26]. The peak width and height can be interpreted as a weighted sum of the average slopes of the data between the respective peaks. On low-rate data, the position and magnitude of peaks in the incremental capacity analysis correspond to different degradation modes [27]. The peaks and peak shifts of the incremental capacity analysis blur out at higher C-rates, as expected from the suppression of phase separation by driven auto-inhibitory reactions [28]. In particular, the decreasing reaction rate with increasing lithium concentration in the LFP cathode, which has been predicted theoretically [29] and confirmed experimentally [30], erases the voltage plateaus at high rates and causes more homogeneous reactions that are likely favorable for battery lifetime [24]–[26]. However, the obtained regression coefficients indicate that there is degradation information in this region even in ∆Q100−10 (i.e., the discharge capacity difference of cycle 100 and 10, both at 4C) that is important for capturing past degradation and forecasting future degradation. On the other hand, if the 4C current is well into the regime of suppressed phase separation, then we would expect a negative correlation between lifetime and internal resistance of the intercalation reaction, which in turn is correlated with larger ∆Q100−10. 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 Voltage (V) 0.02 0.00 0.02 Coefficients a) PLS 9 z-scored 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 Voltage (V) b) FL D1 z-scored Figure 7: Cross-validated regression coefficients, z-scored data (cf. Fig. 3c): a) PLS coefficients, b) fused lasso coefficients. The coefficients regressed on the z-scored data have similar peaks and characteristics as the coefficients regressed on the original data (cf. Figs. 6ab, and 7ab). The z-scoring of columns introduces a linear transformation that significantly changes the regression coefficients in the range from 3.2 and 3.4 V. The fused lasso based on the z-scored columns yields high prediction accuracy and interpretable coefficients (cf. Tab. 1 and Fig. 7b). In the higher voltage region, an additional peak appears around 3.35 V, which could not be learned from the original data because of the very small variance of the data prior to rescaling in combination with regularization. Moreover, the coefficients estimated on the z-scored data have the highest prediction performance on the training, primary, and secondary test sets (Tab. 1), showcasing that there is valuable information in the higher voltage 10Nullspace Perspective on Regression Coefficients Table 1: Root-Mean-Square Error (RMSE) prediction accuracies associated with the coefficients in Figs. 6–7. Low Cycle Life (CL): yi ≤ 1200 cycles; high CL: yi > 1400 cycles. All models were trained on the entire training data. Original Scale Z-Scored Feature Set FL D1 PLS 5 Comp. FL D1 PLS 9 Comp. [31] Variance Model [15] Training (41) 68 83 62 57 104 Test 1 (42) 115 116 105 102 138 Test 2 (40) 198 217 192 174 196 Training Low CL (39) 62 82 53 50 103 Test 1 Low CL (39) 96 101 76 80 96 Test 2 Low CL (34) 135 202 115 132 119 Training High CL (2) 138 106 150 139 115 Test 1 High CL (3) 258 231 280 252 385 Test 2 High CL (6) 395 285 412 322 419 region above 3.2 V. Furthermore, both models on the z-scored data outperform the variance model suggested in [15]. The PLS model with nine components, suggested first in [31], slightly outperforms the fused lasso model when all cells are considered. However, the fused lasso yields the lowest RMSE error for both test sets when only evaluated on the shorter-lived cells. The higher performance of the PLS model with nine components on the secondary test set is thus mainly associated with the longest-living cells that are more difficult to predict (cf. [15], [31]). But, the coefficients associated with the PLS model are challenging to interpret because their sign changes frequently. What is more, the secondary test set was impacted by a longer calendar aging due to an extended storing period before the cycling started (cf. SI of [32]), making it tough to understand the higher prediction accuracy of the PLS model on the secondary test set. In particular, the PLS coefficients show further peaks in the voltage region above 3.4 V influenced at least partially by noise because the SNR in this region is very low (cf. SI Sec. S5). Similarly to the parabolic data set, we observe that the fused lasso coefficients are more interpretable than the PLS coefficients. Not requiring the coefficients orthogonal to the nullspace improves interpretability (Fig. 8ab). The 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 Voltage (V) 2 0 2 Coefficients a) FL D1 FL D1 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 Voltage (V) 0.01 0.00 0.01 0.02 0.03 b) FL D1 z-scored FL D1 z-scored Figure 8: Comparison of fused lasso coefficients (blue) and their component orthogonal to the nullspace (red): a) original data, b) z-scored data. component of the regression coefficients orthogonal to the nullspace (cf. red coefficients in Fig. 8ab) are less interpretable while making identical predictions on the training data. 5 Conclusion The article proposes a nullspace perspective for gaining insights to help make informed design choices for building regression models on high-dimensional data and for reasoning about potential underlying linear models. We demonstrate the nullspace method on a fully synthetic dataset and lithium-ion battery data with a designed linear response. Applying the nullspace insights for predicting the cycle life led to further insights into how degradation manifests itself for LFP batteries during discharge at 4C. The nullspace allows different-looking regression coefficients to yield similar predictions (Fig. 1). While z-scoring for high-dimensional functional data can be beneficial, it should be an active design choice because it can increase noise 11Nullspace Perspective on Regression Coefficients by scaling up columns with low SNR (Fig. 5). Appropriate regularization can mitigate increased noise after z-scoring. Furthermore, regularization and z-scoring must be carefully considered and correspond to prior physical knowledge to obtain interpretable regression results (Figs. 6, 7). Otherwise, the combination of the nullspace and regularization can hinder interpretability and potentially make it impossible to obtain regression coefficients close to the true coefficients. Regression methods which yield coefficients orthogonal to the nullspace, such as RR, PCR, and PLS, can be challenging to interpret. Methods that yield regression coefficients not orthogonal to the nullspace, such as the fused lasso, can be advantageous for interpretability (Fig. 8). The learnings from the nullspace analysis help to build and interpret linear regression models for high-dimensional functional data. The case studies show how to reason about underlying linear relationships between X and y, which is important for system optimization and to improve scientific understanding. Code and Data Availability The code for this article is available in the corresponding GitHub repository, HDRegAnalytics, https://github. com/JoachimSchaeffer/HDRegAnalytics. The repository contains the source code and notebooks to visualize the results. The repository contains a small subset of the LFP dataset that was published with [15] and is available at https://data.matr.io/1/. Author Contributions Joachim Schaeffer: Conceptualization, Methodology, Software, Validation, Formal Analysis, Investigation, Data Curation, Writing – original draft, Writing – review & editing, Visualization, Funding Acquisition; Eric Lenz: Formal Analysis, Writing – review & editing; William C. Chueh: Writing – review & editing; Martin Z. Bazant: Writing – review & editing; Rolf Findeisen: Resources, Writing – review & editing, Funding Acquisition; Richard D. Braatz: Conceptualization, Methodology, Resources, Writing - original draft, Writing – review & editing, Supervision, Project Administration, Funding Acquisition. Acknowledgements and Funding Initial ideas for this work were conceptualized during Joachim Schaeffer’s time at ETH Zürich, for which we acknowl- edge financial support from the German Academic Exchange Service (DAAD) within the scholarship program for Master studies abroad. The main work was carried out by Joachim Schaeffer at the Technical University of Darmstadt. The work was refined and extended during Joachim Schaeffer’s time at the Massachusetts Institute of Technology, for which we acknowledge financial support by a fellowship within the IFI program of the German Academic Exchange Service (DAAD), funded by the Federal Ministry of Education and Research (BMBF). Furthermore, financial support is acknowledged from the Toyota Research Institute through the D3BATT Center on Data-Driven-Design of Rechargeable Batteries. 12Nullspace Perspective on Regression Coefficients References [1] P. Bühlmann and S. Van De Geer, Statistics for High-Dimensional Data: Methods, Theory and Applications. Heidelberg: Springer Berlin, 2011. [2] I. M. Johnstone and D. M. Titterington, “Statistical challenges of high-dimensional data,” Philosophical Transac- tions of the Royal Society A: Mathematical, Physical and Engineering Sciences, vol. 367, no. 1906, pp. 4237– 4253, 2009. DOI: 10.1098/rsta.2009.0159. [3] T. Hastie, R. Tibshirani, and J. H. Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second. New York: Springer, 2009. [4] D. Kobak, J. Lomond, and B. Sanchez, “The optimal ridge penalty for real-world high-dimensional data can be zero or negative due to the implicit ridge regularization,” Journal of Machine Learning Research, vol. 21, no. 169, pp. 1–16, 2020. [5] N. M. Ralbovsky and I. K. Lednev, “Towards development of a novel universal medical diagnostic method: Raman spectroscopy and machine learning,” Chemical Society Reviews, vol. 49, no. 20, pp. 7428–7453, 2020. [6] J. Schaeffer and R. D. Braatz, “Latent variable method demonstrator – Software for understanding multivariate data analytics algorithms,” Computers & Chemical Engineering, vol. 167, p. 108 014, 2022. DOI: 10.1016/j. compchemeng.2022.108014. [7] A.-L. Boulesteix and K. Strimmer, “Partial least squares: A versatile tool for the analysis of high-dimensional genomic data,” Briefings in Bioinformatics, vol. 8, no. 1, pp. 32–44, 2007. DOI: 10.1093/bib/bbl016. [8] J. Groß, Linear Regression, 175 vols. Berlin Heidelberg: Springer Verlag, 2003. [9] D. C. Montgomery, E. A. Peck, and G. G. Vining, Introduction to Linear Regression Analysis, Fifth. Hoboken, N.J.: John Wiley & Sons, 2012. [10] G. A. F. Seber and A. J. Lee, Linear Regression Analysis. Hoboken, NJ: John Wiley & Sons, 2003. [11] A. Monticelli, “Least-squares and minimum norm problems,” in State Estimation in Electric Power Systems: A Generalized Approach. Boston: Springer US, 1999, pp. 15–37. [12] R. Tibshirani, M. Saunders, S. Rosset, J. Zhu, and K. Knight, “Sparsity and smoothness via the fused lasso,” Journal of the Royal Statistical Society: Series B (Statistical Methodology), vol. 67, no. 1, pp. 91–108, 2005. DOI: 10.1111/j.1467-9868.2005.00490.x. [13] J. O. Ramsay and B. W. Silverman, Functional Data Analysis, Second. New York: Springer, 2005, p. 38. [14] H. Dette and J. Tang, Statistical inference for function-on-function linear regression, arXiv preprint, https://arxiv.org/abs/2109.13603, 2021. DOI: 10.48550/ARXIV.2109.13603. [15] K. A. Severson, P. M. Attia, N. Jin, N. Perkins, B. Jiang, et al., “Data-driven prediction of battery cycle life before capacity degradation,” Nature Energy, vol. 4, no. 5, pp. 383–391, 2019. [16] E. Makalic and D. F. Schmidt, High-dimensional bayesian regularised regression with the bayesreg package, arXiv preprint, https://arxiv.org/abs/1611.06649, 2016. [17] A. E. Hoerl and R. W. Kennard, “Ridge regression: Biased estimation for nonorthogonal problems,” Technomet- rics, vol. 12, no. 1, pp. 55–67, 1970. [18] H. Zou and T. Hastie, “Regularization and variable selection via the elastic net,” Journal of the Royal Statistical Society. Series B: Statistical Methodology, vol. 67, no. 2, pp. 301–320, 2005. DOI: 10 . 1111 / j . 1467 - 9868.2005.00503.x. [19] S. Boyd and S. Lal, EE263: Introduction to Linear Dynamical Systems, Lecture 16: Least-norm Solutions of Underdetermined Equations, Department of Electrical Engineering, Stanford University, Stanford, California, 2022. [Online]. Available: http://ee263.stanford.edu/lectures.html. [20] R. Tibshirani, “Regression shrinkage and selection via the lasso,” Journal of the Royal Statistical Society: Series B (Methodological), vol. 58, no. 1, pp. 267–288, 1996. DOI: 10.1111/j.2517-6161.1996.tb02080.x. [21] R. J. Tibshirani and J. Taylor, “The solution path of the generalized lasso,” The Annals of Statistics, vol. 39, no. 3, pp. 1335–1371, 2011. DOI: 10.1214/11-AOS878. [22] R. Malik, D. Burch, M. Bazant, and G. Ceder, “Particle size dependence of the ionic diffusivity,” Nano Letters, vol. 10, no. 10, pp. 4123–4127, 2010, PMID: 20795627. DOI: 10.1021/nl1023595. [23] P. Xu, Q. Dai, H. Gao, H. Liu, M. Zhang, et al., “Efficient direct recycling of lithium-ion battery cathodes by targeted healing,” Joule, vol. 4, no. 12, pp. 2609–2626, 2020. DOI: 10.1016/j.joule.2020.10.008. [24] T. R. Ferguson and M. Z. Bazant, “Nonequilibrium thermodynamics of porous electrodes,” Journal of The Electrochemical Society, vol. 159, no. 12, A1967–A1985, 2012. DOI: 10.1149/2.048212jes. [25] T. R. Ferguson and M. Z. Bazant, “Phase transformation dynamics in porous battery electrodes,” Electrochimica Acta, vol. 146, pp. 89–97, 2014. DOI: 10.1016/j.electacta.2014.08.083. 13Nullspace Perspective on Regression Coefficients [26] Y. Li, F. El Gabaly, T. R. Ferguson, R. B. Smith, N. C. Bartelt, et al., “Current-induced transition from particle- by-particle to concurrent intercalation in phase-separating battery electrodes,” Nature Materials, vol. 13, no. 12, pp. 1149–1156, 2014. DOI: 10.1038/nmat4084. [27] M. Dubarry, C. Truchot, and B. Y. Liaw, “Synthesize battery degradation modes via a diagnostic and prognostic model,” Journal of Power Sources, vol. 219, pp. 204–216, 2012. DOI: 10.1016/j.jpowsour.2012.07.016. [28] M. Z. Bazant, “Thermodynamic stability of driven open systems and control of phase separation by electro- autocatalysis,” Faraday Discuss., vol. 199, pp. 423–463, 2017. DOI: 10.1039/C7FD00037E. [29] D. Fraggedakis, M. McEldrew, R. B. Smith, Y. Krishnan, Y. Zhang, et al., “Theory of coupled ion-electron transfer kinetics,” Electrochimica Acta, vol. 367, p. 137 432, 2021. DOI: 10.1016/j.electacta.2020.137432. [30] H. Zhao, H. D. Deng, A. E. Cohen, J. Lim, Y. Li, et al., Learning heterogeneous reaction kinetics from x-ray movies pixel-by-pixel, Preprint (Version 1) available at Research Square, 2022. DOI: 10.21203/rs.3.rs- 2320040/v1. [31] P. M. Attia, K. A. Severson, and J. D. Witmer, “Statistical learning for accurate and interpretable battery lifetime prediction,” Journal of The Electrochemical Society, vol. 168, no. 9, p. 090 547, 2021. DOI: 10.1149/1945- 7111/ac2704. [32] P. M. Attia, A. Grover, N. Jin, K. A. Severson, T. M. Markov, et al., “Closed-loop optimization of fast- charging protocols for batteries with machine learning,” Nature, vol. 578, no. 7795, pp. 397–402, 2020. DOI: 10.1038/s41586-020-1994-5. 14Nullspace Perspective on Regression Coefficients Supplementary Information for “Interpretation of High-Dimensional Linear Regression: Effects of Nullspace and Regularization Demonstrated on Battery Data” S1 Ordinary Least Squares and Minimum Norm Solution Ordinary Least Squares Regression finds a solution βOLS that minimizes the L2-norm of the regression error ˆϵ [1], min β ∥y − Xβ∥2 2. (S.1) If p < n and rank(X) = p, it follows that X⊤X is invertible, and (S.1) can be solved analytically to give βOLS = (X⊤X)−1X⊤y. (S.2) OLS estimates have low bias and are optimal under the assumption of the Gauss-Markov theorem. However, they have a very large variance if the condition number for inversion of X⊤X is large, as it the case for many real-world data analytics problems, resulting in low prediction accuracy on unseen data. For p < n and λ → 0, RR converges to OLS. In the more general case, without making assumptions about the dimensionality and rank of the real matrix X, the SVD, X = UΣV⊤, can be used to show that (e.g., [2]) β0 = lim λ→0 βλ = lim λ→0(X⊤X + λI)−1X⊤y = lim λ→0(VΣ⊤ΣV⊤ + λVV⊤)−1VΣ⊤U⊤y = VΣ†U⊤y = X†y. (S.3) S2 Orthogonality of Coefficient Vector and Predictor Nullspace for RR, PCR, and PLS This section shows the orthogonality of regression coefficients and the nullspace for RR, PCR, and PLS. Please note that the notation differs slightly from the main section to improve readability. To show the orthogonality for PCR and PLS, the SVD of X is written in the partitioned form X = [U1 U0] � Σ1 0 0 0 � �V⊤ 1 V⊤ 0 � = U1Σ1V⊤ 1 (S.4) where Σ1 contains only the non-zero singular values. Thus the columns of V0 give an orthonormal basis of N(X). As V is an orthogonal matrix, V⊤ 0 vi = 0 holds for every column vi of V1, or simply V⊤ 0 V1 = 0. The statement that a vector β is orthogonal to N(X) is equivalent to V⊤ 0 β = 0. Ridge regression Ridge regression solves arg min ˆβ ∥y − X ˆβ∥2 2 + γ∥ ˆβ∥2 2 . (S.5) Any β can be written as β = β1 + β0 with β0 ∈ N(X) (i. e. Xβ0 = 0) and β1 orthogonal to N(X). Then ∥y − X(β1 + β0)∥2 2 + γ∥β1 + β0∥2 2 = ∥y − Xβ1∥2 2 + γ∥β1∥2 2 + γ∥β0∥2 2 ≥ ∥y − Xβ1∥2 2 + γ∥β1∥2 2 (S.6) with equality if and only if β0 = 0. Thus the optimal ˆβ is always orthogonal to the nullspace of X. Principal Components Regression The PCR coefficient vector can be calculated by βPCR M = M � m=1 ˆθmvm (S.7) where vm are the first M right singular vectors of X, thus the first M columns of V1 [3]. The actual values of the coefficients ˆθm given by PCR are not necessary to show the orthogonality of βPCR M to N(X): V⊤ 0 βPCR M = M � m=1 ˆθmV⊤ 0 vm = 0 . (S.8) S1Nullspace Perspective on Regression Coefficients Partial Least Squares This analysis is based on the recursive PLS algorithm from [3]. The algorithm starts with X0 := X and ˆy(0) := 0, where the data were assumed to be centered. In contrast to [3], we are using matrix notation instead of explicit sums. One recursion step m ∈ {1, . . . , M} consists of calculating a regression input zm = Xm−1X⊤ m−1y (S.9) and performing a univariate regression of y onto zm, giving ˆθm (the actual value is again not needed in this analysis), leading to an update for the estimated output ˆy(m) = ˆy(m−1) + ˆθmzm . A recursion step ends by orthogonalizing the columns of Xm−1 with respect to zm, Xm = � I − 1 z⊤ mzm zmz⊤ m � Xm−1 . (S.10) The PLS solution after M steps is ˆy(M) = M � m=1 ˆθmzm . (S.11) We show below that each zm can be expressed in the form zm = XX⊤ζm with some ζm. Using this, ˆy(M) = X M � m=1 ˆθmX⊤ζm � �� � ˆβm =: XβPLS M (S.12) defines the regression coefficient βPLS M and furthermore, V⊤ 0 βPLS M = M � m=1 V⊤ 0 ˆβm = M � m=1 ˆθmV⊤ 0 X⊤ζm = M � m=1 ˆθmV⊤ 0 V1Σ1U⊤ 1 ζm = 0 ; (S.13) thus βPLS M is orthogonal to N(X). To show that zm = XX⊤ζm, insert (S.9) in (S.10) to give Xm = � I − 1 z⊤ mzm Xm−1X⊤ m−1yy⊤Xm−1X⊤ m−1 � Xm−1 = Xm−1 � I − 1 z⊤ mzm X⊤ m−1yy⊤Xm−1X⊤ m−1Xm−1 � , (S.14) which implies that the product XmX⊤ m can be written with Xm−1X⊤ m−1 as a left factor, XmX⊤ m = Xm−1X⊤ m−1(· · ·) . (S.15) This expression can be applied recursively, XmX⊤ m = Xm−1X⊤ m−1(· · ·) = Xm−2X⊤ m−2(· · ·) = X0X⊤ 0 (· · ·) , (S.16) and, as X0 = X, zm+1 can always be written as zm+1 = XX⊤ζm+1 . (S.17) S3 Derivation of the Nullspace Projection The optimization min v ∥β∆ + v∥2 2 (S.18a) subject to Xv = 0, (S.18b) is a convex quadratic program with linear constraints. Its solution can be derived by introducing Lagrange multipliers to give the equivalent unconstrained optimization min v,λ 1 2(β∆ + v)⊤(β∆ + v) + λ⊤Xv. (S.19) Set the derivatives of the objective function to zero to give v∗ = −β∆ − X⊤λ Xv∗ = −Xβ∆ − XX⊤λ = 0 λ = −(XX⊤)−1Xβ∆ v∗ = (X⊤(XX⊤)−1X − I)β∆ (S.20) S2Nullspace Perspective on Regression Coefficients S4 Parabolic Data Example Predictions X( PLS 1comp+v ), 10.0, NRMSE : 0.104% X PLS 1comp, NRMSE: 0.105% X * , NRMSE: 0.105% 1 0 1 2 3 4 y 1 0 1 2 3 4 y Predictions Training Data Xv , 10.0 0.008 0.006 0.004 0.002 0.000 0.002 y (a) Predictions associated with the coefficients in Fig. 1b. X( RR CV1SE+v ), 0.1, NRMSE : 0.110% X RR CV1SE, NRMSE: 0.111% X FL CV1SE, NRMSE: 0.104% 1 0 1 2 3 4 y 1 0 1 2 3 4 y Predictions Training Data Xv , 0.1 0.003 0.002 0.001 0.000 0.001 y (b) Predictions associated with the coefficients in Fig. 2. Figure S1: Scatter plots of predictions associated with the parabolic data example. S5 Signal-to-Noise Ratio Approximation 0 20 40 60 SNR [dB] a) 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 Voltage (V) 0.00 0.01 0.02 0.03 0.04 X b) 0 2 4 6 SNR 1e5 0.0 0.2 0.4 0.6 0.8 1.0 Noise Power 1e 7 0.000 0.005 0.010 0.015 0.020 LFP Training Data SNR Analysis Figure S2: a) Approximated SNR of the LFP data set with SNR in dB, SNR ratio, and the noise power. b) Mean and standard deviation of the data. Figure S2a shows the approximated SNR of the LFP data. The signal is estimated by fitting a spline, using scipy.interpolate.splrep with a smoothing parameter s = 10−6 and the polynomial degree k = 3, to the original data. The deviation to the spline is considered noise. The spline parameter choice depends on the expected degree of smoothness of the latent function. Figure S2b shows the associated mean and standard deviation. The SNR decreases strongly in the region 3.2–3.5 V; however, in this region, the standard deviation of the data is also very low. Rescaling the data matrix columns to unit variance thus amplifies the noise in this section. S3Nullspace Perspective on Regression Coefficients The above analysis is only based on the data X without considering y. Although standardization can amplify noise, whether a model based on z-scoring the data yields a higher prediction accuracy also depends on the relationship between X and y. An alternative to mitigate the amplification of noise would be to rescale the data such that the variance of the column matches the normalized SNR ratio. References [1] G. Strang, Introduction to Linear Algebra, Fifth. Wellesley, Massachusetts: Cambridge Press, 2016. [2] D. Kobak, J. Lomond, and B. Sanchez, “The optimal ridge penalty for real-world high-dimensional data can be zero or negative due to the implicit ridge regularization,” Journal of Machine Learning Research, vol. 21, no. 169, pp. 1–16, 2020. [3] T. Hastie, R. Tibshirani, and J. H. Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second. New York: Springer, 2009. S4