Area-norm COBRA on Conditional Survival Prediction Rahul Goswami1,2 and Arabin Kr. Dey2,3 1*Department, of Mathematics, IIT Guwahati, Guwahati, 781039, Assam, India. 2Department, of Mathematics, IIT Guwahati, Guwahati, 781039, Assam, India. Contributing authors: rahul.goswami@iitg.ac.in; arabin.k.dey@iitg.ac.in; Abstract The paper explores a different variation of combined regression strategy to cal- culate the conditional survival function. We use regression based weak learners to create the proposed ensemble technique. The proposed combined regression strategy uses proximity measure as area between two survival curves. The pro- posed model shows a construction which ensures that it performs better than the Random Survival Forest. The paper discusses a novel technique to select the most important variable in the combined regression setup. We perform a simu- lation study to show that our proposition for finding relevance of the variables works quite well. We also use three real-life datasets to illustrate the model. Keywords: Cox’s model, Weak learners, Survival Tree, Combined Regression Strategy 1 Introduction Prediction of Conditional survival function is an important relevant information for doctors, clinicians, and insurance companies. Function prediction has a large number of applications in many areas of science and technology. This is a challenging problem. Researchers attempted to provide a reasonable generic solution over decades. After the Cox proportional hazard model ([1], [2]), one of the breakthrough contributions to this problem is Random Survival Forest by Ishawaran [3]. The solution encourages researchers to explore the problem through different other neural network models [[4], [5], [4, 5]] and ensemble techniques which include XGboost, Adaboost ([6]) etc. In 1 arXiv:2309.00417v1 [cs.LG] 1 Sep 2023technical language, this is a problem of constructing a functional regression. Goswami et al ([7], [8]) provide different solutions to this problem in a combined regression setup. However, the solution has issues and demands a more detailed exploration to get a complete idea of the different possible implementations and their benefits. The paper addresses a different variation of the COBRA which works for certain aspects and certain types of datasets. The paper proposes to use a different proximity measure for this combined regression strategy and a specific set of weak learners which ensures that the proposed algorithm works much better than the usual Random Survival Forest. It ensures the algorithm will work at least as well as a random survival forest if the dataset consists of the censored observation up to a certain level. All implementation in this paper assumes datasets are sufficiently large. This ensures the proximity measure to include a reasonable number of points in the pre- diction set. It uses Kaplan Meier estimation ([9]) or Nelson-Aalen [10, 11] estimator to estimate the survival function or cumulative hazard function respectively based on all similar observations. We use both concordance measure and integrated Brier score in a censored setup to tune the hyperparameters of the model and use them to under- stand the goodness of fit for each dataset used in this paper. The approach does not use similar weak learners as used by Goswami et al. The weak learner’s choices in this paper follow the same as the original combined regression paper with its similar extension in functional regression setup. Thus we take each base learner as a survival predictor. We use different survival predictors instead of just one type of survival pre- dictor used by Goswami et al. : (1) Cox model (ridge) [12] (2) Cox model (lasso) [13] (3) random survival forest [14] (4) KNN survival ([15], [16] ) (5) Survival Tree [14]. These are exact functional extensions of the following models (1) Ridge (2) Lasso (3) Random Forest [14] (4) KNN [17] (5) Decision tree used respectively in original cobra for usual regression. We observe the choice can help us to ensure that the algorithm works better than a Random survival forest for sufficiently large datasets with lesser censored data. The experimental results open a large number of issues and questions. We address the issues in other research articles. The relevance of the covariates is also an important aspect of survival analysis. Earlier statistical models like Cox Proportional Hazard Model [2] and Accelerated Failure Time Model [18] have a clear idea of the covariates, which are relevant for the happening of the event of interest at a specific time. The importance of the covariates in predicting the survival function measures the relevance of the covari- ates. Random Survival Forest [3] uses the out-of-bag error ([14], [3], [19], [20], [21]) and minimal depth procedure. Feature importance is crucial for providing the inter- pretability of the model. Independent approaches like LIME [22] and Shapley Value [23] use variable importance concepts in ordinary regression setup. The approaches like SurvLIME ([24, 25], [26]) and SurvSHAP ([27], [28]) serves the similar purpose of bringing interpretability of survival model through measuring the importance of features. In this paper, we propose a novel technique to find out the most important variable. It can also find a score to rank the importance. The technique exploits the cobra structure to find the variable importance. The proposition has support on a simulated framework. 2The organization of the paper is as follows. Section 2 provides a brief overview of the notations and basic framework of survival analysis and original combined regressions strategy used for regression. We describe the proposed framework for aggregating the survival-based functional models in Section 3. Numerical results of the experiments conducted to evaluate the performance of the proposed framework are available in Section 4. A novel proposition for ranking the variables or selecting the most important variable exploiting the Cobra Structure is available in Section 5. Finally, Section 6 concludes the paper and discusses future research directions. 2 Background Important Notations and Technical Specification of the Problem Our goal is to develop individual survival function. Each patient i have a d-dimensional covariate xi ∈ X which represents the unique characteristics of the patient. Here, X is the covariate space. An outcome of interest Ti ∈ R+, representing the time of occurrence of the event of interest. The patient may drop out of the study before the event of interest occurs. We denote this dropout time or censored time as Ci ∈ R+. The observations carry another variable, δi ∈ {0, 1}. δi = 1 when event of interest is available, and δi = 0 if the event of interest is not available. We can write S(t | xi) as S(t | xi) = P(Ti > t | xi) (1) It represents the probability of survival of the patient i beyond time t given the covariate xi. Note that, our aim is to estimate the survival function S specific to the patient i, not the unconditional survival function.. The relationship between the patient covariates and survival function is to be estimated by a dataset Dn comprising of n individuals assumed to be drawn from the random tuple (Xi, δiTi +(1−δi)Ci, δi) . For simplicity we denote the random tuple (Xi, δiTi + (1 − δi)Ci, δi) by (Xi, Yi, δi). Since, we do not observe both Ti and Ci concurrently in practice, we denote the observed time by Yi = δiTi + (1 − δi)Ci Combined Regression Strategy for Usual Regression Framework Let’s consider the regression setting, where Dn = (X1, Y1), · · · , (Xn, Yn) are indepen- dent and identically distributed samples from (X, Y ) ∈ Rp ×R. We assume EY 2 < ∞. In a typical regression setup we are interested in estimating the conditional expectation of Y given X = x i.e E(Y | X = x). In COBRA setup, we split the the dataset Dn into two parts, one part Dk = (X1, Y1) · · · (Xk, Yk) and the other part Dl = (Xk+1, Yk+1), · · · , (Xn, Yn) where l = n − k ≥ 1. We denote Dl = (X1, Y1), · · · , (Xl, Yl). Suppose, we have M competing estimators, referred to as machines in COBRA setup. Each machine m is trained on Dk and referred as rk1, rk2, · · · , rkM, here we 3note that rki is a machine trained only on Dk and capable of estimating E(Y | X = x) for any x ∈ Rp. Then the estimate of E(Y | X = x) by COBRA is given by ˆECOBRA(Y | X = x) = l � i=1 Wn,i(x)Yi (2) where Wn,i(x) is the weight of the ith sample in Dl and is given by Wn,i(x) = I(∩M m=1 | rk,m(Xi) − rk,m(x) |≤ ϵ) �l j=1 I(∩M m=1 | rk,m(Xj) − rk,m(x) |≤ ϵ) (3) where, ϵ is a user specified parameter. The intuition behind the weight is that if the ith sample is close to the query point x in all the machines, then the weight of the ith sample is 1 and if the ith sample is far from the query point x in any machine, then the weight of the ith sample is 0. This condition for consensus of a fraction of the machines brings a new parameter α, which tells us how many machines should agree on the closeness of the ith sample to the query point x.The weight of the ith sample is given by Wn,i(x) = I(�M m=1 I(| rk,m(Xi) − rk,m(x) |≤ ϵ) ≥ α) �l j=1 I(�M m=1 I(| rk,m(Xj) − rk,m(x) |≤ ϵ) ≥ α) (4) COBRA gives a good approximation of the conditional expectation of Y given X = x, there is both theoretical and experimental proof that COBRA perform better that individual machine in terms of quadratic loss. 3 Proposed Algorithm for Functional Regression The structure of the survival data takes the shape of Dn : {Xi, Yi, δi}n i=1, where Xi signifies the covariate under consideration. Correspondingly, Yi represents the lesser of the event of interest occurrence time and the censoring time while δi serves as the indicator that offers insight into whether we observe the event of interest or not. Let us consider of the set survival models M = {m1, m2 . . . m|M|}. Each model of the set M provides a survival function. Since we use five fold cross validation, we divide the dataset into five parts, where each part represents test set and rest as train set. Suppose, we denote the train data as Dn′ which is of size n ′ and split the dataset Dn′ into Dk and Dl such that k + l = n ′. Let us assume the train and test dataset as Dk = {(Xi, Yi, δi)}k i=1 and Dl = {(Xi, Yi, δi)}n ′ i=k+1. We use the subdivision of the dataset Dk to train the base models, which splits out individual survival function for a given query point x. Sk,m(t | x) denotes the estimated the survival function based on mth base model, and k is the dataset on which we train the base model. We define a function as follows: Γ(x; Xj, M, ϵ, α) = I � M � m=1 I(d(Sk,m(t | x), Sk,m(t | Xj)) ≤ ϵ) ≥| M | α � (5) 4The k subscript notify that the prediction of the base models are trained on the dataset Dk. Here, I is the indicator function, which is 1 if the condition is true and 0 if the condition is false. ϵ is the threshold distance, α is the fraction of base models in the in consensus of ϵ proximity of the query point x. basically I(.) is the indicator function, which is 1 if the ith individual is in the (ϵ, α)−proximity of the query point x, otherwise 0. In this paper we take the following area-norm to calculate the distance function : d(Si(t), Sj(t)) = 1 t � t 0 | Si(t) − Sj(t) | dt (6) However, computing this area-norm is computationally expensive. We use the following Euler approximation as distance function between survival function : d(Si(t), Sj(t)) = 1 tn′ − t1 n ′ � i=1 | Si(t) − Sj(t) | (ti − ti−1) (7) The other part of the dataset Dl aggregate the survival function of the base mod- els. We calculate the event time, event count and survivor count using the following equations, YΓ(x; h) = � Yj : j ≥ k ∩ δjΓ(x; Xj, M, ϵ, α) = 1 � (8) DΓ(t | x; h) = n ′ � j=k Γ(x; Xj, M, ϵ, α)δjI(Yj = t) (9) RΓ(t | x; h) = n ′ � j=k Γ(x; Xj, M, ϵ, α)I(Yj ≥ t) (10) Then using the equations 8, 9 and 10, we can construct the aggregated survival function ˆSCOBRA(t | x; h) using the following equation ˆSCOBRA(t | x; h) = � t′∈YΓ(x;h) � 1 − DΓ(t ′ | x; h) RΓ(t ′ | x; h) �I(t ′≤t) (11) The proposed algorithm is available in Algorithm 1. 4 Dataset Descriptions and Numerical Experiments 4.1 Datasets We evaluate the performance of the proposed method on the following datasets: • METABRIC : The Molecular Taxonomy of Breast Cancer International Con- sortium (METABRIC) dataset comprises of gene expression profiles and clinical 5Algorithm 1 Overall flow of the proposed Algorithm Require: Dn : Dataset of n individuals Dn′ : Dataset of n ′ train individuals ϵ : Threshold distance α : Fraction of base models in the consensus of ϵ proximity of the query point x. M : Number of base models Ensure: ˆS(t | x) : Estimated survival function of the query point x at time t 1: Partition the whole data into five parts. Take each part as test set. 2: Split train data Dn′ into Dk and Dl such that k + l = n ′ 3: Train M base models on Dk and predict the survival function Sk,m(t | x) for m = 1, 2 . . . M 4: Calculate the distance d(Sk,m(t | x), Sk,m(t | xi)) for m = 1, 2 . . . M and i = k + 1, k + 2, · · · , n ′ using 7 5: Calculate the indicator function Γi(x) for i = k + 1, k + 2, · · · , n ′ using 5 6: Calculate the estimated survival function ˆS(t | x) using 11 attributes aimed at discerning breast cancer subtypes. Among the 1,981 patients included, 888 (44.8%) had complete follow-up data until mortality, while the remain- ing 1,093 (55.2%) were right-censored. Our analysis focuses on 21 publicly accessible clinical features, including critical indicators such as tumor size and lymph node involvement, with detailed descriptions available in Bilal et al. (2013) [29]. Miss- ing values underwent imputation to ensure data integrity, with real-valued features substituted by their mean and categorical features by their mode. We one-hot code the categorical variables, facilitating their integration into subsequent analytical methodologies. • FLCHAIN : The FLCHAIN3 [[30], [31]] dataset, introduced by Dispenzieri et al. [30], constitutes a publicly accessible resource designed to investigate the intricate association between serum free light chain levels and mortality. This dataset encom- passes a diverse array of covariates, including age, gender, serum creatinine levels, and the presence of monoclonal gammopathy, all of which are pivotal factors in com- prehending the dynamics of this relationship. This dataset is a critical resource for elucidating the multifaceted interplay between serum free light chains and pertinent covariates in the context of mortality studies. • RECID : The Recidivism4 [32] dataset is a publicly available dataset, widely utilised in criminology and social sciences, aimed at exploring the intricate dynamics of recidivism – defined as the re-offending or re-incarceration of individuals previ- ously involved with the criminal justice system. This dataset offers a comprehensive array of covariates essential for the analysis of recidivism, including demographic information such as age, gender, socio-economic factors and criminal history vari- ables like prior convictions and offence type. This dataset serves as a fundamental resource for gaining insights into the multifaceted determinants of recidivism and contributes to informed policy and intervention strategies in criminal justice studies. 6Table 1 Datasets Dataset Total Observations # Features∗ % Censored Time Range METABRIC 1981 53 44.78 3 - 9218 FLCHAIN 7874 11 27.55 0 - 5220 RECID 1445 16 61.80 0 - 4.3994 ∗ The total number of features after preprocessing 4.2 Experimental Setup We conduct the following experiments to evaluate the performance of the proposed method. The experiment uses he five fold cross validation are reported for each dataset, 4 and 3. We tune all parameters for each dataset using hyperopt [33]. The parameters are given in table 2. Table 2 Parameters tuned with 1000 trials using hyperopt Parameter Tuning Range ϵ 1e-300 - 0.9 α 1/5,2/5,3/5,4/5,1 l/n 0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9 4.3 Performance Metrics We evaluate the performance of the proposed method using the following two metrics: Concordance Concordance [34] index is popular to evaluate the model performance in survival mod- els/functional regression setup. We define concordance index with right censored data. The expression is as follows : Ctd(t) = Pr( ˆSi(ti) > ˆSj(ti) | di = 1, t ≤ tj, ti < tj) where Si(ti) is the predicted survival probability of the ith individual at time ti and Sj(ti) is the predicted survival probability of the jth individual at time ti. The time- dependent concordance index given above measure the level of agreement between the predicted survival probabilities of two individuals and actual event time of the individuals. It is defined between 0.5 and 1, where 0.5 random prediction and 1 is perfect prediction, represents that every prediction is in agreement with the actual event time. 7Integrated Brier Score We use another benchmark as Integrated Brier Score. Since the data consists of right censored observation we use the following modified version of the Integrated Brier Score, we call it as IBSc. IBSc computed between time interval [t1, tmax] as IBSc = 1 tmax − t1 � tmax t1 BSc(t)dt . We estimate the integration numerically via trapezoidal rule. Expression of Brier Score for the censored observation (BSc) is : BSc(t) = 1 N N � i=1 � I(yi ≤ t, δi = 1)(0 − ˆS(t | xi))2 ˆG(yi) + I(yi > t)(1 − ˆS(t | xi))2 ˆG(t) � where, ˆS(t | xi) is the predicted conditional function and 1 ˆ G(t) is the inverse probability of the censoring weight. We assume C > t, where C is the censoring point. Base Models The algorithm need base models to calculate the estimation on dataset Dl. We have used the following base models: 1. Survival Tree : Survival Tree is a tree based method for survival analysis. We split the data into different groups based on the log-rank test statistic during the construction of survival tree. The log-rank statistic tells us how different the survival curves of two groups are. Further, we grow the tree recursively, till it meets a certain stopping criteria. 2. Random Survival Forest : Random Survival Forest [3] is an extension Survival Tree, The output of Random Survival Forest is the average of the output of Survival Tree, obtained from different bootstrap samples of the data. 3. Cox Model (Lasso and Ridge) : Cox Model is a semi-parametric model for survival analysis, which is based on the proportional hazard assumption. The Cox Model is given by h(t | x) = h0(t) exp(βT x) (12) where h(t | x) is the hazard function of an individual with covariate x at time t, h0(t) is the baseline hazard function and β is the parameter vector. The Cox Model is usually estimated using partial likelihood method. Regularisation brings two variations in the Cox Model, known as Lasso and Ridge Cox Model. 4. K-Nearest Neighbour Survival : K-Nearest Neighbour Survival ( [15]) is a non-parametric method to estimate conditional survival function. In K-Nearest Neighbour Regression, the prediction of the query point is the average of the K-Nearest Neighbours of the query point. Suppose, we have the following right censored survival data, ((X1, Y1, δ1), · · · , (Xn, Yn, δn)) ∈ X × R+ × {0, 1} Then for specified Kernel function K : R+ → R+, and bandwidth h > 0, we can measure 8the proximity between observation Xi and Xj by K � ρ(Xi,Xj) h � , where ρ is the dis- tance function. We can create unique event time, event count and survivor count as follows YK(x; h) = � Yj : j ∈ [n] ∩ δjK �ρ(x, Xj) h � > 0 � DK(t | x; h) = n � j=1 K �ρ(x, Xj) h � δjI(Yj = t) RK(t | x; h) = n � j=1 K �ρ(x, Xj) h � I(Yj ≥ t) Then the survival function is given by ˆSK(t | x; h) = � t′∈YK(x;h) � 1 − DK(t ′ | x; h) RK(t ′ | x; h) �I(t ′≤t) (13) The K-Nearest Neighbour Survival can be derived from the above equation by replacing the kernel function K by the I, where mathematical expression of I(x; k, Xi)1 is as follows : I(x; k, Xi) = I � n � j=1,j̸=i I(ρ(x, Xi) ≤ ρ(x, Xj)) ≥ k � (14) KNN survival implementation is available by George Chen at https://github.com/georgehc/npsurvival. Numerical Results on the Real Datasets Table 3 provides the information of the concordance of all the five models on the three datasets, whereas Table 4 describes the integrated brier score of all the five models. We use 5 fold cross validation to obtain the result We observe that our proposed construction of Combined regression strategy provides maximum concordance index and minimum integrated brier score for all datasets. 5 Covariate Relevance The relevance of the covariates is also an important aspect of survival analysis. COBRA Survival has an inbuilt mechanism to measure the relevance of the covari- ates. We use a logistic regression to find the relevance of the covariates for a query point x. A detailed overview of logistic regression is available at [36], on the indicator function 15, where i = k + 1, k + 2, · · · , n ′. 1Taking I(·) = 1∀i will lead to a population Kaplan-Meier Estimate 9Table 3 Concordance (Higher is Better) Dataset METABRIC FLCHAIN RECID Survival Tree 0.553 0.780 0.544 Random Survival Forest 0.669 0.899 0.629 Cox Model (Lasso) 0.626 0.886 0.599 Cox Model (Ridge) 0.589 0.901 0.551 Kernel Survival Model 0.561 0.829 0.540 proposed 0.677 0.911 0.630 Table 4 Integrated Brier Scoe (Lower is better) Dataset METABRIC FLCHAIN RECID Survival Tree 0.278 0.065 0.262 Random Survival Forest 0.167 0.046 0.175 Cox Model (Lasso) 0.160 0.079 0.183 Cox Model (Ridge) 0.162 0.143 .189 Kernel Survival Model 0.203 0.054 0.211 proposed 0.148 0.043 0.175 The logistic regression is given by log � Pr(Γi(x) = 1) 1 − Pr(Γi(x) = 1) � = β0 + β1x1,i + β2x2,i + · · · + βpxp,i∀i = k + 1, k + 2, · · · , n ′ The coefficients β1, β2 . . . βp are the relevance of the covariates x1, x2 . . . xp respectively. We recall the definition of −(·) : Γ(x; Xj, M, ϵ, α) = I � M � m=1 I(d(Sk,m(t | x), Sk,m(t | Xj)) ≤ ϵ) ≥| M | α � (15) The k subscript notifies that the prediction of the base models takes training on the dataset Dk. Here, I is the indicator function, which is 1 if the condition is true and 0 if the condition is false. ϵ is the threshold distance, α is thetakes fraction of base models in the consensus of ϵ proximity of the query point x. basically I(.) is the indicator function, which is 1 if the ith individual is in the (ϵ, α)−proximity of the query point x, otherwise 0. Simulation Study We use a synthetic population to demonstrate COBRA Survival’s ability to discrimi- nate between different factors and determine their relative importance. Our strategy 10incorporates a non-linear data-generating procedure supported by the following link rule2 Λ(Xi) = 2 + log(13X0,i + 5X1,i + 7X2,i) + X3,i We have generated 2000 sample, where 40 percent of the sample are censored. The censoring time is generated by taking uniform distribution between 0 and time of event. The time of event is generated by following weibuill distribution Ti ∼ W(2, Λ(Xi)) There are 9 covariates generated, where X0 , X1 , X2 , X3 have non-linear effect on true event time, whereas X4 , X5 , X6 , X7 , X8 have no effect on true event time. This is done to show that COBRA Survival can identify the covariates which have effect on true event time, and the covariates which have no effect on true event time. The covariates are generated from uniform distribution. The estimates of the covariate relevance is given in Figure 1. Fig. 1 Relevance of the covariates 2Similar Link rule is taken by Bellot and van der Schaar 116 Conclusion A large number of variations of cobra structure can not able to beat the performance of Random Survival Forest even with weak learners taken as Random Survival Forest. The performance of the Cobra depends on the appropriate choice of its parameters. We observe that the proposed method in the paper works well for all three datasets. It ensures much better performance in most of the cases. Large censoring may degrade the performance. Also, a lesser number of observations in proximity points affect the performance. Our proposed model in the variable selection method works very well in the simulated framework. However, this requires special attention in a separate research. We may pursue further research on weighted version, problems on smaller observations, and more on variable importance in this context. The work is in progress. References [1] Cox, D.R.: Partial likelihood. Biometrika 62(2), 269–276 (1975) [2] Cox, D.R.: Regression models and life-tables. Journal of the Royal Statistical Society: Series B (Methodological) 34(2), 187–202 (1972) [3] Ishwaran, H., Kogalur, U.B., Blackstone, E.H., Lauer, M.S.: Random survival forests (2008) [4] Faraggi, D., Simon, R.: A neural network model for survival data. Statistics in medicine 14(1), 73–82 (1995) [5] Katzman, J.L., Shaham, U., Cloninger, A., Bates, J., Jiang, T., Kluger, Y.: Deepsurv: personalized treatment recommender system using a cox proportional hazards deep neural network. BMC medical research methodology 18(1), 1–12 (2018) [6] Bellot, A., Schaar, M.: Boosted trees for risk prognosis. In: Machine Learning for Healthcare Conference, pp. 2–16 (2018). PMLR [7] Goswami, R., Dey, A.K.: Concordance based survival cobra with regression type weak learners. arXiv preprint arXiv:2209.11919 (2022) [8] Goswami, R., Dey, A.K.: Integrated brier score based survival cobra–a regression based approach. arXiv preprint arXiv:2210.12006 (2022) [9] Kaplan, E.L., Meier, P.: Nonparametric estimation from incomplete observations. Journal of the American statistical association 53(282), 457–481 (1958) [10] Aalen, O.: Nonparametric inference for a family of counting processes. The Annals of Statistics, 701–726 (1978) [11] Nelson, W.: Theory and applications of hazard plotting for censored failure data. 12Technometrics 14(4), 945–966 (1972) [12] Simon, N., Friedman, J., Hastie, T., Tibshirani, R.: Regularization paths for cox’s proportional hazards model via coordinate descent. Journal of statistical software 39(5), 1 (2011) [13] Tibshirani, R.: The lasso method for variable selection in the cox model. Statistics in medicine 16(4), 385–395 (1997) [14] Breiman, L.: Random forests. Machine learning 45, 5–32 (2001) [15] Chen, G.: Nearest neighbor and kernel survival analysis: Nonasymptotic error bounds and strong consistency rates. In: International Conference on Machine Learning, pp. 1001–1010 (2019). PMLR [16] Beran, R.: Nonparametric regression with randomly censored survival data (1981) [17] Fix, E., Hodges, J.L.: Discriminatory analysis: Nonparametric discrimination: Small sample performance (1952) [18] Kalbfleisch, J.D., Prentice, R.L.: The Statistical Analysis of Failure Time Data. John Wiley & Sons, ??? (2011) [19] Ishwaran, H.: Variable importance in binary regression trees and forests (2007) [20] Gr¨omping, U.: Variable importance assessment in regression: linear regression versus random forest. The American Statistician 63(4), 308–319 (2009) [21] Genuer, R., Poggi, J.-M., Tuleau-Malot, C.: Variable selection using random forests. Pattern recognition letters 31(14), 2225–2236 (2010) [22] Ribeiro, M.T., Singh, S., Guestrin, C.: ”why should i trust you?” explaining the predictions of any classifier. In: Proceedings of the 22nd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining, pp. 1135–1144 (2016) [23] Strumbelj, E., Kononenko, I.: An efficient explanation of individual classifications using game theory. The Journal of Machine Learning Research 11, 1–18 (2010) [24] Kovalev, M.S., Utkin, L.V., Kasimov, E.M.: Survlime: A method for explaining machine learning survival models. Knowledge-Based Systems 203, 106164 (2020) [25] Kovalev, M.S., Utkin, L.V.: A robust algorithm for explaining unreliable machine learning survival models using the kolmogorov–smirnov bounds. Neural Networks 132, 1–18 (2020) 13[26] Utkin, L.V., Kovalev, M.S., Kasimov, E.M.: Survlime-inf: A simplified modifi- cation of survlime for explanation of machine learning survival models. arXiv preprint arXiv:2005.02387 (2020) [27] Alabdallah, A., Pashami, S., R¨ognvaldsson, T., Ohlsson, M.: Survshap: A proxy- based algorithm for explaining survival models with shap. In: 2022 IEEE 9th International Conference on Data Science and Advanced Analytics (DSAA), pp. 1–10 (2022). IEEE [28] Krzyzi´nski, M., Spytek, M., Baniecki, H., Biecek, P.: Survshap (t): Time- dependent explanations of machine learning survival models. Knowledge-Based Systems 262, 110–234 (2023) [29] Bilal, E., Dutkowski, J., Guinney, J., Jang, I.S., Logsdon, B.A., Pandey, G., Sauerwine, B.A., Shimoni, Y., Moen Vollan, H.K., Mecham, B.H., et al.: Improv- ing breast cancer survival analysis through competition-based multidimensional modeling. PLoS computational biology 9(5), 1003047 (2013) [30] Dispenzieri, A., Katzmann, J.A., Kyle, R.A., Larson, D.R., Therneau, T.M., Colby, C.L., Clark, R.J., Mead, G.P., Kumar, S., Melton III, L.J., et al.: Use of nonclonal serum immunoglobulin free light chains to predict overall survival in the general population. Mayo Clinic Proceedings 87(6), 517–523 (2012) [31] Kyle, R.A., Therneau, T.M., Rajkumar, S.V., Larson, D.R., Plevak, M.F., Offord, J.R., Dispenzieri, A., Katzmann, J.A., Melton III, L.J.: Prevalence of monoclonal gammopathy of undetermined significance. New England Journal of Medicine 354(13), 1362–1369 (2006) [32] Wooldridge, J.M., et al.: Recid. Boston College Department of Economics, ??? (2000) [33] Bergstra, J., Yamins, D., Cox, D.D., et al.: Hyperopt: A python library for opti- mizing the hyperparameters of machine learning algorithms. In: Proceedings of the 12th Python in Science Conference, vol. 13, p. 20 (2013). Citeseer [34] Gerds, T.A., Kattan, M.W., Schumacher, M., Yu, C.: Estimating a time- dependent concordance index for survival prediction models with covariate dependent censoring. Statistics in medicine 32(13), 2173–2184 (2013) [35] P¨olsterl, S.: scikit-survival: A library for time-to-event analysis built on top of scikit-learn. The Journal of Machine Learning Research 21(1), 8747–8752 (2020) [36] Cramer, J.S.: The origins of logistic regression. Tinbergen Institute Working Paper (2002) 14