Advancing Personalized Federated Learning: Group Privacy, Fairness, and Beyond Filippo Galli1,3*, Kangsoo Jung2*, Sayan Biswas2,4*, Catuscia Palamidessi2,4* and Tommaso Cucinotta3* 1Scuola Normale Superiore, Pisa, Italy. 2INRIA, Palaiseau, France. 3Scuola Superiore Sant’Anna, Pisa, Italy. 4´Ecole Polytechnique, Palaiseau, France. *Corresponding author(s). E-mail(s): filippo.galli@sns.it; gangsoo.zeong@inria.fr; sayan.biswas@inria.fr; catuscia@lix.polytechnique.fr; tommaso.cucinotta@santannapisa.it; Abstract Federated learning (FL) is a framework for training machine learning models in a distributed and collaborative manner. During training, a set of participating clients process their data stored locally, sharing only updates of the statistical model’s parameters obtained by minimizing a cost function over their local inputs. FL was proposed as a stepping-stone towards privacy-preserving machine learn- ing, but it has been shown to expose clients to issues such as leakage of private information, lack of personalization of the model, and the possibility of having a trained model that is fairer to some groups of clients than to others. In this paper, the focus is on addressing the triadic interaction among personalization, privacy guarantees, and fairness attained by trained models within the FL framework. Differential privacy and its variants have been studied and applied as cutting- edge standards for providing formal privacy guarantees. However, clients in FL often hold very diverse datasets representing heterogeneous communities, making it important to protect their sensitive and personal information while still ensur- ing that the trained model upholds the aspect of fairness for the users. To attain this objective, a method is put forth that introduces group privacy assurances through the utilization of d-privacy (aka metric privacy). d-privacy represents a localized form of differential privacy that relies on a metric-oriented obfuscation approach to maintain the original data’s topological distribution. This method, besides enabling personalized model training in a federated approach and pro- viding formal privacy guarantees, possesses significantly better group fairness 1 arXiv:2309.00416v1 [cs.LG] 1 Sep 2023measured under a variety of standard metrics than a global model trained within a classical FL template. Theoretical justifications for the applicability are pro- vided, as well as experimental validation on real-world datasets to illustrate the working of the proposed method. Keywords: Federated Learning, Metric Privacy, Personalized Models, Fairness 1 Introduction The widespread collection of user data in modern machine learning has raised concerns regarding privacy violations and the potential disclosure of sensitive personal infor- mation [1, 2]. To address these concerns, Federated Learning [3] was introduced as a collaborative machine learning paradigm, where users’ devices train a global predic- tive model without transmitting raw data to a central server. While FL offers promises of preserving user privacy and maintaining model performance, the heterogeneity of data distributions among clients can lead to challenges such as reduced model utility and convergence issues during training. In response, personalized federated learning approaches have emerged, aiming to tailor models to clusters of users with similar data distributions [4–6]. Furthermore, it has been demonstrated that avoiding the release of users’ raw data alone does not provide sufficient protection against potential privacy violations [7–9]. To address this issue, researchers have explored the application of differential privacy (DP) [10, 11] to federated learning, providing privacy guarantees for users participat- ing in the optimization process. DP mechanisms introduce randomness in the model updates released by clients, making each user’s contribution to the final model proba- bilistically indistinguishable up to a certain likelihood factor. To bound this factor, the domain of secrets (i.e., the parameter space in FL) is artificially constrained, either to offer central [12, 13] or local DP guarantees [14, 15]. However, constraining the opti- mization process to a subset of Rn can have negative effects, such as when the optimal model parameters for a particular cluster of users lie outside such a bounded domain. To address the challenges of personalization and local privacy protection, this work proposes the adoption of a more general notion of DP called d-privacy or metric-based privacy [16] which has been in the spotlight of late mainly in the context of location- privacy [17–19]. This concept of privacy does not require a bounded domain and provides guarantees based on the distance between any two points in the parameter space. Therefore, assuming that clients with similar data distributions have simi- lar optimal fitting parameters, d-privacy offers strong indistinguishability guarantees. Conversely, privacy guarantees degrade gracefully for clients with significantly different data distributions. In addition to addressing privacy concerns in personalised FL as was studied in [20], this work extends the analysis and investigates the impact of the proposed method on fairness aspects in federated model training. As machine learning-based decision systems become more prevalent, it has become apparent that many of these 2systems exhibit gender and racial biases that disproportionately affect minority pop- ulations [21, 22]. Therefore, beyond protecting user privacy, it is crucial to explore cutting-edge machine learning algorithms that can potentially mitigate this perva- sive lack of fairness among participating clients. However, systems aiming to protect privacy while ensuring fairness often involve a trade-off between the two [23]. This trade-off arises because privacy protection techniques based on DP tend to minimize the impact of outliers or minorities within the overall dataset. In other words, the application of d-privacy, a metric-based generalization of DP, to personalized FL could potentially compromise the fairness of the machine learning model. Building upon [20], this paper presents extensive experimental results demonstrating that the use of per- sonalized FL under group privacy guarantees not only significantly improves fairness compared to the classical (non-personalized) FL framework, but it also maintains a relatively small trade-off between privacy and fairness. In summary, the contributions of this paper are the following: it extends the work pursued in [20] (points 1 and 2) and it investigates the implications of our proposal on the fairness of the model (point 3): 1. A novel algorithm is put forward for collaborative training of machine learning models, leveraging advanced techniques for model personalization and addressing user privacy concerns by formalizing privacy guarantees in terms of d-privacy. 2. This research focuses on studying the Laplace mechanism under Euclidean distance, and providing a closed-form expression for its generalization in Rn, as well as an efficient sampling procedure. 3. It shows that personalized federated learning under formal privacy guarantees improves group fairness significantly compared to the non-personalized federated learning framework and, hence, establishes that this method enhances the trade-off between privacy and fairness. The rest of this paper is organized as follows. Section 2 introduces the relevant foun- dations of federated learning, differential privacy, and fairness notions. Section 3 discusses the related works for our research. Section 4 explains the proposed algorithm for personalized federated learning with group privacy. Section 5 illustrates how the proposed method works in terms of privacy and fairness, and Section 6 provides our concluding remarks. 2 Background 2.1 Personalized Federated Learning The problem of personalized federated learning falls within the framework of stochas- tic optimization, and the notation from [4] is adopted here to determine the set of minimizers θ∗ j ∈ Rn with j ∈ {1, . . . , k} of the cost functions F(θj) = Ez∼Dj [f(θj; z)] , (1) where {D1, . . . , Dk} are the data distributions which cannot be accessed directly but only through a collection of client datasets Zc = {z|z ∼ Dj, z ∈ D} for some j ∈ 3{1, . . . , k} with c ∈ C = {1, . . . , N} the set of clients, and D a generic domain of data points. C is partitioned in k disjoint sets S∗ j = {c ∈ C | ∀z ∈ Zc, z ∼ Dj} ∀ j ∈ {1, . . . , k} (2) The mapping c → j is unknown and it is necessary to rely on estimates Sj of the membership of Zc to compute the empirical cost functions ˜F(θj) = 1 |Sj| � c∈Sj ˜Fc(θj; Zc); ˜Fc(θj; Zc) = 1 |Zc| � zi∈Zc f(θ; zi) (3) The cost function f : Rn × D �→ R≥0 is applied on z ∈ D, parametrized by the vector θj ∈ Rn. Thus, the optimization aims to find, ∀ j ∈ {1, . . . , k}, ˜θ∗ j = arg min θj ˜F(θj) (4) 2.2 Privacy d-privacy, introduced in [16], extends the concept of differential privacy (DP) to any domain X, which represents the original data space and is equipped with a distance measure d: X 2 �→ R≥0, along with a space of secrets Y. A random mechanism R : X �→ Y is considered ε-d-private if, for any x1, x2 ∈ X and measurable S ⊆ Y, the inequality in Equation (5) holds: P [R(x1) ∈ S] ≤ eεd(x1,x2)P [R(x2) ∈ S] (5) It is important to note that when X corresponds to the domain of databases and d represents the distance based on the Hamming graph of their adjacency relation, Equation (5) aligns with the standard definition of DP in [10, 11]. However, in this study, θ ∈ Rn is considered as both the domain X and the space of secrets Y. The primary motivation behind employing d-privacy is to preserve the topology of the parameter distributions among clients. Specifically, it aims to ensure that clients with similar model parameters in the non-privatized space X will communicate approximate model parameters in the privatized space Y, on average. 2.3 Fairness With the recent surge of interest in building ethical ways to train machine learning models, the topic of fairness in machine learning has been in the spotlight and, corre- spondingly, various metrics and algorithms to quantify and establish fairness in model training have been studied from a variety of perspectives and in different contexts [24– 26]. Most fairness metrics consider the simple case of having a privileged group and an unprivileged group in the population. Under this assumption, typically one attribute of the dataset is selected as a sensitive attribute (e.g., gender, race, etc.) that defines 4the privileged and the unprivileged groups. The goal of fairness in machine learn- ing is to ensure fair and non-discriminated results regardless of the membership in a sensitive attribute. The two main notions of fairness considered by the community are individual fairness and group fairness: Individual fairness [27] claims that simi- lar individuals should be treated similarly, and group fairness requires that different demographic subgroups should receive equal treatment with respect to their sensitive attributes. While both notions of fairness are important, this work focus on group fairness because our goal is to analyze and mitigate the potential bias against certain groups (e.g. demographic groups) through personalization techniques. The following metrics are considered for evaluating group fairness as a part of this work. In the rest of the paper, ˆY = 1, ˆY = 0 is used to represent the positive and negative prediction respectively, and S = 1, S = 0 to represent the privileged and unprivileged group. The simplest notion of fairness to be proposed was demographic parity [27]. Definition 2.1. Demographic parity is achieved by a system when the prediction ˆY of the target label Y is statistically independent of the sensitive attributes S, i.e., P � ˆY = 1|S = 1 � = P � ˆY = 1|S = 0 � (6) Imposing demographic parity has often a strong negative impact on accuracy, and, consequently, more refined notions were proposed afterwards. In particular, equalized odds and equal opportunity [28]. Definition 2.2. A system satisfies equalized odds if its prediction ˆY is conditionally independent of the sensitive attribute S given the target label Y , P � ˆY = 1|Y = y, S = 1 � = P � ˆY = 1|Y = y, S = 0 � , y ∈ {0, 1} (7) In other words, the notion of equalized odds requires the privileged and unprivi- leged groups to have equal true positive rates and equal false positive rates. Equal opportunity is a relaxation of equalized odds, in the sense that it only requires equal true positive rates across the groups. Definition 2.3. Equal opportunity is satisfied by a system if its prediction ˆY is conditionally independent of the sensitive attribute S given the target label Y P � ˆY = 1|Y = 1, S = 1 � = P � ˆY = 1|Y = 1, S = 0 � (8) In practice, however, it is difficult to obtain perfect equality for any of the afore- mentioned notions. Hence, typically the aim is to minimize the absolute value of the difference between the privileged and unprivileged groups, rather than requiring this difference to be exactly zero. For instance, the demographic parity difference is defined as ���P � ˆY = 1|S = 1 � − P � ˆY = 1|S = 0 ���� (9) and similarly for the equalized odd difference and equal opportunity difference. 5[13] [33] [14] [20] This Work Central Privacy ✓ ✓ ✓ ✓ ✓ Local Privacy × ✓ ✓ ✓ ✓ Personalization × ✓ × ✓ ✓ Mild Assumptions on Training ✓ × ✓ ✓ ✓ Fairness analysis × × × × ✓ Table 1: Qualitative comparison with the most relevant prior research on the topic. 3 Related Works Federated optimization has demonstrated suboptimal performance when the local datasets consist of samples from non-congruent distributions, resulting in the inability to simultaneously minimize both client-level and global objectives. In previous stud- ies [4–6], researchers examined various meta-algorithms for personalization, but the assertion of preserving user privacy relies solely on clients releasing updated mod- els or model updates, rather than transferring raw data to the server, which can have significant consequences. To address this issue, several works have focused on the privatization of the (federated) optimization algorithm within the framework of DP [12, 13, 29, 30], which adopt DP to provide defences against an honest-but-curious adversary. However, even in this setting, there is no guarantee of protection against sample reconstruction from the local datasets using client updates, as highlighted in [9]. Various strategies have been explored to offer local privacy guarantees, either through cryptographic approaches [31] or within the framework of local DP [14, 32, 33]. Specif- ically, in [33], the authors tackle the problem of personalized and locally differentially private federated learning, but only for the case of simple convex, 1-Lipschitz cost func- tions of the inputs. It is worth noting that this assumption is unrealistic in the majority of machine learning models and excludes many statistical modelling techniques, par- ticularly neural networks. Finally, some research focused on designing architectures capable of providing private computing environments for remote users [34], often mak- ing use of trusted platform modules, secure processors [35], or similar mechanisms [36] improving efficiency by enforcing encryption on network transmissions, rather than memory accesses. For example, the latter work conceptualizes an architecture that could be leveraged to deploy a server that can only reveal the data being processed to clients that instantiated the server. It shall be noted, however, that cryptographic guarantees of security are orthogonal to the privacy notions of differential privacy and its generalizations. To summarize and provide context around this work, Table 1 pro- vides a qualitative evaluation of relevant research and how the contributions presented in this paper fit among them. Of late, a great deal of attention has been devoted to studying and understanding the aspects of fairness in machine learning [23, 37–42]. Most of the research on fairness focuses on developing techniques to mitigate bias in machine learning algorithms. These techniques can be categorized into three main approaches: pre-processing, in- processing, and post-processing. Pre-processing techniques [43, 44] aim to generate a less biased dataset by modifying the values or adjusting the sampling process. In 6the case of in-processing techniques [45, 46], the objective function is optimized while taking into account discrimination-aware regularizers. Post-processing techniques [47, 48] involve adjusting the trained model to produce fairer outcomes. However, it is worth noting that the majority of these studies primarily target centralized machine learning models as opposed to FL. Furthermore, there is a lack of research exploring the interplay between accuracy and fairness [40, 41] or privacy and fairness [23, 49]. In particular, to the best of our knowledge, disproportionately fewer works have focused on investigating the relationship between privacy and fairness. [23] formally proved that privacy and fairness can be at odds with each other with non-trivial accuracy. A few recent works on group fairness in FL have emerged [38, 39] but they do not consider the facet of privacy-fairness trade-off. 4 An Algorithm for Private and Personalized Federated Learning Algorithm 1 aims to enable personalized federated learning while ensuring local privacy guarantees to preserve group privacy. In this context, locality refers to the sanitization of client information before it is shared with the server, while group privacy pertains to the notion of indistinguishability within a specific neighbourhood of clients, defined based on a particular distance metric. To clarify our terminology, we provide definitions for neighbourhood and group as follows: Definition 4.1. For any model parameterized by θ0, ∈, Rn, the r-neighbourhood is defined as the set of points in the parameter space that are within an L2 distance of r or less from θ0, i.e., θ ∈ Rn : ∥θ0 − θ∥2 ≤ r. Clients whose models are parameterized by θ ∈ Rn within the same r-neighbourhood are considered to be part of the same group or cluster. Algorithm 1 is inspired by the Iterative Federated Clustering Algorithm (IFCA) proposed in [4] and extends it by incorporating formal privacy guarantees. The key modifications include the introduction of the SanitizeUpdate function, as described in Algorithm 2, and the utilization of k-means for server-side clustering of the updated models. 4.1 The Laplace mechanism under Euclidean distance in Rn The SanitizeUpdate function in Algorithm 2 is based on a generalization of the Laplace mechanism to Rn under the Euclidean distance, which was originally intro- duced in [50] for geo-indistinguishability in R2. The decision to utilize the L2 norm as the distance measure serves two main purposes. First, clustering is performed on the vector space Rn of parameters, using the k-means algorithm, which relies on the Euclidean distance. By defining clusters or groups of users based on the proximity of their model parameters using the L2 norm, the procedure needs a d-privacy mechanism that obscures the reported values within each group while enabling the server to distinguish among users belonging to different clusters. Second, the use of equidistant noise vectors in the L2 norm for sanitizing the parameters ensures equiprobability by construction. This property leads to the same 7Algorithm 1 An algorithm for personalized federated learning with formal privacy guarantees in local neighbourhoods. Input: number of clusters k; initial hypotheses θ(0) j , j ∈ {1, . . . , k}; number of rounds T; number of users per round U; number of local epochs E; local step size s; user batch size Bs; noise multiplier ν; local dataset Zc held by user c. for t = {0, 1, . . . , T − 1} do ▷ Server-side loop C(t) ← SampleUserSubset(U) BroadcastParameterVectors(C(t); θ(t) j , j ∈ {1, . . . , k}) for c ∈ C(t) do in parallel ▷ Client-side loop ¯j = arg minj∈{1,...,k} Fc(θ(t) j ; Zc) θ(t) ¯j,c ← LocalUpdate(θ(t) ¯j ; s; E; Zc) ˆθ(t) ¯j,c ← SanitizeUpdate(θ(t) ¯j,c; ν) end for {S1, . . . , Sk} = k-means(ˆθ(t) ¯j,c, c ∈ C(t); θ(t) j , j ∈ {1, . . . , k}) θ(t+1) j ← 1 |Sj| � c∈Sj ˆθ(t) ¯j,c, ∀j ∈ {1, . . . , k} end for Algorithm 2 SanitizeUpdate obfuscates a vector θ ∈ Rn, with a Laplacian noise tuned on the radius of a certain neighbourhood and centered in 0. function SanitizeUpdate(θ(t) ¯j ; θ(t) ¯j,c; ν) δ(t) c = θ(t) ¯j,c − θ(t) ¯j ε = n ν∥δ(t) c ∥ Sample ρ ∼ L0,ε(x) ˆθ(t) ¯j,c = θ(t) ¯j,c + ρ return ˆθ(t) ¯j,c end function bound on the increase of the cost function in first-order approximation, as demon- strated in Proposition 4.2. The Laplace mechanism under Euclidean distance in the general space Rn is formally defined in Proposition 4.1. Proposition 4.1. Let Lε : Rn �→ Rn be the Laplace mechanism with distribution Lx0,ε(x) = P [Lε(x0) = x] = Ke−εd(x,x0) with d(.) being the Euclidean distance. If ρ ∼ Lx0,ε(x), then: 1. Lx0,ε is ε-d-private and K = εnΓ( n 2 ) 2π n 2 Γ(n) 2. ∥ρ∥2 ∼ γε,n(r) = εne−εrrn−1 Γ(n) 3. The ith component of ρ has variance σ2 ρi = n+1 ε2 where Γ(n) is the Gamma function defined for positive reals as � ∞ 0 tn−1e−t dt which reduces to the factorial function whenever n ∈ N. 8Proof. The proof can be found in Appendix A of [20]. Proposition 4.2. Let y = f(x, θ) be the fitting function of a machine learning model parameterized by θ, and (X, Y ) = Z the dataset over which the RMSE loss function F(Z, θ) is to be minimized, with x ∈ X and y ∈ Y . If ρ ∼ L0,ε, the bound on the increase of the cost function does not depend on the direction of ρ, in first-order approximation, and: ∥F(Z, θ + ρ)∥2 − ∥F(Z, θ)∥2 ≤ ∥Jf(X, θ)∥2 ∥ρ∥2 + o(∥Jf(X, θ) · ρ∥2) (10) Proof. The proof can be found in Appendix A of [20]. The results in Proposition 4.1 allow to reduce the problem of sampling a point from Laplace to i) sampling the norm of such point according to the result in Item 2 of Proposition 4.1 and then ii) sample uniformly a unit (directional) vector from the hypersphere in Rn. Much like DP, d-privacy provides a means to compute the total privacy parameters in case of repeated queries, a result known as the Compositionality Theorem for d-privacy. Theorem 4.1. Let Ki be (εi)-d-private mechanism for i ∈ {1, 2}. Then their independent composition is (ε1 + ε2)-d-private. Proof. The proof can be found in Appendix A of [20]. 4.2 A Heuristic for defining the Neighbourhood of a Client During the t-th iteration, when a user c invokes the SanitizeUpdate procedure in Algorithm 2, it has already received a set of hypotheses, optimized θ(t) ¯j (the one that fits best its data distribution), and got θ(t) ¯j,c. It is reasonable to assume that clients whose datasets are sampled from the same underlying data distribution D¯j will perform an update similar to δ(t) c . Therefore, points which are within the δ(t) c -neighbourhood of ˆθ(t) ¯j,c are forced to be indistinguishable. To provide this guarantee, the Laplace mechanism is tuned such that the points within the neighbourhood are ε∥δ(t) c ∥2 differentially private. By choosing ε = n/(νδ(t) c ), it results in ε∥δ(t) c ∥2 = n/ν, where ν is referred to as the noise multiplier. Notably, a larger value of ν corresponds to a stronger privacy guarantee. This is because the norm of the noise vector sampled from the Laplace distribution follows the distribution specified in Proposition 4.1, with an expected value of E [γε,n(r)] = n/ε. 5 Experiments The following Section discusses a number of experimental validations of Algorithm 1 on different tasks and datasets. Detailed experimental settings are discussed in Appendix B of [20], but we provide here an overview of the hardware and software stacks: All the following experiments are run on a local server running Ubuntu 20.04.3 LTS with an AMD EPYC 7282 16-Core processor, 1.5TB of RAM and 8× NVIDIA A100 GPUs. 9Python and PyTorch are the main software tools adopted for simulating the federation of clients and their corresponding collaborative training. 5.1 Characterizing privacy In this Section, we aim to evaluate and assess the trade-off in training personalized federated learning models under formal local privacy guarantees. 5.1.1 Synthetic Data Data is generated according to k = 2 different distributions: y = xT θ∗ i + u and u ∼ Uniform [0, 1), ∀i ∈ {1, 2} and θ∗ 1 = [+5, +6]T , θ∗ 2 = [+4, −4.5]T . We then assess how training progresses as we move from the Federated Averaging [51] (Figure 1a, 1b, 1c), to IFCA (Figure 1d, 1e, 1f), and finally Algorithm 1 (Figure 1g, 1h, 1i). When utilizing Federated Averaging, a noticeable issue arises: relying on a single hypothesis fails to capture the diversity present in the data distributions. As a result, the final parameters tend to settle somewhere between the optimal parameter values (see Figure 1b). Conversely, employing IFCA demonstrates that having multiple initial hypotheses enhances performance, particularly when clients possess heterogeneous data. This is evident from the nearly overlapping optimized client parameters with the true optimal parameters (see Figure 1e). By adopting our algorithm instead, not only do we provide formal guarantees, but we also achieve remarkable outcomes in terms of proximity to the optimal parameters (see Figure 1h) and reduction of the loss function (see Figure 1i). To assess privacy infringement, Figure 2 illustrates the maximum level of privacy leakage incurred by clients per cluster. 5.1.2 Hospital Charge Data This experiment utilizes the Hospital Charge Dataset obtained from the Centers for Medicare and Medicaid Services of the US Government [52]. Here, the healthcare providers are regarded as the clients who participate in training a machine learning model through federated learning. The objective is to predict the cost of a medical service based on its location in the country and the specific procedure involved. To evaluate the trade-off between privacy, personalization, and accuracy, we explore various numbers of initial hypotheses since the number of underlying data distribu- tions is unknown a priori. Accuracy is assessed at different levels of the noise multiplier ν. Notably, using Algorithm 1 with only one hypothesis yields the Federated Averag- ing algorithm. As depicted in Figure 3, employing multiple hypotheses significantly reduces the RMSE loss function, particularly when transitioning from one to three hypotheses. Furthermore, we emphasize that increasing the number of hypotheses also helps mitigate the impact of the noise multiplier, even at high levels (as shown on the right-hand side of the figure). This highlights the importance of adopting for- mal privacy guarantees when a slight increase in the cost function is acceptable. The empirical distribution of privacy leakage among clients involved in a specific train- ing configuration is illustrated in Figure 4. Table 2 presents privacy leakage statistics across multiple rounds and configurations. 100 1 2 3 4 5 θ i, 0 −4 −2 0 2 4 6 θ i, 1 real client parameters optimal parameters θ * 1 , θ * 0 (a) First round −2 0 2 4 6 8 θ i, 0 −4 −2 0 2 4 6 θ i, 1 real client parameters optimal parameters θ * 1 , θ * 0 (b) Best round 0 4 8 12 round 1.8 2.0 2.2 2.4 2.6 2.8 RMSE 1.802 (c) Validation loss 0 1 2 3 4 5 θ i, 0 −4 −2 0 2 4 6 θ i, 1 real client parameters optimal parameters θ * 1 , θ * 0 (d) First round −2 0 2 4 6 8 θ i, 0 −4 −2 0 2 4 6 θ i, 1 real client parameters optimal parameters θ * 1 , θ * 0 (e) Best round 0 4 8 12 16 20 24 28 round 0.0 0.5 1.0 1.5 2.0 2.5 RMSE 0.021 (f) Validation loss −4 −2 0 2 4 θ i, 0 −4 −2 0 2 4 6 θ i, 1 real client parameters optimal parameters θ * 1 , θ * 0 sanitized client parameters (g) First round −2.5 0.0 2.5 5.0 7.5 10.0 θ i, 0 −4 −2 0 2 4 6 8 10 θ i, 1 real client parameters optimal parameters θ * 1 , θ * 0 sanitized client parameters (h) Best round 0 4 8 12 16 round 0.0 0.5 1.0 1.5 2.0 2.5 3.0 RMSE 0.093 (i) Validation loss Fig. 1: (From [20]) Learning federated linear models with: (a, b, c) one initial hypothe- sis and non-sanitized communication, (d, e, f) two initial hypotheses and non-sanitized communication, (g, h, i) two initial hypotheses and sanitized communication. The first two figures of each row show the parameter vectors released by the clients to the server. 11Fig. 2: (From [20]) Synthetic data: max privacy leakage among clients. Privacy leakage is constant when clients with the largest privacy leakage are not sampled (by chance) to participate in those rounds. 0 0.1 1 2 3 5 Noise Multiplier 0.80 0.92 1.04 1.16 1.28 1.40 1.52 1.64 1.76 1.88 2.00 RMSE Model Performance T rends Initial Hypotheses 7 5 3 1 Fig. 3: (From [20]) RMSE for models trained with Algorithm 1 on the Hospital Charge Dataset. Error bars show ±σ, with σ the empirical standard deviation. Lower RMSE values are better for accuracy. 5.1.3 FEMNIST Image Classification This task involves character recognition from images using the FEMNIST dataset [53]. When selecting the range of noise multipliers ν, the resulting privacy leakage ε∥δ(t) c ∥2 = n/ν would be exceptionally large, given the CNN’s n = 206590 12Fig. 4: (From [20]) Hospital charge data: the empirical distribution of the privacy budget over the clients for ν = 3, 5 initial hypotheses, seed = 3, r is the radius of the neighbourhood, the total number of clients is 2062. Hypotheses ν 7 5 3 1 0 −, − −, − −, − −, − 0.1 517.0, 1551.0 418.0, 1342.0 473.0, 1386.0 528.0, 1540.0 1 36.3, 126.5 40.7, 127.6 44.0, 138.6 49.5, 147.4 2 15.4, 57.8 14.3, 54.5 22.0, 69.3 21.5, 66.6 3 7.7, 32.3 8.4, 36.7 12.5, 40.0 12.1, 40.0 5 5.7, 21.3 5.9, 22.0 5.5, 21.6 5.3, 20.9 Table 2: (From [20]) Hospital charge data: median and maximum local privacy bud- gets over the whole set of clients, averaged over 10 runs with different seeds. ν = 0 means no privacy guarantee. parameters. Consequently, this renders the mechanism incapable of providing mean- ingful theoretical privacy guarantees. This issue is commonly encountered with local privacy mechanisms [54], as the expected value of the noise vector’s norm, E [γε,n(r)], exhibits a linear dependence on n: n/ε. However, it is still possible to evaluate, in practice, whether this specific general- ization of the Laplace mechanism can effectively defend against a particular attack known as DLG [9]. The outcomes of varying noise multiplier values are presented in Figure 5, and Table 3 provides additional details. Notably, when ν = 10−3, the ground truth image can be fully reconstructed. Partial reconstruction remains possible up to ν = 10−1. However, for ν ≥ 1, experimental results demonstrate the failure of the DLG attack to reconstruct input samples when the communication between the client and server is protected by the mechanism outlined in Proposition 4.1. 13Cross Entropy loss RMSE loss ν Average Accuracy Standard Deviation Average Accuracy Standard Deviation 0 0.832 ± 0.012 0.801 ± 0.001 0.001 0.843 ± 0.006 0.813 ± 0.014 0.01 0.832 ± 0.017 0.805 ± 0.008 0.1 0.834 ± 0.026 0.808 ± 0.019 1 0.834 ± 0.014 0.814 ± 0.012 3 0.835 ± 0.017 0.825 ± 0.010 5 0.812 ± 0.016 0.787 ± 0.003 10 0.692 ± 0.002 0.687 ± 0.014 15 0.561 ± 0.005 0.622 ± 0.003 Table 3: (From [20]) Effects of increasing the noise multiplier on the validation accuracy and standard deviation. 100 101 102 103 iterations 10−5 10−3 10−1 101 103 105 gradient match loss function Noise Multiplier 0.001 0.01 0.1 1 3 5 10 15 Fig. 5: (From [20]) Effects of the Laplace mechanism in Proposition 4.1 with different noise multipliers as a defence strategy against the DLG attack. 5.2 Fairness analysis In this section, we analyze how group fairness improves with the personalization of the trained models under d-privacy guarantees when there are two groups with different data distributions. Experiments were performed on synthetic data and the FEMNIST image classification dataset that was used in Section 5.1. To ensure a thorough evalua- tion, we considered a variety of group fairness metrics in the experiments. In particular, 14we measured the fairness with respect to equal opportunity [28], equalized odds [28], and demographic parity [27] as explained in Section 2.3. In particular, in Figures 7 and 8, the X-axis denotes the noise multiplier ν rep- resenting the amount of d-private noise added to the local updates as explained in Section 4.2 and the Y -axis denotes the absolute value of the difference in fairness between the privileged and unprivileged groups with respect to the different metrics of group fairness that we considered. 5.2.1 Synthetic data Fig. 6: The first two plots from the left illustrate the spatial distribution of the samples in g1 and g2, respectively, and the third plot shows g1 and g2 superimposed together in the same space. Synthetic data was generated in a method similar to that in Section 5.1.1 with the following modifications to enable us to investigate the aspect of group fairness fostered by our method: i) Total number of users is 1000 and each user holds 10 samples. 800 15users have data that is generated according to distributions y = xT θ1 + u and u ∼ Uniform [0, 1), ∀i ∈ {1, 2}, and set as a privileged majority group g1. The remaining 200 users have data that is generated according to distribution y = xT θ2 + 15 + u and u ∼ Uniform [0, 1), ∀i ∈ {1, 2}, and set as an unprivileged minority group g2. In this case, the sensitive attribute considered to evaluate fairness is the group id G where G ∈ {g1, g2}. ii) For binary classification, we set labels by using the z = Sigmoid(Y ), ∀ y, ˆy ∈ Y . In the case of g1, we assign the label 1 if the value of z is greater than or equal to 0.5 and assign the label 0 otherwise. On the other hand, in the case of g2, the label 1 is assigned when the z = Sigmoid(Y − 15), ∀ y, ˆy ∈ Y is less than or equal to 0.5, and the label 0 is assigned otherwise. This setting is to simulate a situation in which discrimination occurs depending on sensitive attributes in the real world such as minorities would have experienced a higher loan rejection rate than white applicants with the same property [55]. Thus, in our experiment, label 1 could be interpreted as “loan approved” and label 0 as “loan denied”. The data generated in this way are shown in Figure 6. We compared the fairness for two cases: one with a single hypothesis (no person- alization) and the other with the number of hypotheses as 2 (with personalization) in the framework of Algorithm 1. The experimental results are demonstrated in Figure 7. The results illustrated by Figure 7 assert that the personalization of models (i.e., Algorithm 1) enhances the group fairness under all the metrics and the levels of formal privacy guarantees compared to that of the non-personalized model. A major reason behind this significant improvement of fairness by the personalized model is that unlike the non-personalized model, which trains using data from both groups that are biased towards the majority group g1, the personalized model training optimizes for each group’s data distribution without disregarding the effect of the minority group g2. We also observe that fairness deteriorates as the value of the noise multiplier increases, as we would expect. This is presumably due to the decreasing influence of the minority group g2 as the amount of noise insertion increases. This is consistent with the philosophy behind and the definition of DP and its variants. Furthermore, interestingly we observe that the personalized model ensures better fairness than the non-personalized model even with the highest level of privacy protection. This shows that personalization in FL under d-privacy can be a comprehensive solution towards privacy-preserving and ethical machine learning as it provides both privacy guarantees and enhanced fairness. 5.2.2 FEMNIST Image Classification To evaluate the fairness of our method on real datasets, we considered FEMNIST image classification dataset in the same form as in Section 5.1.3. As in experiments performed with the synthetic data in Section 5.2.1, the size of the groups considered privileged and unprivileged were different denoting the existence of a majority and a minority in the population. In this part, the rotated images are set as the unprivileged group g2 with a total number of sampled users of 382 forming only 20% of all users. and the un-rotated images are used to represent the privileged group g1 with a total number of users of 1736. Like in the case of synthetic data considered before, the 160.1 1.0 2.0 4.0 Noise multiplier 0.04 0.06 0.08 0.10 0.12 0.14 0.16 0.18 Equal opportunity difference global personal (a) Equal opportunity difference 0.1 1.0 2.0 4.0 Noise multiplier 0.05 0.10 0.15 0.20 0.25 0.30 Equalized odds difference global personal (b) Equalized odds difference 0.1 1.0 2.0 4.0 Noise multiplier 0.00 0.05 0.10 0.15 0.20 0.25 Demographic parity difference global personal (c) Demographic parity difference Fig. 7: The figure shows the comparison between the personalized and non- personalized models for (from left) equal opportunity, equalized odds, and demo- graphic parity, respectively. Experiments were performed for noise multipliers ν of 0.1, 1, 2, and 4. For all the metrics of fairness and the values of the noise multiplier, the per- sonalized model is seen to possess improved fairness over the non-personalized model. group membership was used to denote the sensitive attribute. In the case of g1, we assign label 1 if the FEMNIST image label is even and 0 if it is odd. And for the g2, we assign label 0 if the FEMNIST image label is even and assign 1 if it is odd. The experimental results are given by Figure 8. We observe that the personalized model training harbours significantly better group fairness across all metrics compared to its non-personalized counterpart. The change in fairness due to the amount of noise added was not as notable as in the case of the synthetic dataset but it was still observed to deteriorate with an increase in 170.1 1.0 2.0 4.0 Noise multiplier 0.04 0.06 0.08 0.10 0.12 0.14 Equal opportunity difference global personal (a) Equal opportunity difference 0.1 1.0 2.0 4.0 Noise multiplier 0.02 0.04 0.06 0.08 0.10 Equalized odds difference global personal (b) Equalized odds difference 0.1 1.0 2.0 4.0 Noise multiplier 0.04 0.06 0.08 0.10 0.12 0.14 0.16 0.18 Demographic parity difference global personal (c) Demographic parity difference Fig. 8: The figure shows the comparison between the personalized and non- personalized models for equal opportunity equalized odds, and demographic parity. Experiments were performed for noise multipliers ν of 0.1, 1, 2, and 4. For all metrics of fairness and values of the noise multiplier, the personalized model improved fairness over the non-personalized model. the value of the noise multiplier. Personalized model training in FL under the high- est level of privacy is still observed to have better fairness across all the metrics than (non-personalized) models trained in a classical FL framework even with no privacy, similar to what we observed in the experiments with the synthetic data. 6 Conclusion This work builds upon our previous research on personalized federated learning with metric privacy guarantees. To ensure the privacy of ML model parameters during 18transmission, we employ d-privacy techniques for sanitization. The objective of this process is to generate personalized models that converge to optimal parameters, cater- ing to the diverse datasets present in the federated learning setting. Given the presence of multiple, unknown data distributions among the individuals participating in the federated learning process, we make a reasonable assumption of a mixture of these dis- tributions. To effectively aggregate clients with similar data distributions, we employ a clustering approach using k-means on the sanitized parameter vectors. This method proves suitable because d-private mechanisms preserve the underlying topology of the true value domain. Notably, our mechanism shows particular promise for machine learning models with a relatively small number of parameters. Although the formal privacy guarantees diminish with larger models, experimental results demonstrate the effectiveness of the Laplace mechanism against the DLG attack. In addition to metric privacy guarantees, we also evaluate the fairness of machine learning models trained using personalized federated learning and d-privacy. Our study assesses various group fairness metrics, including equal opportunity, equalized odds, and demographic parity. The consistent findings demonstrate that personalized models significantly improve group fairness across all evaluated metrics and privacy levels. Moreover, they, unlike non-personalized models, optimize for each group’s specific data distribution, effectively mitigating biases towards the majority group. Consequently, significant advancements in fairness are achieved through this approach. The level of fairness is influenced by the incorporation of d-private noise in the local updates. As the noise increases, the influence of the minority group decreases, resulting in a deterioration of fairness. This behaviour aligns with the principles of differential privacy and the expected impact of noise addition on group fairness. Remarkably, even with the highest level of privacy protection, personalized models consistently maintain superior fairness compared to non-personalized models. This observation highlights the potential of personalized model training in federated learning under d-privacy as a comprehensive solution for privacy-preserving and ethical machine learning. By offer- ing privacy guarantees alongside enhanced fairness, personalized models demonstrate their effectiveness in balancing these critical aspects. 7 Conflict of Interest On behalf of all authors, the corresponding author states that there is no conflict of interest. References [1] Le M´etayer, D., De, S.J.: PRIAM: a Privacy Risk Analysis Methodology. In: Livraga, G., Torra, V., Aldini, A., Martinelli, F., Suri, N. (eds.) Data Privacy Management and Security Assurance. Springer, Heraklion, Greece (2016). https: //hal.inria.fr/hal-01420983 [2] NIST: NIST Privacy Framework Core (2021). https://www.nist.gov/system/ files/documents/2021/05/05/NIST-Privacy-Framework-V1.0-Core-PDF.pdf 19[3] McMahan, B., Moore, E., Ramage, D., Hampson, S., Arcas, B.A.: Communication-efficient learning of deep networks from decentralized data. In: Artificial Intelligence and Statistics, pp. 1273–1282 (2017). PMLR [4] Ghosh, A., Chung, J., Yin, D., Ramchandran, K.: An efficient framework for clustered federated learning. Advances in Neural Information Processing Systems 33, 19586–19597 (2020) [5] Mansour, Y., Mohri, M., Ro, J., Suresh, A.T.: Three approaches for personal- ization with applications to federated learning. arXiv preprint arXiv:2002.10619 (2020) [6] Sattler, F., M¨uller, K.-R., Samek, W.: Clustered federated learning: Model- agnostic distributed multitask optimization under privacy constraints. IEEE transactions on neural networks and learning systems 32(8), 3710–3722 (2020) [7] Hitaj, B., Ateniese, G., Perez-Cruz, F.: Deep models under the gan: informa- tion leakage from collaborative deep learning. In: Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, pp. 603–618 (2017) [8] Nasr, M., Shokri, R., Houmansadr, A.: Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning. In: 2019 IEEE Symposium on Security and Privacy (SP), pp. 739–753 (2019). IEEE [9] Zhu, L., Liu, Z., Han, S.: Deep leakage from gradients. Advances in Neural Information Processing Systems 32 (2019) [10] Dwork, C., McSherry, F., Nissim, K., Smith, A.: Calibrating noise to sensitivity in private data analysis. In: Halevi, S., Rabin, T. (eds.) Theory of Cryptography, pp. 265–284. Springer, Berlin, Heidelberg (2006) [11] Dwork, C., Kenthapadi, K., McSherry, F., Mironov, I., Naor, M.: Our data, our- selves: Privacy via distributed noise generation. In: Vaudenay, S. (ed.) Advances in Cryptology - EUROCRYPT 2006, pp. 486–503. Springer, Berlin, Heidelberg (2006) [12] Andrew, G., Thakkar, O., McMahan, B., Ramaswamy, S.: Differentially pri- vate learning with adaptive clipping. Advances in Neural Information Processing Systems 34 (2021) [13] McMahan, H.B., Ramage, D., Talwar, K., Zhang, L.: Learning differentially private recurrent language models. In: International Conference on Learning Representations (2018). https://openreview.net/forum?id=BJ0hF1Z0b 20[14] Truex, S., Liu, L., Chow, K.-H., Gursoy, M.E., Wei, W.: Ldp-fed: Federated learn- ing with local differential privacy. In: Proceedings of the Third ACM International Workshop on Edge Systems, Analytics and Networking, pp. 61–66 (2020) [15] Zhao, Y., Zhao, J., Yang, M., Wang, T., Wang, N., Lyu, L., Niyato, D., Lam, K.-Y.: Local differential privacy-based federated learning for internet of things. IEEE Internet of Things Journal 8(11), 8836–8853 (2020) [16] Chatzikokolakis, K., Andr´es, M.E., Bordenabe, N.E., Palamidessi, C.: Broadening the scope of differential privacy using metrics. In: International Symposium on Privacy Enhancing Technologies Symposium, pp. 82–102 (2013). Springer [17] Biswas, S., Palamidessi, C.: PRIVIC: A privacy-preserving method for incremen- tal collection of location data (2023) [18] Fernandes, N., McIver, A., Palamidessi, C., Ding, M.: Universal optimality and robust utility bounds for metric differential privacy. In: 2022 IEEE 35th Computer Security Foundations Symposium (CSF), pp. 348–363 (2022). https://doi.org/10. 1109/CSF54842.2022.9919647 [19] Atmaca, U.I., Biswas, S., Maple, C., Palamidessi, C.: A privacy preserving querying mechanism with high utility for electric vehicles (2022) [20] Galli, F., Biswas, S., Jung, K., Cucinotta, T., Palamidessi, C.: Group Privacy for Personalized Federated Learning. In: Proceedings of the 9th International Conference on Information Systems Security and Privacy - ICISSP, pp. 252–263 (2023). https://doi.org/10.5220/0011885000003405 . SciTePress - INSTICC [21] Berk, R., Heidari, H., Jabbari, S., Kearns, M., Roth, A.: Fairness in criminal justice risk assessments: The state of the art. Sociological Methods & Research 50(1), 3–44 (2021) [22] Chouldechova, A.: Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big data 5(2), 153–163 (2017) [23] Agarwal, S.: Trade-Offs between fairness and privacy in machine learning. IJCAI 2021 Workshop on AI for Social Good. 2021 (2022) [24] Verma, S., Rubin, J.: Fairness definitions explained. In: Proceedings of the International Workshop on Software Fairness, pp. 1–7 (2018) [25] Hanna, R., Linden, L.: Measuring discrimination in education. Technical report, National Bureau of Economic Research (2009) [26] Makhlouf, K., Zhioua, S., Palamidessi, C.: On the applicability of machine learning fairness notions. ACM SIGKDD Explorations Newsletter 23(1), 14–23 (2021) 21[27] Dwork, C., Hardt, M., Pitassi, T., Reingold, O., Zemel, R.: Fairness through awareness. In: Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, pp. 214–226 (2012) [28] Hardt, M., Price, E., Srebro, N.: Equality of opportunity in supervised learning. Advances in neural information processing systems 29 (2016) [29] Abadi, M., Chu, A., Goodfellow, I., McMahan, H.B., Mironov, I., Talwar, K., Zhang, L.: Deep learning with differential privacy. In: Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, pp. 308– 318 (2016) [30] Geyer, R.C., Klein, T., Nabi, M.: Differentially private federated learning: A client level perspective. arXiv preprint arXiv:1712.07557 (2017) [31] Bonawitz, K.A., Ivanov, V., Kreuter, B., Marcedone, A., McMahan, H.B., Patel, S., Ramage, D., Segal, A., Seth, K.: Practical secure aggregation for federated learning on user-held data. In: NIPS Workshop on Private Multi-Party Machine Learning (2016). https://arxiv.org/abs/1611.04482 [32] Agarwal, N., Suresh, A.T., Yu, F.X.X., Kumar, S., McMahan, B.: cpsgd: Communication-efficient and differentially-private distributed sgd. Advances in Neural Information Processing Systems 31 (2018) [33] Hu, R., Guo, Y., Li, H., Pei, Q., Gong, Y.: Personalized federated learning with differential privacy. IEEE Internet of Things Journal 7(10), 9530–9539 (2020) [34] Bonawitz, K.A., Ivanov, V., Kreuter, B., Marcedone, A., McMahan, H.B., Patel, S., Ramage, D., Segal, A., Seth, K.: Practical secure aggregation for federated learning on user-held data. In: NIPS Workshop on Private Multi-Party Machine Learning (2016). https://arxiv.org/abs/1611.04482 [35] Chhabra, S., Solihin, Y., Lal, R., Hoekstra, M.: An analysis of secure processor architectures. Transactions on computational science VII, 101–121 (2010) [36] Cucinotta, T., Cherubini, D., Jul, E.: Confidential execution of cloud services. In: CLOSER, pp. 616–621 (2014) [37] Chhabra, A., Masalkovait˙e, K., Mohapatra, P.: An overview of fairness in clustering. IEEE Access 9, 130698–130720 (2021) [38] Ezzeldin, Y.H., Yan, S., He, C., Ferrara, E., Avestimehr, S.: Fairfed: Enabling group fairness in federated learning. In: 1st NeurIPS Workshop on New Frontiers in Federated Learning (2021). https://arxiv.org/abs/1611.04482 [39] Chu, L., Wang, L., Dong, Y., Pei, J., Zhou, Z., Zhang, Y.: Fedfair: Training fair models in cross-silo federated learning. arXiv preprint arXiv:2109.05662 (2021) 22[40] Menon, A.K., Williamson, R.C.: The cost of fairness in binary classification. In: Conference on Fairness, Accountability and Transparency, pp. 107–118 (2018). PMLR [41] Wick, M., Tristan, J.-B., et al.: Unlocking fairness: a trade-off revisited. Advances in neural information processing systems 32 (2019) [42] Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., Galstyan, A.: A survey on bias and fairness in machine learning. ACM Computing Surveys (CSUR) 54(6), 1–35 (2021) [43] Biswas, S., Rajan, H.: Fair preprocessing: towards understanding compositional fairness of data transformers in machine learning pipeline. In: Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pp. 981–993 (2021) [44] Kamiran, F., Calders, T.: Data preprocessing techniques for classification without discrimination. Knowledge and information systems 33(1), 1–33 (2012) [45] Wan, M., Zha, D., Liu, N., Zou, N.: In-processing modeling techniques for machine learning fairness: A survey. ACM Transactions on Knowledge Discovery from Data 17(3), 1–27 (2023) [46] Hashimoto, T., Srivastava, M., Namkoong, H., Liang, P.: Fairness without demo- graphics in repeated loss minimization. In: International Conference on Machine Learning, pp. 1929–1938 (2018). PMLR [47] Petersen, F., Mukherjee, D., Sun, Y., Yurochkin, M.: Post-processing for individ- ual fairness. Advances in Neural Information Processing Systems 34, 25944–25955 (2021) [48] Noriega-Campero, A., Bakker, M.A., Garcia-Bulle, B., Pentland, A.: Active fair- ness in algorithmic decision making. In: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pp. 77–83 (2019) [49] Cummings, R., Gupta, V., Kimpara, D., Morgenstern, J.: On the compatibility of privacy and fairness. In: Adjunct Publication of the 27th Conference on User Modeling, Adaptation and Personalization, pp. 309–315 (2019) [50] Andr´es, M.E., Bordenabe, N.E., Chatzikokolakis, K., Palamidessi, C.: Geo- indistinguishability: Differential privacy for location-based systems. In: Proceed- ings of the 2013 ACM SIGSAC Conference on Computer & Communications Security, pp. 901–914 (2013) [51] Koneˇcn´y, J., McMahan, H.B., Yu, F.X., Richtarik, P., Suresh, A.T., Bacon, D.: Federated learning: Strategies for improving communication efficiency. In: NIPS Workshop on Private Multi-Party Machine Learning (2016). https://arxiv.org/ 23abs/1610.05492 [52] CMMS: Centers for Medicare and Medicaid Services. Accessed: 2022-09-21 (2021). https://www.cms.gov/mmrr/News/mmrr-news-2013-03-hosp-chg-data.html [53] Caldas, S., Duddu, S.M.K., Wu, P., Li, T., Koneˇcn`y, J., McMahan, H.B., Smith, V., Talwalkar, A.: Leaf: A benchmark for federated settings. Workshop on Federated Learning for Data Privacy and Confidentiality (2019) [54] Bassily, R., Nissim, K., Stemmer, U., Guha Thakurta, A.: Practical locally private heavy hitters. In: Guyon, I., Luxburg, U.V., Bengio, S., Wal- lach, H., Fergus, R., Vishwanathan, S., Garnett, R. (eds.) Advances in Neural Information Processing Systems, vol. 30. Curran Associates, Inc., Red Hook, NY, USA (2017). https://proceedings.neurips.cc/paper/2017/file/ 3d779cae2d46cf6a8a99a35ba4167977-Paper.pdf [55] Bartlett, R., Morse, A., Stanton, R., Wallace, N.: Consumer-lending discrimina- tion in the fintech era. Journal of Financial Economics 143(1), 30–56 (2022) 24