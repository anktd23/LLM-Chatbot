arXiv:2308.16192v1 [econ.EM] 27 Aug 2023 High Dimensional Time Series Regression Models: Applications to Statistical Learning Methods* Christis Katsouris Department of Economics, University of Southampton University of Exeter Business School September 1, 2023 Abstract These lecture notes provide an overview of existing methodologies and recent developments for estimation and inference with high dimensional time series regression models. First, we present main limit theory results for high dimensional dependent data which is relevant to covariance matrix structures as well as to dependent time series sequences. Second, we present main aspects of the asymptotic theory related to time series regression models with many covariates. Third, we discuss various applications of statistical learning methodologies for time series analysis purposes. *Part of these lecture notes were prepared during my Ph.D. studies at the Department of Economics, University of Southampton. Moreover, these notes were motivated by discussions and presentations during the weekly sessions of the Time Series and Machine Learning Reading Group at the School of Mathematical Sciences, Statistics Division, University of Southampton. The author is grateful to Zudi Lu and Chao Zheng for stimulating discussions. The author is also grateful for stimulating seminar talks and discussions at the Southampton Statistical Sciences Research Institute seminar series. This draft was prepared during the academic year 2022-2023 at the Department of Economics, University of Exeter Business School. Dr. Christis Katsouris is a Lecturer in Economics, University of Exeter Business School, Exeter EX4 4PU, United Kingdom. Email Address: christiskatsouris@gmail.com 1Contents 1. Introduction 5 2. Limit theory for High Dimensional Dependent Data 6 2.1. Limit theory of Covariance Matrices for Linear Processes . . . . . . . . . . . . . . . . . . . . . . 6 2.1.1. Covariance and Precision matrix estimation for high dimensional time series . . . . . . . 6 2.1.2. Main Results on Probability Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.1.3. Limiting Laws of Coherence of Random Matrices with Applications . . . . . . . . . . . . 13 2.1.4. Large deviations for quadratic forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.1.5. Asymmetry Helps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 2.2. Limit theory for Functional Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 2.2.1. Simultaneous Diagonalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 2.2.2. Common Functional Principal Components . . . . . . . . . . . . . . . . . . . . . . . . . 23 2.3. Limit theorems for Dependent Sequences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 2.3.1. Method of bounded martingale differences . . . . . . . . . . . . . . . . . . . . . . . . . 25 2.3.2. Concentration Inequalities for Dependent Random Variables . . . . . . . . . . . . . . . . 25 2.3.3. Central limit theorems for high dimensional dependent data . . . . . . . . . . . . . . . . 26 2.3.4. Sub-Weibull random vectors under β−mixing . . . . . . . . . . . . . . . . . . . . . . . . 29 2.3.5. Approximation Theorems for Strongly Mixing Random Variables . . . . . . . . . . . . . 29 2.4. Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 3. Time Series Regression Models with Many Covariates 31 3.1. A forecasting Application . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 3.1.1. Best Linear Predictor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 3.1.2. Forecast Combination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 3.2. Statistical Theory for Lasso Regression Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 3.2.1. Statistical Principles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 3.2.2. Dantzig Selector . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 3.2.3. Oracle inequalities for prediction loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 3.3. Parameter Estimation and Model Selection Consistency Properties . . . . . . . . . . . . . . . . . 40 3.3.1. Consistency Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 3.4. A Lasso-based Time Series Regression Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 3.4.1. Oracle Property . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 3.5. Lasso shrinkage with long memory regression errors . . . . . . . . . . . . . . . . . . . . . . . . 45 3.5.1. Results with ﬁnite sample . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 3.6. Residual empirical process based on the ALasso . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 3.6.1. Main results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 23.6.2. Sparsity in high dimensional estimation problems . . . . . . . . . . . . . . . . . . . . . . 50 3.7. Lasso shrinkage for Predictive Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 3.8. Lasso estimation with a structural break . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 3.9. Classical Shrinkage Type Estimation Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 3.9.1. Limiting Distributional Risk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 3.9.2. Asymptotic representation of estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 3.10. Lasso Inference for High Dimensional Time Series under NED . . . . . . . . . . . . . . . . . . . 56 3.10.1. Inference on low-dimensional parameters . . . . . . . . . . . . . . . . . . . . . . . . . . 59 3.10.2. Error Bound and the Consistency for the Lasso . . . . . . . . . . . . . . . . . . . . . . . 59 3.10.3. Uniformly Valid Inference via the Disparsiﬁed Lasso . . . . . . . . . . . . . . . . . . . . 60 4. High Dimensional Feature Selection Methods 61 4.1. Ultra-high dimensionality under dependence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 4.1.1. SIS with dependent observations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 4.1.2. Experimental design examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 4.2. Uniform Asymptotic Inference and Model Selection . . . . . . . . . . . . . . . . . . . . . . . . . 66 4.2.1. Uniform-in-Submodel Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 4.3. Non-nested Regressions and Variable Selection Speciﬁcation Testing . . . . . . . . . . . . . . . . 68 4.4. Divide and Conquer Variable Selection Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 70 4.5. Sample-Splitting and Variable Importance Algorithm . . . . . . . . . . . . . . . . . . . . . . . . 73 4.5.1. The projection parameter β�S . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 4.5.2. The LOCO parameters γ�S and φ�S . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 4.5.3. The prediction parameter ρ�S . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 4.5.4. Conﬁdence sets for the projection parameters: The bootstrap . . . . . . . . . . . . . . . . 76 4.6. Multiple Testing Procedure and Variable Selection . . . . . . . . . . . . . . . . . . . . . . . . . . 77 4.6.1. Rejection Principle of Familywise Error Control . . . . . . . . . . . . . . . . . . . . . . 78 4.6.2. Subgroup Selection Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80 4.7. Model Selection in Cointegrating Regressions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82 4.7.1. Model Selection Consistency and Oracle Property . . . . . . . . . . . . . . . . . . . . . 83 4.7.2. Model Selection and Rank of a Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 5. Statistical Learning Methods in Time Series Analysis 85 5.1. Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 5.2. Non-Asymptotic Probability Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86 5.3. Shallow Neural Network Estimate learned by Gradient Descent . . . . . . . . . . . . . . . . . . . 87 5.3.1. Learning of linear penalized least squares estimates by gradient descent . . . . . . . . . . 89 5.4. Deep Neural Network Estimate learned by Gradient Descent . . . . . . . . . . . . . . . . . . . . 92 5.5. Deep Neural Network Architecture Approximations . . . . . . . . . . . . . . . . . . . . . . . . . 96 35.5.1. Error Bounds of DNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96 5.5.2. Topological Space for DNN Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . 99 5.6. Statistical Inference with Stochastic Gradient Descent . . . . . . . . . . . . . . . . . . . . . . . . 100 5.6.1. Literature Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 5.6.2. Preliminary Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 5.6.3. Assumptions and error bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102 5.6.4. Estimators for asymptotic covariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103 5.6.5. Averaged SGD Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103 5.7. Sieve Estimation Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 5.7.1. Asymptotic Properties of Sieve Estimation . . . . . . . . . . . . . . . . . . . . . . . . . 104 5.7.2. Sieve Estimation for Panel Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 5.7.3. Sieve M inference on irregular parameters . . . . . . . . . . . . . . . . . . . . . . . . . . 106 5.8. Further Statistical Algorithms in Economic Applications . . . . . . . . . . . . . . . . . . . . . . 108 A Elements of Weak Convergence of Empirical Processes 109 A1. Sub-Gaussian processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109 A2. Clivenko-Cantelli theorems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110 A3. Donsker Theorems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111 B Elements of Stochastic Processes 112 B1. Asymptotic Equicontinuity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112 B2. LLNs for Hilbert Space-Valued Mixingales . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114 C Elements of Bayesian Statistics 115 References 129 41. Introduction These lecture notes series present a uniﬁed framework for recent developments of deep learning method- ologies in time series econometrics and statistics. Speciﬁcally, statistical machine learning methods and theory are now widespread in social sciences, economics1, ﬁnance and mathematical modelling. Neural networks can be considered as a type of linear sieve estimation where the basis functions them- selves are ﬂexibly learned from the data by optimizing over many combinations of simple functions (see, Farrell et al. (2021)). Deep reinforcement learning has gain attention in recent years especially when modelling dynamic interactions of economic agents. In particular deep neural nets contain many hidden layers of neurons between the input and output layers, which has been found to exhibit superior perfor- mance across a variety of contexts. Our goal is to consolidate the theoretical and practical underpinnings of machine learning methods when modelling time series data for estimation, inference and forecasting purposes. In order to express precise conditions for ergodicity, we turn to the Lyapounov exponent, a concept which is well-known by those studying stability of dynamical systems. Therefore, in the context of nonlinear time series, nonstability means explosive behaviour. Thus, the Lyapounov exponent, as we deﬁne it for the state space model {Xt} of a time series, is given by γ = lim inf n→∞ lim sup ∥x∥→∞ 1 n (1.1) which measures the drift of the process when the sample size is large. Deﬁne with ℓ1 = � (xn)∞ n=1 : ∑ n≥1 |xn| < ∞ � (1.2) ℓ2 = � (xn)∞ n=1 : ∑ n≥1 x2 n < ∞ � (1.3) The sets ℓ1,ℓ2 and ℓ∞ are all vector spaces and it holds that ∥(xn)∥1 = ∑n≥1 |xn|, ∥(xn)∥2 = ∑n≥1 � ∑n≥1 x2 n �1/2 deﬁne norms on ℓ1 and ℓ2 spaces respectively. For any x ∈ Rn, ∥x∥ = � ∑n i=1x2 i , ∥x∥ℓ1 = ∑n i=1 |xi| and ∥x∥∞ = max 1≤i≤n|xi|, denote ℓ2, ℓ1 and ℓ∞−norms respectively. We also denote with ∥x∥ℓ0 = ∑n i=11{xi ̸= 0} which is simply the number of non-zero entries of x. For any n×n matrix M, ∥M∥ℓ∞ = max1≤i≤n ∑n j=1|Mi, j| denotes the induced ℓ∞ matrix norm. Detailed statistical theory for high dimensional statistics applica- tions are presented in Wainwright (2019) (see also Vershynin (2018)). Let {Xt,n}n t=1, n ∈ N be a random array and {Ft}∞ t=−∞ a ﬁltration such that Xt,n is Ft−measurable for all t and n. Based on the aforemen- tioned notation we discuss important theoretical results from the statistics and econometrics literature. 1The idea of representing the dynamic interactions of economic agents similar to the structure of brain neural networks dates back to Ashby (1957). Furthermore the application of neural networks for modelling purposes dates back to the funda- mental works of Hebb (1949) and Rosenblatt (1958). 52. Limit theory for High Dimensional Dependent Data 2.1. Limit theory of Covariance Matrices for Linear Processes 2.1.1. Covariance and Precision matrix estimation for high dimensional time series Following the framework of Chen et al. (2013), suppose we have n temporally observed p−dimensional vectors (zi)n i=1 having mean zero and covariance matrix Σi = E(zi,zi) whose dimension is p × p. Our goal is to estimate the covariance matrices Σi and their inverses Ωi = Σ−1 i based on the data matrix Zp×n = (z1,...,zn). In the classical situation where p is ﬁxed, n → ∞ and zi are mean zero independent and identically distributed i.i.d random vectors, it is well known that the sample covariance matrix ˆΣn = 1 n n ∑ i=1 ziz⊤ i (2.1) is a consistent and well behaved estimator of Σ, and ˆΩn = ˆΣ−1 n is a natural and good estimator of Ω. However, when the dimensionality p grows with n, random matrix theory asserts that ˆΣn is no longer a consistent estimate of Σ in the sense that its eigenvalues do not converge to those of Σ, as the Marcenko- Pastur law states. Moreover, it is clear that ˆΩn is not deﬁned when ˆΣn is not invertible in the high- dimensional case with p >> n. • Let Tu � �Σu � = Q ˆΛQ⊤ = ∑p j=1 ˆλ jqjq⊤ j be its eigen-decomposition, where Q is an orthonormal matrix and ˆΛ is a diagonal matrix. For v > 0, consider ˜Sv = p ∑ j=1 � ˆλ j ∨v � qjq⊤ j , (2.2) where 0 < v ≤ √p ¯ω and ω2 is the rate of convergence. • Let µ1,...,µp be the diagonal elements of Q⊤ΣQ. Then, by Theorem 2.1 in Chen et al. (2013), it holds that ∑p j=1 � ˆλ j − µj �2 ≤ p2 ¯ω2, and consequently �� ˜Sv −Σ ��2 F ≤ 2 �� ˜Sv −Tu �ˆΣ ���2 F +2 ��Tu �ˆΣu � −Σ ��2 F ≤ 2 p ∑ j=1 � ˆλ j − � ˆλ j ∨v ��2 +2 ¯ω2p2 ≤ 2 p ∑ j=1 � 2ˆλ 2 j 1 � ˆλ j ≤ 0 � +2v2� +2 ¯ω2p2. Furthermore, if ˆλ j ≤ 0, since µi ≥ 0, we have that ���ˆλ j ��� ≤ ���ˆλ j − µi ���. Then, �� ˜Sv −Σ ��2 F ≤ 4v2p+6 ¯ω p2 ≤ 10 ¯ω2p2. (2.3) 6Notice that the eigenvalues of ˜Sv are bounded below by v, and thus it is positive deﬁnite such that v = � p−1 p ∑ j,k=1 u2 ×1 ��� ˆσjk �� ≥ u � �1/2 The same positive-deﬁnization procedure also applies to the spectral norm and its rate can be similarly preserved. Sparsity Conditions We begin by presenting the commonly used sparsity condition deﬁned in terms of the strong ℓq−ball such that Gr (M ) = � Σ ���� max j≤p σj j ≤ 1; max 1≤k≤p p ∑ j=1 ��σj j ��r ≤ ˜M � ,0 ≤ r ≤ 1. (2.4) In particular, when r = 0, the sparsity condition implies that max 1≤k≤p∑p j=1 1 � σjk ̸= 0 � ≤ ˜M. Example 1. (Stationary linear process). An important special class is the vector linear process as deﬁned zi = ∞ ∑ j=0 Ajei−j (2.5) where Aj are p × p matrices, and ei are i.i.d mean zero random vectors with ﬁnite covariance matrix Σe = E � eie⊤ i � . A linear process representation is a ﬁlter which ensures the dependence on the innovation sequences. Then, zi exists almost surely with covariance matrix Σ = ∑∞ j=0 AjΣeA⊤ j if the latter converges. Assume that the innovation vector ei = (e1i,...,epi)⊤, where eji are i.i.d with mean zero, variance 1 and eji ∈ L 2q,q > 2, and the coefﬁcient matrices Ai = � ai, jk � 1≤j,k≤p satisfy maxj≤p p ∑ k=1 a2 i, jk = O � i−2−2γ� ,γ > 0 (2.6) Take the AR(1) process, zi = Azi−1 +ei, where A is the real matrix with spectral norm ρ(A) < 1, it is of the form as in (2.5) with Aj = Aj. and the functional dependence measure θi,2q, j = O � ρ(A)i� . Example 2. Consider the estimation framework for the graphical Lasso estimator with off-diagonal en- tries peranilzed by the 1-norm. In particular, for i.i.d p−dimensional vectors with polynomial moment condition, it can be shown that p = O �� n d2 � q 2τ � for some τ > 2, where d is the maximum degree in the Gaussian graphical model, then 1 p2 �� ˆΩn −Ω �� = Op � s+ p p2 . p2τ/q n � , (2.7) where s is the number of non-zero off-diagonal entries in Ω. 7We now compare the results with the CLIME (constrained L1−minimization for inverse matrix estima- tion) method, a non-Lasso type estimator which is estimated as below minimize |Θ|1 subject to ��ˆΣnΘ−I �� ∞ ≤ λn. (2.8) Notice that we can also consider the slightly modiﬁed version of the graphical Lasso: LetV = diag � σ1/2 11 ,....,σ1/2 pp � and R be the correlation matrix with ˆV and ˆR be the sample counterparts. We estimate Ω = V −1KV −1 by ˆΩλ = ˆV −1 ˆKλ ˆV −1 (2.9) where ˆKλ = arg min Φ>0 � trace � Φ ˆR � −log det(Ψ)+λ ��Ψ−�� 1 � . (2.10) Furthermore, the structure of the proposed risk matrix has different properties in contrast to the covariance matrix. For instance, the estimation procedure is not invariant to variable permutations. In contrast the covariance matrix itself has the property of being invariant to variable permutations. Therefore, this property allows the implementation of high dimensional techniques in inference problems. For instance, in the case of the covariance matrix Ledoit and Wolf (2004) proposed a way to compute an optimal linear combination of the sample covariance with the identity matrix, which also results in shrinkage of eigenvalues. Furthermore, shrinkage estimators are invariant to variable permutations, but they do not affect the eigenvectors of the covariance, only the eigenvalues, and it has been shown that the sample eigenvectors are also not consistent when p is large. Therefore, developing also estimation methods which are invariant to variable permutations is important. Deﬁnition 1. For any sequence X, the φ−mixing coefﬁcient φk is deﬁned as follows: φk(X) = sup � P(B|A)−P(B)| : A ∈ σℓ,B ∈ σ′ ℓ+k,ℓ ≥ 1 � . (2.11) Remark 1. Notice that the sub-Gaussianity assumption can be still employed in a time series setting (e.g., see Wong et al. (2020)). To do this, we combine the Rini-mixing condition with the sub-Gaussianity assumption which ensures that there is enough weak dependence captured as well as that the vectors of the model have good properties for estimation and inference. Lemma 1. Let Zi be i.i.d N (0,Σp) and λmax (Σp) ≤ ¯k < ∞. Then if Σp = [σab], P ������ n ∑ i=1 � Zi jZik −σjk � ����� ≥ nν � ≤ c1exp � −c2nν2� for |ν| ≤ δ (2.12) where c1,c2 and δ depend on ¯k only. 82.1.2. Main Results on Probability Bounds Following the framework of Chen et al. (2013) we focus in how to establish the convergence theory for covariance matrix estimates, we shall use the functional dependence measure of Wu (2005). Recall that Zji = gj(Fi),1 ≤ j ≤ p, where gj(.) is the jth coordinate projection of the Rp−valued measurable function g. For w > 0, the functional dependence measure of Zji is deﬁned by θi,w, j = ��Zji −Z′ ji �� w = � E ��Zji −Z′ ji ��w�1/w , (2.13) where Z′ ji = gj(F ′ i ),F ′ i = � ...,e−1,e′ 0,e1,...,ei � and e′ 0 and e′ 0, eℓ, ℓ ∈ Z, are i.i.d. In other words, Z′ ji is a coupled version of Zji with e0 in the latter replaced by an i.i.d copy of e′ 0. Proof of Theorem 2.1 (Chen et al. (2013)) We ﬁrst assume that α > 1/2−1/q. Notice that E ��Tu �ˆΣu � −Σ ��2 F = p ∑ j,k=1 E � ˆσjk1 ��� ˆσjk �� ≥ u � −σjk �2 ≤ 2 p ∑ j,k=1 E � W 2 jk � +2B(u/2), Wjk = ˆσjk1 ��� ˆσjk �� ≥ u � −σjk1 ���σjk �� ≥ u/2 � B(u) = p ∑ j,k=1 σ2 jk1 ���σjk �� < u � . Since the functional dependence measure for the product process � ZjiZki � i has a bounded inequality, under the decay condition Θm,2q ≤ Cm−α,α > 1/2−1/q. we have by Theorem 2(ii) in Wu (2013) that P ���ξjk �� > v � ≤ C2n (nv)q +C3e−C4nv2 (2.14) holds for all v > 0. Using integration by parts we obtain E � ξ 2 jk1 ���ξjk �� > v �� = v2P ���ξjk �� > v � + � ∞ v2 P ���ξjk �� > √w � dw. (2.15) Remark 2. Note that sparsity assumptions in the literature of high dimensional covariance matrices are usually imposed with respect to the inverse of the covariance matrix. In other words, if the ( j,k)−th component of Σ−1 is zero, then the variables Zj and Zk are partially uncorrelated, given the other vari- ables. More speciﬁcally, the current way of deﬁning sparsity in high dimensional models based on an underline covariance structure is to consider the notion of partial correlation as a measure of conditional independence (or dependence) in graphical based models. On the other hand, Katsouris (2023), deﬁnes a novel tail dependency matrix in which case the absence of a link between two nodes on the network implies the conditional tail independence2 (see, also Engelke and Hitz (2020)). 2Associate Professor Sebastian Engelke (University of Geneva) gave a seminar with title: "Causality for extreme values" at the S3RI Departmental Seminar Series at the University of Southampton, on the 9th of December 2021. 9Moreover, some useful limit results are also presented in the paper of Xiao and Wu (2013). In particular, the sample covariance between columns xi and xj is deﬁned as ˆσi j = 1 n (xi − ¯xi)⊤(xi − ¯xi). (2.16) In high-dimensional covariance inference, a fundamental problem is to establish an asymptotic distribu- tional theory for the maximum deviation Mn = max 1≤i≤j≤m �� ˆσi j −σi j ��. (2.17) Proofs of (iii) and (iv) The ﬁrst step is the truncation step. Step 1. (Truncation Step) We truncate Xn,k,i by ˜Xn,k,i = Xn,k,iI ���Xn,k,i �� ≤ n1/4� log(n) � (2.18) Deﬁne ˜Mn similarly as Mn with Xn,k,i being replaced by its truncated version ˜Xn,k,i, P � ˜Mn ̸= Mn � ≤ nmMn(p)n−p/4 (log(n))p ≤ CMn(p)n−δ/4(log(n))p = o(1). (2.19) Furthermore, since the asymptotics are not affected for notational simplicity, we still use ˜Xn,k,i to denote its centered version with mean zero. Deﬁne ˜σn,i, j = E � ˜Xn,k,i ˜Xn,k, j � and ˜τn,i, j = Var � ˜Xn,k,i ˜Xn,k, j � . Furthermore, denote with Mn,1 = max 1≤i<j≤m 1 �˜τn,i, j ����� 1 n n ∑ k=1 ˜Xn,k,i ˜Xn,k, j − ˜σn,i, j ����� (2.20) Mn,2 = max 1≤i<j≤m 1 �˜τn,i, j ����� 1 n n ∑ k=1 ˜Xn,k,i ˜Xn,k, j −σn,i, j ����� (2.21) Simple calculations are given by max 1≤i≤j≤m �� ˜σn,i, j −σn,i, j �� ≤ Cn−(p−2)/4 (log(n))p, (2.22) max α,β∈In ��Cov � ˜Xn,α ˜Xn,β � −Cov � Xn,αXn,β ��� ≤ Cn−(p−2)/4 (log(n))p , (2.23) Step 2. (Effect of estimated means) Set ¯Xn,i = 1 n ∑n k=1 ˜Xn,k,i. Deﬁne with Mn,3 = max 1≤i≤j≤m 1 �˜τn,i, j ����� 1 n n ∑ k=1 � ˜Xn,k,i − ¯Xn,i �� ˜Xn,k, j − ¯Xn, j � −σn,i, j �����. (2.24) 10Furthermore, observe that ��Mn,3 −Mn,2 �� ≤ max 1≤i≤j≤m �� ¯Xn,i ¯Xn, j �� �˜τn,i, j ≤ max 1≤i≤m| ¯Xn,i|2 . � min 1≤i≤j≤m˜τn,i, j �−1/2 . (2.25) Furthermore, using Bernestein’s inequality we can show that max 1≤i≤m| ¯Xn,i| = Op �� log(n) n � , (2.26) which in together with the previous result implies that ��Mn,3 −Mn,2 �� = Op �log(n) n � . (2.27) Step 3. (Effect of estimated variances) Denote by ˇσn,i, j the estimate of ˜σn,i, j ˇσn,i, j = 1 n n ∑ k=1 � ˜Xn,k,i − ¯Xn,i �� ˜Xn,k, j − ¯Xn, j � . (2.28) Thus, since in the deﬁnition of ˜Mn, ˜τn,i, j is unknown, and is estimated by ˇτn,i, j = 1 n n ∑ k=1 �� ˜Xn,k,i − ¯Xn,i �� ˜Xn,k, j − ¯Xn, j � − ˇσn,i, j �2. (2.29) Therefore, in order to show that the exponential limit holds for ˜Mn, it sufﬁces to verify that max 1≤i≤m ��ˇτn,i, j − ˜τn,i, j �� = op(1/log(n)). (2.30) Set ˇτn,i, j,1 = 1 n n ∑ k=1 �� ˜Xn,k,i − ¯Xn,i �� ˜Xn,k, j − ¯Xn, j � − ˜σn,i, j �2, (2.31) ˇτn,i, j,2 = 1 n n ∑ k=1 � ˜Xn,k,i ˜Xn,k, j − ˜σn,i, j � . (2.32) Using the probability limit, we know that max 1≤i≤j≤m ��ˇτn,i, j,1 − ˇτn,i, j �� = Op(log(n)/n). (2.33) 11Since � ˜Xn,k,i ˜Xn,k, j − ˜σn,i, j �2 ≤ 64n/(log(n))4 . (2.34) We have that max 1≤i≤j≤mP ���ˇτn,i, j,2 − ˜τn,i, j �� ≥ (log(n))−2� ≤   Cn n(log(n))−2. � n(log(n))−3�q∧1   log(n) ≤ � C(log(n))5 nq∧1 �log(n) Then, it follows that max 1≤i≤j≤m ��ˇτn,i, j,2 − ˜τn,i, j �� = Op � (log(n))−2� . (2.35) Thus, it remains to prove that max 1≤i≤j≤m ��ˇτn,i, j,1 − ˇτn,i, j,2 �� (2.36) A normal comparison principle Suppose that for each n ≥ 1, (Xn,i)i∈In is a Gaussian random vector whose entries have mean zero and variance one, where In is an index set with cardinality |In| = sn. Let ±n � rn,i, j � i, j∈In be the covariance matrix of (Xn,i)i∈In. Assume that sn → ∞ as n → ∞. We impose either of the following two conditions. (B1) For any sequence (bn) such that bn → ∞, γn(n,bn) = o(1/log(bn)) and lim sup n→∞ γn < 1. (2.37) (B2) For any sequence (bn) such that bn → ∞, γn(n,bn) = o(1) and ∑ i̸=j∈In r2 n,i, j = O � s2−δ n � for some δ > 0 and lim sup n→∞ γn < 1 (2.38) where γ(n,bn) := sup i∈In sup A ⊂In,|A |=bn inf j∈In ��rn,i, j ��, γn := sup i, j∈In,i̸=j ��rn,i, j ��. (2.39) 122.1.3. Limiting Laws of Coherence of Random Matrices with Applications Furthermore, following the framework of Cai and Jiang (2011), we present some useful lemmas below. Lemma 2. Let hi = ∥xi − ¯xi∥/√n for each i. Then, ���nΓn −X⊤ n Xn ��� ≤ � b2 n,1 +2bn,1 � Wnb−2 n,3 +nb−2 n,3b2 n,4, (2.40) where bn,1 = max 1≤i≤p|hi −1|, Wn = max 1≤i<j≤p ���x⊤ i xj ���, bn,3 = min 1≤i≤phi, bn,4 = max 1≤i≤p|¯xi|. Lemma 3. Let ξ,1 ≤ i ≤ n, be independent random variables with Eξi = 0. Set s2 n = n ∑ i=1 Eξ 2 i , ρ2 n n ∑ i=1 E|ξi|3 , Sn = n ∑ i=1 ξ. (2.41) Assume that max 1≤i≤n|ξi| ≤ cnsn for some 0 < cn ≤ 1. Then, P(Sn > xsn) = eγ(x/sn) (1 = Φ(x)) � 1+θn,x(1+)s−3 n ρn � (2.42) for 0 < x ≤ 1/(18cn), where |γ(x)| ≤ 2x3ρn and |θn,x| ≤ 36. Therefore, we have that P(Wn ≤ an) ≤ e−λn +b1,n +b2,n, (2.43) Suppose we consider an Rd−valued time series process {Xt,t ∈ Z} with Xt = � X1,t,....,Xd,t �⊤, and we have data X1,...,Xn at hand to use for estimation and inference purposes. Corollary 1. The process {Xt,t ∈ Z} is assumed to be strictly stationary and its (d ×d) autocovariance matrix C(h) = � Ci j(h) � i, j=1,...,d at lag h ∈ Z is C(h) = E �� Xt+h − µ �� Xt+h − µ �⊤� , (2.44) where µ = E[Xt], and the sample autocovariance is denoted with ˆC(h) = � ˆCi j(h) � i, j=1,...,d at lag |h| < n and can be deﬁned accordingly. 132.1.4. Large deviations for quadratic forms Large deviations for quadratic forms of stationary processes have been extensively studied in the litera- ture. In particular, we need an upper bound for the tail probability under less restrictive conditions. In this section, we follow the framework proposed by Xiao and Wu (2012). More speciﬁcally, we prove a result on probabilities of large deviations of quadratic forms of stationary processes, which take the form QT = ∑ 1≤s≤t≤T as,tXsXt. (2.45) The coefﬁcients as,t = aT,s,t may depend on T. Throughout this section we assume that sups,t |as,t| ≤ 1, and as,t = 0 when |s−t| > BT, where BT → ∞, and BT = O(T γ) for some 0 < γ < 1. Remark 3. Notice that large deviations for quadratic forms of stationary processes have been extensively examined in the literature, which include: (i) large deviations principle for Gaussian processes, (ii) func- tional large deviation principle, (iii) moderate deviations principle, (iv) moderate deviation principles for quadratic forms of Gaussian processes and (v) moderate deviations of periodograms of linear processes as well as Cramer-type moderate deviation for spectral density estimates of Gaussian processes. Theorem 1. Assume that Xt ∈ L p, p > 4, EXt = 0, and Θp(m) = O(m−a). Set cp = (p+4)ep/4Θ2 4. For any M > 1, let xt = 2cp √TMBTlogBT. Assume that BT → ∞ and BT = O (T γ) for some 0 < γ < 1. Then, for any γ < β < 1, there exists a constant Cp,M,β > 0 such that the following holds P � |E0QT| ≥ xT � ≤ Cp,M,βx−p/2 T (logT) � (TBT)p/4T −αβ p/2 +TBp/2−1−αβ p/2 T +T � +Cp,M,βB−M T . Proof. Without loss of generality, assume BT ≤ T γ. For γ < β < 1, let mT = ⌊T √ β⌋, ˜XT = Ht−mT Xt and QT = ∑ 1≤s≤t≤T as,tXsXt. (2.46) We have that P ���E0 � QT − ˜QT ��� ≥ cpM1/2� TBT (logBT) � (2.47) Split [1,T] into blocks B1,...,BbT of size 2mT, and deﬁne QT = ∑ t∈Bk ∑ 1≤s≤t as,t ˜Xs ˜Xt. (2.48) 142.1.5. Asymmetry Helps In this section, we present the main results obtained by Chen et al. (2021b). We being by considering the low-rank matrix completion of Chen et al. (2021b). Low-rank matrix completion Suppose that M is generated using random partial entries of M⋆ as Mi j =    1 pM⋆ i j with probability p, 0 else, (2.49) where p denotes the fraction of the entries of M⋆ being revealed. We have that H = M−M⋆ is zero-mean and obeys ��Hi j �� ≤ µ np := B and Var � Hi j � ≤ µ2 pn2. For instance, in the case where p ≈ µ2log(n) n , then invoking Corollaries 1-3 of Chen et al. (2021b) yields |λ −λ ⋆| |λ ⋆| ≤ 1 √n � µ3log(n) pn , (2.50) which gives that min � ∥u−u∥∞ ,∥u+u∥∞ � ≤ 1 √n � µ3log(n) pn (2.51) with high probability, where a ∈ Rn is any ﬁxed unit vector. Empirically, eigen-decomposition outper- forms SVD in estimating both the leading eigenvalue and eigenvector of M⋆ Why asymmetry helps? According toChen et al. (2021b), if we consider their Theorem 3, focusing on the case with λ ⋆ = 1 for simplicity, then the key ingredient is the Neumann trick stated in Theorem 2. Speciﬁcally, in the rank-1 case we can expand u = 1 λ � u⋆⊤u � ∞ ∑ s=0 1 λ sHsu⋆. (2.52) We obtain that ����a⊤ � u− u⋆⊤u λ u⋆ ����� = ����� u⋆⊤u λ ∞ ∑ s=1 a⊤Hsu⋆ λ s ����� ≤ ∞ ∑ s=1 ���� a⊤Hsu⋆ λ s ���� (2.53) where the last inequality holds since (i) ��u⋆⊤u �� ≤ 1, and (ii) λ is real-valued and obeys λ ≈ 1 if ∥H∥ << 1. As a result, the perturbation can be well controlled as long as ��a⊤Hsu⋆�� is small for every s ≤ 1. 15• (Asymmetric case) When H is composed of independent zero-mean entries each with variance σ2 n , we obtain that E � a⊤H2u⋆� = a⊤E � H2� u⋆ = a⊤� σ2I � u⋆ = σ2a⊤u⋆ (2.54) • (Symmetric case) When H is symmetric and its upper triangular part consists of independent zero- mean entries with variance σ2 n , it holds that E � a⊤H2u⋆� = a⊤E � H2� u⋆ = a⊤� nσ2I � u⋆ = nσ2a⊤u⋆. (2.55) Pertrubation Analysis for the rank-r case Eigenvalue perturbation for the rank-r case The eigenvalue perturbation analysis can be extended to accommodate the case where M⋆ is symmetric and rank−r. As before, we assume that the r nonzero eigenvalues of M⋆ obey that |λ1| ≥ ... ≥ |λr| Theorem 2 (Chen et al. (2021b)). (Perturbation of linear forms of eigenvectors (rank-r)) Consider a rank−r symmetric matrix M⋆ ∈ Rn×n with incoherence parameter µ. Suppose that σ � n log(n),B log(n) λ ⋆max ≤ c1 κ (2.56) for some sufﬁciently small constants c1 > 0. Then for any ﬁxed unit vector a ∈ Rn and any 1 ≤ ℓ ≤ r, with probability at least 1−O � n−10� one has that �����a⊤ � uℓ − r ∑ j=1 λ ⋆ j u⋆⊤ j uℓ λ j u⋆ j ������ ≤ max � σ � nlog(n),Blog(n) � κ |λℓ| � µr n (2.57) ≤ max � σ � nlog(n),Blog(n) � λ ⋆max κ2 � µr n (2.58) The particular condition allows to control the perturbation of the linear form of eigenvectors. Thus, the perturbation upper bound grows as either the rank r or the condition number κ increases. Corollary 2 (Chen et al. (2021b)). Consider the ℓ−th eigenvalue λℓ for ℓ ∈ {1,...,r} of the matrix M. Under the assumptions of theorem 4, with probability at least 1−O � n−10� , there exists 1 ≤ j ≤ r ��λℓ −λ ⋆ j �� ≤ max � σ � nlog(n),Blog(n) � κr � µ n , max � σ � nlog(n),Blog(n) � λ ⋆max ≤ c1/κ2 (2.59) for sufﬁciently small constant c1 > 0. 16In comparison, the Bauer-Fike theorem (Lemma 2) together with Lemma 1 gives a perturbation bound ��λℓ −λ ⋆ j �� ≤ ∥H∥ ≤ max � σ � nlog(n),Blog(n) � for some 1 ≤ j ≤ r. (2.60) For the low-rank case where r << √n, the eigenvalue pertrubation bound derived in Corollary 5 can be much sharper than the Bauer-Fike theorem. Corollary 3 (Chen et al. (2021b)). Under the same setting of Theorem 4, with probability 1−O � n−9� , ���a⊤U ��� 2 ≤ κ√r ���a⊤U⋆��� 2 + max � σ � nlog(n),Blog(n) � λ ⋆max κ2r � µ n . (2.61) Consequently, by taking a = ei for 1 ≤ i ≤ n in Corollary 6, we arrive at the following statement regarding the alternative deﬁnition of the incoherence of the eigenvalue matrix U. Corollary 4. Under the same setting of Theorem 4, with probability 1−O � n−8� we have ∥U∥2,∞ kr � µ n . (2.62) Proof. Given that ∥U∥2,∞ = max1≤i≤n ��e⊤ i U �� 2 and recalling our assumption implies that the following condition holds ∥U∥2,∞ ≤ � µr/n, we can invoke Corollary 6 and the union bound to derive the advertised entrywise bounds. Proof of Theorem (2) Without loss of generality, we shall assume that λ ⋆ max = λ ⋆ 1 = 1 throughout the proof. To begin with, Lemma 2 implies that for all 1 ≤ ℓ ≤ r, |λℓ| ≤ |λ ⋆ min|−∥H∥ > 1/(2κ) > ∥H∥ (2.63) as long as ∥H∥ < 1/(2κ). In view of the Neumann trick (Theorem 2), we can derive that �����a⊤uℓ − r ∑ j=1 λ ⋆ j u⋆⊤ j uℓ λ j a⊤u⋆ j ����� = ����� r ∑ j=1 λ ⋆ j λ j � u⋆⊤ j uℓ �� ∞ ∑ s=1 1 λ s ℓ a⊤Hsu⋆ j ������ ≤   r ∑ j=1 ���λ ⋆ j ��� ��λ j �� ���u⋆⊤ j uℓ ���   � max 1≤j≤r ∞ ∑ j=1 1 |λℓ|s ���a⊤Hsu⋆ j ��� � ≤ � r r ∑ j=1 ���u⋆⊤ j uℓ ��� 2    max 1≤j≤r ���λ ⋆ j ��� ��λ j ��    � max 1≤j≤r ∞ ∑ j=1 1 |λℓ|s ���a⊤Hsu⋆ j ��� � ≤ √r. 1 |λℓ|. � max 1≤j≤r ∞ ∑ j=1 1 |λℓ|s ���a⊤Hsu⋆ j ��� � 17where the third line follows since ∑r j=1 ���u⋆ juℓ ��� 2 ≤ ∥uℓ∥2 2 = 1, and the last inequality makes use of (49). Apply Corollary 4 in Chen et al. (2021b) to obtain a bound as below ≤ √r |λℓ| ∞ ∑ j=1 � 2c2κ max � Blog(n), � nσ2log(n) ��s � µ n ≤ κ |λℓ|max � Blog(n), � nσ2log(n) �� µr n ≤ κ2max � Blog(n), � nσ2log(n) �� µr n with the provision that |λℓ| > 1/(2κ) and max � Blog(n), � nσ2log(n) � ≤ c1/κ for some sufﬁciently small constant c1 > 0. The condition |λℓ| > 1/(2κ) follows immediately by combining Lemma 2, Lemma 1 and the condition (34). Appendix We justify the existence and uniqueness of an eigenvalue in B(1,∥H∥). Denote with Λ(M) = {λ1,...,λn} and deﬁne a set of auxiliary matrices Mt = M⋆ +tM, 0 ≤ t ≤ 1. (2.64) As we can see the set of eigenvalues of Mt depends continuously on t, we can write as below Λ(Mt) = {λ1(t),λ2(t),...,λn(t)}, (2.65) with each λ j(t), 1 ≤ j ≤ n being a continuous function in t. Meanwhile as ∥H∥ < 1/2 and 0 ≤ t ≤ 1, the two disks B(1,t ∥H∥) and B(0,t ∥H∥) are always disjoint sets. Therefore, the continuity of the spectrum with respect to t requires that λ j(t) to always stay within the same disk where λ j(0) ∈ {0,1} lies, namely, such that λ j(t) ∈ B � λ j(0),t ∥H∥ � . (2.66) Given that M⋆ has n−1 eigenvalues equal to 0 and one eigenvalue equal to 1, we establish the lemma for the rank-1 case. Rank-r Follow the above argument for the case where we have a rank−1 matrix, we can immediately show that: if ∥H∥ < λ ⋆ r /2, then (i) there are exactly n − r eigenvalues lying within B(0,∥H∥), (ii) all other eigenvalues lie within ∪1≤j≤rB � λ ⋆ j ,∥H∥ � , which are exactly top−r leading eigenvalues of M. 18Proof of Theorem 2 [Chen et al. (2021b)] Following the deﬁnition of eigenvectors, we have that (M⋆ +H) = λℓuℓ, or equivalently, 1 λℓ M⋆uℓ = � I− 1 λℓ H � uℓ (2.67) When ∥H∥2 < |λℓ|, one can invert � I− 1 λℓH � and therefore we obtain that uℓ = � I− 1 λℓ H �−1 1 λℓ M⋆uℓ = 1 λℓ � I− 1 λℓ H �−1� r ∑ j=1 λ ⋆ j u⋆ ju⋆⊤ j � uj = r ∑ j=1 λ ⋆ j λ j � u⋆ ju⋆⊤ j �� I− 1 λℓ H �−1 u⋆ j, where the last term follows by rearranging terms. Finally, replacing � I− 1 λℓH �−1 with the Neumann series we obtain that ∑∞ s=0 1 λ sHs, we establish the theorem. Proof of Lemma 3 [Chen et al. (2021b)] We start with the rank-1 case. Towards this, we resort to the Neumann trick in Theorem 2, which in the rank-1 case gives the following expression u = 1 λ � u⋆⊤ j u⋆ j � ∞ ∑ s=0 � 1 λ H �s u⋆. (2.68) From Lemma 2, we know that λ is real-valued and that λ > 1−∥H∥ ≥ 3/4 > ∥H∥ under our assumption. This together yields that ����u− u⋆⊤u λ u⋆ ���� 2 ≤ 1 λ ∞ ∑ s=0 ���� 1 λ H ���� s ∥u⋆∥2 = 1 λ ∞ ∑ s=0 ���� 1 λ H ���� s = ∥H∥ λ (λ −∥H∥) ≤ 8 3 ∥H∥, (2.69) where the last inequality holds since λ ≥ 3/4 and λ −∥H∥ ≥ 1−2∥H∥ ≥ 1/2. Next, by decomposing u into two orthogonal components u = � u⋆⊤u � u⋆ + � u− � u⋆⊤u � u⋆� (2.70) we obtain that ���u⋆⊤u ��� = ��� � u⋆⊤u � u⋆��� 2 = � 1− ��u− � u⋆⊤u � u⋆��2 2 ≥ 1− ���u− � u⋆⊤u � u⋆��� 2 2 ≥ 1− ����u− u⋆⊤u λ u⋆ ���� 2 2 ≥ 1− 64 9 ∥H∥2 . the particular inequality holds since � u⋆⊤u � u⋆ is orthogonal projection of u onto the subspace spanned by u⋆, and hence ���u− � u⋆⊤u � u⋆��� 2 ≤ ����u− 1 λ � u⋆⊤u � u⋆ ���� 2 . (2.71) 19Moreover, since u is real-valued we obtain the following bound: min � ∥u−u⋆∥2,∥u+u⋆∥2 � = � ∥u∥2 2 +∥u⋆∥2 2 −2 ��u⋆⊤u �� ≤ 8 √ 2 3 ∥H∥. (2.72) Next, we consider the rank−r case, since for any 1 ≤ r ≤ r we have that r ∑ j=1 ���u⋆⊤ j uℓ ��� 2 = ����� r ∑ j=1 � u⋆⊤ j uℓ � u⋆ j ����� 2 2 = 1− �����uℓ − r ∑ j=1 � u⋆⊤ j uℓ � u⋆ j ����� 2 2 ≥ 1− �����uℓ − r ∑ j=1 λ ⋆ j u⋆⊤ j uℓ λ j u⊤ j ����� 2 2 where the inequality arises since ∑r j=1 � u⋆⊤ j uℓ � u⋆ j is the Euclidean projection of uℓ onto the space of {u⋆ 1,...,u⋆ r}. Furthermore, we observe that ����� r ∑ j=1 λ ⋆ j � u⋆⊤ j uℓ � u⋆ j ����� 2 ≤ � ∑ j � λ ⋆ j �2���u⋆⊤ j uℓ ��� 2 ≤ λ ⋆ max � ∑ j ���u⋆⊤ j uℓ ��� 2 = λ ⋆ max∥uℓ∥2 = 1. (2.73) This taken collectively with Theorem 2 leads to �����uℓ − r ∑ j=1 λ ⋆ j u⋆⊤ j uℓ λℓ u⋆ j ����� 2 = ����� r ∑ j=1 λ ⋆ j λℓ � u⋆⊤ j uℓ �� ∞ ∑ s=1 1 λ 2 ℓ Hsu⋆ j ������ 2 = ����� 1 λℓ r ∑ j=1 1 λ s ℓ Hs � r ∑ j=1 λ ⋆ j � u⋆⊤ j uℓ � u⋆ j ������ 2 ≤ 1 |λℓ| ∞ ∑ s=1 1 |λℓ|s ∥H∥s ����� r ∑ j=1 λ ⋆ j � u⋆⊤ j uℓ � u⋆ j ����� 2 ≤ 1 |λℓ|. ∥H∥ |λ j|−∥H∥ ≤ 8κ2 3 ∥H∥. The last two lines follow since, when ∥H∥ < 1/(4κ). We can show that |λℓ| > |λ ⋆ min| − ∥H∥ ≥ 3/(4κ) and (ii) |λℓ|−∥H∥ ≥ ��λ ⋆ min ��−2∥H∥ ≥ 1/(2κ). Proof of Lemma 5 on Edge and Vertices [Chen et al. (2021b)] To establish this lemma, we exploit entrywise independence of H and develop a combinatorial trick. To begin with, we expand the quantity of interest as below � a⊤Hsu⋆�k = ∑ 1≤i(b) t ≤n,0≤t≤s,1≤b≤k k ∏ b=1 ai(b) 0 � Hi(b) t−1i(b) t � u⋆ i(b) s (2.74) In this section, we use that I := � i(b) t |0 < t < s,1 < b < k � ∈ [n](s+1)k, to denote such a collection of (s+1)k indices. Therefore, one can write that E �� a⊤Hsu⋆�k� = ∑ I ∈[n](s+1)k E � k ∏ b=1 ai(b) 0 � s ∏ t=1 Hi(b) t−1i(b) t � u⋆ i(b) t−1 � (2.75) Further details of these proofs can be found in the Appendix of Chen et al. (2021b). 202.2. Limit theory for Functional Data 2.2.1. Simultaneous Diagonalization In this section we brieﬂy study the framework proposed by Yuan and Cai (2010). In particular, before studying the asymptotic properties of the regularized estimators, we ﬁrst investigate the relationship be- tween the eigen structures of the covariance operator for X(.) and the reproducing kernel of the functional space H . As observed in earlier studies, eigenvector structures play prominent roles in determining the nature of the estimation problem in functional linear regression. Recall that K is the reproducing kernel of H1. Because K is continuous and square integrable, it follows from Mercer’s theorem that K admits the following spectral decomposition K(s,t) = ∞ ∑ k=1 ρkψk(s)ψk(t). (2.76) Here ρ1 ≥ ρ2 ≥ ... are the eigenvalues of K, and {ψ1,ψ2,...} are the corresponding eigenfunctions, Kψk = ρkψk, k = 1,2,... (2.77) Moreover, ⟨ψi,ψj⟩L2 = δi j and ⟨ψi,ψj⟩L2 = δi j/ρ j, (2.78) where δi j is the Kronecker’s delta. Convergence rates We now turn to the asymptotic properties of the smoothness regularized estimators. To ﬁx ideas, in what follows, we shall focus on the squared error loss. Recall in this case we have that � ˆαnλ, ˆβnλ � = arg min α∈R,βinH � 1 n n ∑ i=1 � yi − � α + � I xi(t)β(t)dt ��2 +λJ(β) � . (2.79) Remark 4. Due to the fact that the risk matrix employed in the optimal portfolio problem is not a typical covariance operator, then in order to investigate the convergence rates and asymptotic properties of the elements of the risk matrix and its eigenvalues, we can simplify the problem by assuming that the entries of the matrix have a linear process representation. This simpliﬁcation allows to model the heavy tailed entries which represent the tail-risk measures without considering the distributional properties of the forecasts based on the quantile speciﬁcations mentioned above (see, Muller and Stadtmuller (2005)). 21Furthermore, we have that ℓn(β) = 1 n n ∑ i=1 � yi − � I xi(t)β(t)dt �2 . (2.80) Observe that ℓ∞ := E[ℓn(β)] = E � Y − � I X(t)β(t)dt �2 = σ2 + � I � I � β(s)−β0(s) � C(s,t) � β(s)−β0(s) � dsdt = σ2 +∥β −β0∥2 0 . Write ¯β∞λ = arg min β∈H � ℓ∞(β)+λJ(β) � . (2.81) For instance, we have the following decomposition ˆβnλ −β0 = � ˆβnλ − ¯β∞λ � + � ¯β∞λ −β0 � . (2.82) we refer to the terms above on the right hand side as the stochastic error and deterministic error respec- tively. Deterministic Error Write β0(.) = ∞ ∑ k=1 akωk(.) and β(.) = ∞ ∑ k=1 bkωk(.) (2.83) Then, Theorem 3 above implies that ℓ∞(β) = σ2 + ∞ ∑ k=1 (bk −ak)2 , J(β) = ∞ ∑ k=1 γ−1 k b2 k. (2.84) Therefore, ¯β∞λ(.) = ∞ ∑ k=1 ak 1+λγ−1 k ωk(.) =: ∞ ∑ k=1 ¯bkωk(.). (2.85) 222.2.2. Common Functional Principal Components According to Benko et al. (2009), the elements of the risk matrix Γ are functionals of the model estimates which can be estimated with asymptotically negligible bias and a parametric rate of convergence T −1/2 i . For example when the data are generated from a balanced, equidistance design, then it can be easily seen that for i ̸= j this rate of convergence is achieved by the following estimator. Notice that a bias corrected estimator may yield negative eigenvalues. However, in practise these values are small and can be interpreted close to zero. For instance, when evaluating eigenvalues and eigenfunctions of the empiri- cal covariance operator of nonparametrically estimated curves ˆXi, then for ﬁxed r ≤ r0 the above rate of convergence for the estimated eigenfunctions may well be achieved for a suitable choice of smoothing parameters. For example, when using standard methods it does not seem to be possible to obtain a corre- sponding rate of convergence, since any smoothing bias ��E � ˆXi � −Xi(t) �� will invariably affect the quality of the corresponding estimate of ˆλr. Main Results In the following, we have that ∥v∥ = �� 1 0 v(t)2dt �1/2 will denote the L2−norm for any square integrable function v. Consider that ˆΣm is the m×m matrix with elements given by � 1 n n ∑ i=1 � β ji − ¯β j �� βki − ¯βk � �m j,k=1 (2.86) Let λ1 �ˆΣm � ≥ λ2 �ˆΣm � ≥ ... ≥ λm �ˆΣm � and ˆζ1,m,..., ˆζm,n denote eigenvalues and corresponding eigen- vectors of ˆΣm. Therefore, some straightforward algebra shows that ˆλr,m = λr �ˆΣm � , ˆγ = gm(t)⊤ ˆζr,m. (2.87) We will use Σm to represent the m×m diagonal matrix with diagonal entries given by λ1 ≤ ... ≤ λm. Then, the corresponding eigevectors are given by the m−dimensional unit vectors denoted by e1,m,...,em,m. Then, by Lemma A of Kneip and Utikal we obtain that the differences between the eigenvalues and eigenvectors of Σm and ˆΣm can be bounded by �ˆλr,m −λr � = trace � er,me⊤ r,m �ˆΣm −Σ �� + ˜Rr,m, ˜Rr,m ≤ 6 sup ∥a∥=1 a⊤ �ˆΣm −Σ �2a min s |λs −λr| (2.88) Moreover, we have that ˆζr,m −er,m = −Sr,m �ˆΣm −Σ � er,m +R∗ r,m, ��R∗ r,m �� ≤ 6 sup ∥a∥=1 a⊤�ˆΣm −Σ �2a min s |λs −λr| , (2.89) where we denote with Sr,m = ∑s̸=r 1 λs−λr es,me⊤ s,m. 23Assumption 1 implies that E � ˆβr � = 0 and Var � ˆβr � = λr r and with δii = 1 as well as δi j = 0 for i ̸= j, we obtain that E � sup ∥a∥=1 a⊤ �ˆΣm −Σ � � ≤ E � trace ��ˆΣm −Σ �2�� = E � m ∑ j,k=1 �1 n � β ji − ¯β j �� βki − ¯βk � −δjkλ j �2� ≤ E � ∞ ∑ j,k=1 �1 n � β ji − ¯β j �� βki − ¯βk � −δjkλ j �2� = 1 n � ∑ j ∑ k E � β 2 jiβ 2 ki � � +o(n−1) = O � n−1� for all m. Furthermore, since trac � er,me⊤ r,m �ˆΣm −Σm �� = 1 n n ∑ i=1 � βri − ¯βr �2 −λr (2.90) Therefore after applying the central limit theorem we obtain that √n � ˆλr −λr � = 1 √n n ∑ i=1 � βri − ˆβr �2 −λr +Op � n−1/2� = 1 √n n ∑ i=1 � (βri)2 −E � (βri)2�� +Op � n−1/2� → N (0,Λr). Obviously the event ˆλr−1 > ˆλr > ˆλr+1 occurs with probability 1. Remark 5. Therefore, via the asymptotic analysis and the econometric model above we aim to verify that the position of the node in the network in terms of centrality, such as for example highly central nodes versus nodes which are on the periphery of the network can have an impact on the stability properties of a community of nodes. For example, if a node is quite central then the impact of the addition of a node to the particular node can dramatically change the dynamics in the network in the sense that that node as well as the structure of the remaining nodes can have higher inteconnectedness. Even though this outlier node is not highly interconnected affects the stability of the remaining of the nodes. 242.3. Limit theorems for Dependent Sequences In this Section, we consider some important results presented in the literature for dependent sequences. We consider a "high-dimensional" probability space (Ω,F,P). For functions of dependent random vari- ables (Xi)i∈N the main challenge is often to quantify and bound the dependence among random variables Xi, in terms of various types of mixing coefﬁcients (see, Kontorovich and Ramanan (2008)). To establish concentration bounds a sufﬁciently rapid decay of the mixing coefﬁcients is necessary. 2.3.1. Method of bounded martingale differences The particular methodology presented in the paper of Kontorovich and Ramanan (2008) provides a suit- able mechanism of establishing concentration inequalities (see also Doukhan et al. (1995)). Theorem 3. Suppose that S is a countable space, F is a set of all subsets and P is a probability measure on (S n,F) and ϕ : S n → R is a Lipschitz function (with respect to the Hamming metric) on S n for some c > 0. Then, for any t > 0, P(|ϕ −Eϕ| ≥ t) ≤ 2exp � − t2 2nc2 ∥∆n∥2 ∞ � . (2.91) Remark 6. Notice that the bound can be simpliﬁed further. More precisely, given any intial probability distribution p0(.) and stochastic transition kernels pi(.|.),1 ≤ i ≤ n−1, then the probability measure is P � (X1,...,Xi) = x � = p0(x1) i−1 ∏ j=1 pj � xj+1|xj � , ∀ 1 ≤ i ≤ n (2.92) (see also Finner (1992)). 2.3.2. Concentration Inequalities for Dependent Random Variables We consider the concept of pointwise inequalities, that is, inequalities that hold uniformly for any param- eter θ ∈ Θ. Deﬁne the function (see, van de Geer (2002)) ψα(x) := exp(xα)−1, for any x > 0. (2.93) For a real-valued random variable ξ, we deﬁne with ∥ξ∥ψα := inf � λ > 0 : E � ψα �|ξ| λ �� ≤ 1 � (2.94) Moreover, we write that ξ ∈ L q for some q > 0 if it holds that ∥ξ∥q := {E(|ξ|q)}1/q (2.95) 25Deﬁnition 2 (Orlicz-norm). For any convex function ψ : R+ → R+ such that ψ(0) = 0 and ψ(x) → ∞ as x → ∞ and (real-valued) random variable X, we denote with ∥x∥ψ the Orlicz-norm, which is deﬁned by ∥X∥ψ := inf � C > 0 : E � ψ �|X| C �� ≤ 1 � . (2.96) • Denote the ℓp Orlicz-norm of X by ∥X∥p for p ∈ [0,+∞) by setting ψ(x) = xp and ∥X∥eγ the exponential Oricz-norm for γ > 0 by setting ψ(x) = exp(xγ)−1 for some γ ≥ 1. • The function ψ(x) is the convex hull of x �→ exp(xγ) − 1 for some γ ∈ (0,1), which ensures con- vexity. • Moreover, when X is a random vector, we deﬁne its Orlicz-norm by ∥X∥ψ := sup∥u∥≤1∥u′X∥ψ. 2.3.3. Central limit theorems for high dimensional dependent data Following the framework proposed by Chang et al. (2021) , recall that we deﬁne with Sn,x = n−1/2 ∑n t=1 Xt. Let G ∼ N (0,Ξ) where Ξ := Cov � n−1/2 ∑n t=1 Xt � . Without loss of generality we assume that G is in- dependent of X = {X1,...,Xn}. We write with Xt = (Xt,1,...,Xt,p)′. Then, the long-run variance of the j−th coordinate marginal sequence � Xt, j �n t=1 is deﬁned as Vn, j = Var � 1 √n n ∑ t=1 Xt, j � . (2.97) Therefore, in order to determine the convergence rate of ρn for the α−mixing sequence {Xt}, we impose additional regularity conditions. The above condition assumes that the partial sum 1 √n ∑n t=1Xt, j is non- degenerated which is necessary to bound the probability of a Gaussian vector taking values in a small region. When � Xt, j � t≥1 is stationary, then it holds that Vn, j := Γj(0)+2 n−1 ∑ k=1 � 1− k n � Γj(k) (2.98) where Γk(k) = Cov � X1, j,Xk+1, j � is the autocovariance of � Xt, j � t≥1 at lag k. Assumption 1 (Subexponential moment). There exists a sequence of constants Bn ≥ 1 and a universal constant γ1 ≥ 1 such that ��Xt, j �� ψγ1 ≤ Bn for all t ∈ [n] and j ∈ [p]. Assumption 2 (Decay of α−mixing coefﬁcients). There exist some universal constants K1 > 1,K2 > 0 and γ2 > 0 such that αn(k) ≤ K1e(−K2kγ2) for any k ≥ 1. Assumption 3 (Non-degeneracy). There exists a universal constant K3 > 0 such that minj∈[p]Vn, j ≥ K3. 26Example 3. Consider the inverse of the covariance matrix such that ��� ��� ��� ����Σ −1 i ��� ��� ��� ��� ψ ≤ C. (2.99) For the polynomial case, applying the union bound followed by Markov’s inequality we conclude that max i ����Σ −1 i ��� ≤P n1/p and max i,t, j ���X(j) i,t ��� ≤P (nkT)1/p . (2.100) Lemma 4. Let X and Y be random elements deﬁned in the same probability space (Ω,F,P) taking values in the metric space (S,d). Then for measurable A and δ ≥ 0 Due to the fact that x �→ exp �� x � |||X|||eγ �γ� is non-decreasing then P(|X| ≥ x) = P � exp �� |X| � |||X|||eγ �γ� ≥ exp �� x � |||X|||eγ �γ�� ≤ exp � − � x � |||X|||eγ �γ� Eexp �� |X| � |||X|||eγ �γ� . It holds that, ψeγ(x) = Kγx1 � 0 ≤ x ≤ aγ � +[exp(xγ)−1]1 � x ≥ aγ � (2.101) where Kγ := (expaγ γ−1) aγ and aγ is deﬁned as below aγ := inf � x ∈ R+ : x ≥ �1−γ γ �1/γ� (2.102) Moreover, it holds that �1−γ γ �1/γ ≤ aγ ≤ �1 γ �1/γ . (2.103) Example 4. Consider the martingale sequence Sn = n ∑ i=1 Xi,n ≥ 1. (2.104) Consider the Fi−1measurable random variables Ki > 0, for i = 1,2,.... Deﬁne with B2 0 = 0 and for any n ≥ 1 such that B2 n = n ∑ i=1 K2 i � 1+E � ψ �|Xi| Ki ���Fi−1 �� (2.105) 27Theorem 4. Let ψ be an Orlicz function such that it holds that supx,y→∞ψ(x)ψ(y)/ψ(cxy) < ∞, for some constant c. Suppose that {Zθ : θ ∈ Θ} is a separable stochastic process indexed by θ in the pseudo-metric space (Θ,τ). Assume that ∥Zθ −Zϑ∥ψ ≤ C′ � diam(Θ) 0 ψ−1� D(δ) � dδ (2.106) where diam(Θ) is the diameter of Θ and D(δ) is the δ−packing number. Corollary 5. Let Wi be Fi−measurable and E(Wi|Fi−1) = 0 for i ≥ 1. Suppose that for some constant c < ∞ it holds that E � ψ �|Wi| c ���Fi−1 � ≤ 1, almost surely i = 1,2,... (2.107) Remark 7. Partitioning entropy could be applied to nonstationary time series? This could be the case when considering a discretenized method, such as block of nonstationary time series (i.e., m−dependence). Notice that this paper doesn’t have in depth explanation of the dependence structure. However, the Orlicz norm provides related moment condition for understanding the asymptotic behaviour. We deﬁne with φ(d) the following quantity φ(d) = � d 0 H1/2(δ,d)dδ ∨d := min �� d 0 H1/2(δ,d)dδ,d � , (2.108) • What type of dependence structure does the entropy integral φ(d) introduce? For example, what form this integral would have in the case of Garch processes or for the autoregressive model? Are there any related results to Hoeffding’s inequality for β−mixing sequences? To derive the proofs of main results presented in the paper we use that P(A) ≤ exp � −βα +2β 2b2� . • All probability bounds are derived with respect to Sn, which the sum of stationary martingale dif- ferences. Under the assumption of stationary sequences we assume that sub-Gaussianity condition holds in order to obtain probability bounds. Deﬁne with g(y1,...,yn) a measurable function of the data, which for example could be extended to sample moments of estimators. • An important related assumption is the Geometric ergodicity which along with β−mixing can facilitate the development of further the asymptotic theory in time series model. Moreover the par- titioning entropy condition clearly holds in the case of stationary sequences but the main challenge in the case of nonstationary time series with a LUR process representation is the presence of the nuisance parameter of persistence. • The theoretical framework presented in the paper shows that the theory can be also extended to the case of M estimators (such as quantile autoregression) using suitable smoothing conditions and deriving the corresponding probability bounds. 282.3.4. Sub-Weibull random vectors under β−mixing Most analysis on lasso assume that data that have sub-Gaussian or subexponential tails. These assump- tions ensure that the moment generating function exists, at least for some values of the free parameter. Nonexistence of the moment generating function is often taken as a deﬁnition of having a heavy tail. Deﬁnition 3 (Sub-Weibull random variables). A sub-Gaussian random variable X can be deﬁned as one for which E(|X|p)1/p ≤ K√p, ∀ p ≥ 1, where K is constant. (2.109) A natural realization which allows for heavier tails is as follows. Fix some γ > 0, and require ∥X∥p := (E|X|p)1/p ≤ Kp1/γ ∀ p ≥ 1∨γ. (2.110) Notice that the condition above requires that the tail is no heavier than that of a Weibull random variable with parameter γ. Deﬁnition 4 (Sub-Weibull Random Variables Properties). Let X be a random variable. Then, the follow- ing statements are equivalent for every γ > 0. The constants K1,K2,K3 differ from each other at most by a constant depending only on γ. (i) The tails of X satisﬁes P(|X| > t) ≤ 2exp � − � t K1 γ�γ� ∀ t ≥ 0. (2.111) 2.3.5. Approximation Theorems for Strongly Mixing Random Variables Theorem 5. Suppose that (Xk) is a strictly stationary sequence of real-valued Random Variables with E(Xk) = 0, E � X2 k � < ∞ and Var(Sn) → ∞ as n → ∞. Suppose that δ > 0 and that λ > 1 +3/δ are real numbers, such that α(n) = o � (log)−λ� as n → ∞, and sup n→∞ E|Sn|2+δ (VarSn) (2+δ) 2 < ∞. (2.112) Then, ∃ σ2, such that 0 < σ2 < ∞, such that lim n→∞ 1 nVar(Sn) = σ2. Thus, without changing its probabil- ity process � S(t),t ≥ 0 � can be redeﬁned on another probability space, together with a Wiener process � W(t),t ≥ 0 � P ���S(t)−W(σ2t) ��� = o � t1/2(loglogt)−1/2� as t → ∞ (2.113) 292.4. Discussion The various mixing properties of time series (either contemporaneously or temporally dependent) are important for understanding how these particular features affect the asymptotic theory for model estima- tors, test statistics and the relevant properties of model selection methods based on dependent data which exhibit these features. Some potential data speciﬁc features such as the presence of serial dependence among errors and regressors, heteroscedasticity and fat tails can lead to misspeciﬁed time series models which especially in a high dimensional setting require to establish validity via error bound determination (see, Adamek et al. (2023) and Wong et al. (2020) among others). In particular, shrinkage methodologies assume a certain structure on the unknown parameter vector of interest. Generally, the sparsity condition implies that a small but unknown subset of the high dimen- sional vector of covariates is considered to have "signiﬁcantly different than zero" coefﬁcients, while the remaining subset of covariates have negligible, or even exactly zero, coefﬁcients. In other words, for Lasso shrinkage methodologies to render meaningful inference the penalty function exploits the under- line sparsity condition. Speciﬁcally, Adamek et al. (2023) obtain novel theoretical results for both point estimation and inference via the desparsiﬁed lasso. Furthermore, they consider a general time series framework where the regressors and error terms are allowed to be non-Gaussian, serially correlated and heteroscedastic and the number of variables can grow faster than the time dimension. Example 5 (Nuclear Norm Regularized Estimation). Denote the Frobenius norm of an (N ×T) matrix A be a ∥A∥2 := � ∑N i=1 ∑T t=1A2 it �1/2 which implies that ∥A∥2 2 = � ∑N i=1∑T t=1 A2 it � . Then, the OLS estimator of β is given by �βOLS := min λ∈RN×R, f ∈RT×R 1 2NT ��Y −β.X −λ f ′��2 2 , (2.114) Relevant research questions of interest: • How does the limiting distribution of the OLS estimator in the above high-dimensional environment is affected under different mixing conditions and distributional assumptions on the error term? • What approach we need to follow in order to obtain error bounds on relevant statistical quantities? Example 6 (Prewhitening Estimators). The idea behind prewhitening goes like this: Suppose one is nonparametrically estimating a function f(λ) at λ0 by taking unbiased estimates of f(λ) at a number of points λ in a neighbourhood of λ0 and averaging them. Additionally, if the function f(λ) is ﬂat in this neighbourhood, then this procedure yields an unbiased estimator of f(λ0). If f(λ) is not ﬂat in this neighbourhood, then the procedure is biased and the magnitude of the bias depends on the degree of non-constancy of f(λ). In the time series literature, the idea of prewhitening has been applied to nonparametric estimators of the spectral density function. In this case, one tries to transform (ﬁlter) the data in such a way that the transformed data is uncorrelated, since an uncorrelated sequence has a ﬂat spectral density function. Understanding the technical tools of learning theory and mixing conditions are useful for this application as well. 303. Time Series Regression Models with Many Covariates A recent growing literature develops econometric frameworks for estimation and inference in regression models with many covariates. Relevant studies include among others Cattaneo et al. (2018), Karmakar et al. (2022) and Wei et al. (2023) (see, also Farrell (2015) although not in a time series setting). 3.1. A forecasting Application Let ˆyt be the forecast of yt based upon information up to t − 1. Notice that when the interested of the researcher is the one-period ahead forecast, (yt − ˆyt)2 is the cost to be minimized. However, there are two situations where the accumulated cost function, denoted with ∑t j=1 (yt − ˆyt)2 is more appropriate. For example, in the sequential forecast case, the forecaster are updated sequentially over many periods and therefore the accumulated cost function is the target to be minimized. Consider for example the one-period expected loss function E(yT+1 − ˆyT+1)2. For instance, for the AR(1) model, under the assumption that E � ε2 t |Ft−1 � = σ2 almost surely for all t, then it can be shown that under appropriate assumptions that 1 T ∑T t=1 (yT+1 − ˆyT+1)2 almost surely. Let ˆβt be the Least square estimate of β, then we have that ˆβt = � t ∑ j=1 Yj−1Y′ j−1 �−1 � t ∑ j=1 Y′ j−1yk � (3.1) where Yt = {y1,...,yt}′, then ˆyt = ˆβ ′ t−1Yt−1 is the least square prediction of yt at time t −1. Theorem 6 (Dhrymes (2013)). Assume that εt are i.i.d random variables with E(εt) = 0 such that it holds 0 < E � ε2 t � < σ2 < ∞. Moreover, let Xt = (xt−1,....,xt−p)′, ST = ∑T t=1 εt. Lemma 5 (Dhrymes (2013)). Assume that Xt+1 = AXt +εt, where εt = (εt,0,....,0)′ and the eigenvalues of A are all inside the unit circle. Then, we have that lim T→∞ T ∑ t=1 XtSt � T T ∑ t=1 S2t = 0, almost surely, (3.2) Proof. It is known that lim T→∞ 1 T T ∑ t=1 XtX′ t = Σ, almost surely, (3.3) where Σ is a positive deﬁnite matrix. Further details can be found in Dhrymes (2013). 313.1.1. Best Linear Predictor The main tool of prediction, or forecasting, in time series relies on the concept of the best linear pre- dictor which is often employed in order to obtain the limiting distribution of an arbitraty size vector of autocovariance estimators (see, Dhrymes (2013)). Deﬁnition 5. Let {Xt : t ∈ T } be a zero mean stationary time series indexed on the linear index set. The best linear predictor of Xt+h,h ≥ 1, given {X1,...,Xn}, is the function given by ˆXt+h = n ∑ j=1 αjXn+1−j (3.4) which minimizes S = E � Xn+h − n ∑ j=1 αjXn+1−j �2 (3.5) The ﬁrst order conditions are given by ∂S ∂α = −2E � Xn+h −α′X(n) � X′ (n) = 0, X(n) =   Xn Xn−1 ... X1   (3.6) Thus, by rearranging we get that E � Xn+hX′ (n) � = α′E � X(n)X′ (n) � (3.7) or α = Cn(i− j)cnh. Proposition 1 (Dhrymes (2013)). The limiting distribution of the coefﬁcient vector of the best linear predictor (BLP) obeys the following limit √n( ˆα −α) d→ N � 0,GVG′� , (3.8) where the matrix G is deﬁned as below: G = C−1 n � F − � α′ ⊗In � (BST,0) � . (3.9) 323.1.2. Forecast Combination Assume that we are interested to construct a forecast combination criterion based a speciﬁc econometric model of interest. Then, a simple approach would be to consider the construction of a criterion which uses OOS forecast combinations. Let ft be a sequence of out-of-sample forecasts for yt+1 derived using a set m different econometric models. The combination forecast is then deﬁned as f(w) = w ′f. Therefore, the OOS empirical MSFE is given by ˆσ2 = 1 P n ∑ t=n−P � yt+1 −w ′ft �2 (3.10) Furthermore, the Granger-Ramanathan forecast combination method implies to select w to minimize the OOS MSFE. However, minimization over w is equivalent to the least-squares regression over yt on the forecasts yt+1 = w ′f+εt+1. Then, the unrestricted least-squares gives the following vector of weights ˆw = � n ∑ t=n−P ftf ′ t �−1 n ∑ t=n−P ftyt+1 (3.11) Notice that the above unrestricted least-squares approach can produce weights which are far outside [0,1], thus an alternative representation can be constructed by subtracting yt+1 from each side, such that 0 = w ′f−yt+1 +εt+1, and by deﬁning et+1 = (ft −yt+1) to be the negative forecast errors. Then, it holds that 0 = w ′et+1 +εt+1, which is the regression of 0 on the forecast errors. Therefore, the constrained GR weights solve the following problem. min w � w ′Aw � subject to M ∑ m=1 w(m) = 1 and 0 ≤ w(m) ≤ 1 (3.12) where A = ∑ t et+1e ′ t+1, the M ×M matrix of forecast error empirical variances/covariances. Proposition 2. In linear models, the combination forecast is the same as the forecast based on the weighted average of the parameter estimates across the different models. That is, ˆyn+1(w) = M ∑ m=1 w(m)ˆyn+1(m) = M ∑ m=1 w(m)xn(m) ′�β(m) ≡ xn(m) ′�β(w) (3.13) where �β(w) = M ∑ m=1 w(m)�β(m). Relevant studies include Hansen (2007), Hansen (2008) and Cheng and Hansen (2015) while a recent approach related to forecasting can be found in Boot and Nibbering (2019) and Swanson et al. (2020). For nonstationary time series models relevant studies include Hansen (2010), Coroneo and Iacone (2020) and Kejriwal and Yu (2021) among others. 333.2. Statistical Theory for Lasso Regression Models 3.2.1. Statistical Principles Example 7. (Predicting inﬂation rates) • Minimization of the empirical risk for a sample {y1,...,yn} : �µ = arg min µ∈R 1 T n ∑ t=1 (yt − µ)2 . (3.14) • Taking the ﬁrst-order conditions give: ∂ ∂µ � 1 T n ∑ t=1 (yt − µ)2 ����� µ=�µ = −2 n n ∑ t=1 (yt − �µ) = 0 (3.15) Thus we obtain that, �µ := 1 T n ∑ t=1 yt ≡ ¯y. Corollary 6 (Shrinkage method). If we drop the requirement of unbiasedness, can we ﬁnd estimators �β with the following property: K ∑ j=1 MSE ��β j � ≤ K ∑ j=1 MSE ��β ols j � (3.16) where MSE ��β j � = E ���βk −βk �2� . • If we aim for (3.16) to hold for all possible β ∈ RK then, this is equivalent to imposing a condition that the OLS estimator is admissible. • If we aim for (3.16) to hold for some given ﬁxed β ∈ RK, then ﬁnding such a �β that improves on �β ols is relatively easy. Corollary 7 (Shrinkage: Ridge Regression). Suppose that there exists a given penalty parameter λ ≥ 0 the ridge regression estimators is given by the following expression �β ℓ2(λ) = arg min b∈RK � 1 n n ∑ t=1 (yt −xtb)2 +λ ∥b∥2 2 � ≡ � X′X +nIk �−1� X′y � (3.17) Recall that the Euclidean norm is given by: ∥b∥2 = √ b′b. In other words, the inverse of (X′X) is regularized by nλIk (called Tikhonov regularization for ill-posed problems). 34Special cases of Ridge Regression include: • When the design matrix X′X is singular then it corresponds to a large variance of the OLS estimator. • When n−1X′X = I p, then simply �β ℓ2(λ) = (1+λ)−1 �β ols. Corollary 8 (Bias of the Ridge Estimator when p < n). Consider the ridge estimator as below: �β ridge(λ) = � X′X +λI �−1 � X′Y � = (R+λI)−1 R � R−1X′Y � = � R � I +λR−1��−1R �� X′X �−1� X′Y �� = � I +λR−1�−1 R−1R�β ols = � I +λR−1�−1 �β ols. Therefore, it holds that E ��β Ridge(λ)|X � = E �� I +λR−1�−1 �β ols|X � = � I +λR−1�−1 β E � �β Ridge(λ) � = E �� I +λR−1�−1� β ̸= β. Based on the above results we can introduce the model selection idea with an ℓ0−penalty. Thus, in the literature various studies mention that one of the main advantages of the Lasso shrinkage is that it "bets on sparsity". Therefore, using an ℓ0 penalty term we have that ∥β∥0 = p ∑ j=1 1 � β j ̸= 0 � (3.18) which implies that �β ℓ0(λ) = arg min β∈Rp � 1 n n ∑ t=1 � Yt −β ′Xt �2 +λ p ∑ j=1 1 � β j ̸= 0 � � = arg min β∈Rp �1 n ∥Y −Xβ∥2 2 +λ ∥β∥0 � . where λ ≥ 0 is the tuning (penalty) parameter which needs to be chosen. Similarly, using an ℓ1 penalty �β ℓ1(λ) = arg min β∈Rp � 1 n n ∑ t=1 � Yt −β ′Xt �2 +λ p ∑ j=1 ��β j �� � = arg min β∈Rp �1 n ∥Y −Xβ∥2 2 +λ ∥β∥1 � where λ ≥ 0 is the tuning (penalty) parameter which needs to be chosen. The lasso shrinkage approach was proposed in the seminal study of Tibshirani (1996). Various studies examine its statistical properties such as Fan and Li (2001) and Zhang (2010) among others, while many variants of the lasso shrinkage have been proposed in the literature such as Zou and Hastie (2005), Zou (2006), Huang et al. (2008), Park and Casella (2008) and James et al. (2009). A relevant empirical study is present by Katsouris (2021). Recently, the literature focuses on applications of the lasso shrinkage approach with group structure (see, also Huang et al. (2012) and Bing et al. (2022)). 35In particular, Klau et al. (2018) propose the priority lasso, which considers the use of different penalty across a baseline group of covariates (see, Breheny and Huang (2015) for case with grouped predictors) and the remaining set of variables while Campbell and Allen (2017) propose a variable selection method using the exclusive Lasso. Now, the Lasso estimates of the slope coefﬁcients in a linear regression model solve the ℓ1−penalized least regression problem: min β n ∑ i=1 � Yi − p ∑ j=1 β jXi j �2 subject to p ∑ j=1 ��β j �� ≤ s, (3.19) or, equivalently we have that min β n ∑ i=1 � Yi − p ∑ j=1 β jXi j �2 +λ p ∑ j=1 ��β j �� (3.20) where β = (β1,...,βp)′, and s and λ are tuning parameters. The lasso uses a constraint in the form of ℓ1−norm: ∑p j=1 ��β j �� ≤ s. Therefore, by using the ℓ1−penalty, the Lasso achieves variable selection and shrinkage simulatneously, while λ controls the amount of regularization. Denote with β 0 the true vector of parameters. There are three types of errors of interest in LASSO regres- sion, that is, (i) the prediction error: ���X � �β −β 0���� 2 2, (ii) the parameter estimation error: ��� � �β −β 0���� 2 2, and (iii) the model selection error: P � supp ��β � = � β 0�� . In other words, the prediction problem im- plies that given a random sample (yi,xi),i = 1,...,n and the covariates xn+1 of an additional observation, we aim to predict the unobserved outcome yn+1. Choice of λ is achieved using Cross-Validation method: λ ∗ = arg min λ>0 n ∑ i=1 � yi −xi �β (−i) (λ) �2 , (3.21) where �β (−i)(λ) is the estimator �β ℓ1(λ) obtained after dropping observation i from the sample. Notice that cross-validation is usually used to choose the tuning parameter λ for in-sample model estimation and variable selection. However, there are theoretical results that show that LASSO performs well for prediction even for large number of covariates p, as long as the true parameter β is sparse. Under appropriate regularity conditions with high probability (for large n) we have that 1 n ���X �β ℓ1(λ ∗)−Xβ ��� 2 2 ≤ C∥β∥1 � log(p) n (3.22) The cross-validation is usually used to choose the tuning parameter λ for in-sample model estimation and variable selection. Furthermore, a key property for model selection implies that under appropriate regularity conditions with high probability (large n) one can show the following relation �S := � j ∈ {1,..., p} : �β ℓ1 j (λ) ̸= 0 � = � j ∈ {1,..., p} : β j ̸= 0 � . (3.23) 36Lemma 6 (Maximum Inequality for Gaussians). For a sample of n Gaussian random variables Zi, for i = 1,...,n, such that E(Zi) = 0 and E(Z2 i ) ≤ ¯σ2 z , ∀ i, then it holds that P � max i=1,...,n|Zi| ≥ z � ≤ 2ne − nz2 2 ¯σ2z . (3.24) Under our assumptions, U′X j T , j = 1,..., p, is Gaussian with mean zero and variance bounded by C σ2 n . Then, it holds that P ����� U′X n ���� ∞ ≥ z � ≤ 2pe− nz2 2C ¯σ2 . (3.25) The aim is to make (bound) the above probability as small as possible. Moreover, to obtain convergence rates and bounds we can use the following deﬁnitions z = σ � 2log( ep δ ) n , where δ > 0 and e := exp(1). Set also C = 1. Therefore, we obtain that P   ���� U′X n ���� ∞ ≥ � 2log �ep δ � n   ≤ 2e−log( ep δ )+log(p) = 2e−log(e)+log(δ) = 2 eδ < δ. Therefore, we get that 1 n ���X � �β −β 0���� 2 2 ≤ 4σ ���β 0��� 1 � 2log(ep/δ) n . (3.26) Another important aspect related to the statistical properties of the lasso shrinkage approach is the sign consistency. Speciﬁcally, Strong Sign Consistency: implies that one can use a pre-selected λ to achieve consistent model selection via the LASSO. On the other hand, a General Sign Consistency: means that for a random realization there exists a correct amount of regularization that selects the true model. Further discussion and related asymptotic results on the consistency and regularization property3 under different distributional assumptions can be found in Medeiros and Mendes (2016). Lemma 7 (Oracle Property). Let �β ols,S denote the OLS estimator of β 0 S. Suppose that a lower bound for β can be established such that βmin > � λ/n1−ξ/2�� s1/2/φmin � , then it holds that √nα′ ��β S −β 0 S � = √nα′ ��β ols,S −β 0 S � (3.27) for any s−dimensional vector α with Euclidean norm 1. 3Professor Marcelo C. Medeiros gave a seminar with title: "Bridging Factor and Sparse Models" (see, Fan et al. (2021b)) at the Department of Economics, University of Southampton on the 6th of October 2021. 37Example 8 (Modelling Unobserved Heterogeneity). Consider the linear regression model below: y = p ∑ j=1 xjβ 0 j +ε, y = Xβ 0 +ε, (3.28) where X = (x1,...,xp) is an (n× p) matrix, y = (y1,...,yn)′ is an (n ×1) vector and β 0 = � β 0 1 ,...,β 0 p � is the true parameter vector. Then, the homogeneity assumption implies that the regression coefﬁcients β j share the same value in their unknown clusters such that under the null hypothesis: H0 : β 0 j = β 0 A,k for all j ∈ Ak (3.29) which also demonstrates the variable selection consistency property. 3.2.2. Dantzig Selector A large stream of literature has focused on the ℓ1 penalized LASSO estimator of parameters in high- dimensional linear regression when the number of variables can be much larger than the sample size. We consider the linear regression with many covariates such as y = Xβ +ε (3.30) where X is the n×M deterministic design matrix, with M possibly much larger than n, and ε is a vector of i.i.d standard normal random variables. In particular, we are interested in the case of sparsity parameters, which implies that the high-dimensional vector β has coefﬁcients that are mostly 0. Denote with �S(β), the residual sum of squares such that �S(β) = 1 n n ∑ i=1 � Yi − fβ (Zi) �2 (3.31) for all β ∈ RM. Deﬁne the Lasso solution such that �βL = ��β1,L,...., �βM,L � by the following expression �βL = arg min β∈RM � �S(β)+2r M ∑ j=1 ��f j �� n ��β j �� � , (3.32) where r > 0 is some tuning constant. Then, the corresponding Lasso estimator �fL(x) = f�βL(x) = M ∑ j=1 �β j,L f j(z) (3.33) A necessary and sufﬁcient condition of the minimizer is that 0 belongs to the subdifferential of the convex function β �→ n−1 |y−Xβ|2 2 +2r ���D1/2β ��� 1. 38This implies that the Lasso selector �βL satisﬁes the constraint ���� 1 nD−1/2X⊤ � y−X �βL ����� ∞ ≤ r, D = diag � ∥ f1∥2 n ,...,∥fM∥2 n � . (3.34) Various studies in the literature present relevant applications and statistical theory of the Dantzig selec- tor such as Osborne et al. (2000), Candes and Tao (2007), Bickel et al. (2009), Koltchinskii (2009) and James et al. (2009) among others. We say that β ∈ RM satisﬁes the Dantzig constraint if β belongs to the set � β ∈ RM : ���� 1 nD1/2X⊤(y−XβL) ���� ∞ ≤ r � . (3.35) The Dantzig estimator is deﬁned by the following expression �fD(z) = f�βD = M ∑ j=1 �β j,D f j(z), (3.36) where �βD = ��β1,D,..., �βM,D � is the Dantzig selector. By the deﬁnition of Dantzig selector, we have that ����βD ��� 1 ≤ ����βL ��� 1. Notice that the Dantzig selector is computationally feasible, since it reduces to a linear programming problem. Finally, for any n ≥ 1, M ≥ 2, we consider the Gram matrix as below Ψn = 1 nX′X = � 1 n n ∑ i=1 f j(Zi) f j′(Zi) � 1≤j, j′≤M (3.37) and let φmax denote the maximal eigenvalue of ψn. 3.2.3. Oracle inequalities for prediction loss In this section, we prove sparsity oracle inequalities for the prediction loss of the Lasso and Dantzig estimators. These inequalities allow us to bound the difference between the prediction errors of the estimators and the best sparse approximation of the regression function (e.g., by an oracle that knows the truth but is constrained by sparsity). We demonstrate that the distance between the prediction losses of the Dantzig and Lasso estimators is of the same order as the distances between them and their oracle approximations. Recall that an estimator has the oracle property if it is variable selection consistent and the limiting distribution of its subvector corresponding to the non-zero coefﬁcients is the same as if their set were known prior to estimation (see, Giurcanu (2016)). Theorem 7. Let Wi be independent N � 0,σ2� random variables with σ2 > 0. Fix some ε > 0 and integers n ≥ 1, M ≥ 2, 1 ≤ s ≤ M. Let Assumption RE (s,3 + 4/ε) be satisﬁed. Consider the Lasso estimator �fL with r = Aσ � logM n , for some A > 2 √ 2. Then, with probability at least 1−M1−A2/8, it can be proved that ��� �fL − f ��� 2 n is bounded. 393.3. Parameter Estimation and Model Selection Consistency Properties Consider the linear regression model as below (see, Amann and Schneider (2018)) y = Xβ +ε (3.38) where y ∈ Rn is the response vector, X ∈ Rn×p the nonstochastic regressor matrix which is assumed to have full column rank, β ∈ Rp the unknown parameter vector, and ε ∈ Rn the unobserved stochastic error term consisting of i.i.d distributed components with mean zero and ﬁnite second moments, deﬁned on some probability space (Ω,F,P). Model consistency results can be found in Shibata (1986) and Potscher (1991). Moreover, to deﬁne the adaptive Lasso estimator, we consider the following expression Ln(b) = ||y−Xb||2 +2 p ∑ j=1 λ j |bj| |ˆbj| (3.39) where ˆβ denotes the OLS estimator. In particular, we assume that the event � ˆβ j = 0 � to have zero probability, for all j = 1,..., p and thus we do not consider this event occurring in the subsequent analysis. Lemma 8 (Equivalence to LS estimator, Amann and Schneider (2018)). If λ ∗ → 0, then ˆβAL and ˆβLS are asymptotically equivalent in the sense that √n � ˆβAL − ˆβLS � → 0, as n → ∞ for all ω ∈ Ω. (3.40) Remark 8. The above Lemma shows that in the case that λ ∗ → ∞, then the adaptive Lasso estimator is asymptotically equivalent to the LS estimator. Furthermore, in terms of consistency in parameter estimation it can be shown that the adaptive lasso estimator is both pointwise and uniformly consistent for the unknown parameter vector of the possibly high dimensional linear model. 3.3.1. Consistency Properties Consider any sequence (βn)n∈N ⊂ Rp converging to φ and let fn(β) = Pβ � β ∈ ˆβ − � λ ∗ n Md � . (3.41) Then, by the Portmanteau Theorem, we have that 0 ≤ lim inf n lim β∈Rp fn(β) ≤ lim sup n lim β∈Rp fn(β) ≤ lim sup n Pβn �� n λ ∗ � ˆβ −βn � ∈ Md � ≤ Pφ � arg min u Vφ(u) ∈ Md � = 1{m ∈ Md} = 0. 40Then, for any ω ∈ Ω, we have that M = � φ∈ ¯Rp arg min u∈Rp Vφ(u)(ω). (3.42) In other words, while the limit of � n λ ∗ � ˆβ −βn � will in general be random, the set M is not random. In particular, Proposition 8 shows that, for any ω, the union of limits over all possible sequences of unknown parameters is always given by the same compact set M . Speciﬁcally, this observation is central for the construction of conﬁdence regions in the following section. It also shows that while in general, a stochastic component will survive in the limit, it is always restricted to have bounded support that depends on the regressor matrix and the tuning parameter through the matrix C and the quantities ψ and λ 0. Interestingly, M only depends on ψ for the components where ψj = ∞, in which case the set M loses a dimension. In particular, this can be seen as a result of the j−th component being penalized much less than the maximal one, so that the scaling factor used in Theorem 7 is not large enough for this component to survive in the limit. Consider the following expression (see, Amann and Schneider (2018)) ����� �XX′ nk � ˆβAL − ˆβLS �� j ����� = λ j nk 1 ��� ˆβLS, j ��� (3.43) where ˆβAL ̸= 0. Notice that the left-hand side is bounded by L, whereas the right-hand side converges to c |β j| in probability. We therefore get Pβ � ˆβAL, j = 0 � → 1, for all β j ∈ R, satisfying the condition ��β j �� < c L. The set M = M1 acts as a benchmark for conﬁdence sets in the sense that if we take a "slightly larger" set, multiplied with the appropriate factor and centered at the adaptive Lasso estimator, we get a conﬁdence region with minimal asymptotic coverage probability equal to 1. But if we base the region on a "slightly larger" set than M , we end up with a conﬁdence set of asymptotic minimal coverage 0. Remark 9 (Amann and Schneider (2018)). In order to provide some further insights of the main pitfalls, we focus on the case where λ 0 ∈ (0,1]p, that is, the case where all components of λ 0 are nonzero (implying that ψ = 0). In other words, this implies that all components are penalized at the same rate, which is obviously fulﬁlled for uniform tuning. However, in the case of uniform tuning (i.e., parameter penalization with the same rate), then the asymptotic distribution is mere point-mass with no stochastic part surviving in the limit. The reason for this, is the fact that when controlling for the bias of the estimator, the stochastic part vanishes asymptotically. In other words, the appropriate scaling factor is simply not large enough to keep the random component in the limit (i.e., it is asymptotically negligible for large samples). This basically illustrates that the bias is of larger order than the stochastic component when viewed under uniform lens - a fact that is generally inherent to penalized estimators. Moreover, the aspect of omitted variable bias in high dimensional settings is discussed by Wüthrich and Zhu (2023). 41Example 9. Suppose that the data {yt,t = 1− p,...,n} is generated by the model yt = pn ∑ j=1 Φjyt−j +εt, t = 1,...,n (3.44) where yt = � yt,1,...,yt,k � , a (k ×1) vector of variables in the model, where εt is a sequence of i.i.d error terms with N (0,Σ) distribution. Furthermore, all roots of �����Ik − p ∑ j=1 Φjz j ����� are assumed to lie outside the unit disc. Deﬁnition 6 (Restricted Eigenvalue Condition). The restricted eigenvalue condition RE(r) is said to sat- isﬁed for some 1 ≤ r ≤ kp if k2 ψn(r) := min R⊂{1,...,kp} min δ∈Rkp\{0} δ ′Φnδ ∥δR∥2 > 0. (3.45) where R ⊂ {1,...,kp} and |R| is its cardinality. Then, one is interested to investigate the properties of the Lasso shrinkage norm when applied to each equation i ∈ {1,..,k} separately. The Lasso estimates are obtained by minimizing the objective function: L(βi) = 1 n ∥yi −Xβi∥2 +2λn∥βi∥ℓ1 (3.46) Let J( ˆβi) = � j : ˆβi, j ̸= 0 � be the indices of the parameters for which the estimator is non-zero. Example 10. Consider the time series vector Yt, which is an N−dimensional random vector generated by the VAR model such that Yt = A1Yt−1 +...+ApYt−p +ut, t ∈ {1,...,T}. Deﬁne the N(p+1) vector Xt = � Y ⊤ t−p,...,Y⊤ t−1,Y ⊤ t �⊤ and let Σx = Var(Xt) = E � XX⊤� and Γi = E � YtY ⊤ t−i � the autocovariance matrix. An equivalence relation between the matrix coefﬁcients of the two multivariate regression models Bi,kℓ = 0 ⇐⇒ Corr � Y k,t,Y ℓ,t−i ��� Xt\ � Y k,t,Y ℓ,t−1 ��� (3.47) for i ∈ {1,..., p} (see, Poignard and Asai (2023)). Thus, to derive the partial correlation coefﬁcient be- tween two variables in Xt, we rely on the inverse of Σx. Denoting the (k.ℓ)−th element of Σ−1 x by σkℓ x , the partial correlation coefﬁcient between the k−th and the ℓ−th elements of Xt is then ρkℓ x = − σkℓ x √ σkk x σℓℓ x for k ̸= ℓ. Also, we denote with σkℓ u the (k,ℓ)−th element of the matrix Σ−1 u . Then, it can be obtained that Bi,kℓ = ρrs x � Σx,ss(1−ρ2 s\r) Σx,rr(1−ρ2 r\s) �1/2 . (3.48) 423.4. A Lasso-based Time Series Regression Model Example 11. Consider the following Dickey-Fuller regression model ∆yt = ρ∗yt−1 + p ∑ j=1 ∆yt−j +εt. (3.49) Notice that in the case that the autoregression parameter ρ∗ = 0, then the model is said to have a unit root and is said to be nonstationary. According to Kock (2016) it can be shown that: (i) The adaptive Lasso possesses the oracle property in stationary and nonstationary autoregressions (see, Kwiatkowski et al. (1992), Muller (2008), Nielsen (2009)). Hence, the ALasso shrinkage approach can distinguish between stationary and nonstationary autoregressions which is extremely important when choosing the right model for forecasting; (ii) Show that choosing the tuning parameter by BIC results is consistent model selection; (iii) Analyze the asymptotic behaviour of the probability of classifying ρ∗ as 0 in the stationary, non- stationary and local to unity setting such that ρ∗ = c/T (see also Caner and Knight (2013)). In the nonstationary setting the problem due to nonuniformity in the asymptotics, can be alleviated if one is willing to tune the adaptive Lasso to perform conservative model selection instead of consistent model selection. We employ the following variant of the adaptive Lasso which is deﬁned as the minimized of ΨT (ρ,β) = T ∑ t=1 � ∆yt −ρyt−1 − p ∑ j=1 β j∆yt−j �2 +λTwγ1 1 |ρ|+λT p ∑ j=1 wγ2 1 ��β j ��, (3.50) where γ1,γ2 > 0 and w1 = 1/| ˆρI| and w2 = 1/| ˆβI, j| for ˆρI and ˆβI, j denote some initial estimators of the parameters of the model. Furthermore, notice that the objective function is modiﬁed compared to the usual adaptive Lasso since it penalized ρ, the coefﬁcient on the potentially nonstationary variable variable yt, different from the coefﬁcients on the stationary variables. We denote with θ = (ρ∗,β ∗′)′ the set of model parameters. The set of active variables is denoted with A = � 1 ≤ j ≤ p+1 : θ∗ j ̸= 0 � , (3.51) Moreover, denote with S = diag � T, √ T,..., √ T � denotes a (p+1× p+1). Denote with ˆθ = � ˆρ, ˆβ ′�′ denote the minimizer of the objective function. Furthermore, denoting M0 the true model and ˆ M the estimated model, then we can say that the shrinkage methodology is consistent if for all � ρ∗, ˆβ ∗� it holds that P � ˆ M = M0 � → 1. Thus, the shrinkage selection methodology is said to be conservative if for all � ρ∗, ˆβ ∗� it holds that P � ˆ M0 ⊂ ˆ M � → 0, which implies that the probability of excluding relevant variables tend to zero. 433.4.1. Oracle Property We discuss the oracle property of the adaptive Lasso for stationary and nonstationary autoregressions. Theorem 8. (Consistent model selection). Assume that εt is i.i.d with E(ε1) = 0 and E � ε4 1 � < ∞. (A) (Nonstationary Case) Then, if ρ∗ = 0, and it holds that λT T 1−γ1 → ∞, λT T 1/2−γ2/2 → ∞, and λT T 1/2 → 0, the following properties hold (i) Consistency: ����ST �� ˆρ, ˆβ ′�′ −(0,β ∗′)′ ����� ℓ2 ∈ Op(1). (ii) Oracle I: P( ˆρ = 0) → 1 and P � ˆβA c = 0 � → 1. (iii) Oracle II: √ T � ˆβT −βA � → N � 0,σ2[ΣA ]−1� . (B) (Stationary Case) If yt is stationary such that ρ∗ ̸= 0, λT T −1/2−γ2/2 → ∞, and λT T 1/2 → 0, (i) Consistency: ����ST �� ˆρ, ˆβ ′�′ −(ρ∗,β ∗′)′ ����� ℓ2 ∈ Op(1). (ii) Oracle I: P( ˆρ = 0) → 1 and P � ˆβA c = 0 � → 1. (iii) Oracle II: �√ T � ˆβT −βA �√ T � ˆβT −βA �� → N � 0,σ2[ΣA ]−1� . Example 12. Consider the framework proposed by Wong et al. (2020). The particular modelling environ- ment corresponds to (i) stationary Gaussian processes with suitably decaying α−mixing coefﬁcients, and (ii) stationary processes with sub-Weibull marginals and geometrically decaying β−mixing coefﬁcients. Moreover, it is well known that guarantees for lasso follow if one can establish the restricted eigenvalue (RE) conditions and provide deviation bounds for the correlation between noise and the regressors. Then, statistical estimation can be achieved as below: Θ⋆ = argmin Θ∈Rp×q E ���Yt −Θ′Xt ��2 2 � . (3.52) Then, the ℓ1penalized least squares estimator �Θ ∈ Rp×q is deﬁned as �Θ = argmin Θ∈Rp×q 1 T ∥vec(Y −XΘ)∥2 2 +λT ∥vec(Θ)∥1 (3.53) Remark 10. The β−mixing condition has been of interest in statistical learning theory for obtaining ﬁnite sample generalization error bounds for empirical risk minimization. Speciﬁcally, the usefulness of β−mixing lies in the fact that by using a simple blocking technique, (e.g., by Yu (1994)), one can reduce the situation to the i.i.d setting. However, there are no results showing that the RE and DB conditions holds under mixing conditions. 443.5. Lasso shrinkage with long memory regression errors In many problems of practical interest in which the lasso shrinkage is applied, such as for example when modelling and forecasting Realized Volatility measures, it is reasonable to consider the existence of long memory errors. We investigate the asymptotic behaviour of Lasso in regression models with long memory errors. Consider Xi = (xi1,....,xip)′ for i = 1,...,n to be the vector of design matrices and Yi to denote its response variable. Thus we have the model (see, Kaul (2014)) Yi = X′ i β +εi, for some β ∈ Rp, 1 ≤ i ≤ n. (3.54) The errors εi are assumed to be long memory moving average with i.i.d innovations, that is, εi = ∞ ∑ k=1 akζi−k = i ∑ k=−∞ ai−kζk, (3.55) where ak = c0k−1+d,∀k ≥ 1, 0 ≤ d ≤ 1 2 and some constant c0 > 0, and ak = 0 for k ≤ 0. Moreover, we have that ζj, j ∈ Z := {0,±1,±2,...}, are i.i.d RV’s with mean zero and variance σ2 ζ . Without loss of generality we assume that σ2 ζ = 1. We denote with X = � xi j � n×p as the design matrix, and ε := (ε1,...,εn)′. Moreover, {εi,i ∈ Z} is a stationary process with autocovariance function γε(k) = ∞ ∑ j=1 ajaj+k = k−1+2dB(d,1−2d) (3.56) Moreover, the Lasso estimate of β is deﬁned as follows ˆβ n(λ) = arg min β �1 n ��Y −X′β ��2 2 +λn∥β∥1 � , λ > 0, (3.57) where Y = (Y1,...,Yn)′ and ∥β∥1 := ∑p j=1 ��β j �� denotes ℓ1 norm of β = (β1,...,βp)′. Notice that the literature in the area of regularized estimation with dependence considerations is scarce. In this paper, we we investigate the asymptotic behaviour of Lasso under strong dependence structure and less restrictive model assumptions. In particular, we assign a long memory structure on the model errors ε, that is, ∑∞ k=1 |γε(k)| = ∞. We provide restrictions on the rate of increase of the design variables as well as the rate of increase of the dimension of p in order to obtain the corresponding ﬁnite sample error bounds. Furthermore, we allow the design variables to grow with the restriction ∑1≤i≤n x2 i j = O(n), and hence the results obtained can also easily be extended to the case of Gaussian random designs. Notice that the framework proposed by Babii et al. (2022) considers a machine learning application for high-frequency time series panel data. The particular time series ﬁlters are constructed based on mixed- frequency data and therefore assumptions on the dependence structure of the errors such as persistence, mixing and long-memory are indeed plausible in such economic and ﬁnance studies. 453.5.1. Results with ﬁnite sample In this Section we prove a ﬁnite sample oracle inequality for the Lasso solution when the design matrix is non-random. We deﬁne with Wnj = n−(1/2+d) n ∑ i=1 xi jεi = n−(1/2+d) n ∑ i=1 i ∑ s=−∞ xi jai−sζs = n ∑ s=−∞ cns, jζs, (3.58) where we have that cns, j := n−(1/2+d) n ∑ i=1 xi jai−s, s ∈ Z, j = 1,..., p, (3.59) cn, j := sup −∞<s≤n ��cns, j ��, cn = max 1≤j≤pcn, j. (3.60) Moreover, we denote with σ2 n, j := Var � Wnj � , σ2 n = max 1≤j≤pσ2 n, j. (3.61) Therefore, we shall prove that with an appropriate choice of λn, the Lasso solution obeys the following oracle inequality in the long memory case, for any n ≥ 1, 1 n ���X � ˆβ −β ���� 2 2 +λn ��� ˆβ −β ��� 1 ≤ 4λ 2 n s0 φ2 0 (3.62) where λn = (O(1))log(p)/n1/2−d, under some conditions on the design matrix. Moreover, s0 denotes the cardinality of the set of non-zero components of β and φ0 is a constant depending on the design matrix X. Therefore, in order to prove the result we need to obtain a probability bound for the set as below Λ = � max 1≤j≤p 2 n ����� n ∑ i=1 xi jεi ����� ≤ λ0n � , (3.63) for a proper choice of λ0n. Thus, once this probability bound is obtained, the oracle inequality follows by deterministic arguments. Remark 11. The presence of long memory errors can affect the convergence rates of estimators as well as the variable selection procedure. Moreover, the presence of serial correlation can be captures by modeling the error term such that εt = ρεt−1+ut. In particular, there are cases in which serial correlation can manifest as structural breaks in high dimensional models (see, Kapetanios and Zikes (2018)). On the other hand, the long memory property is a common feature of mean-revering processes, which implies the signiﬁcance of sample autocorrelations at large lags. In other words, long memory processes implies that when predicting future values this will depend persistently from the past observations. For instance, d = 0.5 is refereed to the fractional or long-memory parameter of the stochastic process. 46Theorem 9. For the long memory regression model suppose that the design variables satisfy the model assumptions. Further, suppose that the tuning parameter λn is such that λn → λ0 ≥ 0, then we have that ˆβn p→ argmin � Z(φ) � (3.64) Z(φ) = (φ −β)′C(φ −β)+λ0 p ∑ j=1 ��φj ��, φ ∈ Rp (3.65) Thus, if λn = o(1) then arg minφ (Z(φ)) = β and ˆβn(λn) is consistent for the unknown parameter β. Proof. To prove this theorem we consider the following objective function Zn(φ) = 1 n n ∑ i=1 � Yi −X′ i φ �2 +λn n ∑ j=1 ��φj ��, (3.66) then Zn(φ) is convex. Therefore, we need to show the pointwise convergence (in probability) of Zn(φ) to Z(φ)+k2 for some constant. Clearly, it holds that λn n ∑ j=1 ��φj �� → λ0 n ∑ j=1 ��φj ��. (3.67) Consider expanding the expression, 1 n n ∑ i=1 � Yi −X′ i φ �2 = 1 n n ∑ i=1 � εi −X′ i (φ −β) �2 = 1 n n ∑ i=1 ε2 i + 1 n n ∑ i=1 (φ −β)′ XiX′ i (φ −β)− 2 n (φ −β)′ n ∑ i=1 Xiεi, = 1 n n ∑ i=1 ε2 i + 1 n n ∑ i=1 (φ −β)′ XiX′ i (φ −β)− 2 n (φ −β)′ X′ε, Notice that the ﬁrst term in the above expression converges to k2 by the ergodic theorem, since the error sequence {εi} forms a stationary ergodic sequence. The second term converges to (φ −β)′C(φ −β) and the last term converges to zero in probability, since we have that 1 n0.5+d X′ε converges in distribution. Theorem 10. For the long memory regression model assume that the design variables satisfy the model assumptions. Suppose that n1/2−dλn → λ0 ≥ 0 as n → ∞, then n1/2−d � ˆβ n −β � →D arg min u V(u), (3.68) V(u) = −2u′W +u′Cu+λ0 p ∑ j=1 � ujsign � β j � I[β j̸=0] + ��uj ��I[β j̸=0] � (3.69) such that W is an N (0,Σ) random variable. 47Proof. We deﬁne the following expression Vn(u) = n1−2d � n ∑ i=1 1 n �� εi − X′ i u n 1 2−d �2 −ε2 i � +λn p ∑ j=1 �����β j + uj n 1 2−d ���� � − ��β j �� � . (3.70) Furthermore, we denote the ﬁrst term of the above expression by (I) and the second term by (II). Then, we obtain that (I) = n1−2d � 1 n n ∑ i=1 ε2 i −2∑n i=1X′ i uεi nn1−2d + u′∑n i=1 XiX′ i u nn 1 2−d − 1 n n ∑ i=1 ε2 i � = �u′∑n i=1 XiX′ i u n −2∑n i=1X′ i uεi n 1 2+d � → u′Cu−2u′W , as n → ∞, where W ∼ N (0,Σ). Moreover, we have that (II) = n 1 2−dλn p ∑ j=1 ����n 1 2−dβ j +uj ���−n 1 2−d ��β j �� � → λ0 p ∑ j=1 � ujsign � β j � I[β j̸=0] + ��uj ��I[β j̸=0] � . Notice that the notion of weak dependence for stationary time series is measured in terms of covariance functions. The following lemma given by Gupta (2012) is useful. Lemma 9. For each ﬁxed n, let A := ������ n ∑ i=−∞ Yni ����� > r � , Bm = ������ n ∑ i=−m Yni ����� > r −δ � , r > 0,δ > 0,m = 1,2,... B = lim inf m→∞ Bm. If ����� n ∑ i=−∞ Yni ����� < ∞, almost surely, then, for each ﬁxed n, A ⊂ B. For the proof see Gupta (2012). The argument is to let ω ∈ A, then it follows that ����� n ∑ i=−∞ Yni ����� > r, ����� n ∑ i=−∞ Yni ����� < ∞. Remark 12. Another relevant aspect for the lasso shrinkage approach regardless of the properties of the error terms is the cross-validation consistency (see, Chetverikov et al. (2021) and Yang (2007)). More- over, although we discuss various cases of the Lasso shrinkage algorithm, those correspond to a different penalization property as well as possibly different-type of thresholding, especially when one considers the pathwise selection approach against the cross-validation approach. 483.6. Residual empirical process based on the ALasso Moreover, the framework proposed by Chatterjee et al. (2015) provides regularity conditions and as- sumptions for deriving the asymptotic behaviour the adaptive Lasso estimator. Speciﬁcally, the Alasso estimator of β is deﬁned as the minimizer of the weighted ℓ1−penalized least squares criterion function �β n = arg min u∈Rp n ∑ i=1 � yi −x′ iu �2 +λn p ∑ j=1 |uj| ˜β j,n γ , (3.71) where, λn > 0 is a regularization parameter, γ > 0 and ˜β j,n is the jth component of �β n, a consistent preliminary estimator of β. 3.6.1. Main results Asymptotic uniform linearity in high dimensions For t ∈ [0,1], we deﬁne �Zn(t) = 1 √n n ∑ i=1 [1(F(ei) ≤ t)−t] (3.72) Zn(t) = 1 √n n ∑ i=1 [1(F(εi) ≤ t)−t] (3.73) We set J(t) = f � F−1(t) � . The ﬁrst result of this section proves the AUL property of the ALASSO based residual empirical distribution function for p. Theorem 11. sup t∈[0,1] ����Zn(t)− � Zn(t)− ¯x′ n √n ��β n −β � J(t) ���� = op(1). (3.74) Notice that Theorem 11 shows that the AUL property holds for the empirical distribution function of the residuals based on the ALASSO ﬁt even for p >> n, provided the regularity conditions hold and the regression model has enough sparsity. Functional oracle property of the ALASSO The AUL property has various important applications in the context of statistical inference in high dimensional regression. For instance, it allows to establish the asymptotic distribution of the residual edf which, in turn, can be used to carry the goodness of ﬁt tests on F and set conﬁdence bands for F. Under standard weak convergence theory we have that Z OR n = n1/2 � �FOR n −F � →w Z∞ on L∞ ([−∞,+∞]) (3.75) where �FOR n is the edf of the residuals based in OLS under the oracle property. 49Moreover, Z∞ is a zero mean Gaussian process with covariance function as below τ(x,y) = Cov(Z∞(x),Z∞(y)), x,y ∈ (−∞,+∞) (3.76) We also deﬁne the ALASSO based residual empirical process as below Zn(x) = √x � �Fn(x)−F(x) � , x ∈ (−∞,+∞). (3.77) Proof. We denote with Γn (t,u) = 1 √n n ∑ i=1 � 1 � F � εi −n1/2x′ iu � ≤ t �� , t ∈ [0,1], and u ∈ Rp. (3.78) Next, we deﬁne the random vector Wn = n−1/2 ∑n i=1 x′ iεi and write Wn = � W(1)′ n ,W(2)′ n � . 3.6.2. Sparsity in high dimensional estimation problems The Lasso estimator appears to be suitable for high dimensional inference problems such as the case when p is much larger than the time series observations n. Some useful results are as below: Regression in the iid case Under the usual assumption that the εi are iid and subGaussian, ∀ s,E � exp � sε2 i �� ≤ exp �s2σ2 2 � (3.79) for some known σ2, then we have that P ������ 1 n n ∑ i=1 W (j) i ����� ≥ t√n � ≤ ψ(t) = exp � − t2 2σ2 � . (3.80) 3.7. Lasso shrinkage for Predictive Regression In this model, the unknown coefﬁcients β ∗ n can be obtained from the data by running OLS �β OLS = arg min β ∥y−Xβ∥2 , (3.81) The asymptotic behaviour of the OLS estimator has been extensively examined in the time series econo- metrics literature (see, Koo et al. (2020), Lin and Tu (2020), Yousuf and Ng (2021), Lee et al. (2022)). A relevant framework for lasso time series regression is proposed by Adamek et al. (2023). 50In particular, the following functional central limit theorem holds 1 √n ⌊nr⌋ ∑ j=1 � e′ j. uj � ⇒ � Be(r) Bu(r) � ≡ BM(Σ). (3.82) To represent the asymptotic distribution of the OLS estimator, deﬁne u+ i = ui −Σ′ euΣeee′ i. By deﬁnition, Cov � ei j,u+ i � = 0 for all j so that X′u n ⇒ ζ := � 1 0 Be(r)dBu+(r)+ � 1 0 Be(r)Σ′ euΣ−1 ee dBe(r)′ (3.83) which is the sum of a mixed normal random vector and a non-standard random vector. The OLS limit distribution is then given by n � ˆβ OLS −β ∗ n � = �X′X n2 �−1 X′u n ⇒ Ω−1ζ, (3.84) where Ω := � 1 0 Be(r)Be(r)′dr. This implies that when we inﬂate �β OLS j by the factor nδ j so that its mag- nitude is comparable to the constant β 0∗ j , we attain consistency in that nδ j ��β OLS −β ∗ jn � = nδ j �β OLS −β 0∗ j = Op � nδ j−1� = op(1) for all j ≤ p. (3.85) Moreover, if β 0∗ j ̸= 0, when δj is close to 1, the signal of xj is weak, and therefore the rate of convergence is slow. However, notice that some true coefﬁcients β ∗ j could be exactly zero, where the associated predictors would be redundant (inactive) in the regression. Let M∗ = � j : β 0∗ j ̸= 0 � be the index set of regressors relevant to the regression, p∗ = |M∗|, and M∗c = {1,..., p}M∗ be the set of redundant regressors. For simplicity, we refer to M∗ as the active set, meaning that it plays an active role in the regression, and we call M∗c the inactive set. Deﬁne �β ora = ��β ora′ M∗ , �β ora′ M∗ � , where �β ora M∗ = arg min β �����y− ∑ j∈M∗ xjβ j ����� 2 (3.86) and the complement of the oracle estimator is just the remaining coefﬁcients which are identical equal, that is, �β ora M∗c = 0. Notice that the oracle estimator is the infeasible oracle information of M∗. The asymp- totic distribution of the estimator is given by n ��β ora −β ∗ n � M∗ ⇒ ΩM∗ζM∗, (3.87) where ΩM∗ is the p∗ × p∗ submatrix � Ω j, j′� j, j′∈M∗ and ζM∗ is the p∗ ×1 subvector � ζj � j∈M∗. 513.8. Lasso estimation with a structural break Relevant studies with frameworks for lasso estimation robust to the presence of structural breaks in the covariates include Chan et al. (2014), Cheng et al. (2016), Qian and Su (2016) and Smith et al. (2019). For any n−dimensional vector W = (W1,...,Wn)′, deﬁne the empirical norm as ∥W∥n := �1 n ∑n i=1W 2 i �1/2. f(α,τ)(x,q) := x′β +x′δ1 � q < τ � (3.88) f0(x,q) := x′β0 +x′δ01 � q < τ0 � (3.89) ˆf(α,τ)(x,q) := x′ ˆβ +x′ ˆδ1 � q < ˆτ � (3.90) Then, we deﬁne the prediction risk as below �� ˆf − f0 �� n := � 1 n n ∑ i=1 � ˆf (Xi,Qi)− f0(Xi,Qi) �2�1/2 . (3.91) Deﬁne with α0 = � β ′ 0,δ ′ 0 �′. Then, we can rewrite the model as below Yi = Xi(τ0)′α0 +Ui, for i = 1,...,n. Let y ≡ (Y1,...,Yn)′. Then, for any ﬁxed τ ∈ T we consider the residual sum of squares given by Sn(α,τ) = 1 n n ∑ i=1 � Yi −X′ i β −X′ i 1{Qi < τ} �2 ≡ ∥y−X(τ)α∥2 n . (3.92) Remark 13. The examples discussed in the previous sections are concerned with the use of the shrinkage approach under the assumption of sparsity in high dimensional time series regression model for estima- tion purposes (see, also Basu and Michailidis (2015) and García-Donato and Paulo (2022)). Consider that we have p−variate time series then we can quantify the stability of the process based on the largest eigenvalue of the transition matrix. Since the largest eigenvalue is bounded, then a condition to roll out the cross-sectional and serial correlation on the data is ensured. The literature on high-dimensional time series modelling allows for dominant cross-sectional autocorrelations to be accounted for, using com- mon factors. In particular, Cho et al. (2023) propose the factor-adjusted VAR model, in which case the time series is decomposed into two latent components. A relevant framework for robust estimation of high-dimensional VAR Models is given by Wang and Tsay (2023). The frameworks mentioned in Sec- tion 3.8. are suitable for statistical estimation in high dimensional regression models under the presence of structural breaks4. One may be interested to develop an econometric framework for structural break detection within these data-dependent model speciﬁcations such as as the piecewise stationary factor- adjusted VAR models (see, Breitung and Eickmeier (2011)). An alternative algorithmic procedure5 for structural change detection in high dimensional time series models is proposed by Li et al. (2021). 4Dr. Haeran Cho (University of Bristol) gave a seminar titled: "High-dimensional time series segmentation via factor- adjusted VAR modelling", at the S3RI Departmental Seminar Series, University of Southampton on the 24th of March 2022. 5Professor Degui Li (University of York) gave a seminar titled: "Detection of Multiple Structural Breaks in Large Covari- ance Matrices", at the S3RI Departmental Seminar Series, University of Southampton on the 2nd of December 2021. 523.9. Classical Shrinkage Type Estimation Approach In this section, we present the Score function approach proposed in the framework of Sen and Saleh (1987), although the particular methodology doesn’t correspond to a high-dimensional environment, it has many possible applications such as a "Shrinkage type Structural Break Estimation in High-Dimensional Predictive Regressions" or "Testing for Predictability in High Dimensional Environments via a Shrinkage type Structural Break Estimator". Consider the score function given by ψ = {ψ(x),x ∈ R} required for the deﬁnition of M-estimators. Moreover, suppose that ψ(x) = � ψ1(x) + ψ2(x) � , where ψ1(.) and ψ2(.) are both non-decreasing and skew-symmetric such that, ψj(x) +ψj(−x) = 0,∀ x ∈ R, j = 1,2. Denote with AT = (a1,..,aT) and for every b ∈ Rp we deﬁne the following p−dimensional functional with T elements in each dimension MT (b) = � MT1(b),...,MT p(b) �′ = T ∑ t=1 atψ � yt −a′ tb � , b ∈ Rp. (3.93) Then, under the assumption of stationarity then, the RME for ˆβ 1T of the model parameter β 1 is a solution to the following equation MT(1) � b1,0 � = 0. (3.94) We introduce a suitable M−test statistic for testing the null hypothesis H0 : β 2 = 0, ˆMT(2) = MT(2) � ˆβ 1,0 � , (3.95) where ˆβ 1T is the restrained M-estimator (RME) of β 1. Notice that under stationarity, it can be proved that the restrained M-estimator (RME) generally performs better than the unrestrained M-estimator (URE) especially when β 2 is close to 0 (see, Sen and Saleh (1987)). Furthermore, denote with S 2 T = T −1 T ∑ t=1 ψ2 � yt − ˆβ ′ 1Tx(j)t−1 � , xt−1 = � x(1)t−1,x(2)t−1 � , (3.96) Cii. j = Cii −Ci jC−1 j j C ji, for i ̸= j = {1,2}. (3.97) Therefore, an appropriate aligned M−test is given by LT = S−2 T � ˆMT(2)C22.1 ˆMT(2) � . (3.98) Under the null hypothesis, H0, LT has asymptotically the chi-square distribution with p2 degrees of freedom. Therefore, based on the signiﬁcance level α such that 0 < α < 1, the preliminary test under the null, H0, is rejected based on the critical values. 53Considering for example one is interested to the conditional mean speciﬁcation of a predictive regression model, focusing on the OLS against the IVX estimation approaches based on the full set of regressors in the model. The, the corresponding expressions are: S 2 T = T −1 T ∑ t=1 ψ2 � yt − ˆβ ′ 1T,olsx(j)t−1 � , xt−1 = � x(1)t−1,x(2)t−1 � , j ∈ {1,2}, (3.99) and � S 2 T = T −1 T ∑ t=1 ψ2 � yt − ˆβ ′ 1T,ivxx(j)t−1 � , xt−1 = � x(1)t−1,x(2)t−1 � , j ∈ {1,2}. (3.100) In other words, the authors of the particular framework focus on preliminary test M-estimation (PTME) formulation, thus the ˆβ PT 1n is chosen as the RME or UME, according as this preliminary test leads to the acceptance or rejection of the null hypothesis H0. Moreover, the shrinkage M−estimator (SME), based on the usual James-Stein rule incorporates the same test statistic in a smoother manner. When β 2 is very close to 0, generally both the PTME and SME perform better than the UME, but the RME may still be better than either of them. On the other hand, for β 2 away from 0, the RME may perform rather poorly, while both the PTME and SME are robust. This relative picture on the performance characteristics of all four versions of M-estimators can be best studies within an asymptotic framework. One can consider the notion of asymptotic distributional risk (ADR) as well as the asymptotic risk efﬁciency (ARE) for the various versions of the M-estimators (see, Sen and Saleh (1987)). 3.9.1. Limiting Distributional Risk In the multivariate location model Sen and Saleh (1985) have pointed out that shrinkage estimation works out well only in a shrinking neighbourhood of the pivot. Since for β 2 the pivot is taken as 0, we consider a shrinking neighbourhood of 0 and we consider a sequence {KT} of alternatives, KT : β 2 = β 2(T) = T −1/2ξ, ξ = � ξp1+1,...,ξp �′ ∈ Rp2, (3.101) which implies that the null hypothesis H0 reduces to H0 : ξ = 0. Using a suitable estimator β ⋆ 1T of β 1, we denote by G⋆(u) = lim T→∞ P � T −1/2� β ⋆ 1T −β 1 � ≤ u ��KT � , u ∈ Rp1, where we assume that u is nondegenerate. Then, with a quadratic error loss such that T � β ⋆ 1T − β 1 �′W � β ⋆ 1T − β 1 � , for a suitable probability distribution function W, the ADR of β ⋆ 1T is deﬁned as below R � β ⋆ 1T;W � = trace � W � Rp ... � Rp uu′dG⋆(u) � ≡ trace{WV ⋆}, (3.102) where V ⋆ is the dispersion matrix for the asymptotic distribution of G⋆(.). 543.9.2. Asymptotic representation of estimators We follow the framework presented in Mukherjee (1998). Notation The i−th row and the j−th column of any matrix S are denoted by si• and s•j, respectively. Then if S has p columns, Ds denotes the maximal diagonal � ∥s•1∥,..., ��s•p ��� , where ��s•j �� is the usual Euclidean norm of the j−th column of S. Moreover, the usual partitions, Sii. j stands for the matrix Sii −Si jSj jS ji. Then, we deﬁne the following matrices which appear as the scaling factors in the asymp- totic distributions of the different estimators. We denote with Ax = n−dDx, Bx = ndDx, Ax1 = n−dDx1. Moreover, we have that lim n→∞n−1X′X exists and equals C, with C being positive deﬁnite. Consequently, Rn := D−1 x X′XD−1 x p→ R := D−1 c CD−1 c (3.103) Denote with Σn to denote the dispersion matrix of ε. Then, the dispersion of B−1 x X′ΣnXB−1 x and it follows that the limit of En exists and equals E, say. As in Sen and Saleh (1987), the shrinkage least-squares estimator (SLSE) ˆβs can be motivated as a smoothed version of a preliminary test estimator. Thus, for each c > 0, this is deﬁned as below ˆβs = ˆβ1 + � 1− κ LT �� ˜β1 − ˆβ1 � ,c > 0. (3.104) For large values of LT, ˆβs yields to ˜β1, whereas it performs quite differently from ˆβpt form small values of LT. Deﬁne the dispersion matrix H := � 0 ... R22.1 � R−1ER−1� 0 ... R22.1 � (3.105) Then, it can be proved that the second term converges to R22.1Dc2η. Therefore, we deﬁne with Hn := � 0 ... Rn22.1 � R−1 n EnR−1 n � 0 ... Rn22.1 � (3.106) implying that Hn converging to H and ξ := Dc2η ∈ Rp2. Then, Ln is deﬁned as Ln := ���H−1/2 n B−1 x2 X′ 2 � Y −X1 ˆβ 1 ���� 2 (3.107) which converges to a noncentral chi-square random variable with p2 degrees of freedom and noncentrality parameter δ := ξ ′L−1 22 ξ. Therefore, under the null hypothesis η = 0 or equivalently, ξ = 0, the test statistic Ln converges to a chi-square random variable with p2 degrees of freedom. Deﬁne the projection matrix Px1 = X1(X′ 1X1)X′ 1. Let Dn := R−1 n and D := R−1. Then, the covariance matrices of the random vectors DnUn and DU are denoted by Ln and L respectively. 553.10. Lasso Inference for High Dimensional Time Series under NED Following the framework proposed by Adamek et al. (2023), consider the linear time series regression yt = x′ tβ 0 +ut, t = 1,...,T, (3.108) where xt = (x1,t,...,xN,t)′ is an (N × 1) vector of explanatory variables and we assume that we have a high-dimensional time series model where N can be larger than T. Assumption 4. Let zt = (x′ t,ut)′ and let there exists some constants ¯m > m > 2 and d ≥ max � 1, ¯m m−1 ¯m−2 � , E � zt � = 0, E � xtut � = 0 and max 1≤j≤N+1,1≤t≤T E|z j,t|2 ¯m ≤ C. (3.109) Assumption 5. Let sT,t denote a k(T)−dimensional triangular array that is α−mixing of size d � 1 m − 1 ¯m �, with σ −ﬁeld F s t := σ � sT,t,sT,t−1,... � (3.110) such that zt is F s t −measurable. The process � z j,t � is L2m−near-epoch dependent (NED) of size −d on sT,t with positive bounded NED constants, uniformly over j ∈ {1,...,N +1}. Remark 14. The ﬁrst assumption above ensures that the error terms are contemporaneously uncorrelated with each of the regressors, and that the process has ﬁnite and constant unconditional moments. The second assumption above implies that sT,t is an underlying shock process driving the regressors and errors in zt, where we assume zt to depend almost nearly entirely on the "near epoch" of sT,t. In other words, the near epoch dependence of the vector zt can be interpreted as zt being "approximately" mixing, in the sense that it can be well-approximated by a mixing process. The NED framework allows for general forms of dependence and that are often encountered in econometrics applications including, but not limited to, strong mixing processes, linear processes including ARMA models, various types of stochastic volatility and GARCH models as well as nonlinear processes (see, Davidson (2002)). The framework proposed by Adamek et al. (2023) is the ﬁrst to utilize the NED framework for establish- ing uniformly valid high-dimensional inference. For instance, Wong et al. (2020) consider time series models with β−mixing errors, which has the advantage of allowing for general forms of dynamic mis- speciﬁcation resulting in serially correlated error terms, but, rules out several relevant data generating processes, and is in addition typically difﬁcult to verify. On the other hand, Masini et al. (2022) use an m.d.s assumption on the innovations in combination with sub-Weibull tails and a mixing assumption on the conditional covariance matrix. However, the particular m.d.s assumption does not allow for dynamic misspeciﬁcation of the full model. Importantly, the NED assumption on ut does allow for misspeciﬁed models as well, in which case we view β 0 as the coefﬁcients of the pseudo-true model when restricting the class of models to those linear in xt. 56Therefore, this econometric intuition allows to view the high-dimensional time series model of Adamek et al. (2023) as simply the linear projection of yt on all the variables in xt, with β 0 in that case representing the best linear projection coefﬁcients, which implies that E[ut] = 0 and E[utxj,t] = 0. On the other hand, ut is not likely to be an m.d.s. Thus, allowing for misspeciﬁed dynamics is crucial for developing the theory for the nodewise regressions underlying the desparsiﬁed lasso. The NED-order m and sequence of size −d play a key role in the development of the limit results in which these parameters enter as the corresponding asymptotic rates of related quantities. Obviously, there is a trade-off between the thickness of the tails allowed for and the moment of dependence - which is measures via the mixing rate. Example 13 (ARCL Model with GARCH errors, see Adamek et al. (2023)). Consider the autoregressive distributed lag (ARDL) model with GARCH errors such that yt = p ∑ i=1 ρiyt−i + q ∑ j=0 θ ′ jwt−j +ut = x′ tβ 0 +ut (3.111) ut = √ htεt, εt ∼ N (0,1), (3.112) ht = β0 +β1ht−1 +β2u2 t−1, (3.113) where the roots of the lag polynomial ρ(z) = 1 −∑ j = 1pρ jz j are outside the unit circle. Then, ut is a strictly stationary geometrically β−mixing process. Moreover, we assume that the vector of exogenous variables wt is stationary and geometrically β−mixing as well with 2 ¯m ﬁnite moments. Given the invertability of the lag polynomial, we may then write yt = ρ−1(L)vt, vt = q ∑ j=0 θ′ jwt−j +ut, (3.114) and the inverse lag polynomial ρ−1(z) has geometrically decaying coefﬁcients. Then, it follows directly that yt is NED on vt, where vt is strong mixing of size −∞ as its components are geometrically β−mixing, and the sum inherits the mixing properties. Example 14 (Equation-by-Equation VAR, see Adamek et al. (2023)). Consider the VAR model below yt = p ∑ j=1 Φjyt−j +ut, (3.115) where yt is a K × 1 vector of dependent variables and E|ut|2 ¯m ≤ C and the K × K matrices Φi satisfy appropriate stationarity and summability conditions. The equivalent equation-by-equation representation yk,t = p ∑ j=1 � Φk,1, j,...,Φk,K, j � yt−j +uk,t = � y′ t−1,...,y′ t−p � β k +uk,t, k ∈ {1,...,K}. (3.116) Therefore, assuming a well-speciﬁed model with E � ut|yt−1,...,yt−p � = 0, then the NED conditions are then satisﬁed trivially. 57Notice that the above examples, provide cases in which although the classical martingale difference sequence framework fails to apply, the more general NED framework extends the applicability of certain time series processes. Example 15 (Misspeciﬁed AR Model, see Adamek et al. (2023)). Consider an autoregressive AR model of order 2 as yt = ρ1yt−1 +ρ2yt−2 +vt, vt ∼ N (0,1). (3.117) where the roots of the characteristic polynomial (1 −ρ1L−ρ2L2) are outside the unit circle. Moreover, deﬁne the misspeciﬁed model yt = ˜ρyt−1 +ut, where ˜ρ = argmin ρ E � (yt −ρyt−1)2� = E � ytyt−1 � E � y2 t−1 � = ρ1 1−ρ2 (3.118) and ut is autocorrelated. Therefore, an m.d.s assumption would be inappropriate in this case, since E � ut|σ {yt−1,yt−2,....} � = E � yt = ˜ρyt−1 +ut|σ {yt−1,yt−2,....} � = − ρ1ρ2 1−ρ2 yt−1 +ρ2yt−2 ̸= 0. (3.119) On the other hand, it can be shown that � yt−1,ut �′ satisﬁes the NED-condition by considering the moving average representation of yt and thus by extension, of ut = yt − ˜ρyt−1. In addition, since the coefﬁcients are geometrically decaying, ut is thus clearly NED on vt and therefore the NED-condition is satisﬁed. Next, we focus on the related assumptions/conditions on the sparsity properties of the parameter vector. Speciﬁcally, the key condition to apply the lasso successfully is that the parameter vector β 0 is (at least approximately) sparse. This is given by the following assumption. Assumption 6. For some 0 ≤ r ≤ 1 and sparsity level sr, deﬁne the N−dimensional sparse compact parameter space such that BN(r,sr) := � β ∈ RN : ∥β∥r r ≤ sr,∥β∥∞ ≤ C,∃ C < ∞ � , (3.120) and assume that β 0 ∈ BN(r,sr) Example 16 (Inﬁnite Order AR,see Adamek et al. (2023)). Consider an inﬁnite order autoregressive model as below yt = ∞ ∑ j=1 ρ jyt−j +εt, (3.121) where εt is a stationary m.d.s with sufﬁcient moments existing, and the lag polynomial � 1−∑∞ j=1 ρ jLj� is invertible and satisﬁes the summability condition ∑∞ j=1 jα|ρ j| < ∞ for some α ≥ 0. 583.10.1. Inference on low-dimensional parameters In this section we establish the uniform asymptotic normality of the desparsiﬁed lasso focusing on low- dimensional parameters of interest (see, Adamek et al. (2023)). Speciﬁcally, we consider testing P joint hypotheses of the form RNβ 0 = q, via a Wald statistic, where RN is an appropriate P ×N matrix whose non-zero columns are indexed by the set H := � j : P ∑ p=1 |rN,p, j| > 0 � (3.122) with cardinality h := |H|. Notice that we can allow for h to increase in N (and therefore T). Focusing on inference on ﬁnite set of parameters such that we can apply a standard central limit theorem. Therefore, given our time series setting, the long-run covariance matrix is ΩN,T = E � 1 T � T ∑ t=1 wt �� T ∑ t=1 w′ t �� , (3.123) where wt = � v1,tut,...,vN,tut �′, enters the asymptotic distribution. Then, ΩN,T can equivalently be written ΩN,T ≡ Ξ(0)+ T−1 ∑ ℓ=1 � Ξ(ℓ)+Ξ′(ℓ) � , (3.124) Ξ(ℓ) ≡ 1 T T ∑ t=ℓ+1 E � wtw′ t−ℓ � . (3.125) In order to estimate the asymptotic variance Ψ, we suggest to estimate ΩN,T with the long-run variance kernel estimator such that Ω ≡ �Ξ(0)+ QT−1 ∑ ℓ=1 K � ℓ QT �� Ξ(ℓ)+Ξ′(ℓ) � , (3.126) Ξ(ℓ) = 1 T −ℓ T ∑ t=ℓ+1 ˆwt ˆw′ t (3.127) with ˆwj,t = ˆvj,t ˆut, the kernel K(.) can be taken as the Bartlett Kernel K( ℓ QT ) = � 1− ℓ QT � . 3.10.2. Error Bound and the Consistency for the Lasso In this section, we consider a new error bound for the lasso in a high-dimensional time series model. Thus, the Lasso estimator of the parameter vector β 0 in the model (see, Adamek et al. (2023)) ˆβ := argmin β∈RN � 1 T ∥y−Xβ∥2 2 +2λ ∥β∥1 � . (3.128) 59where y = (y1,...,yT)′ is the T × 1 response vector, X = � x1,....,xT �′ is the (T × N) design matrix and λ > 0 a tuning parameter. In other words, the above optimization problem adds a penalty term to the least squares objective to penalize parameters that are different from zero. Thus, when deriving this error bound, we typically require that λ is chosen sufﬁciently large to exceed the empirical process such that max j �� 1 T ∑T t=1xj,tut ��, with high probability. Moreover, we deﬁne the following set such that ET(z) := � max j≤N,ℓ≤T ����� ℓ ∑ t=1 utxj,t ≤ z ����� � (3.129) and establish the conditions under which P � ET � Tλ 4 �� → 1. 3.10.3. Uniformly Valid Inference via the Disparsiﬁed Lasso Following the framework of Adamek et al. (2023), we consider an application of the disparsiﬁed Lasso. Example 17 (Sparse Factor Models). Consider the factor model yt = β 0′xt +ut, ut ∼ N (0,1) (3.130) xt = Λf t +vt, vt ∼ N (0,Σv), f t ∼ (0,Σf ). (3.131) Moreover, we assume that Σ = ΛΣf Λ′ +Σv. Example 18 (Sparse VAR(1)). Consider a stationary VAR(1) model for zt = (yt,x′ t)′ zt = Φzt−1 +ut, E � utu′ t � := Ω, E � utu′ t � = 0, ∀ ℓ ̸= 0. (3.132) Then, the regression yt = φ1zt−1 +u1,t, where φ j is the j−th row of Φ, i.e., the ﬁrst line of the VAR. Example 19. Consider the population nodewise regressions deﬁned by the linear projections below xj,t = x′ −j,tγ0 j +vj,t, γ0 j := argmin γ E � 1 T T ∑ t=1 � xj,t −x′ −j,tγ �2 � , j = 1,....,N. (3.133) with τ2 j := 1 T ∑T t=1E � v2 j,t � . Speciﬁcally, if we let ΦφI, with |φ| < 1, and let Ω have a Toeplitz structure, which implies that the elements of the matrix are deﬁned such that ωi, j = ρ|i−j|,|ρ| < 1. Therefore, the nodewise regression parameter γ0 j is only weakly sparse, in that it contains no zeroes, but its entries follow a geometrically decaying pattern6, meaning that max j ���γ0 j ��� r r ≤ C. 6Notice that in high dimensional settings relevant conditions that capture features such as sub-Gaussianity, heavy tailed observations with slowy decaying temporal dependence are essential (see, Baek et al. (2021) and Guillaumin et al. (2022). Dr. Adam Sykulski gave a seminar with title: "The Debiased Spatial Whittle Likelihood", at the S3RI Departmental Seminar Series at the University of Southampton on the 5th of May 2022. 604. High Dimensional Feature Selection Methods High dimensional statistical problems address the challenge of conducting robust estimation and in- ference when the number of unknown parameters, p, is much larger than the number of observations, n. The literature on high-dimensional feature selection methods include the conditional sure inde- pendence screening7 approach as discussed by Fan and Lv (2008), Wang (2009), Ke et al. (2014) and Barut et al. (2016) as well as He et al. (2013) and Kong et al. (2019) in the case of a conditional quan- tile functional form is employed. The covariate screening approach as a tool for variable selection works well in ultra high-dimensional linear regression models with stationary covariates, without re- quiring distributional restrictions. Statistical estimation methodologies in high-dimensional settings in- clude Belloni and Chernozhukov (2011), Belloni et al. (2014), Wang et al. (2012), Chernozhukov et al. (2015), Fan et al. (2017) as well as Sun et al. (2015) and Wang and Leng (2016) for the cases of projec- tion screening. A fundamental research question remains in most cases when considering modelling a high-dimensional vector of regressors is determining the statistical signiﬁcance of either individual covariates or the model selection mechanism. Lockhart et al. (2014) propose a test for detemining statistical signiﬁcance with the Lasso, while Chudik et al. (2018) propose a methodology for selecting covariates in a high-dimensional environment using the multiple testing approach. Furthermore, Battey et al. (2018) considers hypothesis testing and parameter estimation in the context of the divide-and-conquer algorithm. In particular, in a uniﬁed likelihood-based framework, we propose new test statistics and point estimators obtained by aggregating various statistics from k subsamples of size n/k, where n is the sample size. Speciﬁcally, in low dimensional and sparse high dimensional settings, Battey et al. (2018) examine how large k can be, as n grows large, such as the loss of efﬁciency due to the divide-and-conquer algorithm is negligible. In other words, the resulting estimators have the same inferential efﬁciencies and estimation rates as an oracle with access to the full sample. While hypothesis testing in a low dimensional context is straightforward, in the sparse high dimensional setting, nuisance parameters introduce a nonnegligible bias, causing classical low dimensional theory to break down. Therefore, in their high dimensional Wald construction, the phenomenon is remedied through debiasing of the estimator, which gives rise to a test statistic with tractable limiting distribution. Thus, they ﬁnd that the theoretical upper bound on the number of subsamples guaranteeing the same inferential or estimation efﬁciency as the whole-sample procedure is k = o((slogd)−1√n) in the linear model, where s is the sparsity of the parameter vector. Lemma 10 (Battey et al. (2018)). Assume that Σ = E � XiX⊤ i � satisﬁes Cmin < λmin(Σ) ≤ λmin(Σ) ≤Cmax as well as ���Σ−1/2X1 ��� ψ2 = κ, then it holds that P � max j=1,...,k ���M(j)�Σ(j) −I ��� max ≤ α � logd n � ≥ 1−2kd−c2, c2 = α2Cmin 24e2κ4Cmax −2. (4.1) 7Note that the covariate screening approach is widely used in the applied statistics literature, see Aschard et al. (2017). 61On the other hand, in time series regression models incorporating high dimensionality features requires to develop further tools such as Gaussian approximations (e.g., see Chernozhukov et al. (2014) and Zhang and Wu (2017)) and consistent model selection techniques robust to the presence of heteroscedas- ticity (see, Halunga et al. (2017)) and nonstationarity. In particular in the time series econometrics lit- erature many open problems remain such as robust model selection methodologies for ultra-high di- mensional environments with heterogenous data. We return back to Vector Autoregression Processes, although not necessarily a high-dimensional VAR(p) process. In practise, we consider the case where the dimension of the time-series vector is less than the sample size n. Formally, a p−dimensional vector- valued stationary time series Xt = (X1t,...,Xpt), with t = 1,...,n, can be modelled using a VAR represen- tation of lag d with serially uncorrelated Gaussian errors, which takes the following form Xt = A1Xt−1 +...+AdXt−d +et, et ∼ N (0,Σe) (4.2) where A1,...,Ad are (p x p) matrices and et is a p-dimensional vector of possibly correlated innovation shocks. Therefore, the main objective in VAR models is to estimate the transition matrices A1,...,Ad, together with the order of the model d, based on time series realizations (X0,X1,...,Xn). Then, the structure of the transition matrices provides insights into the complex temporal relationships amongst the p time series and the particular representation provides a way to apply forecasting techniques (see, Michailidis and d’Alché Buc (2013), Basu and Michailidis (2015) and Basu and Rao (2021)). 4.1. Ultra-high dimensionality under dependence According to Yousuf (2018), it is possible to achieve the sure screening property in the ultrahigh di- mensional setting with dependent errors and covariates. However, in order to do that we need to make stronger assumptions on the moments of both the error and covariate processes. Speciﬁcally, if the error and covariate processes are assumed to follow a stronger moment condition, such as, ∆0,q(ε) < ∞ and Φ0,q(x) < ∞ for arbitrary q > 0, we can then achieve a much larger range of pn which will cover the ultra-high dimensional case. Speciﬁcally, we need a condition that implies that the tails of the covariate and error processes are exponentially tight. A wide range of processes satisfy the above condition. Suppose that εi is a linear process such that εi = ∞ ∑ j=0 f jei−j, ei ∼ i.i.d, with ∑∞ ℓ=0| fℓ| < ∞ then it holds that ∆0,q(ε)(εℓ) = ∥e0 −e∗ 0∥q ∞ ∑ ℓ=0 | fℓ| (4.3) Thus, if we assume that e0 is sub-Gaussian, then ˜αε = 1 2 since ∥e0∥q = Op(√q) then, when ei is sub- exponential then it holds that ˜αε = 1. 62More generally, for ei := ∑∞ j=0 f jep i−j and thus if ei is sub-exponential, we have that ˜αε = p, where p is the number of components of the high-dimensional vector. The high dimensionality as well as the non- parametric aspect can slow down the rate of convergence and thus especially the variance of the estimator can have slower convergence to the corresponding variance of the process. Thus, for any ﬁxed q, we are not placing additional assumptions on the temporal decay rate of the co- variate error processes besides requiring ∆0,q(ε),Φ0,q(ε) < ∞. On the other hand, the ultrahigh dimen- sional setting requires geometrically decaying strong mixing coefﬁcients, in addition to requiring sub- exponential tails for the response. Consider for example the case that εi = ∑∞ j=0 f jei−j, geometrically decaying strong mixing coefﬁcients would require the coefﬁcients, f j, to decay geometrically. Theorem 12 (Theorem 2 in Yousuf (2018)). Deﬁne with α = 2 1+4 ˜αx , then it holds that for any c2 > 0 P � max j≤pn �� ˆρ j −ρ j �� > c2n−k � ≤ Op  snpnexp � −n1/2−κ ν2x sn � ˜α +Op  pnexp � −n1/2−κ νxνε � ˜α′  (4.4) 4.1.1. SIS with dependent observations Sure Independence Screening, is a method of variable screening based on ranking the magnitudes of the pn marginal regression estimates. Under appropriate conditions, this simple procedure is shown to possesses the sure screening property. Deﬁne with ˆρ = ( ˆρ1,..., ˆρpn), where ˆρ j = � n ∑ t=1 X2 t j �−1� n ∑ t=1 Xt jYt � . (4.5) Therefore, ˆρ j is the OLS estimate of the linear projection of Yt onto Xt j. Let M∗ = � 1 ≤ i ≤ pn : βi ̸= 0 � (4.6) and let |M∗| = sn << n be the size of the true sparse model. Moreover, we sort the elements of ˆρ by their magnitudes. Thus, for any given γn, deﬁne a sub-model such that ˆ Mγn = � 1 ≤ i ≤ pn : | ˆρi| ≥ γn � (4.7) and let �� ˆ Mγn �� = dn be the size of the selected model. Furthermore, notice that the screening property states that for an appropriate choice of γn, we have that P � M∗ ⊂ ˆ Mγn � → 1. 634.1.2. Experimental design examples Experimental Design I: Uncorrelated Features Consider the model for the covariate process we have xt = A1xt−1 +ηt (4.8) where A1 = diag{γ} and γ ∼ Unif[0.4,0.6]. Moreover, we set with ηt ∼ N (0,Ση). In particular, when we are dealing with uncorrelated predictors we set Ση = Ipn which represents the no correlation case. Experimental Design II: Correlated Features One can compare the performance of SIS and GLSS in the case of correlated predictors. The covariate process is generated with A1 = diag � 0.4|i−j|+1� i, j≤pn such that ηt ∼ N (0,Ση) and ηt ∼ t5(0,V) , with Ση = � 0.3|i−j|� (4.9) Proof. Recall that it holds that ∑pn k=1 1{|βk| > 0} = sn. Hence, we obtain the following expression P � |S1 −E(S1)| > c2n−κ 2 � ≤ ∑ k∈M∗ P ����� Xt j (Xtkβk) n −βkE � Xt jXtk ����� > c2n−κ 2sn � (4.10) Moreover, it holds that ��Xi j �� r ≤ ∆0,r(X j) ≤ Φ0,r(x). Using this we compute the cumulative functional dependence measure of XtkXt j such that +∞ ∑ t=m ��Xt jXtk −X∗ t jX∗ tk �� r/2 ≤ ∞ ∑ t=m ���Xt j �� r ∥Xtk −X∗ tk∥r ��Xt j −X∗ t j �� r � ≤ 2Φ0,r(x)Φm,r(x) = Op � m−αx� . Therefore, we obtain that sup m (m+1)αx ∞ ∑ t=m ��Xt j �� r ∥Xtk −X∗ tk∥r/2 ≤ 2K2 x,r. (4.11) Using the above results we get the following probability bound P � |S1 −E[S1]| > c2n−κ 2 � ≤ Csn × � nωKr x,r (n/sn)r/2−rk/2 +exp � −n1−2κ s2nK4x,r �� (4.12) Hence, by our choice of γn, we obtain that P � M∗ ⊂ ˆ Mγn � > P(An). In other words, this allow us to obtain a probability bound with respect to the spectral radius of the corresponding covariance matrix pn ∑ k=1 ρ2 k = Op(λmax(Σ)). (4.13) 64Then, we extend the above result to the corresponding set deﬁned by Bn such that Bn := � max k≤pn | ˆρk −ρk| ≤ c4n−κ � (4.14) Therefore, we obtain a measurability condition for the number of � k : | ˆρk| > 2c4n−4� cannot exceed the number of � k : |ρk| > c4n−k� which is bounded by Op � n2kλmax(Σ) � . Thus, by choosing c4 = c3/2 we can obtain the correct probability bound such that P ��� ˆ Mγn �� < Op � n2kλmax(Σ) �� > P(Bn) (4.15) We are interested to bound the following probability event A1 := P ��� ˆγi,k −γi,k �� > cn−k ℓn � (4.16) where a probability bound for the event A1, can be obtained as below: A1 ≤ P ������ 1 n n−|i| ∑ t=1 εt,kεt+|i|,k −E � 1 n n−|i| ∑ t=1 εt,kεt+|i|,k ������ � +P ������E � 1 n n−|i| ∑ t=1 εt,kεt+|i|,k � −γi,k ����� > cn−k/4ℓn � +P ������ 1 n n−|i| ∑ t=1 εt+|i|,kXt+|i|,k � ∑n j=1 Xjkε j,k/n ∑n j=1X2 jk/n ������ > cn−k/4ℓn � +P   ������ 1 n n−|i| ∑ t=1 XtkXt+|i|,k � ∑n j=1 Xjkε j,k/n ∑n j=1X2 jk/n �2������ > cn−k/4ℓn   Furthermore, we can determine the bias of the term as below �����E �n−|i| ∑ t=1 εt,kεt+|i|,k n � −γi,k ����� ≤ iγi,k n . (4.17) Moreover, it holds that P ������ 1 n n−|i| ∑ t=1 εt,kXt+|i|,k > M ����� � ≤ P ������ 1 n n−|i| ∑ t=1 εt,kXt+|i|,k −E � 1 n n−|i| ∑ t=1 εt,kXt+|i|,k ������ > M − �����E � 1 n n−|i| ∑ t=1 εt,kXt+|i|,k ������ � M > max k≤pn max i≤ℓn 2 ��E � εt,kXt+|i|,k ���+ε, for some ε > 0. (4.18) 654.2. Uniform Asymptotic Inference and Model Selection This is another important topic which we cannot cover here extensively (see, Tibshirani et al. (2018)). 4.2.1. Uniform-in-Submodel Bounds Following the framework proposed by Kuchibhotla et al. (2021), suppose that M = {M1,...,ML} denotes a collection of submodels, where Mj represents a subset of covariates for 1 ≤ j ≤ L. Also, denote with �βMj to represent the least-squares estimator for the linear regression of the response on the covariates of the model Mj. Then, by simultaneous consistency, we mean the existence of target vectors � βMj : 1 ≤ j ≤ L � such that the following result holds sup M∈M ����βM − �β ��� = op(1), as n → ∞, for some ∥.∥. (4.19) More speciﬁcally, if � M is a selected model, then one can perform inference of β � M by estimating the distribution of �β � M. According to Kuchibhotla et al. (2021), in practise, even though various model- selection criteria like Cp (AIC), (BIC) and lasso have been recommended for covariate selection in linear regression, developing more general asymptotic results to the asymptotic uniform linear representation in the special case of the least-squares linear regression estimator are still an ongoing research ﬁeld. The framework of variable selection and linear regression is often considered in the context of high- dimensional linear regressions. Although, the procedure uses only a reduced set of variables in the ﬁnal regression, it uses all variables in the proceding covariate selection stage of estimation. Suppose that � M ∈ M is the ﬁnal selected submodel, where M is some ﬁnite and countable collection of models, with �β � M the corresponding least-squares estimator. The estimator �β � M is known as the post- regularization estimator in the high-dimensional statistics literature when � M is obtained from a regular- ized least-squares procedure. A relevant question regarding the asymptotic behaviour of the particular estimator is "What does �β � M estimate consistently?". These considerations are relevant to the literature of variable and model selection, which includes: methodologies for determining the statistical signiﬁcance of predictors in settings with many covariates (see, Chudik et al. (2018)) as well as the uniform inference post-model selection which includes among others the studies of Belloni et al. (2016) and Farrell (2015)). Here we focus on the framework proposed by Kuchibhotla et al. (2021) and present key results. Suppose that for any M ⊂ {1,2,..., p} the ordinary least-squares (OLS) empirical risk (or objective) function is ˆRn (θ;M) := 1 n n ∑ i=1 � Yi −X⊤ i (M)θ �2 , for θ ∈ R|M|. (4.20) Then, by expanding the square function, it is clear that ˆRn (θ;M) = 1 n n ∑ i=1 Y 2 i − 2 n n ∑ i=1 YiX⊤ i (M)θ +θ⊤ � 1 n n ∑ i=1 Xi(M)X⊤ i (M) � θ. (4.21) 66Furthermore, deﬁne the following quantities ˆΣn := 1 n n ∑ i=1 XiX⊤ i ∈ Rp×p, and ˆΓn := 1 n n ∑ i=1 XiYi ∈ Rp×p. (4.22) Then, the least-squares linear regression estimator ˆβn,M is deﬁned as below ˆβn,M = arg min θ∈R|M| ˆRn (θ;M) = arg min θ∈R|M| � θ⊤ ˆΣn(M)θ −2θ⊤ ˆΓn(M) � . (4.23) The notation arg minθ f(θ) denotes the minimized of f(θ). Based on the quadratic expansion of the empirical objective function ˆRn(θ;M), the estimator �βn,M is given by the closed form expression below �βn,M = �ˆΣn(M) �−1 ˆΓn(M), (4.24) assuming nonsingularity of ˆΣn(M). Therefore, it is clear that �βn,M is a smooth (nonlinear) function of two averages ˆΣn(M) and ˆΓn(M). Then, assuming that the random vectors (Xi,Yi) are i.i.d with ﬁnite fourth moments, it follows that ˆΣn(M) and ˆΓn(M) converge in probability to their expectations. Thus, we can deﬁne the expected value in terms of matrices and vectors as below Σn := 1 n n ∑ i=1 E � XiX⊤ i � ∈ Rp×p, and Γn := 1 n n ∑ i=1 E[XiYi] ∈ Rp×p. (4.25) Following the notation above �βn,M = �ˆΣn(M) �−1 ˆΓn(M), and if �ˆΣn − ˆΣn, ˆΓn −Γn � p→ 0 as n → ∞, using a Slutsky-type argument, it follows that ��βn,M −βn,M � p→ 0, as n → ∞, where βn,M is: βn,M := [Σn(M)]−1Γn(M) ≡ arg min θ∈R|M| � θ⊤Σn(M)θ −2θ⊤Γn(M) � . (4.26) Thus, the convergence statement above only concerns a single submodel M and is not uniform over M. By uniform-in-submodel ∥.∥2 −norm consistency of ˆβn,M to βn,M, for M ∈ M (k), we mean that sup M∈M (k) ����βn,M −βn,M ��� = op(1) as n → ∞. (4.27) Thus, converges of �βn,M to βn,M only requires convergence of �Σn(M) to Σn(M) and �Γn(M) to Γn(M). Remark 15. A relevant stream of literature to uniform inference for submodel selection, is the test- ing methodologies which consider the nested property between a comparison of model as a mechanism for speciﬁcation testing. In particular, Hagemann (2012) considers a non-nested speciﬁcation test (see, also MacKinnon (1983)) while Rinaldo et al. (2019) develops a framework for model selection based on sample-splitting for high dimensional assumption lean inference purposes. 674.3. Non-nested Regressions and Variable Selection Speciﬁcation Testing Following the framework proposed by Hagemann (2012), suppose that we observe covariates �� x⊤ i,1,...,x⊤ i,M �⊤ ∈ Rd1+...+dM : i = 1,...,n � that give rise to M ≥ 2 different possible linear regression models for y := (y1,...,yn)⊤ ∈ Rn, such that y = Xmβm +um, m ∈ M := {1,...,M}, (4.28) where Xm := � x1,m,...,xn,m �⊤ ∈ Rn×dm is the design matrix of model m. Then, the matrices X1,....,XM are assumed to be non-nested, which implies that for any two matrices with index m ̸= ℓ ∈ M , no matrix can be obtained by another by a linear transformation. Furthermore, the particular assumption does not rule out the possibility that some of the columns of Xm and Xℓ are identical or that they may be nonlinear transformations of another. In particular, suppose that there is an unobserved design matrix Xm∗ := � x1,m∗,...,xn,m∗�∗ associated with the correct model m∗. Suppose that F := � Xm : m ∈ M ∪{m∗} � has the following properties: Assumption 7. Consider that � yi,(xi,m)⊤ m∈M ∪{m∗} : i ≥ 1 � is a sequence of i.i.d random vectors. We have that E|xi,m|4 < ∞ for all m ∈ M ∪{m∗}, where the number of elements of M (number of covariates) does not depend on n (sample size). For all m ∈ M , the matrices E � xi,mx⊤ i,m � are positive deﬁnite. Next, we impose a condition for the existence of a correct model speciﬁcation. Assumption 8. Model m∗ satisﬁes E � y|F � = Xm∗βm∗. Deﬁne with ui,m∗ := yi −x⊤ i,mβm∗ for all i ≥ 1 and Eu4 i,m∗ < ∞. The proposed J test presumes that for some predetermined m ∈ M , the null hypothesis is formulated such as m = m∗ against the alternative hypothesis m ̸= m∗ in the presence of non-nested alternatives ℓ ∈ M \{m}. To do this, we artiﬁcially nesting the models via an additional parameter vector αm := � αℓ,m � ℓ∈M \{m} ∈ RM−1 such that y = Xmbm + ∑ ℓ∈M \{m} αℓ,mXℓβℓ +u, (4.29) bm := � 1− ∑ ℓ∈M \{m} αℓ,m � βm. (4.30) Therefore, since the vectors � αℓ,m,βℓ � ℓ∈M \{m} of the nesting model may not be identiﬁed, we can replace the βℓ by the OLS estimates such that ˆβℓ = � X⊤ ℓ Xℓ �−1X⊤ ℓ y. (see, Hagemann (2012)). 68Remark 16. In particular, the minimum J-test proposed by Hagemann (2012) does not require the correct model to be among the considered speciﬁcations and avoids ambiguous test outcomes. More speciﬁcally, the MJ test determines with asymptotically correct size if the correct model is among the speciﬁcations under consideration. Furthermore, if the correct model is present, it is chosen with probability approach- ing one as the sample size becomes large. After redeﬁning the error term appropriately, this yields that y = Xmbm + ∑ ℓ∈M \{m} αℓ,mXℓ ˆβℓ +u, (4.31) Therefore, a Wald test for testing the null hypothesis that αm = 0 is a Jtest for the validity of model m in the presence of the alternatives M \{m}. Thus, to construct the test statistic, let λn,m := n−1/2 � y⊤PℓMmy � ℓ∈M \{m} (4.32) ˆΣn,m := n−1 � y⊤PℓMm ˆΩMmPℓ′y � ℓ,ℓ′∈M ∈M \{m} (4.33) Pm := Xm � X⊤ m Xm �−1 X⊤ m and Mm := In −(Pm)n×n (4.34) are the usual projection matrices and ˆΩn,m is an estimate of the conditional expectation of E � u∗ mu∗⊤ m |F � . The J test statistic for model m is then given by Jn,m := λ ⊤ n,m ˆΣ−1 n,mλn,m (4.35) In other words, the hypothesis that the model m is the true model is rejected for large values of Jn,m. The following asymptotic results holds for obtaining the empirical size and power of the test statistic. Lemma 11 (Hagemann (2012)). Suppose that Assumptions are satisﬁed. Then, it holds that (i). If m∗ ∈ M , then Jn,m∗ ⇒ χ2 M−1. (ii). For every m ∈ M \{m} and every B ∈ R, we have that limn→∞P � Jn,m > B � = 1. Lemma 12 (Hagemann (2012)). Suppose that the conditions of the Theorem hold. If m∗ ∈ M , then plim n→∞ sup x∈R ��P∗� MJ∗ n ≤ x � −P∗� MJ∗ n ≤ x ��� = 0. (4.36) Proof. Let ˆm := argminJ ∗ n . We ﬁrst show that if m∗ ∈ M , then ˆm∗ approximates m∗. Without loss of generality, ﬁx any 0 < ε < 1, then P � 1{∃ m ∈ M \{ ˆm} : Jn,m ≤ B} > ε � ≤ P � ∃ m ∈ M \{ ˆm} : Jn,m ≤ B � +P � ˆm ̸= m∗� , (4.37) which converges to zero as n → ∞. 69In summary, it can be proved that plim n→∞ P∗� J∗ n, ˆm∗ ≤ x � ≡ K(x), (4.38) where K(x) is the distribution function of a χ2 M−1 variable. Remark 17. Notice that although the methodology proposed by Hagemann (2012) considers testing for correct model speciﬁcation it differs from the literature of conditional subvector testing. Speciﬁcally, the subvector testing method considers the roots of the following characteristic polynomial ��� ˆκIp −n−1 �G−1/2 n � ¯Y0,W �′Z �HnZ′�¯Y0,W � �G−1/2 n ��� = 0. (4.39) Remark 18. A different stream of literature considers the classiﬁcation of non-overlapping and over- lapping models and discuss how the relationship between candidate models affects the asymptotic dis- tributions of the test statistics. The particular stream of literature was initiated by Vuong (1989). This approach considers measuring the distance from the model under investigation to the true distribution, which requires to solve the minimization problem infP∈Pd(P,µ). 4.4. Divide and Conquer Variable Selection Algorithm Further algorithmic procedures for model selection purposes in high dimensional linear regression models includes the framework of Meinshausen and Bühlmann (2010) propose a variable selection methodology where the algorithm repeatedly employs a subset selection and apply the backward algorithm. Then, the ﬁnal stage provides all the signiﬁcantly selected covariates with a pre-speciﬁed control rate. Therefore, the particular stability selection procedure is designed to address the issue of choosing the amount of regularization such that a certain familywise type I error rate in multiple testing can be conservatively controlled for ﬁnite sample size. The issue of correlated covariates can affect the behaviour of stability selection for high correlated designs. Speciﬁcally, the stability selection algorithm puts a large emphasis on avoiding false positive selections and, as a consequence, might miss important variables if they are highly correlated with irrelevant variables. Moreover, Battey et al. (2018) (see, also Fan et al. (2015)) consider the so-called "divide and conquer" variable selection algorithm as we brieﬂy explain below. On each subset Dj, we compute the debiased estimator of β ∗ such that �β d = �β d Lasso(Dj)+ 1 nk M(j) � X(j)�⊤� Y (j) −X(j)�β d Lasso(Dj) � , (4.40) where d is used to indicate the debiased version of the estimator, such that M(j) = � m(j) 1 ,...,m(j) d �⊤ and mv is the solution of m(j) v = argmin m m⊤�Σ (j)m s.t ����Σ (j)m−ev ��� ∞ ≤ ϑ1, ����Σ (j)m ��� ≤ ϑ2. (4.41) 70Theorem 13. Suppose that E � ε4 1 � < ∞ and choose ϑ1,ϑ2 and k such that θ1 ≈ � klogd/n,θ2n−1/2 = o(1) and k = o �√n(slogd)−1� . For any v ∈ {1,...,d}, √n k k ∑ j=1 �β d v (Dj)−β ∗ v �Q(j) v ⇒ N (0,σ2), where �Qv = � m(j)⊤ v �Σ (j)m(j) v �1/2 (4.42) The above procedure implies a divide-and-conquer Wald statistic of the following form ¯Sn = √n k k ∑ j=1 �β dv(Dj)−β H v ¯σ � m(j⊤) v �Σ (j)m(j) v �1/2. (4.43) for β ∗ v , where ¯σ is an estimator for σ based on the k subsamples. Consider the desparsiﬁed estimator for sub-sample Dj is given by �β d(Dj) = �β λ(Dj)− �Θ(j)∇ℓ(j) nk � β λ(Dj) � , (4.44) where �Θ(j) is a regularized inverse of the Hessian matrix of second-order derivatives of ℓ(j) nk (β) evaluated at β λ(Dj), denote by �J(j) = ∇2ℓ(j) nk � �β λ(Dj) � (4.45) The proposed approach for estimating �J(j), reduces to the empirical covariance of the design matrix in the case of the linear model. The, the aggregated debiased estimator over the k subsamples is ¯β d := 1 k k ∑ j=1 �β d � Dj � . (4.46) Thus, to approximate the nodewise Lasso requires to approximately invert �J(j) via L1−regularization. The basic idea is to ﬁnd the regularized invert row via a penalized L1−regression, which is the same as regressing the variable Xν on X−ν but expressed in the sample covariance form. In particular for each row ν ∈ {1,...,d}, consider the optimization as below: �κν � Dj � = argmin κ∈Rd−1 � �J(j) νν −2 �J(j) ν,−νκ +κ⊤ �J(j) −ν,−νκ +2λν ∥κ∥1 � , (4.47) where �J(j) ν,−ν denotes the ν−th row of �J(j) without the (ν,ν)−th diagonal element and �J(j) −ν,−ν is the principal submatrix without the ν−th row and ν−th column. Remark 19. Notice that Meinshausen and Bühlmann (2006) established a link between the nodewise regression and the optimal linear prediction of excess asset returns under the assumption that the returns are normally distributed. Relevant studies include Callot et al. (2021) and Caner et al. (2023). 71Introduce the following matrix �C :=       1 −�κ1,2(Dj) ... −�κ1,d(Dj) −�κ2,1(Dj) 1 ... −�κ1,d(Dj) ... ... ... ... −�κd,1(Dj) −�κd,2(Dj) ... 1       . (4.48) Denote with �Ξ(j) = diag � τ1(Dj),...,τd(Dj) � , where �τν � Dj �2 ≡ �J(j) νν − �J(j) ν,−ν�κν(Dj). (4.49) Moreover, the estimation of the matrix �Θ(j) is given by �Θ(j) := � �Ξ(j)�−2 �C(j), (4.50) Therefore, we consider establishing the limit distribution of the following term ¯Sn := √n k k ∑ j=1 β d ν (Dj)−β H ν √Θ∗νν (4.51) for any ν ∈ {1,...,d} under the null hypothesis that H0 : βν = β H ν . Remark 20. Therefore, the above statistical procedure provides the basis for the statistical testing based on divide and conquer. On the other hand, the particular methodology is only implemented for estimation and inference purposes in high dimensional sparse models. In the sparse high dimensional setting, nui- sance parameters introduce a nonnegligible bias, causing classical low dimensional theory to break down. On the other hand, in their high dimensional Wald construction, the phenomenon is remedied through a debiasing of the estimator, which gives rise to a test statistic with tractable limiting distribution. Open Problems An open problem in econometrics and statistics when considering model comparison and selection methodologies remains the aspect of robust statistical inference for nonnested environ- ments. More speciﬁcally, various studies in the literature have considered methodologies for establishing the consistency of the Bayes factor in these settings. A particular interesting solution is the approach of converting a nonnested problem to a nested statistical problem which can provide a pseudo-distance be- tween the base model and the full model. Another approach is to have on overlapping window where the comparisons between the nested models is made (see, Berger and Pericchi (1996), Berger and Mortera (1999)). Moreover, the framework proposed by Vuong (1989) relates the probabilistic model selection approach to the classical nested-hypothesis testing situation. Lastly, in the next section we discuss an application from the statistics literature on sample-splitting to distinguish it from subsampling. Although the particular implementation corresponds to i.i.d data, an extension to dependent data (e.g., time series data with a weak form of dependence) could be an interesting fruitful avenue for future research. 724.5. Sample-Splitting and Variable Importance Algorithm In this section, we follow and discuss the framework proposed by Rinaldo et al. (2019). Speciﬁcally, consider a distribution-free regression framework, where the pair Z = (X,Y) ∈ Rd ×R of d−dimensional covariates and response variables has an unknown distribution belonging to a large non-parametric class of Q. Then, using minimal assumptions (assumption-lean inference) on the regression function x ∈ Rd �→ µ(x) := E � Y|X = x � where µ(x) describes the relationship between the vector of covariates and the expected value of the response variable. We observe the set of data observations Dn = (Z1,...,Zn) where Zi = (Xi,Yi) ∈ Rd+1, where i = 1,...,n and the class Q = Qn, which may depend on the sample size. Then, we apply to the data a procedure wn, which returns both a subset of the covariates and an estimator of the regression function over the selected covariates. Formally, we have that Dn �→ wn (Dn) = � �S, �µ�S � , (4.52) where �S is the selected model, is a random, nonempty subset of {1,...,d} and �µ�S is an estimator of the regression function x ∈ Rd �→ E � Y|X�S = x�S � restricted to �S, where (X,Y) ∼ P independent of Dn and, for a vector x = (x(1),...,x(d)) ∈ Rd, we set x�S = � x( j), j ∈ �S � . The only assumption that is imposed on wn is that the maximum size of the selected model is controlled by the experimenter, that is, 1 ≤ |�S| ≤ k, for a predeﬁned positive integer k ≤ d, where k and d can both increase with the sample size. Therefore, Rinaldo et al. (2019) propose a methodology for deﬁning �S such that it contains any optimal model. In particular, their framework allows for arbitrary procedures including sparse variable selection, and stepwise-forward regression. Thus, the goal of the framework proposed by Rinaldo et al. (2019), is to provide statistical guarantees for various measures of variable importance applied to the covariates in �S, uniformly over the choice of wn and over all the distributions P ∈ Qn. Then conﬁdence sets are considered for four random parameters taking values in R�S, each providing a different assessment of the level of statistical signiﬁcance of the variables in �S from a purely predictive standpoint. All the random parameters under consideration are function of the data generating distribution P, of the sample Dn and its size n, as well as of the mechanism for model selection and the estimation procedure of wn. 4.5.1. The projection parameter β�S Consider a linear estimator of the form x �→ �µ�S(x) = �β ⊤ �S x�S, where �β�S is any estimator of the linear regression coefﬁcients for the selected predictors based on OLS. Then, the linear projection parameter �β�S is deﬁned to be the vector of coefﬁcients of the best linear predictor of Y using X�S, given by �β�S = arg min β∈R�S EX,Y � Y −β ⊤X�S �2 , (4.53) 73where EX,Y denote the expectation with respect to the joint distribution of the random variables (X,Y). In other words, based on the framework proposed by Rinaldo et al. (2019), these projection parameters correspond to the fact that X⊤β�S is the L2 projection of Y into the linear space of all random variables that can be obtained as linear functions of X�S. The projection parameter is well-deﬁned even though the true regression function µ is not linear. Indeed, it is immediate that β�S = Σ−1 �S Q�S, with Q�S := � Q�S, j ∈ �S � (4.54) where Q�S = EX,Y � YX�S( j)|Dn � and Σ�S = E � X�SX⊤ �S |Dn � 4.5.2. The LOCO parameters γ�S and φ�S A commonly used measure of the importance of the selected covariates is β�S, but there are of course other ways to quantify variable signiﬁcance. We consider two parameters of variable importance, which we refer to as Leave Out COvariate Inference-or LOCO-parameters (see, Rinaldo et al. (2019)). The ﬁrst LOCO parameters is γ�S = � γ�S( j) : j ∈ �S � , where γ�S( j) = E ���Y − �β ⊤ �S(j)X�S(j) ��− ��Y − �β ⊤ �S X�S �� ����Dn � . (4.55) In the last expression, �β�S is any estimator of the projection parameter β�S and �S( j) and �β�S(j) are obtained by rerunning the model selection and estimation procedure after removing the j−th predictor. In other words, for each j ∈ �S, �S( j) is a subset of size at most k of {1,...,d} { j}. Notice that the selected model can be different when the j−th covariate is held out from the data, so that the intersection between �S( j) and �S can be smaller than k−1. The interpretation of γ�S( j) is simple: it is the increase in prediction error by not including the j−th predictor in the model. For instance, it is easy to extend the deﬁnition of this parameter by leaving out several variables from �S at once without additional conceptual difﬁculties. Moreover, the parameter γ�S has advantages over the projection parameter β�S and this is because it refers directly to prediction error and as also demonstrate the accuracy of the Normal approximation and the bootstrap is much higher. The second type of LOCO parameters that we consider are the median LOCO parameters, φ�S = � φ�S( j), j ∈ �S � with φ�S( j) = median ���Y − �β ⊤ �S(j)X�S(j) ��− ��Y − �β ⊤ �S X�S �� ����Dn � . (4.56) As with γ�S, we may leave out multiple predictors at the same time. 744.5.3. The prediction parameter ρ�S Another interesting measure of variable importance is an omnibus parameter that measures how well the selected model will predict future observations. To this end, we deﬁne the future prediction error as ρ�S = E ���Y − �β ⊤ �S X�S �� ����Dn � , (4.57) where �β�S is computed based on Dn. In particular, the main idea of the approach proposed by Rinaldo et al. (2019) relies on sample splitting: assuming for notational convenience that the sample size is 2n, we randomly split the data D2n into two halves, D1,n and D2,n. Next, we run the model selection and estimation procedure wn on D1,n, obtaining both �S and �µ�S. We then use the second half of the sample D2,n to construct an estimator �θ�S and a conﬁdence set �C�S for θ�S satisfying the following properties: • Concentration: lim sup n→∞ sup wn∈Wn sup P∈Qn P ����θ�S −θ�S �� > rn � → 0, (4.58) • Coverage Validity: lim inf n→∞ inf wn∈Wn inf P∈QnP � θ�S ∈ �C�S � ≥ 1−α, (4.59) • Accuracy: lim sup n→∞ sup wn∈Wn sup P∈Qn P � ν � �C�S � > εn � → 0, (4.60) where α ∈ (0,1) is a prespeciﬁed level of signiﬁcance, Wn is the set of all the model selection and estimation procedures on samples of size n, rn and εn both vanish as n → ∞ and ν is the volume (Lebsegue measure) of the set. The probability statements above take into account both the randomness in the sample Dn and the randomness associated to splitting it into halves. Remark 21. The property that the coverage of �C�S is guaranteed uniformly over the entire class Qn is known as (asymptotic) honesty. Moreover, the conﬁdence sets are for the random parameters (based on half the data) but the uniform coverage , accuracy and concentration guarantee to hold with respect to the distribution of the entire sample and the randomness associated to the splitting of the full sample. Remark 22. Since we are particularly interested to study suitable variable selection methodologies for time series regression models, notice that usually the subsampling approach in such modelling settings requires to consider the dependence structure of the data. For example, the greater the number of sub- samples, the larger the bias due to the fact that consistent estimation of long-memory parameters requires larger samples. Determining the length of subsamples under the presence of long memory in time series data is another aspect of concern. 754.5.4. Conﬁdence sets for the projection parameters: The bootstrap Following Rinaldo et al. (2019), the conﬁdence set based on the Normal approximation require the eval- uation of both the matrix �Γ�S and the quantile�tα which may be computationally inconvenient. The authors show that the paired bootstrap can be deployed to construct analogous conﬁdence sets, centered at �β�S, without knowledge of �Γ�S. Thus, the bootstrap distribution corresponds to the empirical probability mea- sure associated to the subsample D2,n and conditionally on D1,n and the outcome of the sample splitting procedure. Denote �β ∗ �S the estimator of the projection parameters β�S arising from i.i.d sample of size n drawn from the bootstrap distribution. For a given α ∈ (0,1), let �t∗ α be the smallest positive number P �√n ����β ∗ �S − �β�S ��� ≤�t∗ α ��D2,n � ≥ 1−α. (4.61) Next, let � �t∗ j , j ∈ �S � be such that P �√n ����β ∗ �S ( j)− �β�S( j) ��� ≤�t∗ j , ∀ j ��D2,n � ≥ 1−α. (4.62) By the union bound, each �t∗ j can be chosen to be the largest positive number such that P �√n ����β ∗ �S ( j)− �β�S( j) ��� >�t∗ j , ∀ j ��D2,n � ≤ α k . (4.63) Consider the following two bootstrap conﬁdence sets: �C∗ �S = � β ∈ R�S : ���β − �β�S ��� ≤ �t∗ α √n � , (4.64) �C∗ �S = � β ∈ R�S : ���β( j)− �β�S( j) ��� ≤ �t∗ j √n, ∀ j ∈ �S � . (4.65) It is immediate that �C∗ �S and �C∗ �S are just the bootstrap equivalent of the conﬁdence sets above. Therefore, the case of sparse ﬁtting where k = O(1) implies that the size of the selected model is not allowed to increase with n. The standard central limit theorem shows that √n ��β −β � → N (0,Γ), Γ = Σ−1E �� Y −β ⊤X �2 � Σ−1. (4.66) Note that Γ can be consistently estimated by the sandwich estimator �Γ = �Σ−1A�Σ−1, where A = n−1X⊤RX, Xi j = Xi( j), R is the k ×k diagonal matrix with Rii = � Yi −X⊤ i �β �2 . By Slutsky’s theorem, valid asymp- totic conﬁdence sets can be based on the Normal distribution with �Γ in place of Γ. However, as Rinaldo et al. (2019) points out there is a clear prediction/accuracy trade-off when employing the sample splitting approach for variable selection. In other words, the selected model may be less accurate because only part of the data are used to select the model. Thus, although splitting creates gains in accuracy and robustness for inference it with some loss of prediction accuracy. 76Open Problems An interesting application would be to consider the feasibility of using sample split- ting as an estimation and inference approach in time series regression models. For instance as in the case, when quantile regression models are employed for modelling risk measures such as the VaR and the CoVaR. In that case, sample splitting can be seen as a methodology for out-of-sample forecasting of the unknown risk quantities. Thus, developing robust methodologies for accommodating not only the uncertainty induced by the selection of the optimal size of the out-of-sample period but also to be able to capture correctly the persistence properties of regressors included in the model. Another example, consider the block bootstrap which is commonly which as a resampling method to preserve the depen- dence structure in predictive regression models. Thus, in the case of time-varying predictive regression models, essentially one models the effect of time-varying persistence using a rolling window. Combining these sets of information in a meaningful way is crucial in understanding how the persistence properties of predictors affect the asymptotic efﬁciency of statistics such as the bootstrap conﬁdence set or other methods of variable selection in high-dimensional settings with dependent data. 4.6. Multiple Testing Procedure and Variable Selection In this section, we consider an alternative methodology proposed in the statistical literature especially, since the pioneered work of Abraham Wald on Sequential tests of statistical hypotheses. Towards the direction of the multiple testing approach applied to high dimensional regression models for variable selection and statistical inference purposes, a relevant framework is proposed by Chudik et al. (2018). Therefore, in this section we discuss their proposed statistical methodology, so-called One Covariate at a Time Multiple Testing (OCMT) (see, also Zhang (1993)). Speciﬁcally, the particular procedure is computationally simple and fast even for extremely large data sets (see, also Romano and Wolf (2005)). Thus, when a Lasso penalization is allowed then the estimator of β is obtained via ˆβ = arg min β∈Rp T ∑ t=1 �� yt −x′ ntβ �2 +Pλ(β) � . (4.67) A certain degree of sparsity is required in order to apply standard penalized linear models in high- dimensional settings. However, the methodology does not require that the regressor vector xnt to have a sparse covariance matrix, and therefore it is still applicable even if the covariance matrix of the noise variables, is not sparse. The particular algorithm can be thought as a model selection device since the main idea of this procedure is to test the statistical signiﬁcance of the net contribution of all n available potential covariates in explaining yt individually, while accounting for the multiple testing nature of the problem under consideration. However, the OCMT procedure is not sequential and selects in a single step all covariates whose t−ratios exceed a given threshold. Moreover, using the OCMT procedure post- selection is only applied when there are still covariates whose net contribution to yt is zero, despite the fact that they belong to the true model for yt. From the statistics perspective sequential model selection is discussed by Fithian et al. (2015). These approaches construct regression models by selecting variables from active sets, based on a sequence of sets. 774.6.1. Rejection Principle of Familywise Error Control From the statistical theory perspective the partitioning principle is a powerful tool in multiple decision theory. The following theorem explains the concept of familywise error rejection, (FWER). Theorem 14 (Finner and Strassburger (2002)). Let α,γ ∈ (0,1). If the null hypothesis H0, j : β j = 0 gets rejected whenever Qj(γ) ≤ α, then the FWER is asymptotically controlled at level α, that is, lim sup n→∞ P � min j∈N Qj(γ) ≤ α � ≤ α. (4.68) Closed testing and partitioning are recognized as fundamental principles of familywise error control, (FWE). In practice, various multiple testing procedures that control the FWE are often sequential, in the sence that rejection of some of the hypotheses may make rejection of the remaining hypothesis eas- ier. Sequential rejective procedures in the literature include that of Romano and Wolf (2005) (see, also Politis et al. (1999)). Moreover, the paper of Goeman and Solari (2010) presents a uniﬁed approach to the class of sequentially rejective multiple testing procedures, emphasizing the sequential aspect. Thus, the authors consider a general sequentially rejective procedure as a sequence of single-step methods, deter- mined by a rule for setting the rejection regions for each null hypothesis based on the current collection of unrejected regions for each null hypothesis based on the current collection of unrejected null hypotheses. Thus, the sequential rejection principle, implies that a single-step familywise error controlling procedure is turned into a sequential one which is a general principle of familywise error control. Theorem 15 (Sequential rejection principle, Goeman and Solari (2010)). Suppose that for every R ⊆ S ⊂ H , almost surely, N (R) ⊆ (S )∪S and that for every M ∈ M, P � N (F(M)) ⊆ F(M) � ≥ 1−α. (4.69) Then, for every M ∈ M, P � R∞ ⊆ F(M) � ≥ 1−α. Remark 23. The ﬁrst condition - monotonicity condition, guarantees that no false rejection in the critical case (during the single-step), which implies no false rejection in situations with fewer rejections than in the critical case so that type I error control in the critical case is sufﬁcient for overall FWE control of the sequential procedure. The second condition above - single-step condition, guarantees FWE control when we have rejected all false null hypotheses and none of the true ones. A general admissibility criterion is the case of restricted combinations8 can be also constructed. Restricted combinations occur if, for some R ⊆ H , there is no model M ∈ M such that R = F(M) (see, Goeman and Solari (2010)). 8A standard example concerns testing pairwise equality of means in a one-way ANOVA model: if any single null hypoth- esis is false, it is not possible that all other null hypotheses are simultaneously true (see, also Vesely et al. (2021)). 78• Resampling-based multiple testing procedures use resampling techniques to let the multiple test- ing procedure estimate or accommodate the actual dependence structure between the test statistics. For example, resampling of a test statistic under the complete null hypothesis, using permutations or the bootstrap, can give consistent estimates of the desired quantiles. However, regardless of the underlying assumptions, consistent estimation of the quantiles of maxH∈T (M)SH only guarantees control of the familywise error in an asymptotic sense. Resampling-based methods with exact FWE control, imply that one can obtain control of the FWE by generalizing the treatment of permutation testing to a multiple testing procedure. In order to deﬁne a resampling-based sequentially rejective multiple testing procedure with exact familywise error control, we choose a set π = {π1,...,πr} of r functions that we shall refer to as null-invariant transformations. • Graph-based procedures. Another application of the sequential rejection principle, which is of interest in the statistics, econometrics and ﬁnance literature, is the development of multiple testing9 procedures for graph-structured hypotheses. Speciﬁc procedures for controlling the familywise error for graph-structured hypotheses have been proposed by several authors. Example 20 (Neighborhood Selection with the Lasso, Meinshausen and Bühlmann (2006)). Consider the p−dimensional multivariate normal distributed random variable X = (X1,...,Xp) ∼ N (µ,Σ). This includes the Gaussian linear models where X1 is the response variable and {Xk,2 ≤ k ≤ p} are the predictor variables. The conditional independence structure of the distribution can be represented by a graphical model G = (V ,E ), where V = {1,..., p} is the set of nodes and E is the set of edges in V ×V . A pair (a,b) is contained in the edge set E if and only if Xa is conditionally dependent on Xb, given all remaining variables XV \{a,b}. Every pair of variables not contained in the edge set is conditionally independent, given all remaining variables, and corresponds to a zero entry in the inverse covariance matrix. When predicting a variable Xa with all remaining variables {Xk;k ∈ Γ(n)\{a}}, the vanishing Lasso coefﬁcient estimates identify asymptotically the neighborhood of a node a in the graph. Let the n × p(n)−dimensional matrix X contain n independent observations of X, so that the columns Xa cor- respond for all a ∈ Γ(n) to the vector of n independent observations of Xa. Let ⟨.,.⟩ be the usual inner product on Rn and ∥ . ∥2 the corresponding norm. The Lasso estimate ˆθa,λ of θa is given by ˆθa,λ = arg min θ:θa=0 � n−1∥Xa −Xθ∥2 2 +λ ∥θ∥1 � , ∥θ∥1 = ∑ b∈Γ(n) |θb|, (4.70) where ℓ1−norm of the coefﬁcient vector. Furthermore, Salgueiro et al. (2005) and Salgueiro et al. (2006) propose a formal statistical framework for edge exclusion in graphical Gaussian models. Moreover, sta- tistical theory for constructing relevant matrix moments functions is presented by Roverato and Whittaker (1998). Recently, Fan et al. (2020) propose a conditional dependence measure in high-dimensional undi- rected graphical models (UGM). In particular, the UGM approach examines the internal conditional dependency structure of a multivariate random vector. 9Notice that for multiple testing procedures researchers are also interested in reporting multiplicity-adjusted p-values. Such multiplicity-adjusted p-values are deﬁned for each null hypothesis as the smallest α−level that allows rejection of that hypothesis (see also the method proposed by McKeague and Qian (2015) and Huang et al. (2019)). 794.6.2. Subgroup Selection Methodology Lastly, we brieﬂy discuss the subgroup selection methodology proposed by Reeve et al. (2021)10. In subgroup selection, the main objective of the statistician is to leverage the underline structure dynamics in data to identify a subset ˆ A of the population to treat. Suppose that we have a distribution P on a covariate-response pairs (X,Y) in Rd ×R. Let µ := µP denote the marginal distribution of the covariate X ∈ Rd. Let η : Rd → R be the regression function deﬁned by η(x) := E � Y|X = x � for some x ∈ Rd. Then, one would like to select a subgroup A ⊂ Rd such that η is above a user-speciﬁed threshold τ ∈ R on A. Hence, we are interested in subsets of the τ−super level set such that the following set holds Xτ(η) := � x ∈ Rd : η(x) ≥ τ � . (4.71) In other words, the statistical mechanism of Reeve et al. (2021) implies that the user chooses a data- dependent subgroup ˆA ≡ ˆA(A ), which is a random subset of Rd, taking values in A . Speciﬁcally, the practitioner has access to a sample D := � (X1,Y1),...,(Xn,Yn) � i.i.d ∼ P. Therefore, from the objective function perspective the aim is to select a data-dependent subgroup ˆA(D) ⊂ Xτ(η), with high-probability where Xτ(η) := � x ∈ Rd : η(x) ≥ x � . Proposition 3 (Type 1 error guarantee,Reeve et al. (2021)). Let P be a family of distributions P on pairs (X,Y) and choose a signiﬁcance level α ∈ (0,1). We say that the data-dependent subgroup ˆA controls Type 1 error at the level α over the class P if inf P∈P PP � ˆA(D) ⊆ Xτ(η) � ≥ 1−α. (4.72) Therefore, our objective is to choose ˆA ≡ ˆA(D) which minimizes regret Rτ( ˆA), subject to inf P∈P PP � ˆA(D) ⊆ Xτ(η) � ≥ 1−α. Deﬁnition 7 (Holder Class, Reeve et al. (2021)). Given that (β,CS) ∈ (0,1]×[1,∞), we let PHol,τ(β,CS) denote the class of all distributions P on Rd × [0,1] with marginal µ on Rd such that the regression function η is (β,CS)−Holder on Xτ(η)∩supp(µ) in the sence that ��η(x′)−η(x) �� ≤ CX. ��x′ −x ��β ∞ , (4.73) for all x,x′Xτ(η)∩supp(µ). Further details on the implementation of this methodology as well as related statistical theory and nu- merical illustrations are presented in Reeve et al. (2021). More recently the isotonic subgroup selection approach is proposed by Müller et al. (2023). 10Dr. Henry Reeve gave a seminar with title: "Subgroup Selection in nonparametric regimes", at the S3RI Departmental Seminar Series at the University of Southampton on the 10th of November 2022. 80Remark 24. Notice that according to Reeve et al. (2021), the subgroup selection approach requires from the statistician to use both FWE control as well as the selection step. In other words, the subgroup selec- tion approach corresponds to a data-dependent selection methodology, such that ˆ A = ˆ A (D). Therefore, a special class of functions needs to be considered which allows to test for local null hypothesis. Thus, the statistical problem corresponds to minimizing the regret that controls the type I error. Although one in practice needs to check the exact set that corresponds to the minimized regret. Therefore, the following expression holds: inf p∈P PP � ˆ A (D) ⊂ Xτ(n) � ≥ 1−α. (4.74) Then, the Type I error guarantee implies that the statistical subgroup selection procedure is constructed by sequential partial ordering of multiple covariates. Then, the statistician chooses only the one null hypothesis with the highest partial ordering. In other words, using a graph with acyclic structure allows to expand the multiple testing procedures (the null hypothesis of all ancestors is true). Thus, if the null at that node is false, then the null hypothesis associated with all ancestors is also false. 814.7. Model Selection in Cointegrating Regressions According to Mendes (2011), usually under the presence of cointegration in the VAR system, which implies that there are (k − m) linear combinations of Xt which are I(0), a bias analysis is necessary to obtain a robust econometric estimation methodology. To do this, we consider the m common stochastic trends and at most (k − m) cointegrating relations amongst the I(1) components are represented by the same generating process as ∆Xt−1 = (A − Ik)Xt−1 + εt, where εt ∼ N (0,Ω). In particular, these m components are represented by m unit roots, and the (k − m) stable roots of the stochastic difference equation represent the I(0) components of Xt as well as cointegrating relations between the I(1) variates of Xt (see, Abadir et al. (1999)). Example 21. Consider the following cointegration regression model given by the expression yt = α0 +β ′ 0xt +γ′ 0zt +ut (4.75) where β0 is an n1 × 1 vector of parameters and γ0 is an n2 × 1 vector of parameters. Furthermore, the process {xt}∞ t=1 satisﬁes the following integrated stochastic process xt = xt−1 +vt, (4.76) Therefore, the main aim shall be to control the number of I(0) variables in the model and we assume that n ≡ n1 +n2 to possibly be grater than T, but only a fraction of those coefﬁcients are in fact nonzero. Furthermore, we assume without loss of generality that each coefﬁcient vectors can be partitioned into zero and non-zero coefﬁcients, such that β0 = � β0(1)′,β0(2)′�′ and γ0 = � γ0(1)′,γ0(2)′�′, with all non- zero coefﬁcients stacked ﬁrst, where β0(1) is q1 ×1 and γ0(1) is q2 ×1. In particular, we assume that the number of non-zero coefﬁcients, measured by q1, is ﬁxed (does not depend on T), while the number of zero coefﬁcients, measured by q2, may depend on T, also set q = (q1 +q2). Then, the Adaptive Lasso estimate is given by (see, also Medeiros and Mendes (2017)) � ˆβ, ˆγ � = argmin β,γ ∥Y −Xβ −Zγ∥2 2 +λ1 n ∑ j=1 λ1j|β j|+λ2 n ∑ j=1 λ2j|γj|, (4.77) Lemma 13 (KKT Conditions). The solutions ˆβ = � ˆβ(1)′, ˆβ(2)′� and ˆγ = � ˆγ(1)′, ˆγ(2)′� to the minimiza- tion problem above exists if: ∂ ∥Y −Xβ −Zγ∥2 2 ∂β j ���� β j(1)= ˆβ j(1) = sgn � ˆβ j(1) � λ1λ1j (4.78) ∂ ∥Y −Xβ −Zγ∥2 2 ∂β j ���� γ j(1)= ˆγ j(1) = sgn � ˆγj(1) � λ2λ2j (4.79) 824.7.1. Model Selection Consistency and Oracle Property We focus on showing that under certain conditions on n, p and λ ′s the Adaptive Lasso selects the correct subset of variables - sign consistency and it has the oracle property, meaning that our estimate has the same asymptotic distribution of the OLS as if we knew a priori the selected model and at optimal rate. Lemma 14. Let Ω∞ = � ΩX,∞ 0 0′ ΩX,∞ � (4.80) ΩX,∞ = � 1 0 BX(1)BX(1)′(r)dr and ΩZ,∞ = ΣZ(1)2, (4.81) where for any 0 ≤ r ≤ 1, BX(1)(r) = lim T→∞∑ ⌊rT⌋ t=1 vt(1). Similarly, split the matrix Ω11 into the partition Ω11 = � ΩX(1)2 ΩZ(1)X(1) Ω′ Z(1)X(1) ΩZ(1)2 � = � 1 T 2X(1)′X(1) 1 T 3/2Z(1)′X(1) 1 T 3/2X(1)′Z(1) 1 T Z(1)′Z(1) � . (4.82) Hence, P � A c T (X) � = P �����T −1Ω−1 X,∞X(1)′U �� � j > T|β0j| � , j = 1,...,q1 � +op(1), ≤ q1 ∑ j=1 P ����T −1Ω−1 X,∞X(1)′U �� � j > T|β0j| � +op(1), ≤ q1 T 2β 2∗ max 1≤j≤q1 E �� T −1��Ω−1 X,∞X(1)′U �� �2 j � → 0. Further aspects of consideration in the more recent literature include the development of robust frame- works for time series regressions within a high-dimensional environment for the purpose of estimation, inference and forecasting (see, Gupta and Seo (2019), Baillie et al. (2022)). Moreover, the development of a framework that allows for the use of an ultra-high dimensional environment in nonstationary time series models can be useful when considering statistical properties such as model selection consistency and hypothesis testing accuracy. An econometric framework for high-dimensional quantile predictive regressions is proposed by Fan et al. (2023). Various studies consider methods for variable screening in high dimensional linear models, however less attention is paid on how these methodologies perform in the case of nonstationary time series models, especially when for the conditional quantile functional form. Relevant research aspects include the development of a statistical mechanism for dimension re- duction or screening methodology in the presence of a high-dimensional vector of possibly nonstationary predictors, which can improve forecasting performance (see, Pitarakis (2023) and Gonzalo and Pitarakis (2023)). Moreover, choosing variables from irrelevant cointegrating relations is another important issue (see, Khalaf and Richard (2020) and Richard (2023)). 834.7.2. Model Selection and Rank of a Matrix In many econometric and statistic applications knowing the rank of a matrix is crucial to ensure robust inference and testing. For example, from the ﬁnancial economics literature one can test the implications of Arbitrage Pricing Theory by testing the corresponding rank restrictions. Moreover, the identiﬁcation of parameters in econometric models depends on conditions and restrictions regarding the rank of related moment matrices such as the Jacobian matrix. Speciﬁcally, various tests for the rank of a matrix can be found in the literature. In particular, these testing methodologies are constructed under the assumption that the unrestricted matrix estimator has a kronecker covariance matrix, however this approach can be sensitive to the ordering of the variables (see, Kleibergen and Paap (2006)). Furthermore, Cragg and Donald (1997) propose suitable testing pro- cedures for determining the rank of a matrix. In particular, the authors examine the use of model selection criterion and sequential hypothesis testing methods to estimate the rank consistently. On the other hand, Kleibergen and Paap (2006) propose a novel rank statistic which uses a √n−consistent estimator of the unrestricted matrix which does not have to have a kronecker covariance matrix. To do this, the authors decompose the estimator of the unrestricted matrix using the spectral vector decomposition. Testing for the rank of the matrix can be also used in cointegration testing for non-stationary time series models. Speciﬁcally, in this case the limiting distribution is found to be functional of Brownian motions and is equal to the asymptotic distribution of the Johansen (1991) trace test. Example 22. Consider the following ECM model ∆xt = Πxt−1 + k−1 ∑ i=1 Ψi∆xt−i +Φdt +εt (4.83) where Π = αβ ′ and β is the p × r matrix with the cointegration vectors and α is the p × r matrix of adjustment coefﬁcients. The number of long-rank relations is equal to the rank of Π, which is called the cointegration rank. Both α and β are full rank matrices. Moreover, the symmetric p × p, Ψi matrix governs the short-term dynamics of the system. 845. Statistical Learning Methods in Time Series Analysis 5.1. Motivation In this section we discuss some key applications of statistical learning methodologies in time series analysis. In particular the use of Neural Networks is now widely spread in the ﬁnance, economics and actuarial statistics ﬁelds. Some applications worth mentioning include: (i) asset pricing modelling (see, Feng et al. (2018), Guijarro-Ordonez et al. (2021), Fan et al. (2022), Chen et al. (2023) and Caner and Daniele (2023)). Relevant applications of the Lasso shrinkage in asset pricing and ﬁnance theory include the studies of Feng et al. (2020) and Chinco et al. (2019). (ii) heterogeneity in mortality modelling(see, Pitacco (2019)). A relevant question for the latter is: "What is the link between heterogeneity of unobservable factors and mortality deceleration?". (iii) credit risk and correlated defaults modelling in ﬁnancial markets (see, Angelini et al. (2008) and Bhatore et al. (2020)). Furthermore, on the aspect of modelling heterogeneity in mortality, more speciﬁcally the statistician can employ a functional form represented by f ("biometric" function) to represent the age pattern of mortality. Then, the following representation follows: f = w1 · f (1) +w2 · f (2) +...+wN · f (N) and µx = aeβx δeβx +1, (5.1) where � f (1), f (2),..., f (N)� represent risk factors (such as health status, occupation etc.) in order to capture the non-linear effects of mortality over time. In particular, these risk factors are considered as contributing factors that worsens the mortality level or probability of an individual that can be incorpo- rated into a rating system. Therefore, splitting individuals with similar risk into groups allows to gain more information about uncertainty in heterogeneity11 (which is a well-known principle in the statistical literature of credibility theory and risk premium models). Moreover, according to Pitacco (2019) a rating system can be constructed based on the form qspec t+x = qt+h · � 1+∑r j=1 ρ(j)� , in which case peak mortality is achieved after entering disable stage, such that qspec t+x = qt+x ·∆(x+h;α,γ). Then, the unobservable risk factors can be captured via the form µspec x = Φ(µx+t;zx,t); known as random heterogeneity. The aforementioned examples provide some brief illustrations of the various applications from the econo- metrics and statistics literature that can motivate the investigation of relevant research questions both from the theoretical as well as the empirical perspective when we consider statistical learning methods for time series analysis purposes. Further resources related to deep learning theory and applications can be found in Goodfellow et al. (2016) (see, also LeCun et al. (2015)). 11Professor Ermanno Pitacco gave a seminar with title: "Heterogeneity in mortality: A survey with an actuarial focus" at the S3RI Departmental Seminar Series at the University of Southampton on 24 of September 2018. 855.2. Non-Asymptotic Probability Theory One of the main goals of learning theory is the development of stability bounds of algorithmic procedures. Following the framework of Mohri and Rostamizadeh (2010) we employ the following deﬁnitions. Deﬁnition 8. A learning algorithm is said to be (uniformly) �β−stable if the hypotheses it returns for any two training samples S and S′ that differ by removing a single point satisfy ∀ z ∈ X ×Y, ��c(hS,z)−c(h′ S,z) �� ≤ �β. (5.2) Remark 25. Notice that a �β−stable algorithm is also stable with respect to replacing a single point. Let S and Si be two sequences differing in the i−th coordinate, and S|i be equivalent to S and Si but with the i−th point removed. then, for a �β−stable algorithm we have that ��c(hS,z)−c(h′ S,z) �� ≤ 2�β. (5.3) Speciﬁcally, the use of stability allow us to derive generalization bounds and an exponential concentration bounds of the following form P(|Φ−E[Φ]| ≥ ε) ≤ exp � −mε2 τ2 � , (5.4) where the probability is over a sample of size m and where τ m is the Lipschitz parameter of Φ, with τ a function of m. In the ﬁrst section below, we consider the problem of shallow-vs-deep expressiveness from the perspective of approximation theory and general spaces of functions having derivatives up to certain order (Sobolev-type spaces). Speciﬁcally, in this framework the problem of expressiveness is very well studied in the case of shallow networks with a single hidden layer, where it is known, in particular, that to approximate a Cn−function on a d−dimensional set with inﬁnitesimal error ε one needs a network of size about ε−d/n, assuming a smooth activation function. Deﬁnition 9 (Lipschitz functions). Consider the class of Lipschitz functions such that FL := � g : [0,1] → R|g(0) = 0 and ��g(x)−g′�� ≤ L|x−x′| ∀ x,x′ ∈ [0,1] � . where L > 0 is a ﬁxed constant, and all of the functions in the class obey the Lipschitz bound uniformly condition over all of [0,1]. Remark 26. Learning theory and non-asymptotic probability theory is useful for understanding the local behaviour of statistical learners for a class of functions based on regularity conditions. Further applica- tions include the aspect of robustness and generalization for metric learning12 (see, Yang et al. (2022)). 12Dr. Xiaochen Yang gave a seminar with title: "Towards better robustness and generalisation of metric learning methods", at the S3RI Departmental Seminar Series at the University of Southampton on the 3rd of November 2022. 865.3. Shallow Neural Network Estimate learned by Gradient Descent Deﬁnition 10. A shallow neural network with one output is a function f : Rd → R of the form f(x) = m ∑ j=1 cjσ � w⊤ j x+vj � , wj ∈ Rd, vj,cj ∈ R, (5.5) where σ : R → R is the activation function (see, Braun et al. (2019) and Braun et al. (2021)). Consider functions of the form f(x) = σ � d ∑ j=1 wj.x(j) +w0 � , where x = � x(1),...,x(d)�⊤ ∈ Rd. (5.6) where w0,...,wd ∈ R the weights of the neuron and σ : R → R the activation function. Shallow Neural Networks have only one hidden layer where a simple linear combination of neurons is used to deﬁne a function f : Rd → R by f(x) = K ∑ k=1 αK.σ � d ∑ j=1 βk, j.x(j) +βk,0 � +α0. (5.7) Denote the weight between neuron j in layer (s − 1) and neuron i in layer s by w(s) i, j . This leads to the following recursive deﬁnition of a neural network with L layers and ks neurons in layer s ∈ {1,...,L } such that the following representation applies: f(x) = kL ∑ i=1 w(L ) 1,i f (L ) i (x)+w(L ) 1,0 (5.8) for some w(L ) 1,0 ,...,w(L ) 1,kL and for f (L ) i ’s recursively deﬁned by f (s) i (x) = σ � ks−1 ∑ j=1 w(s−1) i, j f (s−1) j (x)+w(s−1) i,0 � . (5.9) Main results in the literature show that NNs can achieve dimension reducion provided the regression function is a composition of sums of functions, where the input dimension of each of the functions is at most d∗ < d. Denote by fnet,w the neural network with weight vector w = � w(s) j,k � s=0,..,L , j=1,...,ks+1,k=0,...,ks and set the following function F(w) = 1 n n ∑ i=1 |Yi − fnet,w|2 , w(0) = v, (5.10) for some randomly chosen initial vector v. 87Then the optimization problem can be written as below: w(t +1) = w(t)−λn.∇wF(w(t)), for t ∈ {0,...tn −1}. (5.11) Example 23. We approximate m by networks with one hidden layer and K.r neurons in this hidden layer: fnet,(a,b)(x) = K.r ∑ k=1 αk.σ � d ∑ j=1 bk, j.x(j) +bk,0 � +α0. (5.12) where K.r ∈ N is the number of neurons and σ : R → R is the activation function. Furthermore, the unknown optimal vector of weights is obtained by employing the gradient descent algorithm. More precisely, we minimize the penalized empirical L2 risk as F(a,b) = 1 n n ∑ i=1 ��fnet,(a,b)(Xi)−Yi ��2 + c1 n . K.r ∑ k=0 a2 k. (5.13) by choosing the appropriate starting value � a(0),b(0)� and by setting � a(t+1) b(t+1) � = � a(t) b(t) � −λn. � ∇(a,b)F �� a(t),b(t)� (5.14) for some λn > 0 chosen below and t ∈ {0,1,...,tn−1}. Theorem 16. Let n ≥ 1, let A ≥ 1 and let {(X1,Y1),...,(Xn,Yn)} be i.i.d random variables with values in [−A,A]d ×R. Set m(x) = E[Y|X = x] and assume that (X,Y) satisﬁes E � ec2.|Y|2� < +∞ (5.15) for some constants c2 > 0, and that m satisﬁes m(x) = r ∑ s=1 gs � c⊤ s x � , x ∈ Rd. (5.16) for some r ∈ N,cs ∈ [−1,1]d, where ∥cs∥ = 1 and gs : R → R for s ∈ {1,...,r}. Remark 27. From the statistical perspective, relevant properties and convergence rates for gradient descent algorithms are given in the studies of Braun et al. (2019), Shao and Zhang (2022) (see, also Toulis and Airoldi (2017)) as well as by Schmidt-Hieber (2020). Furthermore, in recent years the use of ANN and DNN for econometric applications has seen growing attention (see, White (1990) Kuan and White (1994), Farrell et al. (2021)). 885.3.1. Learning of linear penalized least squares estimates by gradient descent Let (x1,y1),...,(xn,yn) ∈ Rd × R, let K ∈ N and let B1,...,BK : Rd → R, with c1 > 0. We consider the problem to minimize as below F(a) = 1 n n ∑ i=1 ����� K ∑ k=1 ak.Bk(xi)−yi ����� 2 + c1 n .∥a∥2 (5.17) where a = (a1,...,aK)⊤ and ∥a∥2 = ∑K j=1a2 j, by gradient descent. To obtain the solution of the optimization problem, we choose a(0) ∈ RK and set with a(t+1) = a(t) −λn.(∇aF) � a(t)� (5.18) for some chosen λn > 0. Consider that a Lipschitz continuity condition holds ∥(∇aF)(α1)−(∇aF)(α2)∥ ≤ Ln.∥a1 −a2∥ with (a1,a2) ∈ RK. (5.19) Then, we have that F � a(t+1) −F � a(t)�� ≤ − 1 2.Ln . ���(∇aF) � a(t)���� 2 . (5.20) Lemma 15. Let σ be the logistic squasher. Let ¯c ∈ [−1,1]d with ∥¯c∥ = 1 and let g : R → R be (p,C)−smooth for some p ∈ (0,1] and C > 0. Let ρn > 0, K ∈ N and choose b1,b2,...,bK ∈ R such that b1 < b2 < ... < bK and b1 ≤ −A. √ d and bK ≥ A. √ d − 4.A. √ d K −1 (5.21) such that A. √ d (n+1).(K −1) ≤ |bk+1 −bk| ≤ 4.A. √ d K −1 , k ∈ {1,...,K −1}. (5.22) Let a0 = g(b1) and ak = g(bk)−g(bk−1) with k ∈ {1,...,K} where K is the number of layers. Then, we have that sup x∈[−A,A]d �����a0 + K ∑ k=1 ak.σ � ρn. � ¯c⊤x−bk �� −g � ¯c⊤x ������ ≤ 3.(4.A. √ d)p.C (K −1)p +C.(4.A. √ d)p.(K −1)1−p.e− ρn.(A. √ d) (n+1).(K−1). 89Proof. Notice that we assume that the logistic activation function σ(x) = 1 1+e−x is approximated by the step function 1{[0,∞)}(x). In other words, for any x ∈ R we have that |σ(x)−1{[0,∞)}(x)| ≤ e−|x|. (5.23) We have that �����a0 + K ∑ k=1 ak.σ � ρn. � ¯c⊤x−bk �� −g � ¯c⊤x ������ ≤ �����a0 + K ∑ k=1 ak.σ � ρn. � ¯c⊤x−bk �� − K ∑ k=1 ak.1[bk,∞) � ¯c⊤x ������ + �����a0 + K ∑ k=1 ak.1[bk,∞) � ¯c⊤x � −g � ¯c⊤x ������ Moreover, for each bj ≤ ¯c⊤x < bj+1, where j{1,...,K −1}, we can conclude that the deﬁnition of ak, from the (p,C)−smoothness of g and from our choice of the bk we obtain �����a0 + K ∑ k=1 ak.1[bk,∞) � ¯c⊤x � −g � ¯c⊤x ������ = �����a0 + j ∑ k=1 ak −g � ¯c⊤x ������ = ���g(bj)−g � ¯c⊤x ���� ≤ C. ���bj − ¯c⊤x ��� p ≤ C. ��bj+1 −bj ��p ≤ C. � 4.A. √ d �p (K −1)p . Therefore, we have shown that sup x∈[−A,A]d �����a0 + K ∑ k=1 ak.1[bk,∞) � ¯c⊤x � −g � ¯c⊤x ������ ≤ C. � 4.A. √ d �p (K −1)p . (5.24) We complete the proof by showing that sup x∈[−A,A]d �����a0 + K ∑ k=1 ak.σ � ρn. � ¯c⊤x−bk �� − K ∑ k=1 ak.1[bk,∞) � ¯c⊤x ������ ≤ 2. � 4.A. √ d �p .C (K −1)p +C. � 4.A. √ d �p .(K −1)1−p.e− ρn.(A. √ d) (n+1).(K−1). 90Lemma 16 (Braun et al. (2019)). Let σ be the logistic activation function. Deﬁne F to be the ridge regression and set with ¯b = b−λn.(∇bF)(a,b) (5.25) for some λn > 0, where a = � a1,...,aK �⊤ ∈ RK and b = � b1,0,b1,1,...,b1,d,...,bK,0,bK,1,...,bK,d �⊤ ∈ RK.(d+1). Then, we have that for any k ∈ {1,...,K} and any j ∈ {0,...,d} such that: ��¯bk, j −bk, j �� ≤ λn.2. � F(a,b).max � 1,max ����x(ℓ) i ��� �� .exp � − min i=1,...,n � d ∑ j=1 bk, j.x(j) i +bk,0 �� . Consider the Cauchy-Schawrz inequality and since the activation function σ is Lipschitz continuous 1 n n ∑ i=1 � fnet,(¯a,b(t))(xi)− fnet,(¯a,b(0))(xi) �2 = 1 n n ∑ i=1 � K ∑ k=1 ¯ak. � σ � d ∑ j=1 b(t) k, j.x(j) i +b(t) k,0 � −σ � d ∑ j=1 b(t) k, j.x(j) i +b(0) k,0 ���2 ≤ K ∑ k=1 ¯a2 k.max � 1,max i, j |x(j) i |2 � .(d +1). K ∑ k=1 d ∑ j=0 ���b(t) k, j −b(0) k, j ��� Remark 28. Notice that dimensionality reduction is crucial concept that commonly discussed in the neural network literature from the perspective of layers and related algorithms via the implementation of feedforward neural networks techniques. Alternative approaches include the implementation of taylor expansions which is more commonly used as a methodology for estimating econometric models (e.g. see Olmo (2022)) as well as the method of sieves and sieve estimators (see, Chen (2007)). Speciﬁcally, focusing on comparing these two estimation methodologies can provide some intuition on the main im- plications of the choice of the contraction mapping (see, Keeler and Meir (1969), Reich (1971)) to the underline asymptotic theory. In particular, the framework of Olmo (2022) corresponds to a nonparamet- ric linear regression model with a lagged regressor. Due to the fact that the functional form is estimated using a taylor expansion with partioning13, implies that the corresponding stochastic approximation has discontinuous increments. Thus, the asymptotic behaviour of estimators and test statistics consists of asymptotic functionals that correspond to the supremum of Gaussian processes (e.g., see Beder (1987)) which might not even satisfy regularity conditions such as tightness. However, this is problematic for several reasons and especially when the interest of the econometrician is the modeling of nonstation- arity in regressors but the partitioning and estimation methodology does not correspond to the relevant stochastic approximations. A good understanding of the principles of contraction mappings is crucial. 13On the large sample properties of partitioning-based series estimators see, Cattaneo et al. (2020). 915.4. Deep Neural Network Estimate learned by Gradient Descent Based on the framework proposed by Shen et al. (2021), we present the following results which are useful to investigate the properties of Deep Neural Networks learned by gradient descent with respect to their dimensionality and complexity. In particular, the concept of pseudo-dimension is considered as a measure of complexity (see, Mohri and Rostamizadeh (2008)). Thus, the framework of Shen et al. (2021) considers the implementation of deep neural network for a high-dimensional quantile regression. An application to forecasting in time series is presented by Chronopoulos et al. (2023). A key ingredient to derive excess risk bounds for the deep quantile regression framework is to consider the properties of the functional classes within which identiﬁcation and estimation holds. Assumption 9 (Shen et al. (2021)). To derive excess risk bounds the following conditions hold: (i) The conditional τ−th quantile of η given X = x is 0 and E � |η||X = x � < ∞ for almost every x ∈ X. (ii) The support of covariates X is a bounded compact set in Rd, and without loss of generality X = [0,1]d. (iii) The response variable Y has a ﬁnite p−th moment for some p > 1, that is, there exists a ﬁnite constant M > 0 such that E|Y|p ≤ M. Lemma 17 (Lemma 2 in Shen et al. (2021)). Consider the d−variate nonparametric regression model with an unknown regression function f0. Let F = FD,W ,U ,S ,B be a Holder class of feed-forward neural networks with a continuous piecewise-linear activation function of ﬁnite elements ("pieces") and ˆfφ ∈ argmin f ∈Fφ Rnτ( f) (5.26) be the empirical risk minimizer over Fφ. Assume that Assumption 9 above holds and that ∥ f0∥∞ ≤ B for B ≥ 1. Then, for 2n ≥ Pdim(Fφ) and any τ ∈ (0,1), it holds that sup f ∈Fφ ��Rτ( f)−Rnτ( f) �� ≤ c0 max{τ,1−τ}B n1−1/p log � N2n � n−1,∥.∥∞ ,Fφ � , (5.27) Notice that this quantity is considered as an upper bound of the empirical risk minimizer, where c0 > 0 is a constant independent (i.e., not depending upon) of n,d,τ and the remaining components. Moreover, its expected value (here an aspect of interest is to determine the underline distribution theory which relates these quantities w.r.t to marginal and joint distributions) has the following upper bound E � Rτ( ˆfφ)−Rnτ( f0) � ≤ C0 max{τ,1−τ}BBBlog(S)log(n) n1−1/p +2 inf f ∈Fφ ��Rτ( f)−Rτ( f0) �� (5.28) Notice that the denominator can be improved to n if the response Y is assumed to be sub-exponentially distributed, that is, there exists a constant σY > 0 such that E � exp � σY|Y| �� . 92Moreover, further interesting aspects here include the technical results with respect to the DNN archi- tecture (e.g., composition), the relation of continuous functionals with the underline quantile regression model and the number of variables as well as linearity of the model and how it relates with the dimen- sionality of the problem. In particular, one can use numerical experiments to evaluate the performance of the standard kernel-based method for quantile regressions against the DQR estimation approach. Remark 29. A statistical procedure should be providing such evidence (such as a formal speciﬁcation test approach). On the other hand, starting with some comparisons of relative efﬁciency and a bias anal- ysis can provide some insights. Various open problems remain in the literature of DNN especially when considering the correct model speciﬁcation. However, such a research endeavour will require to deﬁne what DNN estimable means. Then this could allow us to construct a speciﬁcation test for a testing that a function under the null hypothesis is correctly DNN estimable against the alternative of a nonparametric kernel-estimation approach. To do this we need to deﬁne what a distance function is between a DNN estimable functional form against a nonparametric kernel-based estimation approach. Relevant litera- ture where these aspects are discussed are the studies that present statistical frameworks for consistent speciﬁcation testing (see, Stinchcombe and White (1998) and White (1996)). Nevertheless, the key point is consider a suitable functional class within which a statistical test can be constructed for evaluating differences between a DNN estimable functional form and a nonparametrically ﬁtted functional form. Deﬁnition 11 (Shen et al. (2021)). For a class F of functions: X → R, its pseudo-dimension, denoted by Pdim(F), is deﬁned to be the largest integer m for which there exists � x1,...,xm,y1,...,ym � ∈ X mRm such that for any (b1,...,bm) ∈ {0,1}m there exists f ∈ F such that ∀i : f(xi) > yi ⇐⇒ bi = 1. Remark 30. For a class of real-valued functions generated by neural networks, pseudo dimension is a natural measure of its complexity. In particular, if F is the class of functions generated by a neural network with a ﬁxed architecture and ﬁxed activation functions, we have that Pdim(F) = VCdim(F). Thus, a usual assumption under these settings is to require the sample size n to be greater than the pseudo dimension of the class of neural networks considered. Furthermore, the smoothness index works as a covering number to measure the complexity or dependence structure of the DNN. For instance, a function of the form f d 0 = � hq ◦ ... ◦ h0 � is a recursive function which implies that for each composition of functions, its argument is converted into one-dimensional functional form. In other words, the main intuition here is that by assuming a compositional structure of high-dimensional quantile regressions, this provides an efﬁcient solution to the "curse of dimensionality" problem, since in each composition applied, there is a reduction in the dimensions of the statistical problem. Lemma 18 (Lemma 1 in Shen et al. (2021)). For any random sample S = � (Xi,Yi)n i=1 � , the excess risk of the DQR estimator ˆfn satisﬁes Rτ( ˆfn)−Rτ( f0) ≤ 2 sup f ∈Fn ��Rτ( f)−Rnτ( f) ��+ inf f ∈Fn � Rτ( f)−Rτ( f0) � , (5.29) where Rnτ is deﬁned above. 93Remark 31. The excess risk of the DQR estimator is bounded above by the sum of two terms: the stochastic error 2 sup f ∈Fn ��Rτ( f)−Rnτ( f) �� and the approximation error inf f ∈Fn � Rτ( f)−Rτ( f0) � . Further- more, an interesting conjecture from the above result is that the upper bound no longer depends on the DQR estimators itself, but the function class Fn, the loss function ρτ and the random sample S. Then, the stochastic error 2 sup f ∈Fn ��Rτ( f) −Rnτ( f) �� can be analyzed using the empirical process theory. A key ingredient is to calculate the complexity measure Fn in terms of its covering (bracketing) number (see, Vaart and Wellner (2023), Van der Vaart (2000), Massart (2007) and Wainwright (2019)). Furthermore, for the excess risk of the DQR estimator and the error bounds for the models, based on appropriately speciﬁed network parameters (depth, width and size of the network), we have the following upper bound for the excess risk, E � Rτ( ˆfφ)−Rτ( f0) � ≤ C0Cd,d∗� logn �2n − � 1− 1 p � 2α∗ 2α∗+t∗ (5.30) • C0 is a constant only depending on the model parameters such as the smoothness index (smoothness parameter) of the underlying conditional quantile function. • Cd,d∗ is the prefactor depending on d, the dimension of the predictor variable in the model. Al- though a question remains whether this parameter has a ﬁxed functional form when considering each lower dimension term. • d∗ is determined by the dimensions of the component functions in the composite function. Then, the convergence rate which is part of the error bound, n− � 1− 1 p � 2α∗ 2α∗+t∗ , is determined by the number of moments p of the response Y, the smoothness index of the composite function α∗ and the intrinsic dimension of the model t∗. Other relevant aspects include whether imposing further distributional as- sumptions (such as heavy-tailed errors versus Gaussian errors) can affect the limit theory of estimators as well as its min-max optimality properties. Next, we employ the risk function (i.e., the expectation of the loss function) such that Rτ( f) := E � ρτ(Y − f(x)) � as well as the empirical risk, which is the minimizer of the empirical risk function (i.e., it allows to restrict the functional class). For the proof of the above lemma (Lemma 1 in Shen et al. (2021)) the following two steps are necessary and sufﬁcient: Step 1. Prediction error decomposition (see, Shen et al. (2021)). The "best in class" estimator f ∗ φ is deﬁned as the estimation in the function class F with minimal L risk such that: f ∗ φ = argmax f ∈Fφ Rτ( f). (5.31) Then, the approximation error of f ∗ φ , Rτ( f ∗ φ)−Rτ( f0) only depends on the function class and the distribution of data. Moreover, by the deﬁnition of the empirical risk minimizer, the following inequality result holds 94ES � 1 n n ∑ i=1 g � ˆfφ,Zi � � ≤ ES � 1 n n ∑ i=1 g � f ∗ φ ,Zi � � . (5.32) Therefore, it can be proved that the predictor error is upper bounded by the sum of an expectation of a stochastic term and an approximation error. Step 2. Bounding the stochastic term (see, Shen et al. (2021)). To bound the stochastic term, we obtain an upper bound of the expression which includes the stochastic term, using a truncation argument and the classical chaining technique for empirical processes.Denote with G( f,Zi) := ES′� g( f,Z′ i) � − 2g( f,Zi) for any function such that f ∈ Fφ. Given a δ−uniform covering of Fφ, we denote the centers of the balls by f j, j = 1,2,...N2n, where N2n = N2n � δ,∥.∥∞ ,Fφ � is the uniform covering number with radius δ(δ < B) under the norm ∥.∥∞. By the deﬁnition of covering, there exists a (random) j∗ element such that �� ˆfφ(x)− f j∗(x) �� ∞ ≤ δ on x = � X1,...,Xn,X′ 1,...,X′ n � ∈ X 2n, (5.33) Recall that we have g � f,Zi � = � ρτ � f(Xi)−Yi � −ρτ � f0(Xi)−Yi �� . Denote with λτ = max{τ,1−τ}, then by the Lipschitz property of ρτ, for a,b ∈ R it holds that ��ρτ(a)−ρτ(b) �� ≤ max{τ,1−τ}|a−b| = λτ|a−b|, (5.34) Therefore, for i = 1,...,n we have that ��g � ˆfφ(x),Zi � −g � f j∗(x),Zi ��� ≤ λτδ, ��ES′ � g � ˆfφ(x),Z′ i �� −ES′ � g � f j∗(x),Z′ i ���� ≤ λτδ (5.35) Then, it holds that ES � 1 n n ∑ i=1 g � ˆfφ(x),Z′ i � � ≤ 1 n n ∑ i=1 ES � g � f j∗(x),Z′ i �� +λτδ (5.36) Moreover, denote with βn ≥ B ≥ 1 to be a positive number who may depend on the sample size n. Denote with Tβn as the truncation operator at level βn, such that it holds that for any Y ∈ R, TβnY = Y, if |Y| ≤ βn and TβnY = βn.sign(Y) otherwise. Deﬁne the function f ∗ βn : X → R pointwisely by f ∗ βn = arg min f (x):∥f ∥∞≤βn E � ρτ � f(X)−TβnY � |X = x � , (5.37) for each x ∈ X . Moreover, recall that ∥ f ∗∥∞ ≤ B ≤ βn and f0(x) := arg min f (x):∥ f ∥∞≤βn E � ρτ � f(X)−Y � |X = x � . (5.38) 955.5. Deep Neural Network Architecture Approximations Related studies on the theoretical aspects of network achitecture include Farrell et al. (2021), Zeng (2021) and D˜ung et al. (2021). The approximation by deep ReLU neural networks of functions having a mixed smoothness is related to the high-dimensional sparse-grid approach which was introduced by Zenger for numerical solving partial differential equations (D˜ung et al. (2021)). Next, we consider the mathematical analysis for deriving error bounds of DNNs (see also Yarotsky (2017) and Yarotsky (2018)). 5.5.1. Error Bounds of DNNs Consider the following gs function with 2s−1 uniformly distributed "triangles". The key observation here is that the function f(x) = x2 can be approximated by linear combinations of the functions gs. Speciﬁcally, fm is a piece-wise linear interpolation of f with 2m + 1 uniformly distributed knots (breakpoints) k 2m , k ∈ {0,...,2m}, such that fn � k 2m � = � k 2m �2 , k ∈ {0,...,2m} (5.39) Thus, the function fm approximates f with the error εm = 2−2(m+1). Applying the linear interpolation method from fm−1 to fm amounts to adjusting it by a function proportional to a sawtooth function given by fm−1(x)− fm(x) = gm(x) 22m . (5.40) Hence, it holds that fm(x) = x− m ∑ s=1 gs(x) 22s . (5.41) Proposition 4. Given M > 0 and ε ∈ (0,1), there is a ReLU network η with two input units that imple- ments a function �× : R2 → R such that (a) for any inputs x,y, if |x| ≤ M and |y| ≤ M, then ���×(x,y)−xy �� ≤ ε (5.42) (b) if x = 0 or y = 0, then �×(x,y) = 0. Remark 32. Notice that the derivations of these error bounds relies on piecewise linear (or quadratic) approximations of the function fm. Some relevant discussion is given by Pottmann et al. (2000). 96Proof. Let �fsq,δ be the approximate squaring function such that �fsq,cδ(0) = 0 and ��� �fsq,δ(0)−x2��� < δ, for x ∈ [0,1]. (5.43) Assume without loss of generality that M ≥ 1 and set �×(x,y) = M2 8 � �fsq,δ �|x+y| 2M � − �fsq,δ � |x| 2M � − �fsq,δ � |y| 2M �� (5.44) where δ = 8ε 3M2. Theorem 17. For any d,n and ε ∈ (0,1), there is a ReLU network architecture that (i) is capable of expressing any function from Fd,n with error ε; (ii) has the depth at most c(ln(1/ε) + 1) and at most cε−d/n(ln(1/ε) + 1) wights and computation units, with some constant c = c(d,n). Remark 33. Notice that the main idea of the proof of the above theorem is the use of piecewise linear approximation of quadratic functions (see, Pottmann et al. (2000)). For example, one can consider the squared distance of the two vertices vi,vj in the Caley-Klein metric induced by Q which is deﬁned by d2� vi,vj � = � vi −v j �⊤Q � vi −v j � (5.45) In other words, according to their Theorem 12, an L∞−optimal piece-wise linear approximant over a triangulation of R2 to a quadratic bivariate function f whose quadratic form is indeﬁnite is deﬁned over a triangulation which is regular in the pseudo-Euclidean metric induced by f. Thus, the linear approximant interpolates the function values at the vertices of the triangulation. Discussion about the use of triangu- lation can be found in the studies of Montanelli and Du (2019). In particular, the curse of dimensionality can be lessened by establishing a connection with sparse grids. Proof. The ﬁrst key part of the proof is to approximate f by a sum-product combination f1 of local Taylor polynomials and one-dimensional piecewise-linear functions. Moreover, we can also approximate f1 using a neural network. Let N be a positive integer. Consider a partition of unity formed by a grid of (N +1)d functions φm on the domain [0,1]d such that ∑ m φm(x) ≡ 1, x ∈ [0,1]d. (5.46) 97Here, we have that m = (m1,...,md) ∈ {0,1,...,N}d, and the function φm is deﬁned as the product below φm(x) = d ∏ k=1 ψ � 3N � xk − mk N �� (5.47) where ψ(x) =        1, |x| < 1, 0, 2 < |x|, 2−|x|, 1 ≤ |x| ≤ 2. (5.48) Moreover, it holds that supp φm ⊂ � x : ���xk − mk N ��� < 1 N ∀ k � . (5.49) Then, for any m ∈ {0,...,N}d, consider the degree−(n−1) Taylor polynomial for the function f at x = m N Pm = ∑ n:|n|<n Dn f n! ���� x= m N � x− m N �n , (5.50) with the usual notation n! = d ∏ k=1 nk! and � x− m N �n = d ∏ k=1 � xk − mk N �nk . (5.51) Therefore, an approximation to f by f1 is given by f1 = ∑ m∈{0,...,N}d φmPm. (5.52) Therefore, for the proof we bound the approximation error using the Taylor expansion of f as | f(x)− f1(x)| = ����∑ m φm(x) � f(x)−Pm(x) ����� ≤ ∑ {m:|xk− mk N |< 1 N ∀ k} ��f(x)−Pm(x) �� ≤ 2d max {m:|xk− mk N |< 1 N ∀ k} ��f(x)−Pm(x) �� ≤ 2ddn n! � 1 N �n max n:|n|=n ess sup x∈[0,1]d |Dn f(x)| ≤ 2ddn n! � 1 N �n . 98We have that the coefﬁcients of the polynomials Pm are uniformly bounded for all f ∈ Fd,n : Pm(x) = ∑ n:|n|<n am,n � x− m N �n , |am,n| ≤ 1. (5.53) The main intuition of the above result is that we can construct a network architecture capable of approx- imating with uniform error ε 2 any function with a similar form as the function f1, assuming that N, has the form N = ⌊ � n! 2ddn ε 2 �− 1 n⌋ and the polynomials can be written as expression (5.53). Therefore, expanding f1 based on the above conditions we obtain f1(x) = ∑ m:{0,...,N}d ∑ n:|n|<n am,nφm(x) � x− m N �n . (5.54) Thus, we consider the approximation of the product φm(x) � x− m N �n. 5.5.2. Topological Space for DNN Architecture Assumption 10 (Farrell et al. (2021)). Assume that f∗ lies in the Holder ball W β,∞ � [−1,1]d� , with smoothness β ∈ N such that f∗(x) ∈ W β,∞� [−1,1]d� := � f : max α,|α|≤β ess sup x∈[−1,1]d |Dα f(x)| ≤ 1 � , (5.55) where α = (α1,....,αd), |α| = α1 +...+αd and Dα f, is the weak derivative. Then, we focus based on Assumptions 1 and 2 above, on deriving high-probability bounds. Assumption 11. Let f∗ lie in a class F. For the feedforward network class FDNN, let the approximation error εDNN be εDNN := sup f∗∈F inf f ∈FDNN ∥ f − f∗∥∞. (5.56) Remark 34. In other words, many recent studies focus on the aspects of how DNN solve the problem of the curse of dimensionality. In other words, various studies consider the approximation for compo- sitional functions by deep neural networks. Speciﬁcally, by assuming that the sparse structure in each composition layer, we can show that the total compositional function in high dimensional space owns a low dimensional property, in the sense that it can be approximated by DNN with a convergence rate only dependent on the intrinsic low dimension. 995.6. Statistical Inference with Stochastic Gradient Descent 5.6.1. Literature Review Notice that both for the estimation of Shallow as well as Deep Neural Networks, useful optimization al- gorithms under examination are the Gradient Descent and Stochastic Gradient Descent. Related studies that discuss the implementation and applications of these algorithms are presented by Chen et al. (2020). Furthermore, Toulis and Airoldi (2017) study the asymptotic theory analysis and ﬁnite-sample proper- ties of these estimators (see also Tran et al. (2015)). Moreover, a selective overview of deep learning estimation methodologies are discussed in the study of Fan et al. (2021a) and Braun et al. (2021). • The framework of Zhang and Simon (2022) consider a sieve SGD algorithm which is a nonpara- metric estimation approach in the Sobolev ellipsoid space. Moreover, Chen and Liao (2014) pro- pose a framework for Sieve M estimation on irregular parameters. • SGD procedures involve parameter updates that are implicitly deﬁned. Implicit updates shrink standard SGD updates. The amount of shrinkage depends on the observed Fisher information matrix, which does not need to be explicitly computed. • Theoretical analysis gives a full characterization of the asymptotic behaviour of both standard and implicit SGD-based estimators, including ﬁnite-sample error bounds. Moreover, Shao and Zhang (2022) present estimation error bounds of M-estimators and SGD algorithms. Following Chen et al. (2020), assume that we want to estimate the true parameter θ0 ∈ Rp of a distribution f from i.i.d data points (Xi,Yi) such that conditional on covariate Xi ∈ Rp outcome Yi ∈ Rd is distributed according to f(Yi;Xi,θ0). Such statistical problems reduce to optimization. More speciﬁcally, suppose that our aim is to estimate the true parameter θ0 ∈ Rp of a distribution f from i.i.d data points (Xi,Yi) such that conditional on covariate Xi ∈ Rp outcome Yi ∈ Rd is distributed according to f(Yi;Xi,θ0). Such statistical problems reduce to optimization. θsgd n = θsgd n−1 +γnCn∇log � f � Yn;Xn,θsgd n−1 �� (5.57) where γn is the learning rate sequence. Consider the equivalent moment condition below E � ∇log f(Y;X,θ0)|X � = 0, (5.58) where the expectation corresponds to the true conditional distribution of outcome Y given covariate X. 100Remark 35. From computational perspective, SGD is appealing because it avoids expensive matrix inversions and single data point (Xn,Yn) evaluations. Implicit SGD does not condition on the observed ordering of data points, but conditions on a random ordering instead. θim n = θim n−1 +γnCn∇log � f � Yn;Xn,θ im n �� , (5.59) Therefore, it holds that θim n = argmax θ � − 1 2γn ��θ −θim n−1 ��2 +log[f (Yn;Xn,θ)] � , (5.60) Recall the Fisher Information matrix is denoted ˆ In(θ) = −∇2ℓ(X′ nθ;Yn). Using a Taylor approximation of the gradient ∇log � f � Yn;Xn,θim n �� yields ∆θim n ≈ � I +γn ˆ In(θ0) �−1 ∆θsgd n , (5.61) where ∆θim n = θim n −θ0 and ∆θsgd n = θsgd n −θ0. Remark 36. Notice that these Gradient Descent Algorithms have a required ﬁrst step the initialization of the vector. Although the choice of this initial vector is just an approximation it provides a good starting point so that the algorithmic procedure converges to the optimal choice vector. 5.6.2. Preliminary Theory Throughout, we use ∥x∥p to denote the ℓp−norm of x, ∥X∥ the matrix operator norm of X and ∥X∥∞ = maxi, j|Xi, j| the elementwise ℓ∞−norm of X. For any sequences {an} and {bn} of positive numbers, we write with an ≥ bn and an ≤ bn. Consider the SGD method where the iteration is given by (see, Chen et al. (2021a)) xn = xn−1ηn∇F(xn−1)+ηnξn, (5.62) where ξn := ∇F(xn−1)−∇ f(xn−1,ζn). In particular, the above formulation decomposes the descent into two parts: ∇F(xn−1) which represents the direction of population gradient which is the major driving force behind the convergence of SGD and ξn is a martingale difference sequence under the above as- sumption. Furthermore, it holds that En−1 [ξn] := ∇F(xn−1)−En−1 � ∇ f(xn−1,ζn) � = 0. (5.63) 101Notice that En(.) denotes the conditional expectation En(.|Fn), where Fn is the σ−algebra generated by {ζ1,...,ζn}. Let ∆n := xn −x∗ be the error of the n−th iterate. An equivalent expression gives us ∆n = ∆n−1 −ηn∇F(xn−1)+ηnξn, (5.64) Given the SGD recursion and under suitable assumptions it can be shown that when the step size sequence ni = ηi−α, for i ∈ {1,...,n} with α ∈ (1/2,1), we have that √n.¯∆n ⇒ N � 0,A−1SA−1� (5.65) where ¯∆n = 1 n ∑n i=1 ∆i = ¯xn −x∗. 5.6.3. Assumptions and error bounds Assumption 12 (Strong convexity and Lipschitz continuity of the gradiet). Assume that the objective function F(x) is continuously differentiable and strongly convex with parameter µ > 0, that is, for any x1 and x2, F(x2) ≥ F(x1)+⟨∇F(x1),x2 −x1⟩+ µ 2 ∥x1 −x2∥2 2. (5.66) Further assume that ∇2F(x∗) exists, and ∇F(x) is Lipschitz continuous with a constant LF, that is for any x1 and x2 ∥∇F(x1)−∇F(x2)∥2 2 ≤ LF ∥x1 −x2∥2. (5.67) Remark 37. Notice that strong convexity of F(x) can be assumed to hold in order to derive the limiting distribution of averaged SGD. In fact, the strong convexity of F(x) implies that λmin(A) = λmin � ∇2F(x∗) � ≥ µ is an important condition for parameter estimation and inference. Assumption 13. The following conditions hold for the sequence ξn = ∇F(xn−1)−∇ f(xn−1,ζn): 1. Assume that f(x,ζ) is continuously differentiable in x for any ζ and ∥∇ f(x,ζ)∥2 is uniformly integrable for any x so that En−1ξn = 0. 2. The conditional covariance of ξn has an expansion around x = x∗ such that En−1 � ξnξ ⊤ n � = S +Σ(∆n−1) (5.68) 102Lemma 19. If there is a function H(ζ) with bounded fourth moment, such that the Hessian of f(x,ζ) is bounded by ��∇2 f(x,ζ) �� ≤ H(ζ) (5.69) for all x and ∇ f(x∗,ζ) have a bounded fourth moment. 5.6.4. Estimators for asymptotic covariance Consider two consistent estimators, the plug-in estimator and the batch-means estimator (see, Chen et al. (2021a)). Consider the sample estimate as below An := 1 n n ∑ i=1 ∇2 f(xi−1,ζi), Sn := 1 n n ∑ i=1 ∇ f(xi−1,ζi)∇ f(xi−1,ζi)⊤ (5.70) Consider the following nodewise Lasso approach �γ j = argmin γ j∈Rd−1 1 2n ��D., j −D.,−jγ j��2 2 +λ j ��γ j�� 1 , (5.71) where D., j is the j−th column of the design matrix D and D.,−j is the design submatrix without the j−th column (see also Guo et al. (2022)). Further, one can estimate Ω j, j by �τ j = 1 n � D., j −D.,−j�γ j�⊤D., j. (5.72) Remark 38. Notice that �γ j is the output of a stochastic gradient-based algorithm. 5.6.5. Averaged SGD Algorithms Consider the statistical problem of searching for the minimum point θ0 of a smooth function f(θ) where θ ∈ Θ ⊂ Rd. The stochastic GD method provides a direct way to solve the minimization problem. The algorithm is given as below. Let θ0 ∈ Rd be the initial value and for n ≥ 1, we update θn using θn = θn−1 −γn � ∇ f(θn−1)+ζn � (5.73) where θn = 1 n ∑n−1 i=0 θi. Remark 39. Notice that the convergence rate of E∥θn −θ∗∥2 and E �� ¯θn −θ∗��2 is examined in the literature as well as the normality of √n � ¯θn −θ∗� (see, Shao and Zhang (2022)). Assumption 14. There exists positive constants c2 and β such that ∀ θ with ∥θ −θ∗∥ ≤ β, ��∇2 f(θ)−∇2 f(θ∗) �� ≤ c2∥θ −θ∗∥. (5.74) 1035.7. Sieve Estimation Applications Consider the knowledge transmission through neural networks as a recursive process that focuses on the diffusion of processes. Similar to neurobiological processes in the brain neural networks can activate certain neuros in the brain for transmission of understanding. Stochastic approximations recursively approximate the zeros of an unknown function Ψ(θ), say θ∗, ˆθt+1 = ˆθt +atψ(zt, ˆθt), t = 1,2,... (5.75) where at is a "learning rate" tending to zero and ψ(Zt,θ) is a measurement of Ψ(θ) at time t, inﬂu- enced by random variables Zt. Furthermore, when Ψ(θ) ≡ E � ψ(Zt,θ) � this method yields a recursive implementation of the method of m-estimation of Huber (see, Kuan and White (1994)). The particular methodology can be used to estimate recursively the parameters of nonlinear regression models, such as those arising in neural network applications. Speciﬁcally, ofﬂine nonparametric estimation methods can usually be applied to the case of mixing processes using results for the methods of sieves. On the other hand, online nonparametric estimation methods require convergence to a global optimum of the underlying least squares problem, not just the local optimum. 5.7.1. Asymptotic Properties of Sieve Estimation Another application in which constraint optimization is used is when estimating and simulating neural networks with sieves. In particular, based on a construction of the neural network sieve estimators, in each sieve space Frn, there is a constraint on the ℓ1 norm for which ∑rn i=0|αi| ≤ Vn. Therefore, ﬁnding the nearly optimal function in Frn for Qn( f) is in fact a constrained optimization problem. A classical way to conduct this optimization is through introducing a Lagrange multiplier for each constraint. On the other hand, due to the difﬁculty in ﬁnding an explicit connection between the Lagrange multiplier and the upper bound in the inequality constraint we use instead the subgradient method (see, Shen et al. (2023)). The main idea is to update the parameter {α0,....,αrn} through α(k+1) i = α(k) i −δkg(k), i = 0,...,rn, (5.76) This allows us to investigate the asymptotic properties, including consistency, rate of convergence and asymptotic normality for neural network sieve estimators with one hidden layer. Using the Generalized Dominated Convergence Theorem, we have that E∗ � sup f ∈Fn ����� 1 n n ∑ i=1 εi � f(xi)− f0(xi) � ����� � ≤ 2EεEξ � sup f ∈Fn ����� 1 n n ∑ i=1 ξεi � f(xi)− f0(xi) � ����� � → 0. (5.77) 104Example 24. An application of sieve estimation in econometrics is presented in the study of Su and Hoshino (2016), where the authors propose a framework for sieve instrumental variable quantile regression esti- mation of functional cofﬁcient models. More precisely, one can demonstrate that √n � β 2τ −β2τ � d→ N � 0,τ(1−τ) lim K→∞S2ΩBτΨKΩ⊤ BτS⊤ 2 � . (5.78) Consequently, one can conduct statistical inference on β2τ as usual by estimating the AVC matrix given above. Alternatively, one can apply the boostrap method to obtain standard errors and make inference. Furthermore, we can construct a speciﬁcation test where we are interested in testing the null hypothesis H0 : δ1τ(Ui) ≡ Sδτ(Ui) = δ1τ. (5.79) almost surely for some parameter δ1τ ∈ Rr. Under H0, r of the (k1 +k2) functional coefﬁcients are con- stant, whereas under the alternative hypothesis H1, we have that at least one of the functional coefﬁcients in δ1τ(.) is not constant. Note that within the setting of nonstationary time series a relevant framework is presented by Dong et al. (2021). 5.7.2. Sieve Estimation for Panel Data A growing interest in the estimation of panel data models with cross-section dependence but most of the literature focuses on the linear speciﬁcation of the regression relationship. In particular, let yit, t = 1,...,n and t = 1,...,T be the i−th cross section unit at time t. We suppose that yit is generated according to the following semiparametric panel data generating process (see, Su and Jin (2012)) yit = gi(xit)+γ′ 1i f1t +eit, (5.80) where xit ∈ Xi ∈ Rd is a vector of observed individual-speciﬁc regressors on the i−th cross-section unit at time t. In this section, we focus on the sieve estimation of semiparametric panel data models with multi-factor error structure. We develop the asymptotic theory under fairly general conditions when both the cross-section and time-dimensions are large. For instance, if only homogeneous regression relationships are of interest, the time dimension need not pass to inﬁnity. Moreover one can consider testing the constancy of the nonparametric relationship over individuals in the presence of multi-factor error structure. In addition the individual speciﬁc regressors have the following structure xit = Γ′ 1i f1t +Γ′ 2i f2t +vit, (5.81) In practice, one may also be interested in estimating a restricted submodel of the following form yit = g(xit)+γ′ 1i f1t +eit. (5.82) 1055.7.3. Sieve M inference on irregular parameters We follow the framework proposed by Chen and Liao (2014) who consider plug-in sieve M estimators. More precisely, we assume that the data {Zi}n i=1 is a random sample from the distribution of Z deﬁned on an underlying complete probability space. Let L (.,.) : Z × A �→ R be a measurable function and E[L (Z,α)] be a population criterion. For simplicity we assume that there is a unique α0 ∈ (A ,dA) such that E[L (Z,α0)] > E[L (Z,α)] for all α ∈ (A ,dA) with dA(α,α0) > 0. Different models in economics correspond to different choices of the criterion function E[L (Z,α)] and the parameter space (A ,dA). A model does not need to be correctly speciﬁed and α0 could be pseudo true parameter. In this paper, we are interested in the estimation of and inference of a functional f (α0) via the method of sieves. Let An be a sieve space for the whole parameter space A . Then an approximate sieve M estimator �an ∈ An of α0 solves 1 n n ∑ i=1 L (Zi, �an) ≥ sup �an∈An 1 n n ∑ i=1 L (Zi,an)−op �1 n � (5.83) Example 25 (A partially additive quantile regression model, see Chen and Liao (2014)). Suppose that the i.i.d data � Yi,X′ i = � X′ 0i,X1i...,Xqi ��n i=1 is generated according to the process below: Y −i = X′ 0iθ0 + q ∑ j=1 hj,0 � Xj,i � +Ui, (5.84) with E � 1{Ui ≤ 0}|Xi � = τ ∈ (0,1), where dim(X0) = dθ, dim(Xj) = 1 for j = 1,...,q, dim(X) = dθ + q and dim(Y) = 1. Let α0 = (θ0,h0), where θ0 ∈ Θ and h0 = � h1,0,...,hq,0 � ∈ H . A functional of interest could be for instance, f (α0) = λ ′θ0 for any λ ∈ Rdθ with λ ̸= 0. This is an extension of the parametric quantile regression model of Koenker and Bassett Jr (1978) to allow for unknown additive functions ∑q j=1hj,0 � Xj,i � . We can estimate α0 = (θ0,h0) by the sieve QR estimator �αn = � �θn,�hn � that solves max θ∈Θ,h∈Hn n ∑ i=1 � 1 � Yi ≤ X′ 0,iθ + q ∑ j=1 hj,0 � Xj,i � � −τ � × � Yi −X′ 0,iθ − q ∑ j=1 hj,0 � Xj,i � � (5.85) Speciﬁcally, if A is a Holder, Sobolev or Besov space of functions with bounded supports and An is a linear sieve space consisting of spline, wavelet, or cosine bases, then one typically has dA (πn (α0),α0) = ∥πn(α0)−α0∥sup (5.86) Given the existing results on the convergence rates for sieve M estimators of semi-nonparametric models, we can restrict our attention to a shrinkage neighborhoud of α0. Let δA,n = δ ∗ A,nγn and δs,n = δ ∗ s,nγn, where γn is a positive sequence that diverges to inﬁnity very slowly (say log log n) such that δA,n = 0(1). 106In the example above, we have that L (Z,α) = � 1{Y ≤ α(Y)−τ} �� Y −α(Y) � (5.87) with α(Y) = X′ 0,iθ +∑q j=1hj,0 � Xj,i � . For any α ∈ A , we deﬁne a strong metric ∥.∥s as below ∥α −α0∥s = E ������X′ 0,i(θ −θ0)+ q ∑ j=1 � hj � Xj � −hj,0 � Xj,i �� ����� � (5.88) By the deﬁnitions of the metrics, we have that for any α ∈ A , (see, Chen and Liao (2014)) ∥α −α0∥2 = E � f(0|X)|α(X)−α0(X)|2 � (5.89) Let v∗ n = � v∗ θ,n,v∗ h,n � be the Riesz representer of the functional ∂ f (α0) ∂α [v] on Vn. Denote with v∗ n(X) = X′ 0,iv∗ θ,n + q ∑ j=1 v∗ hj,n � Xj � (5.90) Then, the variance of the plug-in sieve M estimator f (�αn) of f (α0) is expressed as below ∥v∗ n∥2 sd = τ(1−τ)E ���v∗ n(X) ��2 � = τ (1−τ)∥v∗ n∥2 s . (5.91) Proposition 5 (Chen and Liao (2014)). Under Assumptions and Conditions we have that √n f (�αn)− f (α0)   τ (1−τ)E   �����X′ 0v∗ θ,n + q ∑ j=1 v∗ hj,n � Xj � ����� 2     1/2 → N (0,1). (5.92) An application to speciﬁcation testing in econometrics using invariance principles on Sobolev spaces is proposed by Kuersteiner (2019). Furthermore, an application of general sieves for inference purposes in nonparametric time series regression models identiﬁed with conditional moment restrictions is studied by Chen et al. (2022). In particular, nonlinear sieve learning employs a more general functional class of sieves that can approximate nonlinear functions of high dimensional variables (such as in the case of an inﬁnite-dimensional parameter space). 1075.8. Further Statistical Algorithms in Economic Applications This section is motivated from the Seminar of Dennis Kristensen, Professor of Economics at the Depart- ment of Economics of University College London (UCL), who presented the paper with title: "Iterative estimation of structural models with an application to perturbed utility models", in May 2023 at the Department of Economics, University of Exeter Business School. Speciﬁcally, the particular steam of literature is based on the framework proposed by Hotz et al. (1994) and is relevant to estimators for dynamic models of discrete choice, that is, a general class of iterative estimators. Take for example, the classical likelihood estimation, which is a statistical methodology that iteratively solves the statistical problem until convergence to an estimate that is close to the true unknown parameter of interest. In the same spirit, without sacriﬁcing efﬁcient statistical estimation we assume that such a sequence of estimators converge to the full solution of the optimization problem. These optimization steps are summarized as below: 1. Obtain a nonparametric estimate of the solution as the global optimizer from a local estimator ˆθ. 2. Use ﬁrst order conditions to update the initial conditions estimate. 3. Use the updated solution path based on the estimated parameters of the previous steps. 4. Update solution using parameter estimates. 5. Repeat Steps 2-4 until convergence. Remark 40. Notice that the existence of some noise due to the parametric estimation is unavoidable. Therefore, the larger the presence of noise during the ﬁrst stage of estimation the more iterations will be needed for algorithmic convergence (uncertainty quantiﬁcation). Furthermore, the error bound of the true parameter vector based on the estimated parameters from the above algorithmic procedure is heavily based on the assumed estimator of the ﬁrst stage (large sample theory). Thus, the resulting sequence of iterative estimators is assumed to have asymptotic behaviour that is within the vicinity of the domain of attraction; although might not necessarily result in solving the structural model under examination. A different stream of literature considers doubly robust estimation methods which have been developed in various semiparametric problems, including partially linear models, instrumental variable analysis, and dimension reduction among others. Therefore, as a somewhat under-appreciated limit result, we point out that the familiar least-squares estimator for each individual coefﬁcient in linear regression is doubly robust in the context of a partially linear model. This result is also closely related to debiased Lasso estimation in high-dimensional linear regression (e.g., see Guo et al. (2022)). 108A Elements of Weak Convergence of Empirical Processes Example 26. (M-dependent sequences) Let Xn,n ∈ Z be a stationary sequence with EXn = 0, EX2 n < ∞. Assume that σ �� Xj, j ≤ 0 �� and σ �� Xj, j ≥ M �� are independent. A1. Sub-Gaussian processes Notice that Sub-Gaussian processes satisfy the increment bound ∥Xs −Xt∥ψ2 ≤ √ 6d(s,t). Therefore, the general maximal inequality leads for sub-Gaussian processes to a bound in terms of an entropy integral. Lemma 20. Let {Xt : t ∈ T} be a separable sub-Gaussian process. Then, for every δ > 0, E sup d(s,t)≤δ |Xs −Xt| ≤ K � δ 0 � logD(ε,d)dε, (A.1) for a constant K. In particular, for any t0, E sup t |Xt| ≤ E|Xt0|+K � ∞ 0 � logD(ε,d)dε. (A.2) Proof. Apply the general maximal inequality with ψ2(x) = ex2 −1 and η = δ. Since ψ−1 2 (m) = � log(1+m), we have that ψ−1 2 � D2(δ,d) � ≤ √ 2ψ−1 2 � D(δ,d) � . Thus, the second term in the maximal inequality can ﬁrst be replaced by √ 2δψ−1� D(δ,d) � , (at the cost of increasing the constant) gives ����� sup d(s,t)≤δ |Xs −Xt| ����� ψ2 ≤ K � δ 0 � log(1+D(ε,d))dε. (A.3) Symmetrization Let ε1,...,εn be i.i.d Rademacher random variables. Replace the empirical process f �→ (Pn −P) f = 1 n n ∑ i=1 � f(Xi)−P f � , (A.4) with the corresponding symmetrized empirical process deﬁned by f �→ Po n f = 1 n n ∑ i=1 εi f(Xi),where ε1,...,εn are independent of (X1,...,Xn). Both processes have mean function zero. Lemma 21. For every nondecreasing, convex Φ : R → R and class of measurable function F E∗Φ � ∥Pn −P∥F � ≤ E∗Φ � 2 ��P0 n �� F � (A.5) 109A2. Clivenko-Cantelli theorems In this section, we prove two types of Clivenko-Cantelli theorems. The ﬁrst theorem is the simplest and is based on entropy with bracketing. The second theorem, uses random L1−entropy numbers and is proved through symmetrization followed by a maximal inequality. Deﬁnition 12. (Covering numbers) The covering numbers N (ε,F,∥.∥) is the minimal number of balls {g : ∥g− f∥ < ε} of radius ε needed to cover the set F. The entropy (without bracketing) is the loga- rithm of the covering numbers. Deﬁnition 13. (bracketing numbers) Given two functions l and u, the bracket [l,u] is the set of all func- tions f with l ≤ f ≤ u. An ε−bracket is a bracket [l,u] with ∥u−l∥ < ε. Then, the bracketing number N[ ] (ε,F,∥.∥) is the minimum number of ε−brackets needed to cover F. The entropy with bracketing is the logarithm of the bracketing number. Theorem 18. Let F be a class of measurable functions such that N[ ] (ε,F,L1(P)) < ∞ for every ε > 0. Then, F is Glivenko-Cantelli. Proof. Fix ε > 0. Choose ﬁnitely many ε−brackets [ℓi,ui] whose union contains F and such that P(ui − ℓi) < ε for every i. Then, for every f ∈ F, there is a bracket such that (Pn −P) f ≤ (Pn −P)ui +P(ui − f) ≤ (Pn −P)ui +ε (A.6) Consequently, sup f ∈F (Pn −P) ≤ max i (Pn −P)ui +ε. (A.7) The right side converges almost surely to ε by the strong law of large numbers for real variables. Theorem 19. Let F be a P−measurable class of measurable functions with envelope F such that P∗F < ∞. Let FM be the class of functions f1{F ≤ M} when f ranges over F. If logN[ ] (ε,F,L1(Pn)) = o∗ p(n) for every ε and M > 0, then ∥Pn −P∥∗ F → 0 both almost surely and in mean. In particular, F is GC. Proof. By the symmetrization lemma, measurability of the class F, and Fubini’s theorem E∗ ∥Pn −P∥F ≤ 2EXEε ����� 1 n n ∑ i=1 εi f(Xi) ����� F ≤ 2EXEε ����� 1 n n ∑ i=1 εi f(Xi) ����� FM +2P∗F {F > M} by the triangle inequality, for every M > 0. 110Thus, for sufﬁciently large M, the last term is arbitrarily small. To prove convergence in mean, it sufﬁces to show that the ﬁrst term converges to zero for ﬁxed M. Fix X1,...,Xn. If G is an ε−net in L1(Pn) over FM, then the following inequality holds Eε ����� 1 n n ∑ i=1 εi f(Xi) ����� FM ≤ Eε ����� 1 n n ∑ i=1 εi f(Xi) ����� G +ε. (A.8) The cardinality of G can be chosen equal to N(ε,FM,L1(Pn)). Bound the L1−norm on the right using the Orlicz-norm for ψ2(x) = exp(x2)−1, and using the maximal inequality to ﬁnd that the last expression does not exceed a multiple of � 1+logN(ε,FM,L1(Pn)) sup f ∈G ����� 1 n n ∑ i=1 εi f(Xi) ����� ψ2|X +ε, (A.9) where the Orlicz-norm ∥.∥ψ2|X are taken over ε1,...,εn with X1,...,Xn ﬁxed. By Hoeddding’s inequality, then can be bounded by � 6/n � Pn f 2�1/2, which is less than � 6/nM. A3. Donsker Theorems Uniform Entropy: We establish the weak convergence of the empirical process under the condition that the envelope function F be square integrable, combined with the uniform entropy bound � ∞ 0 � logN � ε,FQ,2,L2(Q) � dε < ∞. (A.10) Theorem 20. Let F be a class of measurable functions that satisﬁes the uniform entropy bound. Let the class Fδ = � f −g : f,g,∈ F,∥ f −g∥P,2 < δ � and F 2 ∞ be P−measurable for every δ > 0. If P∗F2 < ∞, then F is P−Donsker. Proof. Let δn → 0 be a ﬁxed constant. Using Markov’s inequality and the symmetrization lemma: P∗ � ∥Gn∥Fδn > x � ≤ 2 xE∗ ����� 1 √n n ∑ i=1 εi f(Xi) ����� Fδn . (A.11) Therefore, we can see that the inner expectation is bounded as below Eε ����� 1 √n n ∑ i=1 εi f(Xi) ����� Fδn ≤ � ∞ 0 � logN � ε,Fδn,L2(Pn) � dε. (A.12) Notice that for large values of ε, the set Fδn ﬁts in a single ball of radius ε around the origin, in which case the integrand is zero.Furthermore, we have that the covering numbers of the class Fδn are bounded by covering numbers of F∞ = {f −g : f,g ∈ F} (see, Vaart and Wellner (2013)). 111B Elements of Stochastic Processes B1. Asymptotic Equicontinuity A class of measurable functions is called pre-Gaussian if the (tight) limit process G in the uniform central limit theorem exists. We focus on Brownian bridge processes. Firstly, it is desirable to have a more concrete description of the tightness property of a Brownian bridge and hence of the motion of pre-Gaussianity. More speciﬁcally, tightness of a random map into ℓ∞(F) is closely connected to continuity of its sample paths. A Donsker class F satisﬁes a stronger condition that the sequence Gn is asymptotically tight. Therefore, this entails replacing the condition that the sample paths of the limit process are continuous by the condition that the empirical process is asymptotically continuous. Thus, for every ε > 0 it holds that lim δ→0 lim sup n→∞ P∗ � sup ρp(f −g)<δ ��Gn( f −g) �� > ε � = 0. (B.1) Related discussion can be found in Newey (1991) and Hagemann (2014) among others. Maximal Inequalities Therefore, it follows that the law of large numbers and the central limit theorem are concerned with showing that the supremum of real-valued variables are converges to zero. Thus, to show these results we need to make use of maximal inequalities that bound probabilities involving suprema of random variables. Notice that bounds on ﬁnite suprema can be extended to general maximal inequalities with the help of the chaining method. CLT in Banach Spaces In this particular section it is shown that any CLT in a Banach space can be stated in terms of empirical processes. Furthermore, a class of maximal inequalities can be used to establish the asymptotic equicontinuity of the empirical process. Theorem 21. Let ψ be convex, nondecreasing, nonzero function with ψ(0) = 0 and lim sup x,y→∞ ψ(x)ψ(y)/ψ(cxy) < ∞, for some constant c. Let � Xt : t ∈ T � be a separable stochastic process with ∥Xs −Xt∥ψ ≤ Cd(s,t), for every s,t, (B.2) for some semi-metric d on T and a constant C. Then for any η,δ > 0, ����� sup d(s,t)≤δ ��Xs −Xt �� ����� ψ ≤ K �� η 0 ψ−1� D(ε,d) � dε +δψ−1� D2(ε,d) �� , (B.3) for a constant K depending on ψ and C only. 112Corollary 9. The constant K can be chosen such that ∥Xs −Xt∥ψ ≤ K � diamT 0 ψ−1� D(ε,d) � dε, (B.4) where diamT is the diameter of T. In practise, maximal inequality means that no point can be added without destroying the validity of the inequality. Furthermore, a stochastic process is called sub-Gaussian with respect to the semimetric d on its index set if P � |Xs −Xt| � ≤ 2e− 1 2x2/d2(s,t), for every s,t ∈ T,x > 0. (B.5) Another example, is the Rademacher process given by Xα = n ∑ i=1 αiεi, α ∈ Rn, (B.6) for Rademacher variables ε1,...,εn. Therefore, by Hoeffding’s inequality, this is sub-Gaussian for the Euclidean distance d(a,b) = ∥a−b∥. Tightness under an Increment Bound We focus on deriving a general CLT for empirical processes through the application of maximal inequalities. Example 27. Let � Xn(t) :t ∈ [0,1] � be a sequence of separable stochastic processes with bounded sample paths and increments satisfying the following condition E ��Xn(s)−Xn(t) ��p ≤ K |s−t|1+r , (B.7) for constants p,K,r > 0 independent of n. Assume that the sequences of martingales � Xn(t1),...,Xn(tk) � converge weakly to the corresponding marginals of a stochastic process � X(t) : t ∈ [0,1] � . Then, there exists a version of X with continuous sample paths and Xn ⇒ X in ℓ∞[0,1]. Hence, also in D[0,1] or C[0,1], provided every Xn has all its sample paths in these spaces. 113B2. LLNs for Hilbert Space-Valued Mixingales Example 28. (Regression via Orthonormal Bases) Let {Xt} and {Yt} be real-valued random sequences (see, also Hu (2004)): E[Yt|Xt = x] := θ0(x) (B.8) Deﬁne εt ≡ Yt −θ0(Xt), and suppose E � ε2 t |Xt � = ρ2 t ≤ σ2 < ∞. Suppose that, for all t, Xt has the same marginal density f with bounded support and 0 < inf f(x) ≤ sup f(x) < ∞ where M0(x) ≡ θ0(x) and f(x) belong to an inﬁnite-dimensional separable Hilbert space H with inner product induced norm | . |. Let � gj � be an orthonormal basis for H and {Jn} a nondecreasing integer sequence. Estimate M0 as ˆMn(x) = ∑ 1≤j≤Jn � n−1 n ∑ t=1 Ytgj(Xt) � gj(x). (B.9) For known f, estimate θ0 by ˆθn = ˆMn/ f. For unknown f, estimate θ0 by ˆθn = ˆMn/ ˆfn, ˆfn ≡ ∑ 1≤j≤Jn � n−1 n ∑ t=1 gj(Xt) � gj(x). (B.10) Furthermore, if {Xt} and {Yt} are R−valued near-epoch dependent (NED) functions of some mixing ran- dom sequences, � ˆMn −E ˆMn � and � ˆfn −E ˆfn � , become weighted sums of Hilbert space-valued mixingale arrays. Our results, in the next section can establish the convergence of E � | ˆθn −θ0| � . For example, when H = L2(R), we obtain that � R � ˆθn(x)−θ0(x) �2 dx → 0, in probability. Theorem 22. Let � Wn,i,F n,i� be an Lp(H) mixingale with p ≥ 1. (i) If p ≥ 2, then there exists a doubly inﬁnite summable sequence of positive constants deﬁned with a ≡ {am : −∞ < m < ∞} where am = a−m such that E  max j≤k ����� j ∑ i=1 Wn,i ����� 2  ≤ K(ψ,a) k ∑ i=1 c2 n,i, (B.11) K(ψ,a) = 4 � ∑ −∞<m<∞ am �� a−1 0 � ψ2 0 +ψ2 1 � +2 ∞ ∑ m=1 ψ2 m|a−1 m −a−1 m−1| � (B.12) (ii) If 1 < p ≤ 2, then there exists a constant Cp > 0 depending only on p such that �����max j≤k ����� j ∑ i=1 Wn,i ����� ����� p ≤ Cp ∑ −∞<m<∞ � k ∑ i=1 E ���W m n,i ��p� �1/p ≤ 2Cp ∞ ∑ m=1 ψm k ∑ i=1 � cp n,i �1/p , where W m n,i ≡ E � Wn,i|F n,i−m� −E � Wn,i|F n,i−m−1� . 114C Elements of Bayesian Statistics "...the simple idea of splitting a sample in two and then developing the hypothesis on the basis of one part and testing it on the remainder may perhaps be said to be one of the most seriously neglected ideas in statistics...". The applications of the Bayesian framework in economics, econometrics and statistics are widely spread and it is worth mentioning some key elements here, especially due to the fact that model selection is commonly presented using the Bayesian approach. Generally, statistical problems are concerned for the model f(x|θ), such that θ ∈ Θ with the associated hypothesis testing of interest is formulated as below: H0 : θ = θ0 vs θ ̸= θ0, (C.1) where the data X = � X1,...,Xn � i.i.d ∼ f(X|θ). Denote the prior probability function with π(θ), then one can construct the observed posterior π(θ|X), assumed to be a proper density function even if the prior is im- proper (see, ). Next, one needs to consider what we compare π(θ|X) with. Assume that we can generated data Y = � Y1,...,Yn � i.i.d ∼ f(Y|θ0) under the null hypothesis. Using the same prior we can construct π(θ|Y) and hence we can deﬁne the expected posterior under the null; π0(θ) = � π(θ|y) f(y|θ0)dy, where y is an n−vector. Then, the test statistic of interest is the KL divergence between the expected posterior under the null and the observed posterior such that T(X) = � π0(θ)log � π0(θ) π(θ|X) � dθ. (C.2) Therefore, it can be shown that T(X) is related to the Bayes factor. Example 29. Consider the probability distributions P(x|H0) and P(x|H1). Moreover, assume some prior knowledge on prior probabilities P(H0) and P(H1). Then, Bayes theorem combines the prior probabilities and the data to produce posterior probabilities P(H0|x) and P(H1|x). Therefore, the transformation of prior to posterior itself represents evidence provided by the data, which takes the following form P(Hk|x) = P(x|Hk)P(Hk) P(x|H0)P(H0)+P(x|H1)P(H1) (C.3) In other words, the Bayes factor represents the the likelihood ratio. When there is an unknown param- eter θk, corresponding to hypothesis Hk, the Bayes factor is the marginal likelihood ratio, where the marginal likelihood densities mk(x) are obtained by integrating over the parameter space with respect to the speciﬁed prior π(θk|Hk), such that mk(x) = � fk(x|θk)πk(θk)dθk, for k ∈ {0,1}. (C.4) However, there are difﬁculties with the Bayes factor when prior information about the unknown parame- ters of the models is weak, in particular, with the use of improper priors Chen and Walker (2021). 115Below, we present some popular econometric models. Some relevant literature that studies Bayesian asymptotics within the time series econometric context include among others Kim (1994). Example 30. Consider the autoregressive model yt = ρyt−1 +εt (C.5) Without imposing the normality assumption on the innovation sequences, we can consider the asymptotic posterior distribution. A posterior density is deﬁned as below (see, e.g., Kim (1994)) PT(g,YT(ω)) = � G π(θ|YT(ω))dθ (C.6) The Bayesian posterior is determined by the prior and the likelihood function. Therefore, in order to verify the asymptotic normality of the posterior distribution, then we need to determine the limiting behaviour of each of the components comprising the prior and the posterior distributions. Additional useful reading within the context of model selection and empirical bayes include among others Varin et al. (2011)14, Efron (2014) as well as Jewson and Rossell (2022)15. Moreover, a framework for Bayesian Inference in Econometric Models using Monte Carlo Integration is proposed by Geweke (1989). Example 31. Consider the structural regression model given by Yt = α +βXt−1 +Ut (C.7) Xt = µ +ρXt−1 +Vt (C.8) where (Ut,Vt)⊤ is a sequence of independent and identically distributed random vectors with means zeros and ﬁnite variances. As it is well documented in the literature the least squares estimator for β based on the predictive regression is biased in ﬁnite sample behaviour due to the correlation structure between the innovation sequences of the equations of the system Ut and Vt. Hence, several bias-corrected estimators and tests for both stationary (|ρ| < 1) and nonstationary or nearly nonstationary, ρ = � 1− c n � have been proposed in the literature. In other words, inference on the parameter β of the model is challenging due to the fact that the asymptotic distribution of an estimators or a test statistic depends heavily on whether Xt is stationary or nearly integrated or unit root, and whether the model intercept φ, of the nonstationary autoregressive equation is zero or not. Therefore, is of paramount importance to have a uniﬁed inference approach to avoid making a mistake in characterizing the predicting variable. The persistence endogeneity of covariates in predictive regressions has been studied by several authors. 14Professor Cristiano Varin gave a seminar with title "An approximate empirical Bayes approach to paired comparisons", at the S3RI Departmental Seminar Series at the University of Southampton on the 28th of April 2022. 15Professor David Rossell gave a seminar with title "Improper models for data analysis", at the S3RI Departmental Seminar Series at the University of Southampton on the 10th of February 2022. 116Therefore an estimation methodology is to employ the empirical likelihood approach which is deﬁned: Ln(β) = sup � n ∏ t=1 (npt) : p1 ≥ 0,..., pn ≥ 0, n ∑ t=1 pt = 1, n ∑ t=1 ptZt(β) � (C.9) where Zt(β) = � Yt − βXt−1 � Xt−1 �� 1+X2 t−1. Then, based on the Lagrange multiplier optimization method it follows that ℓn(β) := −2logLn(β) = 2 n ∑ t=1 log � 1+λZt(β) � (C.10) Then, taking the ﬁrst-order-condition for some λ = λ(β) satisﬁes n ∑ t=1 Zt(β) 1+λZt(β) ≡ 0. (C.11) Remark 41. The ﬁrst rigorous work to deﬁne and construct tests which are asymptotically optimal was Wald (1943). He argued that maximum likelihood estimators may be asymptotically sufﬁcient for de- tecting local deviations from the null hypothesis and showed that the Wald test - is asymptotically most stringent: the asymptotic power function is closest to the asymptotic envelope power function in the minimax sense in local neighborhoods of the null hypothesis. In particular, Wald tests can be used for more general problems if we can ﬁnd asymptotically efﬁcient estimates for the parameters of interest - an estimator ˆϑn with √n � ˆϑn −ϑn � asymptotically N � 0,B∗−1� under every � ϑn(hϑ),ηn(hn) � . Moreover, the notion of asymptotically uniformly most powerful (AUMP), AUMPU (unbiased) and AUMPI (invariant) tests are useful in semiparametric econometrics. In particular, characterization is done by stating the asymptotic local power function. Furthermore, sufﬁcient for optimality is that a test be equivalent to a canonical effective score test - an optimal test requiring knowledge of nuisance pa- rameters. In addition, Stein’s notion of adaptation implies replacing, in tests which are optimal when certain nuisance parameters are known, these parameters by estimates without affecting the asymptotic performance of the test. The particular aspect is the equivalent of large-sample studentization, a variance parameter can be replaced by an estimate without large-sample penalty. Note that a statistical problem is invariant under locally linear transformation of the parameter of interest, iff standardized effective score tests are rotation invariant (asymptotically). Furthermore, another large stream of literature in econometrics focuses on estimation and inference meth- ods for conditional moment models which can be extended within a high-dimensional environment using techniques based on large sample approximations (see, Ai and Chen (2003) and Domínguez and Lobato (2004) among others). A Bayesian perspective is presented by Chib et al. (2022). 117References Abadir, K. M., Hadri, K., and Tzavalis, E. (1999). The inﬂuence of var dimensions on estimator biases. Econometrica, 67(1):163–181. Adamek, R., Smeekes, S., and Wilms, I. (2023). Lasso inference for high-dimensional time series. Journal of Econometrics, 235(2):1114–1143. Ai, C. and Chen, X. (2003). Efﬁcient estimation of models with conditional moment restrictions contain- ing unknown functions. Econometrica, 71(6):1795–1843. Amann, N. and Schneider, U. (2018). Uniform asymptotics and conﬁdence regions based on the adaptive lasso with partially consistent tuning. Econometric Theory, pages 1–26. Angelini, E., Di Tollo, G., and Roli, A. (2008). A neural network approach for credit risk evaluation. The quarterly review of economics and ﬁnance, 48(4):733–755. Aschard, H., Guillemot, V., Vilhjalmsson, B., Patel, C. J., Skurnik, D., Ye, C. J., Wolpin, B., Kraft, P., and Zaitlen, N. (2017). Covariate selection for association screening in multiphenotype genetic studies. Nature genetics, 49(12):1789–1795. Ashby, W. R. (1957). An introduction to cybernetics. Babii, A., Ball, R. T., Ghysels, E., and Striaukas, J. (2022). Machine learning panel data regressions with heavy-tailed dependent data: Theory and application. Journal of Econometrics. Baek, C., Düker, M.-C., and Pipiras, V. (2021). Local whittle estimation of high-dimensional long-run variance and precision matrices. arXiv preprint arXiv:2105.13342. Baillie, R. T., Diebold, F. X., Kapetanios, G., and Kim, K. H. (2022). On robust inference in time series regression. arXiv preprint arXiv:2203.04080. Barut, E., Fan, J., and Verhasselt, A. (2016). Conditional sure independence screening. Journal of the American Statistical Association, 111(515):1266–1277. Basu, S. and Michailidis, G. (2015). Regularized estimation in sparse high-dimensional time series models. Annals of statistics, 43(4):1535–1567. Basu, S. and Rao, S. S. (2021). Graphical models for nonstationary time series. arXiv preprint arXiv:2109.08709. Battey, H., Fan, J., Liu, H., Lu, J., and Zhu, Z. (2018). Distributed testing and estimation under sparse high dimensional models. Annals of statistics, 46(3):1352. Beder, J. H. (1987). A sieve estimator for the mean of a gaussian process. The Annals of Statistics, 15(1):59–78. Belloni, A. and Chernozhukov, V. (2011). Penalized quantile regression in high-dimensional sparse models. Annals of Statistics, 39(1):82–130. Belloni, A., Chernozhukov, V., and Hansen, C. (2014). Inference on treatment effects after selection among high-dimensional controls. The Review of Economic Studies, 81(2):608–650. Belloni, A., Chernozhukov, V., and Wei, Y. (2016). Post-selection inference for generalized linear models with many controls. Journal of Business & Economic Statistics, 34(4):606–619. Benko, M., Hardle, W., and Kneip, A. (2009). Common functional principal components. The Annals of Statistics, 37(1):1–34. 118Berger, J. O. and Mortera, J. (1999). Default bayes factors for nonnested hypothesis testing. Journal of the american statistical association, 94(446):542–554. Berger, J. O. and Pericchi, L. R. (1996). The intrinsic bayes factor for model selection and prediction. Journal of the American Statistical Association, 91(433):109–122. Bhatore, S., Mohan, L., and Reddy, Y. R. (2020). Machine learning techniques for credit risk evaluation: a systematic literature review. Journal of Banking and Financial Technology, 4:111–138. Bickel, P. J., Ritov, Y., and Tsybakov, A. B. (2009). Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, 37(4):1705–1732. Bing, X., Bunea, F., and Wegkamp, M. (2022). Inference in latent factor regression with clusterable features. Bernoulli, 28(2):997–1020. Boot, T. and Nibbering, D. (2019). Forecasting using random subspace methods. Journal of Economet- rics, 209(2):391–406. Braun, A., Kohler, M., Langer, S., and Walk, H. (2021). The smoking gun: Statistical theory improves neural network estimates. arXiv preprint arXiv:2107.09550. Braun, A., Kohler, M., and Walk, H. (2019). On the rate of convergence of a neural network regression estimate learned by gradient descent. arXiv preprint arXiv:1912.03921. Breheny, P. and Huang, J. (2015). Group descent algorithms for nonconvex penalized linear and logistic regression models with grouped predictors. Statistics and computing, 25:173–187. Breitung, J. and Eickmeier, S. (2011). Testing for structural breaks in dynamic factor models. Journal of Econometrics, 163(1):71–84. Cai, T. T. and Jiang, T. (2011). Limiting laws of coherence of random matrices with applications to testing covariance structure and construction of compressed sensing matrices. The Annals of Statistics, 39(3):1496–1525. Callot, L., Caner, M., Önder, A. Ö., and Ula¸san, E. (2021). A nodewise regression approach to estimating large portfolios. Journal of Business & Economic Statistics, 39(2):520–531. Campbell, F. and Allen, G. I. (2017). Within group variable selection through the exclusive lasso. Elec- tronic Journal of Statistics, 11(2):4220–4257. Candes, E. and Tao, T. (2007). The dantzig selector: Statistical estimation when p is much larger than n. The Annals of Statistics, 35(6):2313–2351. Caner, M. and Daniele, M. (2023). Deep learning based residuals in non-linear factor models: Precision matrix estimation of returns with low signal-to-noise ratio. arXiv preprint arXiv:2209.04512. Caner, M. and Knight, K. (2013). An alternative to unit root tests: Bridge estimators differentiate between nonstationary versus stationary models and select optimal lag. Journal of Statistical Planning and Inference, 143(4):691–715. Caner, M., Medeiros, M., and Vasconcelos, G. F. (2023). Sharpe ratio analysis in high dimensions: Residual-based nodewise regression in factor models. Journal of Econometrics, 235(2):393–417. Cattaneo, M. D., Farrell, M. H., and Feng, Y. (2020). Large sample properties of partitioning-based series estimators. Annals of Statistics, 48(3):1718–1741. Cattaneo, M. D., Jansson, M., and Newey, W. K. (2018). Inference in linear regression models with many covariates and heteroscedasticity. Journal of the American Statistical Association, 113(523):1350– 1191361. Chan, N. H., Yau, C. Y., and Zhang, R.-M. (2014). Group lasso for structural break time series. Journal of the American Statistical Association, 109(506):590–599. Chang, J., Chen, X., and Wu, M. (2021). Central limit theorems for high dimensional dependent data. arXiv preprint arXiv:2104.12929. Chatterjee, A., Gupta, S., and Lahiri, S. (2015). On the residual empirical process based on the alasso in high dimensions and its functional oracle property. Journal of Econometrics, 186(2):317–324. Chen, H., Lu, W., and Song, R. (2021a). Statistical inference for online decision making via stochastic gradient descent. Journal of the American Statistical Association, 116(534):708–719. Chen, L., Pelger, M., and Zhu, J. (2023). Deep learning in asset pricing. Management Science. Chen, S. and Walker, S. G. (2021). A new statistic for bayesian hypothesis testing. Econometrics and Statistics. Chen, X. (2007). Large sample sieve estimation of semi-nonparametric models. Handbook of economet- rics, 6:5549–5632. Chen, X., Lee, J. D., Tong, X. T., and Zhang, Y. (2020). Statistical inference for model parameters in stochastic gradient descent. The Annals of Statistics, 48(1):251–273. Chen, X., Liao, Y., and Wang, W. (2022). Inference on time series nonparametric conditional moment restrictions using general sieves. arXiv preprint arXiv:2301.00092. Chen, X. and Liao, Z. (2014). Sieve m inference on irregular parameters. Journal of Econometrics, 182(1):70–86. Chen, X., Xu, M., and Wu, W. B. (2013). Covariance and precision matrix estimation for high- dimensional time series. The Annals of Statistics, 41(6):2994–3021. Chen, Y., Cheng, C., and Fan, J. (2021b). Asymmetry helps: Eigenvalue and eigenvector analyses of asymmetrically perturbed low-rank matrices. Annals of statistics, 49(1):435. Cheng, X. and Hansen, B. E. (2015). Forecasting with factor-augmented regression: A frequentist model averaging approach. Journal of Econometrics, 186(2):280–293. Cheng, X., Liao, Z., and Schorfheide, F. (2016). Shrinkage estimation of high-dimensional factor models with structural instabilities. The Review of Economic Studies, 83(4):1511–1543. Chernozhukov, V., Chetverikov, D., and Kato, K. (2014). Gaussian approximation of suprema of empiri- cal processes. The Annals of Statistics, 42(4):1564–1597. Chernozhukov, V., Hansen, C., and Spindler, M. (2015). Post-selection and post-regularization inference in linear models with many controls and instruments. American Economic Review, 105(5):486–490. Chetverikov, D., Liao, Z., and Chernozhukov, V. (2021). On cross-validated lasso in high dimensions. The Annals of Statistics, 49(3):1300–1317. Chib, S., Shin, M., and Simoni, A. (2022). Bayesian estimation and comparison of conditional moment models. Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(3):740–764. Chinco, A., Clark-Joseph, A. D., and Ye, M. (2019). Sparse signals in the cross-section of returns. The Journal of Finance, 74(1):449–492. Cho, H., Maeng, H., Eckley, I. A., and Fearnhead, P. (2023). High-dimensional time series segmentation via factor-adjusted vector autoregressive modelling. Journal of the American Statistical Association, 120(just-accepted):1–28. Chronopoulos, I. C., Raftapostolos, A., and Kapetanios, G. (2023). Forecasting value-at-risk using deep neural network quantile regression. Journal of Financial Econometrics. Chudik, A., Kapetanios, G., and Pesaran, M. H. (2018). A one covariate at a time, multiple testing ap- proach to variable selection in high-dimensional linear regression models. Econometrica, 86(4):1479– 1512. Coroneo, L. and Iacone, F. (2020). Comparing predictive accuracy in small samples using ﬁxed- smoothing asymptotics. Journal of Applied Econometrics, 35(4):391–409. Cragg, J. G. and Donald, S. G. (1997). Inferring the rank of a matrix. Journal of econometrics, 76(1- 2):223–250. Davidson, J. (2002). Establishing conditions for the functional central limit theorem in nonlinear and semiparametric time series processes. Journal of Econometrics, 106(2):243–269. Dhrymes, P. J. (2013). Mathematics for econometrics. Springer. Domínguez, M. A. and Lobato, I. N. (2004). Consistent estimation of models deﬁned by conditional moment restrictions. Econometrica, 72(5):1601–1615. Dong, C., Linton, O., and Peng, B. (2021). A weighted sieve estimator for nonparametric time series models with nonstationary variables. Journal of Econometrics, 222(2):909–932. Doukhan, P., Massart, P., and Rio, E. (1995). Invariance principles for absolutely regular empirical processes. In Annales de le IHP Probabilites et statistiques, volume 31, pages 393–427. D˜ung, D. et al. (2021). Deep relu neural networks in high-dimensional approximation. Neural Networks, 142:619–635. Efron, B. (2014). Two modeling strategies for empirical bayes estimation. Statistical science: a review journal of the Institute of Mathematical Statistics, 29(2):285. Engelke, S. and Hitz, A. S. (2020). Graphical models for extremes. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 82(4):871–932. Fan, J., Feng, Y., and Xia, L. (2020). A projection-based conditional dependence measure with applica- tions to high-dimensional undirected graphical models. Journal of Econometrics, 218(1):119–139. Fan, J., Ke, Z. T., Liao, Y., and Neuhierl, A. (2022). Structural deep learning in conditional asset pricing. Available at SSRN 4117882. Fan, J., Li, Q., and Wang, Y. (2017). Estimation of high dimensional mean regression in the absence of symmetry and light tail assumptions. Journal of the Royal Statistical Society Series B: Statistical Methodology, 79(1):247–265. Fan, J. and Li, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American statistical Association, 96(456):1348–1360. Fan, J., Liao, Y., and Yao, J. (2015). Power enhancement in high-dimensional cross-sectional tests. Econometrica, 83(4):1497–1541. Fan, J. and Lv, J. (2008). Sure independence screening for ultrahigh dimensional feature space. Journal of the Royal Statistical Society Series B: Statistical Methodology, 70(5):849–911. Fan, J., Ma, C., and Zhong, Y. (2021a). A selective overview of deep learning. Statistical science: a review journal of the Institute of Mathematical Statistics, 36(2):264. 121Fan, J., Masini, R., and Medeiros, M. C. (2021b). Bridging factor and sparse models. arXiv preprint arXiv:2102.11341. Fan, R., Lee, J. H., and Shin, Y. (2023). Predictive quantile regression with mixed roots and increasing dimensions: The alqr approach. Journal of Econometrics. Farrell, M. H. (2015). Robust inference on average treatment effects with possibly more covariates than observations. Journal of Econometrics, 189(1):1–23. Farrell, M. H., Liang, T., and Misra, S. (2021). Deep neural networks for estimation and inference. Econometrica, 89(1):181–213. Feng, G., Giglio, S., and Xiu, D. (2020). Taming the factor zoo: A test of new factors. The Journal of Finance, 75(3):1327–1370. Feng, G., He, J., Polson, N. G., and Xu, J. (2018). Deep learning in characteristics-sorted factor models. arXiv preprint arXiv:1805.01104. Finner, H. (1992). A generalization of holder’s inequality and some probability inequalities. The Annals of probability, pages 1893–1901. Finner, H. and Strassburger, K. (2002). The partitioning principle: a powerful tool in multiple decision theory. Annals of statistics, pages 1194–1213. Fithian, W., Taylor, J., Tibshirani, R., and Tibshirani, R. (2015). Selective sequential model selection. arXiv preprint arXiv:1512.02565. García-Donato, G. and Paulo, R. (2022). Variable selection in the presence of factors: a model selection perspective. Journal of the American Statistical Association, 117(540):1847–1857. Geweke, J. (1989). Bayesian inference in econometric models using monte carlo integration. Economet- rica: Journal of the Econometric Society, pages 1317–1339. Giurcanu, M. (2016). Thresholding least-squares inference in high-dimensional regression models. Elec- tronic Journal of Statistics, 10(2):2124–2156. Goeman, J. J. and Solari, A. (2010). The sequential rejection principle of familywise error control. The Annals of Statistics, pages 3782–3810. Gonzalo, J. and Pitarakis, J.-Y. (2023). Out of sample predictability in predictive regressions with many predictor candidates. arXiv preprint arXiv:2302.02866. Goodfellow, I., Bengio, Y., and Courville, A. (2016). Deep learning. MIT press. Guijarro-Ordonez, J., Pelger, M., and Zanotti, G. (2021). Deep learning statistical arbitrage. arXiv preprint arXiv:2106.04028. Guillaumin, A. P., Sykulski, A. M., Olhede, S. C., and Simons, F. J. (2022). The debiased spatial whittle likelihood. Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(4):1526– 1557. Guo, Z., Cevid, D., and Buhlmann, P. (2022). Doubly debiased lasso: High-dimensional inference under hidden confounding. Annals of statistics, 50(3):1320–1347. Gupta, A. and Seo, M. H. (2019). Robust inference on inﬁnite and growing dimensional time series regression. arXiv preprint arXiv:1911.08637. Gupta, S. (2012). A note on the asymptotic distribution of lasso estimator for correlated data. Sankhya A, 74(1):10–28. 122Hagemann, A. (2012). A simple test for regression speciﬁcation with non-nested alternatives. Journal of econometrics, 166(2):247–254. Hagemann, A. (2014). Stochastic equicontinuity in nonlinear time series models. The Econometrics Journal, 17(1):188–196. Halunga, A. G., Orme, C. D., and Yamagata, T. (2017). A heteroskedasticity robust breusch–pagan test for contemporaneous correlation in dynamic panel data models. Journal of econometrics, 198(2):209– 230. Hansen, B. E. (2007). Least squares model averaging. Econometrica, 75(4):1175–1189. Hansen, B. E. (2008). Least-squares forecast averaging. Journal of Econometrics, 146(2):342–350. Hansen, B. E. (2010). Averaging estimators for autoregressions with a near unit root. Journal of Econo- metrics, 158(1):142–155. He, X., Wang, L., and Hong, H. G. (2013). Quantile-adaptive model-free variable screening for high- dimensional heterogeneous data. Annals of Statistics, 41(1):342–369. Hebb, D. O. (1949). The organization of behavior: A neuropsychological theory. Wiley. Hotz, V. J., Miller, R. A., Sanders, S., and Smith, J. (1994). A simulation estimator for dynamic models of discrete choice. The Review of Economic Studies, 61(2):265–289. Hu, Y. (2004). Complete convergence theorems for lp-mixingales. Journal of mathematical analysis and applications, 290(1):271–290. Huang, J., Breheny, P., and Ma, S. (2012). A selective review of group selection in high-dimensional models. Statistical science: a review journal of the Institute of Mathematical Statistics, 27(4). Huang, J., Ma, S., and Zhang, C.-H. (2008). Adaptive lasso for sparse high-dimensional regression models. Statistica Sinica, pages 1603–1618. Huang, T.-J., McKeague, I. W., and Qian, M. (2019). Marginal screening for high-dimensional predictors of survival outcomes. Statistica Sinica, 29(4):2105. James, G. M., Radchenko, P., and Lv, J. (2009). Dasso: connections between the dantzig selector and lasso. Journal of the Royal Statistical Society Series B: Statistical Methodology, 71(1):127–142. Jewson, J. and Rossell, D. (2022). General bayesian loss function selection and the use of improper models. Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(5):1640–1665. Johansen, S. (1991). Estimation and hypothesis testing of cointegration vectors in gaussian vector au- toregressive models. Econometrica: journal of the Econometric Society, pages 1551–1580. Kapetanios, G. and Zikes, F. (2018). Time-varying lasso. Economics Letters, 169:1–6. Karmakar, S., Chud`y, M., and Biao Wu, W. (2022). Long-term prediction intervals with many covariates. Journal of Time Series Analysis, 43(4):587–609. Katsouris, C. (2021). Forecast evaluation in large cross-sections of realized volatility. arXiv preprint arXiv:2112.04887. Katsouris, C. (2023). Statistical estimation for covariance structures with tail estimates using nodewise quantile predictive regression models. arXiv preprint arXiv:2305.11282. Kaul, A. (2014). Lasso with long memory regression errors. Journal of Statistical Planning and Infer- ence, 153:11–26. Ke, T., Jin, J., and Fan, J. (2014). Covariance assisted screening and estimation. Annals of statistics, 12342(6):2202. Keeler, E. and Meir, A. (1969). A theorem on contraction mappings. J. Math. Anal. Appl, 28:326–329. Kejriwal, M. and Yu, X. (2021). Generalized forecast averaging in autoregressions with a near unit root. The Econometrics Journal, 24(1):83–102. Khalaf, L. and Richard, F. (2020). Simulation-based multiple testing for many non-nested multivariate models. Kim, J.-Y. (1994). Bayesian asymptotic theory in a time series model with a possible nonstationary process. Econometric Theory, pages 764–773. Klau, S., Jurinovic, V., Hornung, R., Herold, T., and Boulesteix, A.-L. (2018). Priority-lasso: a simple hi- erarchical approach to the prediction of clinical outcome using multi-omics data. BMC bioinformatics, 19(1):322. Kleibergen, F. and Paap, R. (2006). Generalized reduced rank tests using the singular value decomposi- tion. Journal of econometrics, 133(1):97–126. Kock, A. B. (2016). Consistent and conservative model selection with the adaptive lasso in stationary and nonstationary autoregressions. Econometric Theory, 32(1):243–259. Koenker, R. and Bassett Jr, G. (1978). Regression quantiles. Econometrica: journal of the Econometric Society, pages 33–50. Koltchinskii, V. (2009). The dantzig selector and sparsity oracle inequalities. Bernoulli, pages 799–828. Kong, Y., Li, Y., and Zerom, D. (2019). Screening and selection for quantile regression using an alterna- tive measure of variable importance. Journal of Multivariate Analysis, 173:435–455. Kontorovich, L. and Ramanan, K. (2008). Concentration inequalities for dependent random variables via the martingale method. The Annals of Statistics, 36(6):2126–2158. Koo, B., Anderson, H. M., Seo, M. H., and Yao, W. (2020). High-dimensional predictive regression in the presence of cointegration. Journal of Econometrics, 219(2):456–477. Kuan, C.-M. and White, H. (1994). Artiﬁcial neural networks: An econometric perspective. Econometric reviews, 13(1):1–91. Kuchibhotla, A. K., Brown, L. D., Buja, A., George, E. I., and Zhao, L. (2021). Uniform-in-submodel bounds for linear regression in a model-free framework. Econometric Theory, pages 1–47. Kuersteiner, G. M. (2019). Invariance principles for dependent processes indexed by besov classes with an application to a hausman test for linearity. Journal of econometrics, 211(1):243–261. Kwiatkowski, D., Phillips, P. C., Schmidt, P., and Shin, Y. (1992). Testing the null hypothesis of station- arity against the alternative of a unit root: How sure are we that economic time series have a unit root? Journal of econometrics, 54(1-3):159–178. LeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. nature, 521(7553):436–444. Ledoit, O. and Wolf, M. (2004). A well-conditioned estimator for large-dimensional covariance matrices. Journal of multivariate analysis, 88(2):365–411. Lee, J. H., Shi, Z., and Gao, Z. (2022). On lasso for predictive regression. Journal of Econometrics, 229(2):322–349. Li, Y., Chan, N. H., Yau, C. Y., and Zhang, R. (2021). Group orthogonal greedy algorithm for change- point estimation of multivariate time series. Journal of Statistical Planning and Inference, 212:14–33. 124Lin, Y. and Tu, Y. (2020). Robust inference for spurious regressions and cointegrations involving pro- cesses moderately deviated from a unit root. Journal of Econometrics, 219(1):52–65. Lockhart, R., Taylor, J., Tibshirani, R. J., and Tibshirani, R. (2014). A signiﬁcance test for the lasso. Annals of statistics, 42(2):413. MacKinnon, J. G. (1983). Model speciﬁcation tests against non-nested alternatives. Econometric Re- views, 2(1):85–110. Masini, R. P., Medeiros, M. C., and Mendes, E. F. (2022). Regularized estimation of high-dimensional vector autoregressions with weakly dependent innovations. Journal of Time Series Analysis, 43(4):532–557. Massart, P. (2007). Concentration inequalities and model selection: Ecole de Ete de Probabilites de Saint-Flour XXXIII-2003. Springer. McKeague, I. W. and Qian, M. (2015). An adaptive resampling test for detecting the presence of signiﬁ- cant predictors. Journal of the American Statistical Association, 110(512):1422–1433. Medeiros, M. C. and Mendes, E. F. (2016). Norm 1-regularization of high-dimensional time-series mod- els with non-gaussian and heteroskedastic errors. Journal of Econometrics, 191(1):255–271. Medeiros, M. C. and Mendes, E. F. (2017). Adaptive lasso estimation for ardl models with garch inno- vations. Econometric Reviews, 36(6-9):622–637. Meinshausen, N. and Bühlmann, P. (2006). High-dimensional graphs and variable selection with the lasso. Annals of statistics, 34(3):1436–1462. Meinshausen, N. and Bühlmann, P. (2010). Stability selection. Journal of the Royal Statistical Society Series B: Statistical Methodology, 72(4):417–473. Mendes, E. F. (2011). Model selection consistency for cointegrating regressions. arXiv preprint arXiv:1104.5667. Michailidis, G. and d’Alché Buc, F. (2013). Autoregressive models for gene regulatory network infer- ence: Sparsity, stability and causality issues. Mathematical biosciences, 246(2):326–334. Mohri, M. and Rostamizadeh, A. (2008). Rademacher complexity bounds for non-iid processes. Ad- vances in Neural Information Processing Systems, 21. Mohri, M. and Rostamizadeh, A. (2010). Stability bounds for stationary ϕ-mixing and β-mixing pro- cesses. Journal of Machine Learning Research, 11(2). Montanelli, H. and Du, Q. (2019). New error bounds for deep relu networks using sparse grids. SIAM Journal on Mathematics of Data Science, 1(1):78–92. Mukherjee, K. (1998). On preliminary test and shrinkage estimation in linear models with long-memory errors. Journal of statistical planning and inference, 69(2):319–328. Muller, H.-G. and Stadtmuller, U. (2005). Generalized functional linear models. The Annals of Statistics, 33(2):774–805. Müller, M. M., Reeve, H. W., Cannings, T. I., and Samworth, R. J. (2023). Isotonic subgroup selection. arXiv preprint arXiv:2305.04852. Muller, U. K. (2008). The impossibility of consistent discrimination between i (0) and i (1) processes. Econometric Theory, 24(3):616–630. Newey, W. K. (1991). Uniform convergence in probability and stochastic equicontinuity. Econometrica: 125Journal of the Econometric Society, pages 1161–1167. Nielsen, M. Ø. (2009). A powerful test of the autoregressive unit root hypothesis based on a tuning parameter free statistic. Econometric Theory, 25(6):1515–1544. Olmo, J. (2022). A nonparametric predictive regression model using partitioning estimators based on taylor expansions. Journal of Time Series Analysis. Osborne, M. R., Presnell, B., and Turlach, B. A. (2000). On the lasso and its dual. Journal of Computa- tional and Graphical statistics, 9(2):319–337. Park, T. and Casella, G. (2008). The bayesian lasso. Journal of the American Statistical Association, 103(482):681–686. Pitacco, E. (2019). Heterogeneity in mortality: a survey with an actuarial focus. European Actuarial Journal, 9:3–30. Pitarakis, J.-Y. (2023). A novel approach to predictive accuracy testing in nested environments. Econo- metric Theory, pages 1–44. Poignard, B. and Asai, M. (2023). Estimation of high-dimensional vector autoregression via sparse precision matrix. The Econometrics Journal, 26(2):307–326. Politis, D. N., Romano, J. P., and Wolf, M. (1999). Subsampling. Springer Science & Business Media. Potscher, B. M. (1991). Effects of model selection on inference. Econometric Theory, 7(2):163–185. Pottmann, H., Krasauskas, R., Hamann, B., Joy, K., and Seibold, W. (2000). On piecewise linear approx- imation of quadratic functions. Journal for Geometry and Graphics, 4(1):31–53. Qian, J. and Su, L. (2016). Shrinkage estimation of regression models with multiple structural changes. Econometric Theory, 32(6):1376–1433. Reeve, H. W., Cannings, T. I., and Samworth, R. J. (2021). Optimal subgroup selection. arXiv preprint arXiv:2109.01077. Reich, S. (1971). Some remarks concerning contraction mappings. Canadian Mathematical Bulletin, 14(1):121–124. Richard, F. (2023). Model conﬁdence sets in multivariate systems. Rinaldo, A., Wasserman, L., and G’Sell, M. (2019). Bootstrapping and sample splitting for high- dimensional, assumption-lean inference. Annals of statistics, 47(6):3438–3469. Romano, J. P. and Wolf, M. (2005). Exact and approximate stepdown methods for multiple hypothesis testing. Journal of the American Statistical Association, 100(469):94–108. Rosenblatt, F. (1958). The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review, 65(6):386. Roverato, A. and Whittaker, J. (1998). The isserlis matrix and its application to non-decomposable graphical gaussian models. Biometrika, 85(3):711–725. Salgueiro, M. F., Smith, P. W., and McDonald, J. W. (2005). Power of edge exclusion tests in graphical gaussian models. Biometrika, 92(1):173–182. Salgueiro, M. F., Smith, P. W., and McDonald, J. W. (2006). Power of edge exclusion tests for graphical log-linear models. Journal of multivariate analysis, 97(8):1691–1701. Schmidt-Hieber, J. (2020). Nonparametric regression using deep neural networks with relu activation function. Annals of Statistics, 48(4):1875–1897. 126Sen, P. K. and Saleh, A. E. (1987). On preliminary test and shrinkage m-estimation in linear models. The Annals of Statistics, pages 1580–1592. Sen, P. K. and Saleh, A. M. E. (1985). On some shrinkage estimators of multivariate location. The Annals of Statistics, 13(1):272–281. Shao, Q.-M. and Zhang, Z.-S. (2022). Berry–esseen bounds for multivariate nonlinear statistics with applications to m-estimators and stochastic gradient descent algorithms. Bernoulli, 28(3):1548–1576. Shen, G., Jiao, Y., Lin, Y., Horowitz, J. L., and Huang, J. (2021). Deep quantile regression: Mitigating the curse of dimensionality through composition. arXiv preprint arXiv:2107.04907. Shen, X., Jiang, C., Sakhanenko, L., and Lu, Q. (2023). Asymptotic properties of neural network sieve estimators. Journal of Nonparametric Statistics, pages 1–30. Shibata, R. (1986). Consistency of model selection and parameter estimation. Journal of applied proba- bility, 23(A):127–141. Smith, S. C., Timmermann, A., and Zhu, Y. (2019). Variable selection in panel models with breaks. Journal of econometrics, 212(1):323–344. Stinchcombe, M. B. and White, H. (1998). Consistent speciﬁcation testing with nuisance parameters present only under the alternative. Econometric theory, 14(3):295–325. Su, L. and Hoshino, T. (2016). Sieve instrumental variable quantile regression estimation of functional coefﬁcient models. Journal of Econometrics, 191(1):231–254. Su, L. and Jin, S. (2012). Sieve estimation of panel data models with cross section dependence. Journal of Econometrics, 169(1):34–47. Sun, Q., Zhu, H., Liu, Y., and Ibrahim, J. G. (2015). Sprem: sparse projection regression model for high- dimensional linear regression. Journal of the American Statistical Association, 110(509):289–302. Swanson, N. R., Xiong, W., and Yang, X. (2020). Predicting interest rates using shrinkage methods, real- time diffusion indexes, and model combinations. Journal of Applied Econometrics, 35(5):587–613. Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Methodological), 58(1):267–288. Tibshirani, R. J., Rinaldo, A., Tibshirani, R., and Wasserman, L. (2018). Uniform asymptotic inference and the bootstrap after model selection. The Annals of Statistics, 46(3):1255–1287. Toulis, P. and Airoldi, E. M. (2017). Asymptotic and ﬁnite-sample properties of estimators based on stochastic gradients. The Annals of Statistics, 45(4):1694–1727. Tran, D., Toulis, P., and Airoldi, E. M. (2015). Stochastic gradient descent methods for estimation with large data sets. arXiv preprint arXiv:1509.06459. Vaart, A. v. d. and Wellner, J. A. (2013). Weak convergence and empirical processes: with applications to statistics. Springer Science & Business Media. Vaart, A. v. d. and Wellner, J. A. (2023). Empirical processes. In Weak Convergence and Empirical Processes: With Applications to Statistics, pages 1–403. Springer. van de Geer, S. A. (2002). On hoeffding’s inequality for dependent random variables. In Empirical process techniques for dependent data, pages 161–169. Springer. Van der Vaart, A. W. (2000). Asymptotic statistics, volume 3. Cambridge university press. Varin, C., Reid, N., and Firth, D. (2011). An overview of composite likelihood methods. Statistica Sinica, 127pages 5–42. Vershynin, R. (2018). High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press. Vesely, A., Finos, L., and Goeman, J. J. (2021). Permutation-based true discovery guarantee by sum tests. arXiv preprint arXiv:2102.11759. Vuong, Q. H. (1989). Likelihood ratio tests for model selection and non-nested hypotheses. Economet- rica: journal of the Econometric Society, pages 307–333. Wainwright, M. J. (2019). High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam- bridge university press. Wald, A. (1943). On the efﬁcient design of statistical investigations. The annals of mathematical statis- tics, 14(2):134–140. Wang, D. and Tsay, R. S. (2023). Rate-optimal robust estimation of high-dimensional vector autoregres- sive models. The Annals of Statistics, 51(2):846–877. Wang, H. (2009). Forward regression for ultra-high dimensional variable screening. Journal of the American Statistical Association, 104(488):1512–1524. Wang, L., Wu, Y., and Li, R. (2012). Quantile regression for analyzing heterogeneity in ultra-high dimension. Journal of the American Statistical Association, 107(497):214–222. Wang, X. and Leng, C. (2016). High dimensional ordinary least squares projection for screening vari- ables. Journal of the Royal Statistical Society Series B: Statistical Methodology, 78(3):589–611. Wei, W., Zhou, Y., Zheng, Z., and Wang, J. (2023). Inference on the best policies with many covariates. Journal of Econometrics, page 105460. White, H. (1990). Connectionist nonparametric regression: Multilayer feedforward networks can learn arbitrary mappings. Neural networks, 3(5):535–549. White, H. (1996). Estimation, inference and speciﬁcation analysis. Number 22. Cambridge university press. Wong, K. C., Li, Z., and Tewari, A. (2020). Lasso guarantees for β-mixing heavy-tailed time series. The Annals of Statistics, 48(2):1124–1142. Wüthrich, K. and Zhu, Y. (2023). Omitted variable bias of lasso-based inference methods: A ﬁnite sample analysis. The review of economics and statistics, 105(4):982–997. Xiao, H. and Wu, W. B. (2012). Covariance matrix estimation for stationary time series. The Annals of Statistics, 40(1):466–493. Xiao, H. and Wu, W. B. (2013). Asymptotic theory for maximum deviations of sample covariance matrix estimates. Stochastic Processes and their Applications, 123(7):2899–2920. Yang, X., Guo, Y., Dong, M., and Xue, J.-H. (2022). Toward certiﬁed robustness of distance metric learning. IEEE Transactions on Neural Networks and Learning Systems. Yang, Y. (2007). Consistency of cross validation for comparing regression procedures. Annals of statis- tics, 35(6):2450–2473. Yarotsky, D. (2017). Error bounds for approximations with deep relu networks. Neural Networks, 94:103–114. Yarotsky, D. (2018). Optimal approximation of continuous functions by very deep relu networks. In 128Conference on learning theory, pages 639–649. PMLR. Yousuf, K. (2018). Variable screening for high dimensional time series. Electonic Journal of Statistics, 12(1):667–702. Yousuf, K. and Ng, S. (2021). Boosting high dimensional predictive regressions with time varying parameters. Journal of Econometrics, 224(1):60–87. Yu, B. (1994). Rates of convergence for empirical processes of stationary mixing sequences. The Annals of Probability, pages 94–116. Yuan, M. and Cai, T. T. (2010). A reproducing kernel hilbert space approach to functional linear regres- sion. The Annals of Statistics, 38(6):3412–3444. Zeng, G. L. (2021). A deep-network piecewise linear approximation formula. IEEE Access, 9:120665– 120674. Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty. Annals of Statistics, 38(2):894–942. Zhang, D. and Wu, W. B. (2017). Gaussian approximation for high dimensional time series. Annals of statistics, 45(5):1895–1919. Zhang, P. (1993). Model selection via multifold cross validation. The annals of statistics, pages 299–313. Zhang, T. and Simon, N. (2022). A sieve stochastic gradient descent estimator for online nonparametric regression in sobolev ellipsoids. The Annals of Statistics, 50(5):2848–2871. Zou, H. (2006). The adaptive lasso and its oracle properties. Journal of the American statistical associ- ation, 101(476):1418–1429. Zou, H. and Hastie, T. (2005). Regularization and variable selection via the elastic net. Journal of the royal statistical society: series B (statistical methodology), 67(2):301–320. 129