Learning multi-modal generative models with permutation-invariant encoders and tighter variational bounds Marcel Hirt1,2 Domenico Campolo1 Victoria Leong1 Juan-Pablo Ortega1 1Nanyang Technological University, Singapore 2 Corresponding author: marcelandre.hirt@ntu.edu.sg Abstract Devising deep latent variable models for multi-modal data has been a long- standing theme in machine learning research. Multi-modal Variational Autoen- coders (VAEs) have been a popular generative model class that learns latent rep- resentations which jointly explain multiple modalities. Various objective func- tions for such models have been suggested, often motivated as lower bounds on the multi-modal data log-likelihood or from information-theoretic consid- erations. In order to encode latent variables from different modality subsets, Product-of-Experts (PoE) or Mixture-of-Experts (MoE) aggregation schemes have been routinely used and shown to yield different trade-offs, for instance, regard- ing their generative quality or consistency across multiple modalities. In this work, we consider a variational bound that can tightly lower bound the data log- likelihood. We develop more flexible aggregation schemes that generalise PoE or MoE approaches by combining encoded features from different modalities based on permutation-invariant neural networks. Our numerical experiments illustrate trade-offs for multi-modal variational bounds and various aggregation schemes. We show that tighter variational bounds and more flexible aggregation models can become beneficial when one wants to approximate the true joint distribution over observed modalities and latent variables in identifiable models. 1 Introduction Multi-modal data sets where each sample has features from distinct sources have grown in re- cent years. For example, multi-omics data such as genomics, epigenomics, transcriptomics and metabolomics can provide a more comprehensive understanding of biological systems if multiple modalities are analysed in an integrative framework [7, 74, 90]. However, annotations or labels in such data sets are often rare, making unsupervised or semi-supervised generative approaches partic- ularly attractive as such methods can be used in these settings to (i) generate data, such as missing modalities, and (ii) learn latent representations that are useful for down-stream analyses or that are of scientific interest themselves. The availability of heterogenous data for different modalities promises to learn generalizable rep- resentations that can capture shared content across multiple modalities in addition to modality- specific information. A promising class of weakly-supervised generative models is multi-modal VAEs [117, 138, 110, 115] that combine information across modalities in an often-shared low- dimensional latent representation. Other classes of generative models, such as denoising diffusion or energy-based models, have achieved impressive generative quality. However, these models are not naturally learning multi-modal latent representations and commonly resort to different guidance techniques [24, 47] to generate samples that are coherent across multiple modalities. Non-linear latent variable models often lack identifiability, even up to indeterminacies, which makes it hard to interpret inferred latent representations or model parameters. However, utilizing auxiliary Preprint. Under review. arXiv:2309.00380v1 [stat.ML] 1 Sep 2023variables or additional modalities, recent work [62, 63, 139] has shown that such models can become identifiable up to known indeterminacies with such models adapted, for instance, to neuroscience applications: [151] model neural activity conditional on non-neural labels using VAEs, while [107] model neural recordings conditional on behavioural variables using self-supervised learning. A common route for learning the parameters of latent variable models is via maximization of the marginal data likelihood with various lower bounds thereof suggested in previous work. Setup. We consider a set of M random variables {X1, . . . , XM} with empirical density pd, where each random variable Xs, s ∈ M = {1, . . . , M}, can be used to model a different data modality taking values in Xs. With some abuse of notation, we write X = {X1, . . . , XM} and for any subset S ⊂ M, we set X = (XS, X\S) for two partitions of the random variables into XS = {Xs}s∈S and X\S = {Xs}s∈M\S. We pursue a latent variable model setup, analogous to uni- modal VAEs [66, 100]. For a latent variable Z ∈ Z with prior density pθ(z), we posit a joint generative model1 pθ(z, x) = pθ(z) �M s=1 pθ(xs|z), where pθ(xs|z) is commonly referred to as the decoding distribution for modality s. Observe that all modalities are independent given the latent variable z shared across all modalities. One can introduce modality-specific latent variables by making sparsity assumptions for the decoding distribution. We assume throughout that Z = RD, and that pθ(z) is a Lebesgue density, although the results can be extended to more general settings such as discrete random variables Z with appropriate adjustments, for instance, regarding the gradient estimators. Multi-modal variational bounds and mutual information. Popular approaches to train multi- modal models are based on a mixture-based variational bound [22, 110] given by LMix(θ, ϕ, β) = � ρ(S)LMix S (x, θ, ϕ, β)dS, where LMix S (x, θ, ϕ, β) = � qϕ(z|xS) [log pθ(x|z)] dz − βKL(qϕ(z|xS)|pθ(z)) (1) and ρ is some distribution on the power set P(M) of M and β > 0. For β = 1, one obtains the bound LMix S (x, θ, ϕ, β) ≤ log pθ(x). Variations of (1) have been suggested [114], for example, by replacing the prior density pθ in the KL-term by a weighted product of the prior density pθ and the uni-modal encoding distributions qϕ(z|xs), for all s ∈ M. Maximizing LMix S can be seen as minimizing � H(X|ZS) + β Iqϕ(XS, ZS) = H(X) − Iqϕ(X, ZS) + β Iqϕ(XS, ZS) � , (2) where Iq(X, Y ) = � q(x, y) log q(x,y) q(x)q(y) is the mutual information of random variables X and Y having marginal and joint densities q, whilst H(X|Y ) = − � q(x, y) log q(x|y)]dxdy is the conditional entropy of X given Y . Likewise, the multi-view variational information bottle- neck approach developed in [74] for predicting x\S given xS can be interpreted as minimizing − Iqϕ(X\S, Z) + β Iqϕ(XS, Z). [52] suggested a related bound motivated by a conditional vari- ational bottleneck perspective that aims to maximize the reduction of total correlation of X when conditioned on Z, as measured by the conditional total correlation, see [136, 127, 32], i.e., minimizing � TC(X|Z) = TC(X) − TC(X, Z) = TC(X) + Iqϕ(X, Z) − M � s=1 Iqϕ(Xs, Z) � , (3) where TC(X) = KL(p(x)| �d i=1 p(xi)) for d-dimensional X. Resorting to variational lower bounds and using a constant β > 0 that weights the contributions of the mutual information terms, approxi- mations of (3) can be optimized by maximizing LTC(θ, ϕ, β) = � ρ(S) � {qϕ(z|x) [log pθ(x|z)] dz − βKL(qϕ(z|x)|qϕ(z|xS))} dS, where ρ is concentrated on the uni-modal subsets of M. Similar bounds have been suggested in [114] and [117] by considering different KL-regularisation terms, see also [116]. [111] add a con- trastive term to the maximum likelihood objective and minimize − log pθ(x) − β Ipθ(XS, X\S). 1We usually denote random variables using upper-case letters, and their realizations by the corresponding lower-case letter. 2Multi-modal aggregation schemes. In order to optimize the variational bounds above or to allow for flexible conditioning at test time, we need to learn encoding distributions qϕ(z|xS) for any S ∈ P(M). The typical aggregation schemes that are scalable to a large number of modalities are based on a choice of uni-modal encoding distributions qϕs(z|xs) for any s ∈ M, which are then used to define the multi-modal encoding distributions as follows: • Mixture of Experts (MoE), see [110], qMoE ϕ (z|xS) = 1 |S| � s∈S qϕs(z|xs). • Product of Experts (PoE), see [137], qPoE ϕ (z|xS) = 1 Z pθ(z) � s∈S qϕs(z|xs), for some Z ∈ R. Contributions. This paper contributes (i) a new variational bound that addresses known limita- tions of previous variational bounds. For instance, mixture-based bounds (1) may not provide tight bounds on the joint log-likelihood if there is considerable modality-specific variation [22]. In con- trast, the novel variational bound becomes a tight lower bound of both the marginal log-likelihood log pθ(xS) as well as the conditional log pθ(x\S|xS) for any choice of S ∈ P(M), provided that we can learn a flexible multi-modal encoding distribution. This paper then contributes (ii) new multi-modal aggregation schemes that yield more expressive multi-modal encoding distributions when compared to MoEs or PoEs. These schemes are motivated by the flexibility of permutation- invariant architectures such as DeepSets [144] or attention models [125, 75]. We illustrate that these innovations (iii) are beneficial when learning identifiable models, aided by using flexible prior and encoding distributions consisting of mixtures and (iv) yield higher log-likelihoods in experiments. Further related work. Canonical Correlation Analysis [49] is a classical approach for multi- modal data that aims to find projections of two modalities by maximally correlating them and has been interpreted in a probabilistic or generative framework [9]. Furthermore, it has been extended to include more than two modalities [6, 119] or to allow for non-linear transformations [2, 42, 132, 61]. Probabilistic CCA can also be seen as multi-battery factor analysis (MBFA) [17, 69], wherein a shared latent variable models the variation common to all modalities with modality-specific latent variables capturing the remaining variation. Likewise, latent factor regression or classification mod- els [113] assume that observed features and response are driven jointly by a latent variable. [126] considered a tiple-ELBO for two modalities, while [115] introduced a generalised variational bound that involves a summation over all modality subsets. A series of work has developed multi-modal VAEs based on shared and private latent variables [133, 76, 84, 85, 97]. [123] proposed a hybrid generative-discriminative objective and minimized an approximation of the Wasserstein distance between the generated and observed multi-modal data. [60] consider a semi-supervised setup of two modalities that requires no explicit multi-modal aggregation function. Extending the Info-Max principle [81], maximizing mutual information Iq(g1(X1), g(X2)) ≤ Iq((X1, X2), (Z1, Z2)) based on representations Zs = gs(Xs) for modality-specific encoders gs from two modalities has been a motivation for approaches based on (symmetrised) contrastive objectives [120, 148, 23] such as InfoNCE [96, 98, 131] as a variational lower bound on the mutual information between Z1 and Z2. 2 A tighter variational bound with arbitrary modality masking For S ⊂ M and β > 0, we define LS(xS, θ, ϕ, β) = � qϕ(z|xS) [log pθ(xS|z)] dz − βKL(qϕ(z|xS)|pθ(z)). (4) This is simply a standard variational lower bound [59, 14] restricted to the subset S for β = 1, and therefore LS(xS, θ, ϕ, 1) ≤ log pθ(xS). To obtain a lower bound on the log-likelihood of all modalities, we introduce an (approximate) conditional lower bound L\S(x, θ, ϕ, β) = � qϕ(z|x) � log pθ(x\S|z) � dz − βKL(qϕ(z|x)|qϕ(z|xS)). (5) For some fixed density ρ on P(M), we suggest the overall bound L(x, θ, ϕ, β) = � ρ(S) � LS(xS, θ, ϕ, β) + L\S(x, θ, ϕ, β) � dS, which is a generalisation of the bound suggested in [138] to an arbitrary number of modalities. This bound can be optimised using standard Monte Carlo techniques, for example, by computing unbi- ased pathwise gradients [66, 100, 122] using the reparameterisation trick. For variational families 3such as Gaussian mixtures2, one can employ implicit reparameterisation [29]. It is straightforward to adapt variance reduction techniques such as ignoring the score term of the multi-modal encoding densities for pathwise gradients [101], see Algorithm 1 in Appendix K for pseudo-code. Neverthe- less, a scalable approach requires an encoding technique that allows to condition on any masked modalities with a computational complexity that does not increase exponentially in M. Remark 1 (Optimization, multi-task learning and the choice of ρ). For simplicity, we have chosen to sample S ∼ ρ in our experiments via the hierarchical construction γ ∼ U(0, 1), mj ∼ Bern(γ) iid for all j ∈ [M] and setting S = {s ∈ [M]: mj = 1}. The distribution ρ for masking the modali- ties can be adjusted to accommodate various weights for different modality subsets. Indeed, (2) can be seen as a linear scalarisation of a multi-task learning problem [30, 109]. We aim to optimise a loss vector (LS + L\S)S⊂M, where the gradients for each S ⊂ M can point in different directions, making it challenging to minimise the loss for all modalities simultaneously. Consequently, [56] used multi-task learning techniques (e.g., as suggested in [19, 142]) for adjusting the gradients in mixture based VAEs. Such improved optimisation routines are orthogonal to our approach. Simi- larly, we do not analyse optimisation issues such as initialisations and training dynamics that have been found challenging for multi-modal learning [134, 51]. Multi-modal distribution matching. Likelihood-based learning approaches aim to match the model distribution pθ(x) to the true data distribution pd(x). Variational approaches achieve this by matching in the latent space the encoding distribution to the true posterior as well as maximizing a tight lower bound on log pθ(x), see for instance [103]. We show here analogous results for the multi- modal variational bound. Consider therefore the densities pθ(z, x) = pθ(z)pθ(xS|z)pθ(x\S|z) and qϕ(z, x) = pd(x)qϕ(z|x) = pd(xS)qϕ(z|xS)qϕ(x\S|z, xS). The standard interpretation is that the former is the generative density, while the latter is the encoding path consisting of the conditional variational approximation qϕ and the empirical density pd. The following Proposition, proven in Appendix A, shows that maximizing the variational lower bound L leads to a joint distribution matching of qϕ(z, x) and pθ(z, x), analogously to the uni-modal setting [150]. Proposition 2 (Joint distribution matching). For any S ∈ P(M), we have that � pd(x) � LS(xS, θ, ϕ, 1) + L\S(x, θ, ϕ, 1) � dx + H(pd(x)) = −KL(qϕ(z, x)|pθ(z, x)). In particular, LS(xS, θ, ϕ, 1) + L\S(x, θ, ϕ, 1) is a lower bound on log pθ(x). Moreover, Proposition 12 in Appendix A illustrates that maximizing � pd(xS)LS(xS, θ, ϕ)dxS drives (i) the joint inference distribution qϕ(z, xS) = pd(xS)qϕ(z|xS) of the S submodalities to the joint generative distribution pθ(z, xS) = pθ(z)pθ(xS|z) and (ii) the generative marginal pθ(xS) to its empirical counterpart pd(xS). Analogously, maximizing � pd(x\S|xS)L\S(x, θ, ϕ)dx\S drives (i) the distribution pd(x\S|xS)qϕ(z|x) to the distribution pθ(x\S|z)qϕ(z|xS) and (ii) the conditional pθ(x\S|xS) to its empirical counterpart pd(x\S|xS), provided that qϕ(z|xS) approximates pθ(z|xS) exactly. In this case, Proposition 12 implies that L\S(x, θ, ϕ) is a lower bound of log pθ(x\S|xS). Furthermore, it shows that L\S(x, θ, ϕ) contains a Bayes-consistency matching term for the multi- modal encoders where a mismatch can yield poor cross-generation, as an analogue of the prior not matching the aggregated posterior [87] leading to poor unconditional generation, see Remark 13. Our problem setup recovers meta-learning with (latent) Neural processes [34] when only optimizing the variational term L\S, where S is determined by context-target splits, cf. Appendix B. Information-theoretic perspective. Beyond generative modelling, β-VAEs [46] have been pop- ular for representation learning and data reconstruction. [3] suggest learning a latent representation that achieves certain mutual information with the data based on upper and lower variational bounds of the mutual information. A Legendre transformation thereof recovers the β-VAE objective and allows a trade-off between information content or rate versus reconstruction quality or distortion. We show that the proposed variational objective gives rise to an analogous perspective for multiple modalities. We recall first that mutual information Iqϕ(XS, Z) can be bounded by standard [11, 4, 3] lower and upper bounds using the rate and distortion: HS − DS ≤ HS − DS + ∆1 = Iqϕ(XS, Z) = RS − ∆2 ≤ RS, (6) 2For MoE aggregation schemes, [110] considered a stratified ELBO estimator as well as a tighter bound based on importance sampling, see also [93], that we do not pursue here for consistency with other aggregation schemes that can likewise be optimised based on importance sampling ideas. 4with ∆1, ∆2 ≥ 0 for the rate RS = � pd(xS)KL(qϕ(z|xS)|pθ(z))dxS measuring the information content that is encoded by qϕ into the latents, and the distortion DS = − � qϕ(xS, z) log pθ(xS|z)dzdxS given as the negative reconstruction log-likelihood. Observe that − � pd(xS)L(xS)dxS = DS + βRS and for any β > 0, it holds that HS ≤ RS + DS. To arrive at a similar interpretation for the conditional bound L\S, we set R\S = � pd(x)KL(qϕ(z|x)|qϕ(z|xS))dx for a conditional or cross rate. Similarly, set D\S = − � pd(x)qϕ(z|x) log pθ(x\S|z)dzdx. One obtains the following bounds, see Appendix A. Lemma 3 (Variational bounds on the conditional mutual information). It holds that − � L\S(x, θ, ϕ, β)pd(dx) = D\S + βR\S and for ∆\S,1, ∆\S,2 ≥ 0, H\S − D\S + ∆\S,1 = Iqϕ(X\S, ZM|XS) = R\S − ∆\S,2. Using the chain rules for entropy, we obtain that the suggested bound can be seen as a relaxation of bounds on marginal and conditional mutual information. Corollary 4 (Lagrangian relaxation). It holds that H − DS − D\S ≤ Iqϕ(XS, ZS) + Iqϕ(X\S, ZM|XS) ≤ RS + R\S and minimizing L for fixed β = ∂(DS+D\S) ∂(RS+R\S) minimizes the rates RS+R\S and distortions DS+D\S. Remark 5 (Mixture based variational bound). Rephrasing the arguments in [22], we can write − � pd(dx)LMix S (x) = DS + Dc \S + βRS, where Dc \S = � pd(xS)qϕ(z|xS) log pθ(x\S|z)dzdxS is a cross-distortion term. Due to H(XM|ZS) = −H(XM) + Iqϕ(XM, ZS) ≤ DS + Dc \S, we can view minimizing LMix S as minimizing H(XM) − Iqϕ(XM, ZS) + β Iqϕ(XS, ZS), see (2). Optimal variational distributions. Consider the annealed likelihood ˜pβ,θ(xS|z) ∝ pθ(xS|z)1/β as well as the adjusted posterior ˜pβ,θ(z|xS) ∝ ˜pβ,θ(xS|z)pθ(z). The minimum of the bound � pd(dx)LS(x) is attained at any xS for the variational density q⋆(z|xS) ∝ exp � 1 β [log pθ(xS|z) + β log pθ(z)] � ∝ ˜pβ,θ(z|xS), (7) see also [50]. Similarly, if (7) holds, then it is readily seen that the minimum of the bound � pd(dx)L\S(x) is attained at any x for the variational density q⋆(z|x) = ˜pβ,θ(z|x). In contrast, as shown in Appendix D, the optimal variational density for the mixture-based (1) multi-modal objective is attained at q⋆(z|xS) ∝ ˜pβ,θ(z|xS) exp �� pd(x\S|xS) log ˜pβ,θ(x\S|z)dx\S � . 3 Permutation-invariant modality encoding In order to optimize multi-modal bounds, we need to learn variational densities with different con- ditioning sets. To unify the presentation, let hs,φ : Xs �→ RDE be some modality-specific feature function that maps into a shared parameter space RDE. Fixed multi-modal aggregation schemes. We recall the following multi-modal encoding func- tions suggested in previous work where usually hs,φ(xs) = � µs,φ(xs)⊤, vec(Σs,φ(xs))⊤�⊤ with µs,φ and Σs,φ being the mean, respectively the (often diagonal) covariance, of a uni-modal encoder of modality s. Accommodating more complex variational families, such as mixture distributions for the uni-modal encoding distributions, can be more challenging for these approaches. • Mixture of Experts (MoE), see [110], qMoE φ (z|xS) = 1 |S| � s∈S qN (z|µs,φ(xs), Σs,φ(xs)), where qN (z|µ, Σ) is a Gaussian density with mean µ and covariance Σ. • Product of Experts (PoE), see [137], qPoE φ (z|xS) = 1 Z pθ(z) � s∈S qN (z|µs,φ(xs), Σs,φ(xs)), for some normalising constant Z. Under the assumption that the prior is Gaussian pθ(z) = qN (z|µθ, Σθ) with mean µθ ∈ RD and covariance matrix Σθ, the multi-modal encod- ing distribution qPoE φ (z|xS) is Gaussian with mean (µθΣθ + � s∈S µs,φ(xs)Σs,φ(xs))(Σ−1 1,θ + � s∈S Σs,φ(xs)−1)−1 and covariance (Σ−1 1,θ + � s∈S Σs,φ(xs)−1)−1. 5Learnable multi-modal aggregation schemes. We aim to learn a more flexible aggregation scheme under the constraint that the encoding distribution is invariant [15] with respect to the or- dering of encoded features of each modality. Put differently, for all (Hs)s∈S ∈ R|S|×DE and all permutations π ∈ SS of S, we assume that the conditional distribution is SS-invariant, i.e. q′ ϑ(z|h) = q′ ϑ(z|π · h) for all z ∈ RD, where π acts on H = (Hs)s∈S via π · H = (Hπ(s))s∈S. We set qϕ(z|xS) = q′ ϑ(z|hs,φ(xs)s∈S), ϕ = (φ, ϑ) and remark that the encoding distribution is not invariant with respect to the modalities, but becomes only invariant after applying modality-specific encoder functions hs,φ. Observe that such a constraint is satisfied by the aggregation schemes above for hs,φ being the encoding parameters for the uni-modal variational approximation. A variety of invariant (or equivariant) functions along with their approximation properties have been considered previously, see for instance [106, 144, 99, 75, 108, 94, 88, 105, 143, 18, 129, 147, 78, 12], and applied in different contexts such as meta-learning [27, 34, 64, 45, 38], reinforcement learning [118, 146] or generative modeling of (uni-modal) sets [77, 80, 65, 13, 79]. We can use such constructions to parameterise more flexible encoding distributions that allow for applying a reparameterisation trick [67, 100, 122]. Indeed, the results from [15] imply that for an exchangable sequence HS = (Hs)s∈S ∈ R|S|×DE and random variable Z, the distribution q′(z|hS) is SS- invariant if and only if there is a measurable function3 f ⋆ : [0, 1] × M(RDE) → RD such that (HS, Z) a.s. = (HS, f ⋆(Ξ, MHS)), where Ξ ∼ U[0, 1] and Ξ ⊥⊥ HS with MHS(·) = � s∈S δHs(·) being the empirical measure of hS, which retains the values of hS, but discards their order. For variational densities from a location-scale family such as a Gaussian or Laplace distribution, we find it more practical to consider a different reparameterisation in the form Z = µ(hS) + σ(hS) ⊙ Ξ, where Ξ is a sample from a parameter-free density p such as a standard Gaussian and Laplace distribution, while [µ(hS), log σ(hS)] = f(hS) for a permutation-invariant function f : R|S|×DE → R2D. Likewise, for mixture distributions thereof, we assume that [µ1(hS), log σ1(hS), . . . , µK(hS), log σK(hS), log ω(hS)] = f(hS) for a permutation-invariant function f : RDE → R2DK+K and Z = µL(hS) + σL(hS) ⊙ Ξ with L ∼ Cat(ω(hS)) denoting the sampled mixture component out of K mixtures. For simplicity, we consider here only two examples of permutation-invariant functions f that have representations with parameter ϑ in the form fϑ(hS) = ρϑ �� s∈S gϑ(hS)s � for a function ρϑ : RDP → RDO and permutation-equivariant function gϑ : RN×DE → RN×DP . Example 6 (Sum Pooling Encoders). The Deep Set [144] construction fϑ(hS) = ρϑ �� s∈S χϑ(hs) � applies the same neural network χϑ : RDE → RDP to each encoded feature hs. For simplicity, we assume that χϑ is a feed-forward neural network, and remark that pre-activation ResNets [43] have been advocated in [147] when χϑ contains multiple layers. For exponential fam- ily models, the optimal natural parameters of the posterior solve an optimisation problem where the dependence on the generative parameters from the different modalities decomposes as a sum, see Appendix G. Example 7 (Set Transformer Encoders). Let MTBϑ be a multi-head pre-layer-norm transformer block [130, 140], see Appendix E for precise definitions. For some neural network χϑ : RDE → RDP , set g0 S = χϑ(hS) and for k ∈ {1, . . . , L}, set gk S = MTBϑ(gk−1 S ). We then consider fϑ(hS) = ρϑ �� s∈S gL s � . This can be seen as a Set Transformer [75, 146] model without any inducing points as for most applications, a computational complexity that scales quadratically in the number of modalities can be acceptable. In our experiments, we use layer normalisation [8] within the transformer model, although, for example, set normalisation [146] could be used alternatively. Remark 8 (Pooling expert opinions). Combining expert distributions has a long tradition in decision theory and Bayesian inference, see [35] for early works, with popular schemes being linear pooling (i.e., MoE) or log-linear pooling (i.e., PoE with tempered densities). These are optimal schemes for minimizing different objectives, namely a weighted (forward or reverse) KL-divergence between the pooled distribution and the inidividual experts [1]. Log-linear pooling operators are externally Bayesian, that is, they allow for consistent Bayesian belief updates when each expert updates her belief with the same likelihood function [36]. 3The function f ⋆ generally depends on the cardinality of S. Finite-length exchangeable sequences imply a de Finetti latent variable representation only up to approximation errors [25]. 6Permutation-equivariance and private latent variables. Suppose that the generative model factorises as pθ(z, x) = p(z) � s∈M pθ(xs|z′, ˜zs) with z = (z′, ˜z1, . . . , ˜zM), for shared la- tent variables Z′ and private latent variable ˜Zs, s ∈ M. For s ̸= t ∈ [M], it holds that hφ,s(Xs) ⊥⊥ ˜Zt | Z′, ˜Zs. Under the assumption that we have modality-specific feature functions hφ,s such that {Hs = hφ,s(Xs)}s∈S is exchangeable, the results from [15] imply a permutation- equivariant representation of the private latent variables, conditional on the shared latent variables. This suggests to consider encoders for the private latent variables that satisfy q′ ϕ(˜zS|π·hφ(xS), z′) = q′ ϕ(π · ˜zS|hφ(xS), z′) for any permutation π ∈ SS. Details are given in Appendix F, including permutation-equivariant versions of PoEs, SumPooling and SelfAttention aggregations. 4 Identifiability and model extensions 4.1 Identifiability Identifiability of parameters and latent variables in latent structure models is a classic problem [70, 72, 5], that has been studied increasingly for non-linear latent variable models, e.g., for ICA [53, 40, 41], VAEs [62, 151, 135, 92, 82], EBMs [63], flow-based [112] or mixture models [68]. Non-linear generative models such as ICA are generally unidentifiable without imposing some struc- ture [54, 139]. However, identifiability up to some ambiguity can be achieved in some conditional models based on observed auxiliary variables and injective decoder functions wherein the prior den- sity is conditional on auxiliary variables. In our multi-modal setup, observations from different modalities can act as auxiliary variables to obtain identifiability of conditional distributions given some modality subset under an analogous assumption, see Appendix H. Example 9 (Auxiliary variable as a modality). In the iVAE model [62], the latent variable distribu- tion pθ(z|x1) is independently modulated via an auxiliary variable X1 = U. Instead of interpreting this distribution as a (conditional) prior density, we view it as a posterior density given the first modality X1. Assuming observations X2 from another modality, [62] estimate the model by lower bounding log pθ(x2|x1) via L\{1} under the assumption that qϕ(z|x1) is given by the prior density pθ(z|x1). Similarly, [91] optimise log pθ(x1, x2) by a double VAE bound that reduces to L for a masking distribution ρ(s1, s2) = (δ1 ⊗ δ0)(s1, s2) that always masks the modality X2 and choos- ing to parameterise separate encoding functions for different conditioning sets. Our bound thus generalises these procedures to multiple modalities in a scalable way. 4.2 Mixture models An alternative to the choice of uni-modal prior densities pθ has been to use Gaussian mixture priors [58, 57, 26] or more flexible mixture models [28]. Following previous work, we include a latent cluster indicator variable c ∈ [K] that indicates the mixture component out of K possible mixtures with augmented prior pθ(c, z) = pθ(c)pθ(z|c). The classic example is pθ(c) being a categorical distribution and pθ(z|c) a Gaussian with mean µc and covariance matrix Σc. Similar to [28] that use an optimal variational factor in a mean-field model, we use an optimal factor of the cluster indicator in a structured variational density qϕ(c, z|xS) = qϕ(z|xS)qϕ(c|z, xS) with qϕ(c|z, xS) = pθ(c|z) We show in greater detail in Appendix J how one can optimize an augmented multi-modal bound. 5 Experiments 5.1 Linear multi-modal VAEs The relationship between uni-modal VAEs and probabilistic principle component analysis [121] has been studied in previous work [21, 83, 102]. [50] considered a variational rate-distortion analysis for linear VAEs and [89] illustrated that varying β simply scales the inferred latent factors. Our focus will be the analysis of different multi-modal fusion schemes and multi-modal variational bounds in this setting. We perform a simulation study based on two different data generation mechanisms of multi-modal (M = 5) linear Gaussian models wherein i) all latent variables are shared across all modalities and ii) only parts of the latent variables are shared across all modalities with the re- maining latent variables being modality specific. The latter setting can be incorporated by imposing 7sparsity structures on the decoders and allows us to analyse scenarios with considerable modality- specific variation described through private latent variables. We refer to Appendix M for details about the data generation mechanisms. We assess the learned generative models and inferred latent representations by computing the true marginal log-likelihood of the multi-modal data, and addi- tionally assess the tightness of the variational bound. Results for case (i) of shared latent variables are given in Table 1, with the corresponding results for modality-specific latent variables found in Table 5 in Appendix M. In order to evaluate the (weak) identifiability of the method, we follow [62, 63] to compute the mean correlation co-efficient (MCC) between the true latent variables Z and samples from the variational distribution qϕ(·|xM) after an affine transformation using CCA. Our results suggest that first, more flexible aggregation schemes improve the log-likelihood, the tightness of the variational bound and the identifiability for both variational objectives. Second, our new bound provides a tighter approximation to the log-likelihood for different aggregation schemes. Additionally, we compute different rate and distortion terms in Appendix M, Figures 3 and 4 and the KL-divergence between the encoding distribution and the true posterior. Table 1: Multi-modal Gaussian model with dense decoders: LLH Gap is the relative difference of the log-likelihood of the learned model relative to the log-likelihood based on the exact MLE. Bound gap is the relative difference of the variational bound to the log-likelihood based on the MLE. Our bound Mixture bound Aggregation LLH Gap Bound Gap MCC LLH Gap Bound Gap MCC PoE 0.03 (0.058) 0.12 (0.241) 0.75 (0.20) 0.04 (0.074) 0.13 (0.220) 0.77 (0.21) MoE 0.01 (0.005) 0.02 (0.006) 0.82 (0.04) 0.02 (0.006) 0.11 (0.038) 0.67 (0.03) SumPooling 0.00 (0.000) 0.00 (0.000) 0.84 (0.00) 0.00 (0.002) 0.02 (0.003) 0.84 (0.02) SelfAttention 0.00 (0.003) 0.00 (0.003) 0.84 (0.00) 0.02 (0.007) 0.03 (0.007) 0.83 (0.00) 5.2 Non-linear models (a) Observed data X (b) True latents Z (c) Our bound with SumPooling (d) Our bound with PoE (e) Mixture bound with PoE Figure 1: Bi-modal model with label (colour-coded) and continuous modality in (a) with true latent variables in (b). Inferred latent variables in (c) - (e) with a linear transformation inditerminancy. Auxiliary labels as modalities. We construct artificial data following [62], with the latent vari- ables Z ∈ RD being conditionally Gaussian having means and variances that depend on an ob- served index value X2 ∈ [K]. More precisely, pθ(z|x2) = N(µx2, Σx2), where µc ∼ ⊗ U(−5, 5) and Σc = diag(Λc), Λc ∼ ⊗ U(0.5, 3) iid for c ∈ [K]. The marginal distribution over the labels is uniform U([K]) so that the prior density pθ(z) = � [K] pθ(z|x2)pθ(x2)dx2 becomes a Gaussian mixture. We choose an injective decoding function f1 : RD → RD1, D ≤ D1, as a composition of MLPs with LeakyReLUs and full rank weight matrices having monotonically increasing row di- mensions [63] and with iid randomly sampled entries. We assume X1|Z ∼ N(f1(Z), σ2 I). We set σ = 0.1, D = D1 = 2, and f1 has a single hidden layer of size D1 = 2. One realisation of bi-modal data X, the true latent variable Z, as well as inferred latent variables for a selection of different bounds and aggregation schemes, are shown in Figure 1, with more examples given in Figures 6 and 7. Results over multiple repetitions in Table 7 indicate that both a tighter variational bound and more flexible aggregation schemes improve the identifiability of the latent variables and the log-likelihood as estimated using importance sampling with 64 particles. 8Multiple modalities. Considering the same generative model for Z with a Gaussian mixture prior, suppose now that instead of observing the auxiliary label, we observe multiple modalities Xs ∈ RDs, Xs|Z ∼ N(fs(Z), σ2 I), for injective MLPs fs constructed as above, with D = 10, Ds = 25, σ = 0.5 and K = M = 5. We consider a semi-supervised setting where modali- ties are missing completely at random, as in [145], with a missing rate η as the sample average of 1 |M| � s∈M(1 − Ms). Our bound and the suggested permutation-invariant aggregation schemes can naturally accommodate this partially observed setting, see Appendix I for details. Table 2 shows that using the new variational bound improves the log-likelihood and the identifiability of the latent representation. Furthermore, using learnable aggregation schemes benefits both variational bounds. Table 2: Partially observed (η = 0.5) non-linear identifiable model with 5 modalities: The first four rows use a fixed standard Gaussian prior, while the last four rows use a Gaussian mixture prior with 5 components. Mean and standard deviation over 4 repetitions. Our bound Mixture Aggregation LLH Lower Bound MCC LLH Lower Bound MCC PoE -250.9 (5.19) -256.1 (5.43) 0.94 (0.015) -288.4 (8.53) -328.8 (9.17) 0.93 (0.018) MoE -250.1 (4.77) -255.3 (4.90) 0.92 (0.022) -286.2 (7.63) -325.1 (8.03) 0.90 (0.019) SumPooling -249.6 (4.85) -253.1 (4.84) 0.95 (0.016) -275.6 (7.35) -317.7 (8.72) 0.92 (0.031) SelfAttention -249.7 (4.83) -253.1 (4.84) 0.95 (0.014) -275.5 (7.45) -317.6 (8.68) 0.93 (0.022) SumPooling -247.3 (4.23) -251.9 (4.31) 0.95 (0.009) -269.6 (7.42) -311.5 (8.47) 0.94 (0.018) SelfAttention -247.5 (4.22) -252.1 (4.21) 0.95 (0.013) -269.9 (6.06) -311.6 (7.72) 0.93 (0.022) SumPoolingMixture -244.8 (4.44) -249.5 (5.84) 0.95 (0.011) -271.9 (6.54) -313.4 (7.30) 0.93 (0.021) SelfAttentionMixture -245.4 (4.55) -248.2 (4.80) 0.96 (0.010) -270.3 (5.96) -312.1 (7.61) 0.94 (0.016) 5.3 MNIST-SVHN-Text Following previous work [114, 115, 56], we consider a tri-modal dataset based on augmenting the MNIST-SVHN dataset [110] with a text-based modality comprised of the string with the English name of the digit at different starting positions. Herein, SVHN consists of relatively noisy images, whilst MNIST and text are clearer modalities. Multi-modal VAEs have been shown to exhibit dif- fering performances relative to their multi-modal coherence, latent classification accuracy or test log-likelihood, see Appendix L for definitions. Previous works often differ in their hyperparam- eters, from neural network architectures, latent space dimensions, priors and likelihood families, modality-specific likelihood weightings, fixed decoder variances, etc. However, we have chosen the same hyperparameters for all models, thereby providing a clearer disentanglement of how either the variational objective or the aggregation scheme affect different multi-modal evaluation measures. In particular, we consider multi-modal generative models with (i) shared latent variables and (ii) private and shared latent variables. As an additional benchmark we also consider PoE or MoE schemes (de- noted PoE+, resp., MoE+) with additional neural network layers in their modality-specific encoding functions so that the number of parameters matches or exceeds those of the introduced permutation- invariant models, see Appendix P.5 for details. For models without private latent variables, estimates of the test log-likelihoods in Table 3 suggest that our bound improves the log-likelihood across dif- ferent aggregation schemes for all modalities and differnet βs (Table 9), with similar results for permutation-equivariant schemes, except for a Self-Attention model. Furthermore, more flexible fusion schemes yield higher log-likelihoods for both bounds. We provide qualitative results for the reconstructed modalities in Figures 10 - 12. We believe that the clearest observation here is that realistic cross-generation of the SVHN modality is challenging for the mixture-based bound with all aggregation schemes. In contrast, our bound, particularly when combined with the learnable aggre- gation schemes, improves the cross-generation of SVHN. No bound or aggregation scheme performs best across all modalities by the generative coherence measures (see Table 4 for uni-modal inputs, Table 10 for bi-modal ones and Tables 11 - 14 for models with private latent variables and differ- ent βs). Overall, our bound is slightly more coherent for cross-generating SVHN or Text, but less coherent for MNIST. Furthermore, mixture based bounds tend to improve the unsupervised latent classifcation accuracy across different fusion approaches and modalities, see Table 15. To provide complementary insights into the trade-offs for the different bounds and fusion schemes, we consider a multi-modal rate-distortion evaluation in Figure 2. Ignoring MoE where reconstructions are simi- lar, observe that our bound improves the full reconstruction, with higher full rates, and across various fusion schemes. In contrast, mixture-based bounds yield improved cross-reconstructions for all ag- 9gregation models, with increased cross-rates terms. Flexible permutation-invariant architectures for our bound improve the full reconstruction, even at lower full rates. (a) Full Reconstr. −DM (b) Cross Reconstr. −Dc \S (c) Full Rates RM (d) Cross Rates R\S Figure 2: Rate and distortion terms for MNIST-SVHN-Text with shared latent variables (β = 1). Table 3: Test log-likelihood estimates for the joint data (M+S+T) and marginal data (importance sampling with 512 particles). The first part of the table is based on the same generative model with shared latent variable Z ∈ R40, while the second part of the table is based on a restrictive generative model with a shared latent variable Z′ ∈ R10 and modality-specific latent variables ˜Zs ∈ R10. Our bound Mixture bound Aggregation M+S+T M S T M+S+T M S T PoE+ 6872 (9.62) 2599 (5.6) 4317 (1.1) -9 (0.2) 5900 (10) 2449 (10.4) 3443 (11.7) -19 (0.4) PoE 6775 (54.9) 2585 (18.7) 4250 (8.1) -10 (2.2) 5813 (1.2) 2432 (11.6) 3390 (17.5) -19 (0.1) MoE+ 5428 (73.5) 2391 (104) 3378 (92.9) -74 (88.7) 5420 (60.1) 2364 (33.5) 3350 (58.1) -112 (133.4) MoE 5597 (26.7) 2449 (7.6) 3557 (26.4) -11 (0.1) 5485 (4.6) 2343 (1.8) 3415 (5.0) -17 (0.4) SumPooling 7056 (124) 2478 (9.3) 4640 (114) -6 (0.0) 6130 (4.4) 2470 (10.3) 3660 (1.5) -16 (1.6) SelfAttention 7011 (57.9) 2508 (18.2) 4555 (38.1) -7 (0.5) 6127 (26.1) 2510 (12.7) 3621 (8.5) -13 (0.2) PoE+ 6549 (33.2) 2509 (7.8) 4095 (37.2) -7 (0.2) 5869 (29.6) 2465 (4.3) 3431 (8.3) -19 (1.7) SumPooling 6337 (24.0) 2483 (9.8) 3965 (16.9) -6 (0.2) 5930 (23.8) 2468 (16.8) 3491 (18.3) -7 (0.1) SelfAttention 6662 (20.0) 2516 (8.8) 4247 (31.2) -6 (0.4) 6716 (21.8) 2430 (26.9) 4282 (49.7) -27 (1.1) Table 4: Conditional coherence with shared latent variables and uni-modal inputs. The letters on the second line represent the generated modality based on the input modalities on the line below it. Our bound Mixture bound M S T M S T Aggregation M S T M S T M S T M S T M S T M S T PoE 0.97 0.22 0.56 0.29 0.60 0.36 0.78 0.43 1.00 0.96 0.83 0.99 0.11 0.57 0.10 0.44 0.39 1.00 PoE+ 0.97 0.15 0.63 0.24 0.63 0.42 0.79 0.35 1.00 0.96 0.83 0.99 0.11 0.59 0.11 0.45 0.39 1.00 MoE 0.96 0.80 0.99 0.11 0.59 0.11 0.44 0.37 1.00 0.94 0.81 0.97 0.10 0.54 0.10 0.45 0.39 1.00 MoE+ 0.93 0.77 0.95 0.11 0.54 0.10 0.44 0.37 0.98 0.94 0.80 0.98 0.10 0.53 0.10 0.45 0.39 1.00 SumPooling 0.97 0.48 0.87 0.25 0.72 0.36 0.73 0.48 1.00 0.97 0.86 0.99 0.10 0.63 0.10 0.45 0.40 1.00 SelfAttention 0.97 0.44 0.79 0.20 0.71 0.36 0.61 0.43 1.00 0.97 0.86 0.99 0.10 0.63 0.11 0.45 0.40 1.00 6 Conclusion Limitations. A drawback of our bound is that computing a gradient step is more expensive as it requires drawing samples from two encoding distributions. Similarly, learning aggregation functions is more computationally expensive compared to fixed schemes. Mixture-based bounds might be preferred if one is interested primarily in cross-modal reconstructions. Outlook. Using modality-specific encoders to learn features and aggregating them with a permutation-invariant function is clearly not the only choice for building multi-modal encoding distributions. However, it allows us to utilize modality-specific architectures for the encoding func- tions. Alternatively, our bounds could also be used, e.g., when multi-modal transformer architectures [141] encode a distribution on a shared latent space. Our approach applies to general prior densities if we can compute its cross-entropy relative to the multi-modal encoding distributions. An extension would be to apply it with more flexible prior distributions, e.g., as specified via score-based genera- tive models [124]. The ideas in this work might also be of interest for other approaches that require flexible modeling of conditional distributions, such as in meta-learning with Neural processes. 10Acknowledgements This work is supported by funding from the Wellcome Leap 1kD Program and by the RIE2025 Human Potential Programme Prenatal/Early Childhood Grant (H22P0M0002), administered by A*STAR. The computational work for this article was partially performed on resources of the Na- tional Supercomputing Centre, Singapore (https://www.nscc.sg). References [1] A. E. Abbas. A Kullback-Leibler view of linear and log-linear pools. Decision Analysis, 6 (1):25–37, 2009. [2] S. Akaho. A kernel method for canonical correlation analysis. In International Meeting of Psychometric Society, 2001, 2001. [3] A. Alemi, B. Poole, I. Fischer, J. Dillon, R. A. Saurous, and K. Murphy. Fixing a broken elbo. In International conference on machine learning, pages 159–168. PMLR, 2018. [4] A. A. Alemi, I. Fischer, J. V. Dillon, and K. Murphy. Deep Variational Information Bottle- neck. arXiv preprint arXiv:1612.00410, 2016. [5] E. S. Allman, C. Matias, and J. A. Rhodes. Identifiability of parameters in latent structure models with many observed variables. The Annals of Statistics, 37(6A):3099–3132, 2009. [6] C. Archambeau and F. Bach. Sparse probabilistic projections. Advances in neural information processing systems, 21, 2008. [7] R. Argelaguet, B. Velten, D. Arnol, S. Dietrich, T. Zenz, J. C. Marioni, F. Buettner, W. Huber, and O. Stegle. Multi-Omics Factor Analysis—a framework for unsupervised integration of multi-omics data sets. Molecular systems biology, 14(6):e8124, 2018. [8] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [9] F. R. Bach and M. I. Jordan. A Probabilistic Interpretation of Canonical Correlation Analysis. 2005. [10] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014. [11] D. Barber and F. Agakov. The IM Algorithm: a variational approach to Information Maxi- mization. Advances in neural information processing systems, 16(320):201, 2004. [12] S. Bartunov, F. B. Fuchs, and T. P. Lillicrap. Equilibrium aggregation: Encoding sets via optimization. In Uncertainty in Artificial Intelligence, pages 139–149. PMLR, 2022. [13] M. Biloˇs and S. G¨unnemann. Scalable normalizing flows for permutation invariant densities. In International Conference on Machine Learning, pages 957–967. PMLR, 2021. [14] D. M. Blei, A. Kucukelbir, and J. D. McAuliffe. Variational inference: A review for statisti- cians. Journal of the American Statistical Association, 112(518):859–877, 2017. [15] B. Bloem-Reddy and Y. W. Teh. Probabilistic symmetries and invariant neural networks. J. Mach. Learn. Res., 21:90–1, 2020. [16] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable trans- formations of Python+NumPy programs, 2018. URL http://github.com/google/jax. [17] M. Browne. Factor analysis of multiple batteries by maximum likelihood. British Journal of Mathematical and Statistical Psychology, 1980. [18] A. Bruno, J. Willette, J. Lee, and S. J. Hwang. Mini-batch consistent slot set encoder for scalable set encoding. Advances in Neural Information Processing Systems, 34:21365–21374, 2021. 11[19] Z. Chen, V. Badrinarayanan, C.-Y. Lee, and A. Rabinovich. Gradnorm: Gradient normaliza- tion for adaptive loss balancing in deep multitask networks. In International conference on machine learning, pages 794–803. PMLR, 2018. [20] J. Chung, K. Kastner, L. Dinh, K. Goel, A. C. Courville, and Y. Bengio. A recurrent latent variable model for sequential data. In Advances in neural information processing systems, pages 2980–2988, 2015. [21] B. Dai, Y. Wang, J. Aston, G. Hua, and D. Wipf. Connections with robust PCA and the role of emergent sparsity in variational autoencoder models. The Journal of Machine Learning Research, 19(1):1573–1614, 2018. [22] I. Daunhawer, T. M. Sutter, K. Chin-Cheong, E. Palumbo, and J. E. Vogt. On the Limitations of Multimodal VAEs. In International Conference on Learning Representations, 2022. [23] I. Daunhawer, A. Bizeul, E. Palumbo, A. Marx, and J. E. Vogt. Identifiability results for multimodal contrastive learning. arXiv preprint arXiv:2303.09166, 2023. [24] P. Dhariwal and A. Nichol. Diffusion models beat GANs on image synthesis. Advances in Neural Information Processing Systems, 34:8780–8794, 2021. [25] P. Diaconis and D. Freedman. Finite exchangeable sequences. The Annals of Probability, pages 745–764, 1980. [26] N. Dilokthanakul, P. A. Mediano, M. Garnelo, M. C. Lee, H. Salimbeni, K. Arulkumaran, and M. Shanahan. Deep unsupervised clustering with Gaussian Mixture Variational Autoen- coders. arXiv preprint arXiv:1611.02648, 2016. [27] H. Edwards and A. Storkey. Towards a neural statistician. arXiv preprint arXiv:1606.02185, 2016. [28] F. Falck, H. Zhang, M. Willetts, G. Nicholson, C. Yau, and C. C. Holmes. Multi-facet clus- tering Variational Autoencoders. Advances in Neural Information Processing Systems, 34: 8676–8690, 2021. [29] M. Figurnov, S. Mohamed, and A. Mnih. Implicit reparameterization gradients. In Advances in Neural Information Processing Systems, pages 441–452, 2018. [30] J. Fliege and B. F. Svaiter. Steepest descent methods for multicriteria optimization. Mathe- matical methods of operations research, 51:479–494, 2000. [31] A. Foong, W. Bruinsma, J. Gordon, Y. Dubois, J. Requeima, and R. Turner. Meta-learning stationary stochastic process prediction with convolutional neural processes. Advances in Neural Information Processing Systems, 33:8284–8295, 2020. [32] S. Gao, R. Brekelmans, G. Ver Steeg, and A. Galstyan. Auto-encoding total correlation explanation. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1157–1166. PMLR, 2019. [33] M. Garnelo, D. Rosenbaum, C. Maddison, T. Ramalho, D. Saxton, M. Shanahan, Y. W. Teh, D. Rezende, and S. A. Eslami. Conditional neural processes. In International conference on machine learning, pages 1704–1713. PMLR, 2018. [34] M. Garnelo, J. Schwarz, D. Rosenbaum, F. Viola, D. J. Rezende, S. Eslami, and Y. W. Teh. Neural processes. arXiv preprint arXiv:1807.01622, 2018. [35] C. Genest and J. V. Zidek. Combining probability distributions: A critique and an annotated bibliography. Statistical Science, 1(1):114–135, 1986. [36] C. Genest, K. J. McConway, and M. J. Schervish. Characterization of externally Bayesian pooling operators. The Annals of Statistics, pages 487–501, 1986. [37] S. Ghalebikesabi, R. Cornish, L. J. Kelly, and C. Holmes. Deep generative pattern-set mixture models for nonignorable missingness. arXiv preprint arXiv:2103.03532, 2021. 12[38] G. Giannone and O. Winther. Scha-vae: Hierarchical context aggregation for few-shot gen- eration. In International Conference on Machine Learning, pages 7550–7569. PMLR, 2022. [39] Y. Gong, H. Hajimirsadeghi, J. He, T. Durand, and G. Mori. Variational selective autoen- coder: Learning from partially-observed heterogeneous data. In International Conference on Artificial Intelligence and Statistics, pages 2377–2385. PMLR, 2021. [40] H. H¨alv¨a and A. Hyvarinen. Hidden markov nonlinear ica: Unsupervised learning from nonstationary time series. In Conference on Uncertainty in Artificial Intelligence, pages 939– 948. PMLR, 2020. [41] H. H¨alv¨a, S. Le Corff, L. Leh´ericy, J. So, Y. Zhu, E. Gassiat, and A. Hyvarinen. Disentangling identifiable features from noisy data with structured nonlinear ICA. Advances in Neural Information Processing Systems, 34:1624–1633, 2021. [42] D. R. Hardoon, S. Szedmak, and J. Shawe-Taylor. Canonical correlation analysis: An overview with application to learning methods. Neural computation, 16(12):2639–2664, 2004. [43] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV 14, pages 630–645. Springer, 2016. [44] J. Heek, A. Levskaya, A. Oliver, M. Ritter, B. Rondepierre, A. Steiner, and M. van Zee. Flax: A neural network library and ecosystem for JAX, 2023. URL http://github.com/ google/flax. [45] L. B. Hewitt, M. I. Nye, A. Gane, T. Jaakkola, and J. B. Tenenbaum. The variational homoen- coder: Learning to learn high capacity generative models from few examples. arXiv preprint arXiv:1807.08919, 2018. [46] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Ler- chner. β-VAE: Learning basic visual concepts with a constrained variational framework. In International conference on learning representations, 2017. [47] J. Ho and T. Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [48] M. D. Hoffman and M. J. Johnson. ELBO surgery: yet another way to carve up the variational evidence lower bound. In Workshop in Advances in Approximate Bayesian Inference, NIPS, 2016. [49] H. Hotelling. Relations between two sets of variates. Biometrika, 28(3/4):321–377, 1936. [50] S. Huang, A. Makhzani, Y. Cao, and R. Grosse. Evaluating lossy compression rates of deep generative models. arXiv preprint arXiv:2008.06653, 2020. [51] Y. Huang, J. Lin, C. Zhou, H. Yang, and L. Huang. Modality competition: What makes joint training of multi-modal network fail in deep learning?(provably). arXiv preprint arXiv:2203.12221, 2022. [52] H. Hwang, G.-H. Kim, S. Hong, and K.-E. Kim. Multi-view representation learning via total correlation objective. Advances in Neural Information Processing Systems, 34:12194–12207, 2021. [53] A. Hyvarinen and H. Morioka. Unsupervised feature extraction by time-contrastive learning and nonlinear ICA. Advances in neural information processing systems, 29, 2016. [54] A. Hyv¨arinen and P. Pajunen. Nonlinear Independent Component Analysis: Existence and uniqueness results. Neural networks, 12(3):429–439, 1999. [55] N. B. Ipsen, P.-A. Mattei, and J. Frellsen. not-MIWAE: Deep Generative Modelling with Missing not at Random Data. In ICLR 2021-International Conference on Learning Repre- sentations, 2021. 13[56] A. Javaloy, M. Meghdadi, and I. Valera. Mitigating Modality Collapse in Multimodal VAEs via Impartial Optimization. arXiv preprint arXiv:2206.04496, 2022. [57] Z. Jiang, Y. Zheng, H. Tan, B. Tang, and H. Zhou. Variational deep embedding: an unsuper- vised and generative approach to clustering. In Proceedings of the 26th International Joint Conference on Artificial Intelligence, pages 1965–1972, 2017. [58] M. J. Johnson, D. Duvenaud, A. B. Wiltschko, S. R. Datta, and R. P. Adams. Structured vaes: Composing probabilistic graphical models and variational autoencoders. arXiv preprint arXiv:1603.06277, 2016. [59] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for graphical models. Machine learning, 37(2):183–233, 1999. [60] T. Joy, Y. Shi, P. H. Torr, T. Rainforth, S. M. Schmon, and N. Siddharth. Learning multimodal VAEs through mutual supervision. arXiv preprint arXiv:2106.12570, 2021. [61] M. Karami and D. Schuurmans. Deep probabilistic canonical correlation analysis. In Pro- ceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 8055–8063, 2021. [62] I. Khemakhem, D. Kingma, R. Monti, and A. Hyvarinen. Variational Autoencoders and nonlinear ICA: A unifying framework. In International Conference on Artificial Intelligence and Statistics, pages 2207–2217. PMLR, 2020. [63] I. Khemakhem, R. Monti, D. Kingma, and A. Hyvarinen. ICE-BeeM: Identifiable Conditional Energy-Based Deep Models Based on Nonlinear ICA. Advances in Neural Information Pro- cessing Systems, 33:12768–12778, 2020. [64] H. Kim, A. Mnih, J. Schwarz, M. Garnelo, A. Eslami, D. Rosenbaum, O. Vinyals, and Y. W. Teh. Attentive neural processes. In International Conference on Learning Representations, 2018. [65] J. Kim, J. Yoo, J. Lee, and S. Hong. Setvae: Learning hierarchical composition for generative modeling of set-structured data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15059–15068, 2021. [66] D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [67] D. P. Kingma and M. Welling. Auto-Encoding Variational Bayes. Proceedings of the 2nd International Conference on Learning Representations (ICLR), 2014. [68] B. Kivva, G. Rajendran, P. K. Ravikumar, and B. Aragam. Identifiability of deep genera- tive models without auxiliary information. In Advances in Neural Information Processing Systems, 2022. [69] A. Klami, S. Virtanen, and S. Kaski. Bayesian canonical correlation analysis. Journal of Machine Learning Research, 14(4), 2013. [70] T. C. Koopmans and O. Reiersol. The identification of structural characteristics. The Annals of Mathematical Statistics, 21(2):165–181, 1950. [71] D. Kramer, P. L. Bommer, D. Durstewitz, C. Tombolini, and G. Koppe. Reconstructing nonlinear dynamical systems from multi-modal time series. In International Conference on Machine Learning, pages 11613–11633. PMLR, 2022. [72] J. B. Kruskal. More factors than subjects, tests and treatments: An indeterminacy theorem for canonical decomposition and individual differences scaling. Psychometrika, 41(3):281–293, 1976. [73] T. A. Le, H. Kim, M. Garnelo, D. Rosenbaum, J. Schwarz, and Y. W. Teh. Empirical eval- uation of neural process objectives. In NeurIPS workshop on Bayesian Deep Learning, vol- ume 4, 2018. 14[74] C. Lee and M. van der Schaar. A variational information bottleneck approach to multi-omics data integration. In International Conference on Artificial Intelligence and Statistics, pages 1513–1521. PMLR, 2021. [75] J. Lee, Y. Lee, J. Kim, A. Kosiorek, S. Choi, and Y. W. Teh. Set Transformer: A framework for attention-based permutation-invariant neural networks. In International conference on machine learning, pages 3744–3753. PMLR, 2019. [76] M. Lee and V. Pavlovic. Private-shared disentangled multimodal vae for learning of latent representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1692–1700, 2021. [77] C.-L. Li, M. Zaheer, Y. Zhang, B. Poczos, and R. Salakhutdinov. Point cloud GAN. arXiv preprint arXiv:1810.05795, 2018. [78] Q. Li, T. Lin, and Z. Shen. Deep neural network approximation of invariant functions through dynamical systems. arXiv preprint arXiv:2208.08707, 2022. [79] Y. Li and J. Oliva. Partially observed exchangeable modeling. In International Conference on Machine Learning, pages 6460–6470. PMLR, 2021. [80] Y. Li, H. Yi, C. Bender, S. Shan, and J. B. Oliva. Exchangeable neural ode for set modeling. Advances in Neural Information Processing Systems, 33:6936–6946, 2020. [81] R. Linsker. Self-organization in a perceptual network. Computer, 21(3):105–117, 1988. [82] C. Lu, Y. Wu, J. M. Hern´andez-Lobato, and B. Sch¨olkopf. Invariant causal representation learning for out-of-distribution generalization. In International Conference on Learning Rep- resentations, 2022. [83] J. Lucas, G. Tucker, R. B. Grosse, and M. Norouzi. Don’t Blame the ELBO! A Linear VAE Perspective on Posterior Collapse. In Advances in Neural Information Processing Systems, pages 9408–9418, 2019. [84] Q. Lyu and X. Fu. Finite-sample analysis of deep CCA-based unsupervised post-nonlinear multimodal learning. IEEE Transactions on Neural Networks and Learning Systems, 2022. [85] Q. Lyu, X. Fu, W. Wang, and S. Lu. Understanding latent correlation-based multiview learn- ing and self-supervision: An identifiability perspective. arXiv preprint arXiv:2106.07115, 2021. [86] C. Ma, S. Tschiatschek, K. Palla, J. M. Hernandez-Lobato, S. Nowozin, and C. Zhang. EDDI: Efficient Dynamic Discovery of High-Value Information with Partial VAE. In International Conference on Machine Learning, pages 4234–4243. PMLR, 2019. [87] A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and B. Frey. Adversarial Autoencoders. In ICLR, 2016. [88] H. Maron, E. Fetaya, N. Segol, and Y. Lipman. On the universality of invariant networks. In International conference on machine learning, pages 4363–4371. PMLR, 2019. [89] E. Mathieu, T. Rainforth, N. Siddharth, and Y. W. Teh. Disentangling disentanglement in Variational Autoencoders. In International Conference on Machine Learning, pages 4402– 4412. PMLR, 2019. [90] K. Minoura, K. Abe, H. Nam, H. Nishikawa, and T. Shimamura. A mixture-of-experts deep generative model for integrated analysis of single-cell multiomics data. Cell reports methods, 1(5):100071, 2021. [91] G. Mita, M. Filippone, and P. Michiardi. An identifiable double VAE for disentangled rep- resentations. In International Conference on Machine Learning, pages 7769–7779. PMLR, 2021. [92] G. E. Moran, D. Sridhar, Y. Wang, and D. M. Blei. Identifiable deep generative models via sparse decoding. arXiv preprint arXiv:2110.10804, 2021. 15[93] W. Morningstar, S. Vikram, C. Ham, A. Gallagher, and J. Dillon. Automatic differentiation variational inference with mixtures. In International Conference on Artificial Intelligence and Statistics, pages 3250–3258. PMLR, 2021. [94] R. Murphy, B. Srinivasan, V. Rao, and B. Riberio. Janossy pooling: Learning deep permutation-invariant functions for variable-size inputs. In International Conference on Learning Representations (ICLR 2019), 2019. [95] A. Nazabal, P. M. Olmos, Z. Ghahramani, and I. Valera. Handling incomplete heterogeneous data using VAEs. Pattern Recognition, 107:107501, 2020. [96] A. v. d. Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. [97] E. Palumbo, I. Daunhawer, and J. E. Vogt. Mmvae+: Enhancing the generative quality of mul- timodal vaes without compromises. In The Eleventh International Conference on Learning Representations, 2023. [98] B. Poole, S. Ozair, A. Van Den Oord, A. Alemi, and G. Tucker. On variational bounds of mutual information. In International Conference on Machine Learning, pages 5171–5180. PMLR, 2019. [99] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 652–660, 2017. [100] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1278–1286, 2014. [101] G. Roeder, Y. Wu, and D. Duvenaud. Sticking the landing: An asymptotically zero-variance gradient estimator for variational inference. arXiv preprint arXiv:1703.09194, 2017. [102] M. Rolinek, D. Zietlow, and G. Martius. Variational Autoencoders pursue PCA directions (by accident). In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12406–12415, 2019. [103] M. Rosca, B. Lakshminarayanan, and S. Mohamed. Distribution matching in variational inference. arXiv preprint arXiv:1802.06847, 2018. [104] D. B. Rubin. Inference and missing data. Biometrika, 63(3):581–592, 1976. [105] A. Sannai, Y. Takai, and M. Cordonnier. Universal approximations of permutation invari- ant/equivariant functions by deep neural networks. arXiv preprint arXiv:1903.01939, 2019. [106] A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski, R. Pascanu, P. Battaglia, and T. Lilli- crap. A simple neural network module for relational reasoning. Advances in neural informa- tion processing systems, 30, 2017. [107] S. Schneider, J. H. Lee, and M. W. Mathis. Learnable latent embeddings for joint behavioural and neural analysis. Nature, pages 1–9, 2023. [108] N. Segol and Y. Lipman. On universal equivariant set networks. In International Conference on Learning Representations, 2019. [109] O. Sener and V. Koltun. Multi-task learning as multi-objective optimization. Advances in neural information processing systems, 31, 2018. [110] Y. Shi, B. Paige, P. Torr, et al. Variational Mixture-of-Experts Autoencoders for Multi-Modal Deep Generative Models. Advances in Neural Information Processing Systems, 32, 2019. [111] Y. Shi, B. Paige, P. Torr, and N. Siddharth. Relating by Contrasting: A Data-efficient Frame- work for Multimodal Generative Models. In International Conference on Learning Repre- sentations, 2020. 16[112] P. Sorrenson, C. Rother, and U. K¨othe. Disentanglement by nonlinear ICA with general incompressible-flow networks (GIN). arXiv preprint arXiv:2001.04872, 2020. [113] J. H. Stock and M. W. Watson. Forecasting using principal components from a large number of predictors. Journal of the American statistical association, 97(460):1167–1179, 2002. [114] T. Sutter, I. Daunhawer, and J. Vogt. Multimodal generative learning utilizing Jensen- Shannon-divergence. Advances in Neural Information Processing Systems, 33:6100–6110, 2020. [115] T. M. Sutter, I. Daunhawer, and J. E. Vogt. Generalized multimodal elbo. In 9th International Conference on Learning Representations (ICLR 2021), 2021. [116] M. Suzuki and Y. Matsuo. Mitigating the Limitations of Multimodal VAEs with Coordination- based Approach. 2022. [117] M. Suzuki, K. Nakayama, and Y. Matsuo. Joint multimodal learning with deep generative models. arXiv preprint arXiv:1611.01891, 2016. [118] Y. Tang and D. Ha. The sensory neuron as a transformer: Permutation-invariant neural net- works for reinforcement learning. Advances in Neural Information Processing Systems, 34: 22574–22587, 2021. [119] A. Tenenhaus and M. Tenenhaus. Regularized generalized Canonical Correlation Analysis. Psychometrika, 76:257–284, 2011. [120] Y. Tian, D. Krishnan, and P. Isola. Contrastive multiview coding. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XI 16, pages 776–794. Springer, 2020. [121] M. E. Tipping and C. M. Bishop. Probabilistic Principal Component Analysis. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):611–622, 1999. [122] M. Titsias and M. L´azaro-Gredilla. Doubly stochastic variational bayes for non-conjugate inference. In Proceedings of the 31st International Conference on Machine Learning (ICML- 14), pages 1971–1979, 2014. [123] Y.-H. H. Tsai, S. Bai, P. P. Liang, J. Z. Kolter, L.-P. Morency, and R. Salakhutdinov. Mul- timodal transformer for unaligned multimodal language sequences. In Proceedings of the conference. Association for Computational Linguistics. Meeting, volume 2019, page 6558. NIH Public Access, 2019. [124] A. Vahdat, K. Kreis, and J. Kautz. Score-based generative modeling in latent space. Advances in Neural Information Processing Systems, 34, 2021. [125] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [126] R. Vedantam, I. Fischer, J. Huang, and K. Murphy. Generative models of visually grounded imagination. In International Conference on Learning Representations, 2018. [127] G. Ver Steeg and A. Galstyan. Maximally informative hierarchical representations of high- dimensional data. In Artificial Intelligence and Statistics, pages 1004–1012. PMLR, 2015. [128] S. Virtanen, A. Klami, S. Khan, and S. Kaski. Bayesian group factor analysis. In Artificial Intelligence and Statistics, pages 1269–1277. PMLR, 2012. [129] E. Wagstaff, F. B. Fuchs, M. Engelcke, M. A. Osborne, and I. Posner. Universal approxima- tion of functions on sets. Journal of Machine Learning Research, 23(151):1–56, 2022. [130] Q. Wang, B. Li, T. Xiao, J. Zhu, C. Li, D. F. Wong, and L. S. Chao. Learning deep transformer models for machine translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1810–1822, 2019. 17[131] T. Wang and P. Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Machine Learning, pages 9929–9939. PMLR, 2020. [132] W. Wang, R. Arora, K. Livescu, and J. Bilmes. On deep multi-view representation learning. In International conference on machine learning, pages 1083–1092. PMLR, 2015. [133] W. Wang, X. Yan, H. Lee, and K. Livescu. Deep Variational Canonical Correlation Analysis. arXiv preprint arXiv:1610.03454, 2016. [134] W. Wang, D. Tran, and M. Feiszli. What makes training multi-modal classification networks hard? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog- nition, pages 12695–12705, 2020. [135] Y. Wang, D. Blei, and J. P. Cunningham. Posterior collapse and latent variable non- identifiability. Advances in Neural Information Processing Systems, 34:5443–5455, 2021. [136] S. Watanabe. Information theoretical analysis of multivariate correlation. IBM Journal of research and development, 4(1):66–82, 1960. [137] M. Wu and N. Goodman. Multimodal generative models for scalable weakly-supervised learning. Advances in Neural Information Processing Systems, 31, 2018. [138] M. Wu and N. Goodman. Multimodal generative models for compositional representation learning. arXiv preprint arXiv:1912.05075, 2019. [139] Q. Xi and B. Bloem-Reddy. Indeterminacy in latent variable models: Characterization and strong identifiability. arXiv preprint arXiv:2206.00801, 2022. [140] R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y. Lan, L. Wang, and T. Liu. On layer normalization in the transformer architecture. In International Conference on Machine Learning, pages 10524–10533. PMLR, 2020. [141] P. Xu, X. Zhu, and D. A. Clifton. Multimodal learning with transformers: A survey. arXiv preprint arXiv:2206.06488, 2022. [142] T. Yu, S. Kumar, A. Gupta, S. Levine, K. Hausman, and C. Finn. Gradient surgery for multi- task learning. Advances in Neural Information Processing Systems, 33:5824–5836, 2020. [143] C. Yun, S. Bhojanapalli, A. S. Rawat, S. Reddi, and S. Kumar. Are transformers universal approximators of sequence-to-sequence functions? In International Conference on Learning Representations, 2019. [144] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. R. Salakhutdinov, and A. J. Smola. Deep Sets. Advances in neural information processing systems, 30, 2017. [145] C. Zhang, Z. Han, H. Fu, J. T. Zhou, Q. Hu, et al. CPM-Nets: Cross partial multi-view networks. Advances in Neural Information Processing Systems, 32, 2019. [146] F. Zhang, B. Liu, K. Wang, V. Y. Tan, Z. Yang, and Z. Wang. Relational Reasoning via Set Transformers: Provable Efficiency and Applications to MARL. arXiv preprint arXiv:2209.09845, 2022. [147] L. Zhang, V. Tozzo, J. Higgins, and R. Ranganath. Set Norm and Equivariant Skip Connec- tions: Putting the Deep in Deep Sets. In International Conference on Machine Learning, pages 26559–26574. PMLR, 2022. [148] Y. Zhang, H. Jiang, Y. Miura, C. D. Manning, and C. P. Langlotz. Contrastive learning of medical visual representations from paired images and text. In Machine Learning for Healthcare Conference, pages 2–25. PMLR, 2022. [149] S. Zhao, C. Gao, S. Mukherjee, and B. E. Engelhardt. Bayesian group factor analysis with structured sparsity. The Journal of Machine Learning Research, 2016. 18[150] S. Zhao, J. Song, and S. Ermon. InfovVAE: Balancing Learning and Inference in Variational Autoencoders. In Proceedings of the aaai conference on artificial intelligence, volume 33, pages 5885–5892, 2019. [151] D. Zhou and X.-X. Wei. Learning identifiable and interpretable latent models of high- dimensional neural activity using pi-VAE. Advances in Neural Information Processing Sys- tems, 33:7234–7247, 2020. 19Contents A Multi-modal distribution matching 21 B Meta-learning and Neural processes 23 C Information-theoretic perspective 24 D Optimal variational distributions 25 E Permutation-invariant architectures 25 F Permutation-equivariance and private latent variables 27 G Multi-modal posterior in exponential family models 29 H Identifiability 29 I Missing modalities 31 J Mixture model extensions for different variational bounds 31 K Algorithm and STL-gradient estimators 32 L Evaluation of multi-modal generative models 32 M Linear models 33 N Non-linear identifiable models 35 N.1 Auxiliary labels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 N.2 Five continuous modalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 O MNIST-SVHN-Text 38 O.1 Training hyperparamters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 O.2 Multi-modal rates and distortions . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 O.3 Log-likelihood estimates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 O.4 Generated modalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 O.5 Conditional coherence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 P Encoder Model architectures 42 P.1 Linear models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 P.2 Linear models with private latent variables . . . . . . . . . . . . . . . . . . . . . . 42 P.3 Nonlinear model with auxiliary label . . . . . . . . . . . . . . . . . . . . . . . . . 42 P.4 Nonlinear model with five modalities . . . . . . . . . . . . . . . . . . . . . . . . . 42 P.5 MNIST-SVHN-Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 P.6 MNIST-SVHN-Text with private latent variables . . . . . . . . . . . . . . . . . . 42 20Q MNIST-SVHN-Text Decoder Model architectures 42 R Compute resources and existing assets 43 A Multi-modal distribution matching Proof of Proposition 2. Our proof extends the arguments in [138]. Observe first that for any S ⊂ M, the encoding distribution is marginally consistent in the sense that it holds that � X\S qϕ(z, x)dx\S = � X\S pd(xS)qϕ(x\S|z, xS)qϕ(z|xS)dx\S = pd(xS)qϕ(z|xS). Consequently, KL(qϕ(z, x)|pθ(z, x)) = � X×Z log pd(xS)pd(x\S|xS)qϕ(z|x)qϕ(z|xS) pθ(z)pθ(xS|z)pθ(x\S|z)qϕ(z|xS) pd(x)qϕ(z|x)dxdz = � XS×Z log pd(xS)qϕ(z|xS) pθ(z)pθ(xS|z) � X\S � qϕ(z, x)dx\S � dxSdz + � X×Z pd(x)qϕ(z|x) log pd(x\S|xS)qϕ(z|x) pθ(x\S|z)qϕ(z|xS) dxdz = � XS×Z pd(xS)qϕ(z|xS) log qϕ(z|xS) pθ(z)pθ(xS|z)dxSdz − H(pd(xS)) + � X×Z pd(x)qϕ(z|x) log qϕ(z|x) pθ(x\S|z)qϕ(z|xS)dxdz − H(pd(x\S|xS)). The claim follows by the chain rule for the entropy. Following the same arguments as for uni-modal VAEs, this establishes a lower bound on the log- likelihood. Corollary 10 (Tight lower bound on multi-modal log-likelihood). For any modality mask S, we have � pd(x) � LS(xS, θ, ϕ, 1) + L\S(x, θ, ϕ, 1) � dx = � pd(x) [log pθ(x) − KL(qϕ(z|x)|pθ(z|x))] dx. Proof. Recall that qϕ(z, x) = pd(x)qϕ(z|x). Proposition 12 implies then that � pd(x) � LS(xS, θ, ϕ, 1) + L\S(x, θ, ϕ, 1) � dx = − KL(qϕ(z, x)|pθ(z, x)) − H(pd(x)) = − � pd(x) � qϕ(z|x) log qϕ(z|x) − log pθ(z, x))dzdx = � pd(x) log pθ(x)dx − � qϕ(z|x) (log pθ(z|x) − log qϕ(z|x)) dzdx. Remark 11. Corollary 10 shows that the variational bound becomes tight if the encoding distri- bution closely approximates the true posterior distribution. A similar result does not hold for the mixture-based multi-modal bound. Indeed, as shown in [22], there is a gap between the variational bound and the log-likelihood given by the conditional entropies that cannot be reduced even for flexible encoding distributions. Moreover, our bound can be tight for an arbitrary number of modal- ities. In contrast, [22] show that for mixture-based bounds, this variational gap increases with each additional modality, if the new modality is ’sufficiently diverse’. 21Proposition 12 (Marginal and conditional distribution matching). For any S ∈ P(M), we have � pd(xS)LS(xS, θ, ϕ)dxS + H(pd(xS)) = − KL(qϕ(z, xS)|pθ(z, xS)) (ZXmarginal) = − KL(pd(xS)|pθ(xS)) − � pd(xS)KL(qϕ(z|xS)|pθ(z|xS))dxS (Xmarginal) = − KL(qagg ϕ,S(z)|pθ(z)) − � qagg ϕ,S(z)KL(q⋆(xS|z)|pθ(xS|z))dxS, (Zmarginal) where qagg ϕ,S(z) = � pd(xS)qϕ(z|xS)dxS is the aggregated prior [87] restricted on modalities from S and q⋆(xS|z) = qϕ(xS, z)/qagg ϕ (z). Moreover, for fixed xS, � pd(x\S|xS)L\S(x, θ, ϕ)dx\S + H(pd(x\S|xS)) = − KL � qϕ(z|x)pd(x\S|xS) ��pθ(x\S|z)qϕ(z|xS) � (ZXconditional) = − KL(pd(x\S|xS)|pθ(x\S|xS)) (Xconditional) − � pd(x\S|xS) � KL(qϕ(z|x)|pθ(z|x)) + � qϕ(z|x) log qϕ(z|xS) pθ(z|xS)dz � dx\S = − KL(qagg ϕ,\S(z|xS)|qϕ(z|xS)) − � qagg ϕ,\S(z|xS) � KL(q⋆(x\S|z, xS)|pθ(xS|z)) � dz, (Zconditional) where qagg ϕ,\S(z|xS) = � pd(x\S|xS)qϕ(z|x)dx\S can be seen as an aggregated encoder conditioned on xS and q⋆(x\S|z, xS) = qϕ(z, x\S|xS)/qagg ϕ,\S(z|xS) = pd(x\S|xS)qϕ(z|x)/qagg ϕ,\S(z|xS). Proof of Proposition 12. The equations for LS(xS) are well known for uni-modal VAEs, see for ex- ample [150]. To derive similar representations for the conditional bound, note that the first equation (ZXconditional) for matching the joint distribution of the latent and the missing modalities conditional on a modality subset follows from the definition of L\S, � pd(x\S|xS)L\S(x, θ, ϕ)dx\S = � pd(x\S|xS) � qϕ(z|x) � log pθ(x\S|z) − log qϕ(z|x) + log qϕ(z|xS)) � dzdx\S = � pd(x\S|xS) log pd(x\S|xS)dx\S + � pd(x\S|xS) � qϕ(z|x) � log pθ(x\S|z)qϕ(z|xS)) qϕ(z|x)pd(x\S|xS) � dzdx\S = − H(pd(x\S|xS)) − KL � qϕ(z|x)pd(x\S|xS) ��pθ(x\S|z)qϕ(z|xS) � . To obtain the second representation (Xconditional) for matching the conditional distributions in the data space, observe that pθ(x\S|xS, z) = pθ(x\S|z) and consequently, − � pd(x\S|xS)L\S(x, θ, ϕ)dx\S − H(pd(x\S|xS)) = � pd(x\S|xS)qϕ(z|x) log pd(x\S|xS)qϕ(z|x) pθ(x\S|z)qϕ(z|xS) dzdx\S = � pd(x\S|xS)qϕ(z|x) log pd(x\S|xS)qϕ(z|x)pθ(z|xS) pθ(x\S|z)pθ(z|xS)qϕ(z|xS) dzdx\S = � pd(x\S|xS)qϕ(z|x) log pd(x\S|xS)qϕ(z|x)pθ(z|xS) pθ(x\S|z, xS)pθ(z|xS)qϕ(z|xS)dzdx\S = � pd(x\S|xS)qϕ(z|x) log pd(x\S|xS)qϕ(z|x)pθ(z|xS) pθ(x\S|xS)pθ(z|xS, x\S)qϕ(z|xS)dzdx\S =KL(pd(x\S|xS)|pθ(x\S|xS)) + � pd(x\S|xS) � qϕ(z|x) � log qϕ(z|x) pθ(z|x) + log pθ(z|xS) qϕ(z|xS) � dzdx\S. 22Lastly, the representation (Zconditional) for matching the distributions in the latent space given a modal- ity subset follows by recalling that pd(x\S|xS)qϕ(z|x) = qagg ϕ,\S(z|xS)q⋆(x\S|z, xS) and consequently, − � pd(x\S|xS)L\S(x, θ, ϕ)dx\S − H(pd(x\S|xS)) = � pd(x\S|xS)qϕ(z|x) log pd(x\S|xS)qϕ(z|x) pθ(x\S|z)qϕ(z|xS) dzdx\S = � qagg ϕ,\S(z|xS)q⋆(x\S|z, xS) log qagg ϕ,\S(z|xS)q⋆(x\S|z, xS) pθ(x\S|z)qϕ(z|xS) dzdx\S =KL(qagg ϕ,\S(z|xS)|qϕ(z|xS)) − � qagg ϕ,\S(z|xS) � KL(q⋆(x\S|z, xS)|pθ(xS|z)) � dz. Remark 13 (Prior-hole problem and Bayes or conditional consistency). In the uni-modal setting, the mismatch between the prior and the aggregated prior can be large and can lead to poor uncondi- tional generative performance, because this would lead to high-probability regions under the prior that have not been trained due to their small mass under the aggregated prior [48, 103]. Equation (Zmarginal) extents this to the multi-modal case and we expect that unconditional generation can be poor if this mismatch is large. Moreover, (Zconditional) extends this conditioned on some modality subset and we expect that cross-generation for x\S conditional on xS can be poor if the mismatch between qagg ϕ,\S(z|xS) and qϕ(z|xS) is large for xS ∼ pd, because high-probability regions under qϕ(z|xS) will not have been trained - via optimizing L\S(x) - to model x\S conditional on xS, due to their small mass under qagg ϕ,\S(z|xS). The mismatch will vanish when the encoders are consistent and correspond to a single Bayesian model where they approximate the true posterior distributions. B Meta-learning and Neural processes Meta-learning. We consider a standard meta-learning setup but use slightly non-standard nota- tions to remain consistent with notations used in other parts of this work. We consider a compact input or covariate space A and output space X. Let D = ∪∞ M=1(A × X)M be the collection of all input-output pairs. In meta-learning, we are given a meta-dataset, i.e., a collection of elements from D. Each individual data set D = (a, x) = Dc ∪ Dt ∈ D is called a task and split into a context set Dc = (ac, xc), and target set Dt = (at, xt). We aim to predict the target set from the context set. Consider, therefore, the prediction map π: Dc = (ac, xc) �→ p(xt|at, Dc) = p(xt, xc|at, ac)/p(xc|ac), mapping each context data set to the predictive stochastic process conditioned on Dc. Variational lower bounds for Neural processes. Latent Neural processes [34, 31] approximate this prediction map by using a latent variable model with parameters θ in the form of z ∼ pθ, pθ(xt|at, z) = � (a,x)∈Dt pϵ(x − fθ(a, z)) for a prior pθ, decoder fθ and a parameter free density pϵ. The model is then trained by (approxi- mately) maximizing a lower bound on log pθ(xt|at, ac, xc). Note that for an encoding density qϕ, we have that log pθ(xt|at, ac, xx) = � qϕ(z|x, a) log pθ(xt|at, z)dz − KL(qϕ(z|a, x)|pθ(z|ac, xc)). 23Since the posterior distribution pθ(z|ac, xc) is generally intractable, one instead replaces it with a variational approximation or learned conditional prior qϕ(z|ac, xc), and optimizes the following objective LLNP \C (x, a) = � qϕ(z|x, a) log pθ(xt|at, z)dz − KL(qϕ(z|a, x)|qϕ(z|ac, xc)). Note that this objective coincides with L\C conditioned on the covariate values a and where C comprises the indices of the data points that are part of the context set. Using this variational lower bound can yield subpar performance compared to other biased log- likelihood objectives [64, 31], possibly because the variational approximation qϕ(z|ac, xc) needs not to be close the posterior distribution pθ(z|ac, xc). It would therefore be interesting to analyze in future work if one can alleviate such issues if one optimizes additionally the variational objective corresponding to LC, i.e., LLNP C (xc, ac) = � qϕ(z|xc, ac) log pθ(xc|ac, z)dz − KL(qϕ(z|ac, xc)|pθ(z)), as we do in this work for multi-modal generative models. Note that the objective LLNP C alone can be seen as a form of a neural statistician model [27] where C coincides with the indices of the target set, while a form of the mixture-based bound corresponds to a neural process bound similar to variational homoencoders [45], see also the discussion in [73]. C Information-theoretic perspective We recall first that the mutual information on the inference path4 is given by Iqϕ(XS, ZS) = � qϕ(xS, z) log qϕ(xS, z) pd(xS)qagg ϕ,S(z)dzdxS, where qagg ϕ,S(z) = � pd(xS)qϕ(z|xS)dxS is the aggregated prior [87]. It can be bounded by standard [11, 4, 3] lower and upper bounds using the rate and distortion: HS − DS ≤ HS − DS + ∆1 = Iqϕ(XS, ZS) = RS − ∆2 ≤ RS, with ∆1 = � qagg ϕ (z)KL(q⋆(xS|z)|pθ(xS|z))dz > 0, ∆2 = KL(qagg ϕ,S(z)|pθ(z)) > 0 and q⋆(xS|z) = qϕ(xS, z)/qagg ϕ (z). Moreover, if the bounds in (6) become tight with ∆1 = ∆2 = 0 in the hypothetical scenario of infinite-capacity decoders and encoders, one obtains � pdLS = (1 − β) Iqϕ(XS, ZS) + HS. For β > 1, maximizing LS yields an auto-decoding limit that minimizes Iqϕ(xS, z) for which the latent representations do not encode any information about the data, whilst β < 1 yields an auto-encoding limit that maximizes Iqϕ(XS, Z) and for which the data is perfectly encoded and decoded. To arrive at a similar interpretation for the conditional bound L\S, recall that we have de- fined R\S = � pd(x)KL(qϕ(z|x)|qϕ(z|xS)dx for a conditional or cross rate term and D\S = − � pd(x)qϕ(z|x) log pθ(x\S|z)dzdx for the distortion term. Bounds on the conditional mutual information Iqϕ(X\S, ZM|XS) = � pd(xS)KL(pd(x\S, z|xS))|pd(x\S|xS)qagg ϕ,\S(z|xS))dxS with qagg ϕ,\S(z|xS) = � pd(x\S|xS)qϕ(z|x)dx\S can be established as follows. Proof of Lemma 3. The proof follows by adapting the arguments in [3]. The law of X\S and Z conditional on XS on the encoder path can be written as qϕ(z, x\S|xS) = pd(x\S|xS)qϕ(z|x) = qagg ϕ,\S(z|xS)q⋆(x\S|z, xS) 4We include the conditioning modalities as an index for the latent variable Z. 24with q⋆(x\S|z, xS) = qϕ(z, x\S|xS)/qagg ϕ,\S(z|xS). To prove a lower bound on the conditional mutual information, note that Iqϕ(X\S, ZM|XS) = � pd(xS) � qagg ϕ,\S(z|xS) � q⋆(x\S|z, xS) log qagg ϕ,\S(z|xS)q⋆(x\S|z, xS) qagg ϕ,\S(z|xS)pd(x\S|x\S) dzdx\SdxS = � pd(xS) � qagg ϕ,\S(z|xS) � q⋆(x\S|z, xS) log pθ(x\S|z)) + KL(q⋆(x\S|z, xS)|pθ(x\S|z)) � dzdxS − � pd(xS) � pd(x\S|xS) log pd(x\S|xS)dx = � pd(x) � qϕ(z|x) log pθ(x\S|z)dzdx − � pd(xS) � pd(x\S|xS) log pd(x\S|xS)dx � �� � =−H\S=−H(X\S|XS) + � pd(xS) � qagg ϕ,\S(z|xS)KL(q⋆(x\S|z, xS)|pθ(x\S|z))dxS � �� � =∆\S,1≥0 =∆\S,1 + D\S + H\S. The upper bound follows by observing that Iqϕ(X\S, ZM|XS) = � pd(xS) � pd(x\S|x\) log qϕ(z|x)pd(x\S|xS) qagg ϕ,\S(z|xS)pd(x\S|xS)dzdx = � pd(x)KL(qϕ(z|x)|qϕ(z|xS))dx − � pd(xS)KL(qagg ϕ,\S(z|xS)|qϕ(z|xS))dxS � �� � =∆\S,2≥0 =R\S − ∆\S,2. D Optimal variational distributions The optimal variational density for the mixture-based (1) multi-modal objective, � pd(dx)LMix S (x) = � pd(xS) � qϕ(z|xS) � pd(x\S|xS) � log pθ(xS|z) + log pθ(x\S|z) − β log pθ(z) − β log qϕ(z|xS) � dx\SdzdxS is attained at q⋆(z|xS) ∝ exp � 1 β � pd(x\S|xS) � log pθ(xS|z) + log pθ(x\S|z) − β log pθ(z) � dx\S � ∝ ˜pβ,θ(z|xS) exp �� pd(x\S|xS) log ˜pβ,θ(x\S|z)dx\S � . E Permutation-invariant architectures Multi-head attention and masking. We introduce here a standard multi-head attention [10, 125] mapping MHAϑ : RI×DX × RS×DY → RI×DY given by MHAϑ(X, Y ) = W O � Head1(X, Y, Y ), . . . , HeadH(X, Y, Y ) � , ϑ = (WQ, WK, WV , WO), with output matrix WO ∈ RDA×DY , projection matrices WQ, WK, WV ∈ RDY ×DA and Headh(Q, K, V ) = Att(QW h Q, KW h K, V W h V ) ∈ RI×D (8) 25where we assume that D = DA/H ∈ N is the head size. Here, the dot-product attention function is Att(Q, K, V ) = σ(QK⊤)V, where σ is the softmax function applied to each column of Q and K⊤, respectively. Masked multi-head attention. In practice, it is convenient to consider masked multi-head atten- tion models MMHAϑ,M : RI×DX × RT ×DY → RI×DY for mask matrix M ∈ {0, 1}I×T that operate on key or value sequences of fixed length T where the h-th head (8) is given by Headh(Q, K, V ) = � M ⊙ σ(QW h Q(KW h K)⊤) � Vt′W h V ∈ RT ×D. Using the softmax kernel function SMD(q, k) = exp(q⊤k/ √ D), we set MMHAϑ,M(X, Y )i = T � t=1 H � h=1 MitSMD(W Q h Xi, W K h Yt) �T t′=1 Mit′SMD(XiW Q h , Yt′W K h ) YtW V h W O h (9) which does not depend on Yt if M·t = 0. Masked self-attention. For mask matrix M = mm⊤ with m = (1{s∈S})s∈M, we write MHAϑ(YS, YS) = MMHAϑ,M(i(YS), i(YS))S. where MMHAϑ,M operates on sequences with fixed length and i(YS))t = Yt if t ∈ S and 0 other- wise. LayerNorm and SetNorm. Let h ∈ RT ×D and consider the normalisation N(h) = h − µ(h) σ(h) ⊙ γ + β where µ and σ standardise the input h by computing the mean, and the variance, respectively, over some axis of h, whilst γ and β define a transformation. LayerNorm [8] standardises inputs over last axis, e.g., µ(h) = 1 D �D d=1 µ·,d, i.e., separately for each element. In contrast, SetNorm [147] stan- dardises inputs over both axes, e.g., µ(h) = 1 T D �T t=1 �D d=1 µt,d, thereby losing the global mean and variance only. In both cases, γ and β share their values across the first axis. Both normalisations are permutation-equivariant. Transformer. We consider a masked pre-layer-norm [130, 140] multi-head transformer block (MMTBϑ,M(iS(YS)))S = (Z + σReLU(LN(Z)))S with σReLU being a ReLU non-linearity and Z = iS(YS) + MMHAϑ,M(LN(iS(YS)), LN(iS(YS))) where M = mm⊤ for m = (1{s∈S})s∈M. Set-Attention Encoders. Set g0 = iS(χϑ(hS)) and for k ∈ {1, . . . , L}, let gk = MMTBϑ,M(gk−1 S ). Then, we can express the self-attention multi-modal aggregation mapping via fϑ(hS) = ρϑ �� s∈S gL s � . Remark 14 (Mixture-of-Product-of-Experts or MoPoEs). [115] introduced a MoPoE aggregation scheme that extends MoE or PoE schemes by considering a mixture distribution of all 2M modality subsets, where each mixture component consists of a PoE model, i.e., qMoPoE ϕ (z|xM) = 1 2M � xS∈P(xM) qPoE ϕ (z|xS). This can also be seen as another permutation-invariant model. While it does not require learning sep- arate encoding models for all modality subsets, it however becomes computationally expensive to evaluation for large M. Our mixture models using components with a SumPooling or SelfAttention aggregation can be seen as an alternative that allows one to choose the number of mixture compo- nents K to be smaller than 2M, with non-uniform weights, while the individual mixture components are not constrained to have a PoE form. 26Remark 15 (Multi-modal time series models). We have introduced our generative model in a gen- eral form that also applies to the time-series setup, such as when a latent Markov process drives multiple time series. For example, consider a latent Markov process Z = (Zt)t∈N with prior dy- namics pθ(z1, . . . , zT ) = pθ(z1) �T t=2 pθ(zt|zt−1) for an initial density pθ(z1) and homogeneous Markov kernels pθ(zt|zt−1). Conditional on Z, suppose that the time-series (Xs,t)t∈N follows the dynamics pθ(xs,1, . . . , xs,T |z1, . . . , zT ) = �T t=2 pθ(xs,t|zt) for decoding densities pθ(xs,t|zt). A common choice [20] for modeling the encoding distribution for such sequential VAEs is to assume the factorisation qϕ(z1, . . . zT |x1, . . . xT ) = qϕ(z1|x1) �T t=2 qϕ(zt|zt−1, xt) for xt = (xs,t)s∈M, with initial encoding densities qϕ(z1|x1) and encoding Markov kernels qϕ(zt|zt−1, xt). One can again consider modality-specific encodings hs = (hs,1, . . . , hs,T ), hs,t = hs,φ(xs,t), now applied separately at each time step that are then used to construct Markov kernels that are permutation- invariant in the form of q′ ϕ(zt|zt−1, πhφ(xt,S)) = q′ ϕ(zt|zt−1, hφ(xt,S)) for permutations π ∈ SS. Alternatively, in absence of the auto-regressive encoding structure with Markov kernels, one could also use transformer models that use absolute or relative positional embeddings across the last tem- poral axis, but no positional embeddings across the first modality axis, followed by a sum-pooling operation across the modality axis. Note that previous works using multi-modal time series such as [71] use a non-amortized encoding distribution for the full multi-modal posterior only. A numerical evaluation of permutation-invariant schemes for time series models is however outside the scope of this work. F Permutation-equivariance and private latent variables In principle, the general permutation invariant aggregation schemes that have been introduced could also be used for learning multi-modal models with private latent variables. For example, suppose that the generative model factorises as pθ(z, x) = p(z) � s∈M pθ(xs|z′, ˜zs) (10) for z = (z′, ˜z1, . . . , ˜zM) ∈ Z, for shared latent variables Z′ and private latent variable ˜Zs for each s ∈ M. Note that for s ̸= t ∈ [M], Xs ⊥⊥ ˜Zt | Z′, ˜Zs. (11) Consequently, pθ(z′, ˜zS, ˜z\S|xS) = pθ(z′, ˜zS, |xS)pθ(˜z\S|z′, ˜zS, xS) = pθ(z′, ˜zS, |xS)pθ(˜z\S|z′, ˜zS). (12) An encoding distribution qϕ(z|xS) that approximates pθ(z|xS) should thus be unaffected by the inputs xS when encoding ˜zs for s /∈ S, provided that, a priori, all private and shared latent variables are independent. Observe that for fϑ with the representation fϑ(hS) = ρϑ �� s∈S gϑ(hS)s � , where ρϑ has aggregated inputs y, the gradients of its i-th dimension with respect to the modality values xs is ∂ ∂xs [fϑ(hS(xS))i] = ∂ρϑ,i ∂y �� s∈S gϑ(hS(xS)) � ∂ ∂xs �� t∈S gϑ(hS(xS))t � . In the case of a SumPooling aggregation, the gradient simplifies to ∂ρϑ,i ∂y �� s∈S χϑ(hS(xS)) � ∂χϑ ∂h (hs(xs)) ∂hs(xs) ∂xs . Notice that only the first factor depends on i so that ρϑ,i has to be constant around y = � s∈S χϑ(hS(xS)) if some other components have a non-zero gradient with respect to xs. However, the specific generative model also lends itself to an alternative parameterisation. The assumption of private latent variables suggests an additional permutation-equivariance into the en- coding distribution that approximates the posterior in (12), in the sense that for any permutation π ∈ SS, it holds that q′ ϕ(˜zS|π · hφ(xS), z′) = q′ ϕ(π · ˜zS|hφ(xS), z′), 27assuming that all private latent variables are of the same dimension D.5 Indeed, suppose we have modality-specific feature functions hφ,s such that {Hs = hφ,s(Xs)}s∈S is exchangeable. Clearly, (11) implies for any s ̸= t that hφ,s(Xs) ⊥⊥ ˜Zt | Z′, ˜Zs. The results from [15] then imply, for fixed |S|, the existence of a function f ⋆ such that for all s ∈ S, almost surely, (HS, ˜Zs) = (HS, f ⋆(Ξs, Z′, Hs, MHS)), where Ξs ∼ U[0, 1] iid and Ξs ⊥⊥ HS. (13) This fact suggests an alternative route to approximate the posterior distribution in (12): First, pθ(˜z\S|z′, ˜zS) can often be computed analytically based on the learned or fixed prior distribu- tion. Second, a permutation-invariant scheme can be used to approximate pθ(z′|xS). Finally, a permutation-equivariant scheme can be employed to approximate pθ(˜zS|xS, z′) with a reparame- terisation in the form of (13). Three examples of such permutation-equivariant schemes are given below with pseudocode for optimising the variational bound given in Algorithm 2. Example 16 (Permutation-equivariant PoE). Similar to previous work [133, 76, 114], we consider an encoding density of the form qϕ(z′, ˜zM|xS) = qPoE φ (z′|xS) � s∈S qN (˜zs|˜µs,φ(xs), ˜Σs,φ(xs)) � s∈M\S pθ(˜zs), where qPoE φ (z′|xS) = 1 Z pθ(z′) � s∈S qN (z′|µ′ s,φ(xs), Σ′ s,φ(xs)) is a (permutation-invariant) PoE aggregation, and we assumed that the prior density factorises over the shared and different private variables. For each modality s, we encode different features h′ s,φ = (µ′ s,φ, Σ′ s,φ) and ˜hs,φ = (˜µs,φ, ˜Σs,φ) for the shared, respectively, private, latent variables. Example 17 (Permutation-equivariant Sum-Pooling). We consider an encoding density that writes as qϕ(z′, ˜zM|xS) = qSumP ϕ (z′|xS)qEquiv-SumP ϕ (˜zS|z′, xS) � s∈M\S pθ(˜zs|z′). Here, we use a (permutation-invariant) Sum-Pooling aggregation scheme for constructing the shared latent variable Z′ = µ′(hS)+σ′(hS)⊙Ξ′ ∼ qSumP ϕ (z′|xS), where Ξ′ ∼ p and fϑ : R|S|×DE → RD given as in Exanoke (6) with [µ′(h), log σ′(h)] = fϑ(h). To sample ˜ZS ∼ qEquiv-SumP ϕ (˜zS|z′, xS), consider functions χj,ϑ : RDE → RDP , j ∈ [3], and ρϑ : RDP → RDO, e.g., fully-connected neural networks. We define f Equiv-SumP ϑ : Z × R|S|×DE → R|S|×DO via f Equiv-SumP ϑ (z′, hS)s = ρϑ ��� t∈S χ0,ϑ(ht) � + χ1,ϑ(z′) + χ2,ϑ(hs) � . With � ˜µ(hS)⊤, log ˜σ(hS)⊤�⊤ = f Equiv-SumP ϑ (z′, hS), we then set ˜Zs = ˜µ(hS)s + ˜σ(hS)s ⊙ ˜Ξs for ˜Ξs ∼ p iid, hs = hφ,s(xs) for modality-specific feature functions hφ,s : Xs → RDE. Example 18 (Permutation-equivariant Self-Attention). Similar to a Sum-Pooling approach, we con- sider an encoding density that writes as qϕ(z′, ˜zM|xS) = q SA ϕ (z′|xS)qEquiv-SA ϕ (˜zS|z′, xS) � s∈M\S pθ(˜zs|z′). Here, the shared latent variable Z′ is sampled via the permutation-invariant aggregation above by summing the elements of a permutation-equivariant transformer model of depth L′. For encoding the private latent variables, we follow the example above but set � ˜µ(hS)⊤, log ˜σ(hS)⊤�⊤ = f Equiv-SA ϑ (z′, hS)s = gL S, with gk S = MTBϑ(gk−1 S ) an g0 = (χ1,ϑ(hs) + χ2,ϑ(z′))s∈S. 5The effective dimension can vary across modalities in practice if the decoders are set to mask redundant latent dimensions. 28Remark 19 (Cross-modal context variables). In contrast to the PoE model, where the private encod- ings are independent, the private encodings are dependent in the Sum-Pooling model by conditioning on a sample from the shared latent space. The shared latent variable Z′ can be seen as a shared cross- modal context variable, and similar probabilistic constructions to encode such context variables via permutation-invariant models have been suggested in few-shot learning algorithms [27, 38] or neural process models [34, 33, 64]. Remark 20 (Variational bounds with private latent variables). To compute the multi-modal varia- tional bounds, notice that the required KL-divergences can be written as follows: KL(qϕ(z′, ˜z|xS)|pθ(z′, ˜z)) = KL(qϕ(z′|xS)|pθ(z′))+ � qϕ(z′|xS)KL(qϕ(˜zS|z′, xS)|pθ(˜zS|z′))dz′ and KL(qϕ(z′, ˜z|xM)|qϕ(z′, ˜z|xS)) =KL(qϕ(z′|xM)|(qϕ(z′|xS)) + � qϕ(z′|xM)KL(qϕ(PS ˜z|z′, xM)|qϕ(PS ˜z|z′, xS))dz′ + � qϕ(z′|xM)KL(qϕ(P\S ˜z|z′, xS)|pθ(P\S ˜z|z′))dz′ where PS : (˜z1, . . . ˜zM) �→ (˜zs)s∈S projects all private latent variables to those contained in S. G Multi-modal posterior in exponential family models Consider the setting where the decoding and encoding distributions are of the exponential family form, that is pθ(xs|z) = µs(xs) exp [⟨Ts(xs), fs,θ(z)⟩ − log Zs(fs,θ(z))] for all s ∈ M, while for all S ⊂ M, qϕ(z|xS) = µ(z) exp [⟨V (z), λϕ,S(xS)⟩ − log ΓS(λϕ,S(xS))] where µs and µ are base measures, Ts(xs) and V (z) are sufficient statistics, while the natural pa- rameters λϕ,S(xS) and fs,θ(z) are parameterised by the decoder or encoder networks, respectively, with Zs and ΓS being normalising functions. Note that we made a standard assumption that the multi-modal encoding distribution has a fixed base measure and sufficient statistics for any modality subset. For fixed generative parameters θ, we want to learn a multi-modal encoding distribution that minimises, see Remark 5, over xS ∼ pd, KL(qϕ(z|xS)|pθ(z|xS)) = � qϕ(z|xS) � log qϕ(z|xS) − log pθ(z) − � s∈S log pθ(xs|z) � dz − log pθ(xS) = � qϕ(z|xS) � ⟨V (z), λϕ,S(xS)⟩ − log ΓS(λϕ,S(xS)) − � s∈S log µs(xs) − � � s∈S ⟨Ts,θ(xs), fs,θ(z)⟩ + log pθ(z) − � s∈S Zs(fs,θ(z)) �� dz − log pθ(xS) = � qϕ,ϑ(z|xS) �� � V (z) 1 � , � λϕ,ϑ,S(xS) − log ΓS(λϕ,ϑ,S(xS)) � � − � s∈S � � Ts(xs) 1 � , � fθ,s(z) bθ,s(z) � �� dz, with bθ,s(z) = 1 |S|pθ(z) − log Zs(fs,θ(z)). H Identifiability We are interested in identifiability, conditional on having observed some non-empty modality subset S ⊂ M. For illustration, we translate an identifiability result from the uni-modal iVAE setting in [82], which does not require the conditional independence assumption from [62]. We assume 29that the encoding distribution qϕ(z|xS) approximates the true posterior pθ(z|xS) and belongs to a strongly exponential family, i.e., pθ(z|xS) = qϕ(z|xS) = pEF Vϕ,S,λϕ,S(z|xS), (14) with pEF VS,λS(z|xS) = µ(z) exp [⟨VS(z), λ(xS)⟩ − log ΓS(λS(xS))] , where µ is a base measure, VS : Z → Rk is the sufficient statistics, λS(xS) ∈ Rk the natural param- eters and ΓS a normalising term. Furthermore, one can only reduce the exponential component to the base measure on sets having measure zero. In this section, we assume that pθ(xs|z) = ps,ϵ(xs − fθ,s(z)) (15) for some fixed noise distribution ps,ϵ with a Lebesgue density, which excludes observation models for discrete modalities. Let ΘS be the domain of the parameters θS = (f\S, VS, λS) with f\S : Z ∋ z �→ (fs(z))s∈M\S ∈ ×s∈M\SXs = X\S. Assuming (14), note that pθS(x\S|xS) = � pVS,λS(z|xS)p\S,ϵ(x\S − f\S(z))dz, with p\S,ϵ = ⊗s∈M\Sps,ϵ. We define an equivalence relation on ΘS by (f\S, VS, λS) ∼AS ( ˜f\S, ˜VS, ˜λS) iff there exist invertible AS ∈ Rk×k and cS ∈ Rk such that VS(f −1 \S (x\S)) = AS ˜VS( ˜f −1 \S (x\S)) + cS for all x\S ∈ X\S. Proposition 21 (Weak identifiability). Consider the data generation mechanism pθ(z, x) = pθ(z) � s∈M pθ(xs|z) where the observation model satisfies (15) for an injective f\S. Sup- pose further that pθ(z|xS) is strongly exponential and (14) holds. Assume that the set {x\S ∈ X\S|φ\S,ϵ(x\S) = 0} has measure zero, where φ\S,ϵ is the characteristic function of the density p\S,ϵ. Furthermore, suppose that there exist k + 1 points x0 S, . . . , xk S ∈ XS such that L = � λS(x1 S) − λS(x0 S), . . . , λS(xk S) − λS(x0 S) � ∈ Rk×k is invertible. Then pθS(x\S|xS) = p˜θS(x\S|xS) for all x ∈ X implies θ ∼AS ˜θ. This result follows from Theorem 4 in [82]. Note that pθS(x\S|xS) = p˜θS(x\S|xS) for all x ∈ X implies with the regularity assumption on φ\S,ϵ that the transformed variables Z = f −1 \S (X\S) and ˜Z = ˜f −1 \S (X\S) have the same density function conditional on XS. Remark 22. The joint decoder function f\S can be injective, even if the individual modality-specific decoder functions are not, suggesting that the identifiability of latent variables can be improved when training a multi-modal model compared to separate uni-modal models. Remark 23. The identifiability result above is about conditional models and does not contradict the un-identifiability of VAEs: When S = ∅ and we view x = xM as one modality, then the parameters of pθ∅(x) characterised by the parameters V∅ and λ∅ of the prior pθ∅(z|x∅) and the encoders fM will not be identifiable as the invertibility condition will not be satisfied. Remark 24. Note that the identifiablity concerns parameters of the multi-modal posterior distri- bution. We believe that our inference approach is beneficial for this type of identifiability because (a) unlike some other variational bounds, the posterior is the optimal variational distribution with L\S(x) being a lower bound on log pθ(x\S|xS) for flexible encoders, and (b) the trainable aggrega- tion schemes can be more flexible for approximating the optimal encoding distribution. Remark 25. For models with private latent variables, we might not expect that conditioning on XS helps to identify ˜Z\S as pθ(z′, ˜zS, ˜z\S|xS) = pθ(z′, ˜zS|xS)pθ(˜z\S|z′, ˜z\S). Indeed, Proposition 21 will not apply in such models as f\S will not be injective. 30I Missing modalities In practical applications, modalities can be missing for different data points. We describe this missingness pattern by missingness mask variables ms ∈ {0, 1} where ms = 1 indicates that observe modality s, while ms = 0 means it is missing. The joint generative model that ex- tends (16) will be of the form pθ(z, x, m) = pθ(z) � s∈M pθ(xs|z)pθ(m|x) for some distribu- tion pθ(m|x) over the mask variables m = (ms)s∈M. For S ⊂ M, we denote by xo S = {xs : ms = 1, s ∈ S} and xm S = {xs : ms = 0, s ∈ S} the set of observed, respectively missing, modalities. The full likelihood of the observed and missingness masks becomes then pθ(xo S, m) = � pθ(z) � s∈S pθ(xs|z)pθ(m|x)dxm s dz. If pθ(m|x) does not depend on the obser- vations, that is, observations are missing completely at random [104], then the missingness mech- anisms pθ(m|x) for inference approaches maximizing pθ(xo, m) can be ignored. Consequently, one can instead concentrate on maximizing log pθ(xo) only, based on the joint generative model pθ(z, xo) = pθ(z) � {s∈M: ms=1} pθ(xs|z). In particular, one can employ the variational bounds above by considering only the observed modalities. Since masking operations are readily supported for the considered permutation-invariant models, appropriate imputation strategies [95, 86] for the encoded features of the missing modalities are not necessarily required. Settings allowing for not (completely) at random missingness have been considered in the uni-modal case, for instance, in [55, 37, 39], and we leave multi-modal extensions thereof for future work. J Mixture model extensions for different variational bounds We consider the optimization of an augmented variational bound L(x, θ, ϕ) = � ρ(S) � � qϕ(c, z|xS) [log pθ(c, xS|z)] dzdc − KL(qϕ(c, z|xS)|pθ(c, z)) + � qϕ(c, z|xS) � log pθ(x\S|z) � dzdc − KL(qϕ(c, z|x)|qϕ(c, z|xS)) � dS. We will pursue here an encoding approach that does not require modelling the encoding distribution over the discrete latent variables explicitly, thus avoiding large variances in score-based Monte Carlo estimators or resorting to advanced variance reduction techniques or alternatives such as continuous relaxation approaches. Assuming a structured variational density of the form qϕ(c, z|xS) = qϕ(z|xS)qϕ(c|z, xS), we can express the augmented version of (4) via LS(xS, θ, ϕ) = � qϕ(c, z|xS) [log pθ(c, xS|z)] dz − βKL(qϕ(c, z|xS)|pθ(c, z)) = � qϕ(z|xS) [fx(z, xS) + fc(z, xS)] dz, where fx(z, xS) = log pθ(xS|z) − β log qϕ(z|xS)) and fc(z, xS) = � qϕ(c|z, xS) [−β log qϕ(c|z, xS) + β log pθ(c, z)] dc. (16) We can also write the augmented version of (5) in the form of L\S(x, θ, ϕ) = � qϕ(c, z|xS) � log pθ(x\S|z) � dz − βKL(qϕ(c, z|x)|qϕ(c, z|xS)) = � qϕ(z|x)gx(z, x)dz where gx(z, x) = log pθ(x\S|z) − β log qϕ(z|x) + β log qϕ(z|xS) 31which does not depend on the encoding density of the cluster variable. To optimize the variational bound with respect to the cluster density, we can thus optimize (16), which attains its maximum value of f ⋆ c (z, xS) = β log � pθ(c)pθ(z|c)dc = β log pθ(z) at qϕ(c|z, xS) = pθ(c|z) due to Remark 26 below with g(c) = β log pθ(c, z). Remark 26 (Entropy regularised optimization). Let q be a density over C, exp(g) be integrable with respect to q and τ > 0. The maximum of f(q) = � C q(c) [g(c) − τ log q(c)] dc that is attained at q⋆(c) = 1 Z eg(c)/τ with normalising constant Z = � C eg(c)/τ dc is f ⋆ = f(q⋆) = τ log � C eg(c)/τ dc We can derive an analogous optimal structured variational density for the mixture-based and total- correlation-based variational bounds. First, we can write the mixture-based bound (1) as LMix S (x, θ, ϕ) = � qϕ(z|xS) [log pθ(c, x|z)] dz − βKL(qϕ(c, z|xS)|pθ(c, z)) = � qϕ(z|xS) � f Mix x (z, x) + fc(z, x) � dz, where f Mix x (z, x) = log pθ(x|z) − β log qϕ(z|xS) and fc(z, x) has a maximum value of f ⋆ c (z, x) = β log pθ(z). Second, we can express the corresponding terms from the total-correlation-based bound as LTC S (θ, ϕ) = � qϕ(z|x) [log pθ(x|z)] dz − βKL(qϕ(c, z|x)|qϕ(c, z|xS)) = � qϕ(z|x) � f TC x (z, x) � dz, where f TC x (z, x) = log pθ(x|z) − β log qϕ(z|x) + β log qϕ(z|xS). K Algorithm and STL-gradient estimators We consider a multi-modal extension of the sticking-the-landing (STL) gradient estimator [101] that has also been used in previous multi-modal bounds [110]. The gradient estimator ignores the score function terms when sampling qϕ(z|xS) for variance reduction purposes due to the fact that it has a zero expectation. For the bounds (2) that involves sampling from qϕ(z|xS) and qϕ(z|xM), we thus ignore the score terms for both integrals. Consider the reparameterisation with noise vari- ables ϵS, ϵM ∼ p and transformations zS = tS(ϕ, ϵS, xS) = finvariant-agg(ϑ, ϵS, S, hS), for hS = hφ,s(xs)s∈S and zM = tM(ϕ, ϵM, xM) = finvariant-agg(ϑ, ϵM, M, hM), for hM = hφ,s(xs)s∈M . We need to learn only a single aggregation function that applies that masks the modalities appropri- ately. Pseudo-code for computing the gradients are given in Algorithm 1. If the encoding distribution is a mixture distribution, we apply the stop-gradient operation also to the mixture weights. Notice that in the case of a mixture prior and an encoding distribution that includes the mixture component, the optimal encoding density over the mixture variable has no variational parameters and is given as the posterior density of the mixture component under the generative parameters of the prior. In the case of private latent variables, we proceed analogously and rely on reparameterisations z′ S = t′ S(ϕ, ϵ′ S, xS) for the shared latent variable z′ S ∼ qϕ(z′|xS) as above and ˜zS = ˜tS(ϕ, z′, ϵS, xS) = fequivariant-agg(ϑ, ˜ϵS, z′, S, hS) for the private latent variables ˜zS ∼ qϕ(˜zS|z′, xS). Moreover, we write PS for a projection on the S-coordinates. Pseudo-code for computing unbiased gradient esti- mates for our bound is given in Alorithm 2. L Evaluation of multi-modal generative models We evaluate models using different metrics suggested previously for multi-modal learning, see for example [110, 138, 115]. 32Algorithm 1 Single training step for computing unbiased gradients of L(x). Input: Multi-modal data point x, generative parameter θ, variational parameters ϕ = (φ, ϑ). Sample S ∼ ρ. Sample ϵS, ϵM ∼ p. Set zS = tS(ϕ, ϵS, xM) and zM = tM(ϕ, ϵM, xM). Stop gradients of variational parameters ϕ′ = stop grad(ϕ). Set �LS(θ, ϕ) = log pθ(xS|zS) + β log pθ(zS) − β log qϕ′(zS|xS). Set �L\S(θ, ϕ) = log pθ(x\S|zM) + β log qϕ(zM|xS) − β log qϕ′(zM|xM). Output: ∇θ,ϕ � �LS(θ, ϕ) + �L\S(θ, ϕ) � Algorithm 2 Single training step for computing unbiased gradients of L(x) with private latent vari- ables. Input: Multi-modal data point x, generative parameter θ, variational parameters ϕ = (φ, ϑ). Sample S ∼ ρ. Sample ϵ′ S, ϵS, ϵ\S, ϵ′ M, ϵM, ϵ\M ∼ p. Set z′ S = t′ S(ϕ, ϵ′ S, xS), ˜zS = ˜tS(ϕ, z′ S, ϵS, xS). Set z′ M = t′ M(ϕ, ϵ′ M, xM), ˜zM = ˜tM(ϕ, z′ M, ϵM, xM). Stop gradients of variational parameters ϕ′ = stop grad(ϕ). Set �LS(θ, ϕ) = log pθ(xS|z′ S, ˜zS) + β log pθ(z′ S) − β log qϕ′(z′ S|xS) + β log pθ(˜zS|z′ S) − β log qϕ′(˜zS|z′ S, xS). Set �L\S(θ, ϕ) = log pθ(x\S|z′ M) + β log qϕ(z′ M|xS) − β log qϕ′(˜zM|z′ M, xM) + β log qϕ(PS(˜zM)|z′ M, xS) + β log pθ(P\S(˜zM)|z′ M, ˜zM) − β log qϕ′(˜zM|z′ M, xM). Output: ∇θ,ϕ � �LS(θ, ϕ) + �L\S(θ, ϕ) � Marginal, conditional and joint log-likelihoods. We can estimate the marginal log-likelihood using classic importance sampling log pθ(xS) ≈ log 1 K K � k=1 pθ(zk, xS) qϕ(zk|xS) for zk ∼ qϕ(·|xS). This also allows to approximate the joint log-likelihood log pθ(x), and conse- quently also the conditional log pθ(x\S|xS) = log pθ(x) − log pθ(xS). Generative coherence with joint auxiliary labels. Following previous work [110, 115, 22, 56], we assess whether the generated data share the same information in the form of the class labels across different modalities. To do so, we use pre-trained classifiers clfs : Xs → [K] that classify values from modality s to K possible classes. More precisely, for S ⊂ M and m ∈ M, we compute the self- (m ∈ S) or cross- (m /∈ S) coherence CS→m as the empirical average of 1{clfm(ˆxm)=y}, over test samples x with label y where ˆzS ∼ qϕ(z|xS) and ˆxm ∼ pθ(xm|ˆzS). The case S = M \ {m} corresponds to a leave-one-out conditional coherence. Linear classification accuracy of latent representations. To evaluate how the latent represen- tation can be used to predict the shared information contained in the modality subset S based on a linear model, we consider the accuracy AccS of a linear classifier clfz : Z → [K] that is trained to predict the label based on latent samples zS ∼ qϕ(zS|xtrain S ) from the training values xtrain S and evaluated on latent samples zS ∼ qϕ(z|xtest S ) from the test values xtest S . M Linear models Generative model. Suppose that a latent variable Z taking values in RD is sampled from a standard Gaussian prior pθ(z) = N(0, I) generates M data modalities Xs ∈ RDs, D ≤ Ds, 33based on a linear decoding model pθ(xs|z) = N(Wsz + bs, σ2 I) for a factor loading matrix Ws ∈ RDs×D, bias bs ∈ RDs and observation scale σ > 0. Note that the annealed likelihood function ˜pβ,θ(xs|z) = N(Wsz + bs, βσ2 I) corresponds to a scaling of the observation noise, so that we consider only the choice σ = 1, set σβ = σβ1/2 and vary β > 0. It is obvious that for any S ⊂ M, it holds that ˜pβ,θ(xS|z) = N(WSz + bS, σ2 β IS), where WS and bS are given by concatenating row-wise the emission or bias matrices for modalities in S, while σ2 β IS is the diagonal matrix of the variances of the corresponding observations. By standard properties of Gaussian distributions, it follows that ˜pβ,θ(xS) = N(bS, CS) where CS = WSW ⊤ S + σ2 β IS is the data covariance matrix. Furthermore, with KS = W ⊤ S WS + σ2 β Id, the adjusted posterior is ˜pβ,θ(z|xS) = N(K−1 S W ⊤ S (xS − bS), σ2 β Id K−1 S ). We sample orthogonal rows of W so that the posterior covariance becomes diagonal so that it can – in principle – be well approximated by an encoding distribution with a diagonal covariance matrix. Indeed, the inverse of the posterior co- variance matrix is only a function of the generative parameters of the modalities within S and can be written as the sum σ2 β I +W ⊤ S WS = σ2 β I + � s∈S W ⊤ s Ws, while the posterior mean function is xS �→ (σ2 β I + � s∈S W ⊤ s Ws)−1 � s∈S Ws(xs − bs). Data generation. We generate 5 data sets of N = 5000 samples, each with M = 5 modalities. We set the latent dimension to D = 30, while the dimension Ds of modality s is drawn from U(30, 60). We set the observation noise to σ = 1, shared across all modalities, as is standard for a PCA model. We sample the components of bs independently from N(0, 1). For the setting without modality- specific latent variables, Ws is the orthonormal matrix from a QR algorithm applied to a matrix with elements sampled iid from U(−1, 1). The bias coefficients Wb are sampled independently from N(0, 1/d). Conversely, the setting with private latent variables in the ground truth model allows us to describe modality-specific variation by considering the sparse loading matrix WM =   W ′ 1 ˜W1 0 . . . 0 W ′ 2 0 ˜W2 . . . 0 ... ... ... ... ... W ′ M 0 . . . 0 ˜WM   . Here, W ′ s, ˜Ws ∈ RDs×D′ with D′ = D/(M + 1) = 5, Furthermore, the latent variable Z can be written as Z = (Z′, ˜Z1, . . . , ˜ZM) for private and shared latent variables ˜Zs, resp. Z′. We similarly generate orthonormal � W ′ s, ˜Ws � from a QR decomposition. Observe that the general generative model with latent variable Z corresponds to the generative model (10) with shared Z′ and private latent variables ˜Z with straightforward adjustments for the decoding functions. Similar models have been considered previously, particularly from a Bayesian standpoint with different sparsity assumptions on the generative parameters [6, 128, 149]. Maximum likelihood estimation. Assume now that we observe N data points {xn}n∈[N], con- sisting of stacking the views xn = (xs,n)s∈S for each modality in S and let S = 1 N �N n=1(xn − b)(xn − b)⊤ ∈ RDx×Dx, Dx = �M s=1 Ds, be the sample covariance matrix across all modalities. Let Ud ∈ RDx×D be the matrix of the first D eigenvectors of S with corresponding eigenvalues λ1, . . . λD stored in the diagonal matrix ΛD ∈ RD×D. The maximum likelihood estimates are then given by bML = 1 N �N n=1 xn, σ2 ML = 1 N−D �N j=D+1 λj and WML = UD(ΛD − σ2 ML I)1/2 with the loading matrix identifiable up to rotations. Model architectures. We estimate the observation noise scale σ based on the maximum likelihood estimate σML. We assume linear decoder functions pθ(xs|z) = N(W θ s z + bθ, σ2 ML), fixed standard Gaussian prior p(z) = N(0, I) and generative parameters θ = (W θ 1 , bθ 1, . . . , W θ M, bθ M). Details about the various encoding architectures are given in Table 17. The modality-specific encoding functions for the PoE and MoE schemes have a hidden size of 512, whilst they are of size 256 for the learnable aggregation schemes having additional aggregation parameters φ. Simulation results. We show different rate-distortion terms for the learned models where the true data generation mechanism has (see Figure 4 ) or has not (see Figure 3) private latent variables. 34In both settings, we use the general multi-modal model without private latent variables in order to compare different aggregation schemes and bounds. We find that our bound yields encoding distri- butions that are closer to the true posterior distribution across various aggregation schemes. Note that in the case of the mixture-based bound, the posterior distribution is only optimal as an encod- ing distribution that uses all modalities (Sub-figures (c)). The trade-offs between full reconstruction quality and full rates vary across ground truth models, bounds and aggregation. Cross-reconstruction terms are usually better for the mixture-based bound. Moreover, the mixture-based bound has lower cross-modal rates, i.e., the encoding distribution does not change as much if additional modalities are included. Table 5 shows the log-likelihood of the generative model and the value of the lower bound when the true data has private latent variables. Compared to the results in Table 1 with full decoder matrices, there appear to be smaller differences across different bounds and fusion schemes. Finally, we consider permutation-equivariant schemes for learning models with private latent vari- ables as detailed in Appendix F, applied to the setting with sparse variables in the data genera- tion mechanism. Figure 5 shows different rate-distortion terms for β ∈ {0.1, 1, 4.} for PoE and SumPooling and SelfAttention aggregation models. We find that our variational bound tends to obtain higher full reconstruction terms, while the full rates vary for different configurations. Con- versely, the mixture-based bound obtains better cross-model reconstruction, with less clear patterns in the cross-rate terms. Table 6 shows the log-likelihood values for the learned generative model that is similar across different configurations, apart from a PoE scheme that achieves lower log-likelihood for a mixture-based bound. (a) Full Reconstruction −DM (b) Full Rates RM (c) Full Posterior Approximation (d) Cross Reconstruction −Dc \S (e) Cross Rates R\S (f) Uni-Modal Posterior Approxima- tion Figure 3: Linear Gaussian models with dense decoder matrix: Rate and distortion terms and KL- divergence of encoding distributions to posterior distribution from learned generative model. N Non-linear identifiable models N.1 Auxiliary labels Table 19 illustrates first the benefits of our bound that obtain better log-likelihood estimates for different fusion schemes. Second, it demonstrates the advantages of our new fusion schemes that achieve better log-likelihoods for both bounds. Third, it shows the benefit of using aggregation schemes that have the capacity to accommodate prior distributions different from a single Gaussian. Observe also that MoE schemes lead to low MCC values, while PoE schemes had high MCC values. We also show in Figure 6 the reconstructed modality values and inferred latent variables for one realisation with our bound, with the corresponding results for a mixture-based bound in Figure 7. 35(a) Full Reconstruction −DM (b) Full Rates RM (c) Full Posterior Approximation (d) Cross Reconstruction −Dc \S (e) Cross Rates R\S (f) Uni-Modal Posterior Approxima- tion Figure 4: Linear Gaussian models with sparse decoder matrix: Rate and distortion terms and KL- divergence of encoding distributions to posterior distribution from learned generative model. Table 5: Multi-modal Gaussian model with sparse decoders in the ground truth model: LLH Gap is the relative difference of the log-likelihood of the learned model relative to the log-likelihood based on the exact MLE. Bound gap is the relative difference of the variational bound to the log-likelihood based on the MLE. Our bound Mixture bound Aggregation LLH Gap Bound Gap MCC LLH Gap Bound Gap MCC PoE 0.00 (0.000) 0.00 (0.000) 0.84 (0.004) 0.00 (0.007) 0.01 (0.001) 0.87 (0.004) MoE 0.01 (0.001) 0.01 (0.001) 0.81 (0.001) 0.01 (0.002) 0.01 (0.002) 0.83 (0.003) SumPooling 0.00 (0.000) 0.00 (0.000) 0.84 (0.015) 0.01 (0.001) 0.01 (0.002) 0.84 (0.013) SelfAttention 0.00 (0.001) 0.00 (0.000) 0.84 (0.005) 0.01 (0.002) 0.01 (0.002) 0.83 (0.004) Table 6: Multi-modal Gaussian model with sparse decoders in the ground truth model and permutation-equivariant encoders: LLH Gap is the relative difference of the log-likelihood of the learned model relative to the log-likelihood based on the exact MLE. Bound gap is the relative difference of the variational bound to the log-likelihood based on the MLE. Our bound Mixture bound Aggregation LLH Gap Bound Gap MCC LLH Gap Bound Gap MCC PoE (equivariant) 0.00 (0.000) 0.00 (0.000) 0.91 (0.016) 0.01 (0.001) 0.02 (0.001) 0.88 (0.011) SumPooling (equivariant) 0.00 (0.000) 0.00 (0.000) 0.85 (0.004) 0.00 (0.000) 0.00 (0.001) 0.82 (0.003) SelfAttention (equivariant) 0.00 (0.000) 0.00 (0.000) 0.83 (0.006) 0.00 (0.000) 0.00 (0.003) 0.83 (0.003) 36(a) Full Reconstruction −DM (b) Full Rates RM (c) Cross Reconstruction −Dc \S (d) Cross Rates R\S Figure 5: Linear Gaussian models with sparse decoder matrix and permutation-equivariant aggrega- tion: Rate and distortion terms for varying β. Table 7: Non-linear identifiable model with one real-valued modality and an auxiliary label acting as a second modality: The first four rows use a fixed standard Gaussian prior, while the last four rows use a Gaussian mixture prior with 5 components. Mean and standard deviation over 4 repetitions. Our bound Mixture bound Aggregation LLH (β = 1) MCC (β = 1) MCC (β = 0.1) LLH (β = 1) MCC (β = 1) MCC (β = 0.1) PoE -43.4 (10.74) 0.98 (0.006) 0.99 (0.003) -318 (361.2) 0.97 (0.012) 0.98 (0.007) MoE -20.5 (6.18) 0.94 (0.013) 0.93 (0.022) -57.9 (6.23) 0.93 (0.017) 0.93 (0.025) SumPooling -17.9 (3.92) 0.99 (0.004) 0.99 (0.002) -18.9 (4.09) 0.99 (0.005) 0.99 (0.008) SelfAttention -18.2 (4.17) 0.99 (0.004) 0.99 (0.003) -18.6 (3.73) 0.99 (0.004) 0.99 (0.007) SumPooling -15.4 (2.12) 1.00 (0.001) 0.99 (0.004) -18.6 (2.36) 0.98 (0.008) 0.99 (0.006) SelfAttention -15.2 (2.05) 1.00 (0.001) 1.00 (0.004) -18.6 (2.27) 0.98 (0.014) 0.98 (0.006) SumPoolingMixture -15.1 (2.15) 1.00 (0.001) 0.99 (0.012) -18.2 (2.80) 0.98 (0.010) 0.99 (0.005) SelfAttentionMixture -15.3 (2.35) 0.99 (0.005) 0.99 (0.004) -18.4 (2.63) 0.99 (0.007) 0.99 (0.007) N.2 Five continuous modalities Table 8 demonstrates that our bound can yield to higher log-likelihoods and tigher bounds compared to a mixture-based bound, as do more flexible fusion schemes. Similar results for the partially observed case (η = 0.5) have been illustrated in the main text in Table 2. 37Table 8: Fully observed (η = 0) non-linear identifiable model with 5 modalities: The first four rows use a fixed standard Gaussian prior, while the last four rows use a Gaussian mixture prior with 5 components. Mean and standard deviation over 4 repetitions. Our bound Mixture bound Aggregation LLH Lower Bound MCC LLH Lower Bound MCC PoE -473.6 (9.04) -476.9 (9.61) 0.98 (0.005) -497.7 (11.26) -559.0 (2.33) 0.97 (0.008) MoE -477.9 (8.50) -484.3 (8.88) 0.91 (0.014) -494.6 (9.20) -546.8 (7.02) 0.92 (0.004) SumPooling -471.4 (8.29) -472.3 (8.54) 0.99 (0.004) -480.5 (8.84) -530.5 (3.02) 0.98 (0.005) SelfAttention -471.4 (8.97) -472.3 (9.52) 0.99 (0.002) -482.8 (10.51) -532.7 (2.89) 0.98 (0.004) SumPooling -465.4 (8.16) -467.6 (8.25) 0.98 (0.002) -475.1 (7.54) -521.7 (3.55) 0.98 (0.003) SelfAttention -469.3 (4.76) -471.5 (4.99) 0.98 (0.003) -474.7 (8.20) -522.7 (2.79) 0.98 (0.002) SumPoolingMixture -464.5 (8.16) -466.3 (7.91) 0.99 (0.003) -474.2 (7.61) -521.2 (4.13) 0.98 (0.004) SelfAttentionMixture -464.4 (8.50) -466.0 (9.66) 0.99 (0.003) -473.6 (8.24) -520.6 (2.62) 0.98 (0.002) (a) Observed data x (b) True latents z (c) PoE (x) (d) PoE (z) (e) MoE (x) (f) MoE (z) (g) SumP, K = 1 (x) (h) SumP, K = 1 (z) (i) SumP, K = 5 (x) (j) SumP, K = 5 (z) (k) SumPM, K = 5 (z) (l) SumPM, K = 5 (z) Figure 6: Bi-modal non-linear model with label and continuous modality based on our bound. O MNIST-SVHN-Text O.1 Training hyperparamters The MNIST-SVHN-Text data set is taken from the code accompanying [115] with around 1.1 million train and 200k test samples. All models are trained for 100 epochs with a batch size of 250 using Adam [66] and a cosine decay schedule from 0.0005 to 0.0001. 38(a) Observed data x (b) True latents z (c) PoE (x) (d) PoE (z) (e) MoE (x) (f) MoE (z) (g) SumP, K = 1 (x) (h) SumP, K = 1 (z) (i) SumP, K = 5 (x) (j) SumP, K = 5 (z) (k) SumPM, K = 5 (z) (l) SumPM, K = 5 (z) Figure 7: Bi-modal non-linear model with label and continuous modality based on mixture bound. (a) Full Reconstr. −DM (b) Cross Reconstr. −Dc \S (c) Full Rates RM (d) Cross Rates R\S Figure 8: Rate and distortion terms for MNIST-SVHN-Text with shared and private latent variables. Table 9: Test log-likelihood estimates for varying β choices for the joint data (M+S+T) as well as for the marginal data of each modality based on importance sampling (512 particles). Multi-modal generative model with a 40-dimensional shared latent variable. Our bound Mixture bound (β, Aggregation) M+S+T M S T M+S+T M S T (0.1, PoE+) 5433 (24.5) 1786 (41.6) 3578 (63.5) -29 (2.4) 5481 (18.4) 2207 (19.8) 3180 (33.7) -39 (1.0) (0.1, SumPooling) 7067 (78.0) 2455 (3.3) 4701 (83.5) -9 (0.4) 6061 (15.7) 2398 (9.3) 3552 (7.4) -50 (1.9) (1.0, PoE+) 6872 (9.6) 2599 (5.6) 4317 (1.1) -9 (0.2) 5900 (10.0) 2449 (10.4) 3443 (11.7) -19 (0.4) (1.0, SumPooling) 7056 (124.4) 2478 (9.3) 4640 (113.9) -6 (0.0) 6130 (4.4) 2470 (10.3) 3660 (1.5) -16 (1.6) (4.0, PoE+) 7021 (13.3) 2673 (13.2) 4413 (30.5) -5 (0.1) 5895 (6.2) 2484 (5.5) 3434 (2.2) -13 (0.4) (4.0, SumPooling) 6690 (113.4) 2483 (9.9) 4259 (117.2) -5 (0.0) 5659 (48.3) 2448 (10.5) 3233 (27.7) -10 (0.2) 39(a) Full Reconstr. −DM (b) Cross Reconstr. −Dc \S (c) Full Rates RM (d) Cross Rates R\S Figure 9: Rate and distortion terms for MNIST-SVHN-Text with shared latent variables and different β. O.2 Multi-modal rates and distortions O.3 Log-likelihood estimates O.4 Generated modalities (a) Our bound (b) Mixture-based bound Figure 10: Conditional generation for different aggergation schemes and bounds and shared latent variables. The first column is the conditioned modality. The next three columns are the generated modalities using a SumPooling aggregation, followed by the three columns for a SelfAttention ag- gregation, followed by PoE+ and lastly MoE+. O.5 Conditional coherence Table 10: Conditional coherence for models with shared latent variables and bi-modal condition- als. The letters on the second line represent the modality which is generated based on the sets of modalities on the line below it. Our bound Mixture bound M S T M S T Aggregation M+S M+T S+T M+S M+T S+T M+S M+T S+T M+S M+T S+T M+S M+T S+T M+S M+T S+T PoE 0.98 0.98 0.60 0.75 0.58 0.77 0.82 1.00 1.00 0.96 0.97 0.95 0.61 0.11 0.61 0.45 0.99 0.98 PoE+ 0.97 0.98 0.55 0.73 0.52 0.75 0.83 1.00 0.99 0.97 0.97 0.96 0.64 0.11 0.63 0.45 0.99 0.97 MoE 0.88 0.97 0.90 0.35 0.11 0.35 0.41 0.72 0.69 0.88 0.96 0.89 0.32 0.10 0.33 0.42 0.72 0.69 MoE+ 0.85 0.94 0.86 0.32 0.10 0.32 0.40 0.71 0.67 0.87 0.96 0.89 0.32 0.10 0.32 0.42 0.72 0.69 SumPooling 0.97 0.97 0.86 0.78 0.30 0.80 0.76 0.99 1.00 0.97 0.97 0.95 0.65 0.10 0.65 0.45 0.99 0.97 SelfAttention 0.97 0.97 0.82 0.76 0.30 0.78 0.69 1.00 1.00 0.97 0.97 0.99 0.66 0.10 0.65 0.45 0.99 1.00 40(a) Our bound, β = 0.1 (b) Our bound, β = 4 (c) Mixture-based bound, β = 0.1 (d) Mixture-based bound, β = 4 Figure 11: Conditional generation for different β parameters. The first column is the conditioned modality. The next three columns are the generated modalities using a SumPooling aggregation, followed by the three columns for a PoE+ scheme. (a) Our bound (b) Mixture-based bound Figure 12: Conditional generation for permutation-equivariant schemes and private latent variable constraints. The first column is the conditioned modality. The next three columns are the gener- ated modalities using a SumPooling aggregation, followed by the three columns for a SelfAttention scheme and a PoE model. 41Table 11: Conditional coherence for models with private latent variables and uni-modal condition- als. The letters on the second line represent the modality which is generated based on the sets of modalities on the line below it. Our bound Mixture bound M S T M S T Aggregation M S T M S T M S T M S T M S T M S T PoE+ 0.97 0.12 0.13 0.20 0.62 0.24 0.16 0.15 1.00 0.96 0.83 0.99 0.11 0.58 0.11 0.44 0.39 1.00 SumPooling 0.97 0.42 0.59 0.44 0.67 0.40 0.65 0.45 1.00 0.97 0.86 0.99 0.11 0.62 0.11 0.45 0.40 1.00 SelfAttention 0.97 0.12 0.12 0.27 0.71 0.28 0.46 0.40 1.00 0.96 0.09 0.08 0.12 0.67 0.12 0.15 0.17 1.00 Table 12: Conditional coherence for models with private latent variables and bi-modal condition- als. The letters on the second line represent the modality which is generated based on the sets of modalities on the line below it. Our bound Mixture bound M S T M S T Aggregation M+S M+T S+T M+S M+T S+T M+S M+T S+T M+S M+T S+T M+S M+T S+T M+S M+T S+T PoE+ 0.97 0.97 0.14 0.66 0.33 0.67 0.18 1.00 1.00 0.97 0.97 0.94 0.63 0.11 0.63 0.45 0.99 0.96 SumPooling 0.97 0.97 0.54 0.79 0.43 0.80 0.57 1.00 1.00 0.97 0.97 0.93 0.64 0.11 0.63 0.45 0.99 0.97 SelfAttention 0.97 0.97 0.12 0.80 0.29 0.81 0.49 1.00 1.00 0.96 0.96 0.08 0.70 0.12 0.70 0.15 1.00 1.00 Latent classification accuracy. P Encoder Model architectures P.1 Linear models P.2 Linear models with private latent variables P.3 Nonlinear model with auxiliary label P.4 Nonlinear model with five modalities P.5 MNIST-SVHN-Text For SVHN and and Text, we use 2d- or 1d-convolutional layers, respectively, denoted as Conv(f, k, s) for feature dimension f, kernel-size k and stride s. We denote transposed convolu- tions as tConv. We use the neural network architectures as implemented in Flax [44]. P.6 MNIST-SVHN-Text with private latent variables Q MNIST-SVHN-Text Decoder Model architectures For models with private latent variables, we concatenate the shared and private latent variables. We use a Laplace likelihood as the decoding distribution for MNIST and SVHN, where the decoder function learns both its mean as a function of the latent and a constant log-standard-deviation at Table 13: Conditional coherence for models with shared latent variables for different βs and uni- modal conditionals. The letters on the second line represent the modality which is generated based on the sets of modalities on the line below it. Our bound Mixture bound M S T M S T (β, Aggregation) M S T M S T M S T M S T M S T M S T (0.1, PoE+) 0.98 0.11 0.12 0.12 0.62 0.14 0.61 0.25 1.00 0.96 0.83 0.99 0.11 0.58 0.11 0.45 0.39 1.00 (0.1, SumPooling) 0.97 0.48 0.81 0.30 0.72 0.33 0.86 0.55 1.00 0.97 0.86 0.99 0.11 0.64 0.11 0.45 0.40 1.00 (1.0, PoE+) 0.97 0.15 0.63 0.24 0.63 0.42 0.79 0.35 1.00 0.96 0.83 0.99 0.11 0.59 0.11 0.45 0.39 1.00 (1.0, SumPooling) 0.97 0.48 0.87 0.25 0.72 0.36 0.73 0.48 1.00 0.97 0.86 0.99 0.10 0.63 0.10 0.45 0.40 1.00 (4.0, PoE+) 0.97 0.29 0.83 0.41 0.60 0.58 0.76 0.38 1.00 0.96 0.82 0.99 0.10 0.57 0.10 0.44 0.38 1.00 (4.0, SumPooling) 0.97 0.48 0.88 0.35 0.66 0.44 0.83 0.53 1.00 0.96 0.85 0.99 0.11 0.57 0.10 0.45 0.39 1.00 42Table 14: Conditional coherence for models with shared latent variables for different βs and bi- modal conditionals. The letters on the second line represent the modality which is generated based on the sets of modalities on the line below it. Our bound Mixture bound M S T M S T (β, Aggregation) M+S M+T S+T M+S M+T S+T M+S M+T S+T M+S M+T S+T M+S M+T S+T M+S M+T S+T (0.1, PoE+) 0.98 0.98 0.15 0.70 0.14 0.72 0.66 1.00 1.00 0.96 0.96 0.93 0.62 0.11 0.62 0.45 0.99 0.95 (0.1, SumPooling) 0.97 0.97 0.86 0.83 0.31 0.84 0.85 0.99 1.00 0.97 0.97 0.94 0.66 0.11 0.65 0.45 0.99 0.96 (1.0, PoE+) 0.97 0.98 0.55 0.73 0.52 0.75 0.83 1.00 0.99 0.97 0.97 0.96 0.64 0.11 0.63 0.45 0.99 0.97 (1.0, SumPooling) 0.97 0.97 0.86 0.78 0.30 0.80 0.76 0.99 1.00 0.97 0.97 0.95 0.65 0.10 0.65 0.45 0.99 0.97 (4.0, PoE+) 0.97 0.98 0.84 0.76 0.66 0.78 0.82 1.00 1.00 0.97 0.97 0.96 0.62 0.10 0.62 0.45 0.99 0.98 (4.0, SumPooling) 0.97 0.97 0.89 0.77 0.40 0.78 0.86 0.99 1.00 0.97 0.97 0.96 0.61 0.10 0.60 0.45 0.99 0.97 Table 15: Unsupervised latent classification for β = 1 and models with shared latent variables only (top half) and shared plus private latent variables (bottom half). Accuracy is computed with a linear classifier (logistic regression) trained on multi-modal inputs (M+S+T) or uni-modal inputs (M, S or T). Our bound Mixture bound Aggregation M+S+T M S T M+S+T M S T PoE 0.988 (0.000) 0.940 (0.009) 0.649 (0.039) 0.998 (0.001) 0.991 (0.004) 0.977 (0.002) 0.845 (0.000) 1.000 (0.000) PoE+ 0.978 (0.002) 0.934 (0.001) 0.624 (0.040) 0.999 (0.001) 0.998 (0.000) 0.981 (0.000) 0.851 (0.000) 1.000 (0.000) MoE 0.841 (0.008) 0.974 (0.000) 0.609 (0.032) 1.000 (0.000) 0.940 (0.001) 0.980 (0.001) 0.843 (0.001) 1.000 (0.000) MoE+ 0.850 (0.039) 0.967 (0.014) 0.708 (0.167) 0.983 (0.023) 0.928 (0.017) 0.983 (0.002) 0.846 (0.001) 1.000 (0.000) SelfAttention 0.985 (0.001) 0.954 (0.002) 0.693 (0.037) 0.986 (0.006) 0.991 (0.000) 0.981 (0.001) 0.864 (0.003) 1.000 (0.000) SumPooling 0.981 (0.000) 0.962 (0.000) 0.704 (0.014) 0.992 (0.008) 0.994 (0.000) 0.983 (0.000) 0.866 (0.002) 1.000 (0.000) PoE+ 0.979 (0.009) 0.944 (0.000) 0.538 (0.032) 0.887 (0.07) 0.995 (0.002) 0.980 (0.002) 0.848 (0.006) 1.000 (0.000) SumPooling 0.987 (0.004) 0.966 (0.004) 0.370 (0.348) 0.992 (0.002) 0.994 (0.001) 0.982 (0.000) 0.870 (0.001) 1.000 (0.000) SelfAttention 0.990 (0.003) 0.968 (0.002) 0.744 (0.008) 0.985 (0.000) 0.997 (0.001) 0.974 (0.000) 0.681 (0.031) 1.000 (0.000) each pixel. Following previous works [110, 115], we re-weight the log-likelihoods for different modalities relative to their dimensions. R Compute resources and existing assets Our computations were performed on shared HPC systems. All experiments except Section 5.3 were run on a CPU server using one or two CPU cores. The experiments in Section 5.3 were run a GPU server using one NVIDIA A100. Our implementation is based on JAX [16] and Flax [44]. We compute the mean correlation co- efficient (MCC) between true and infered latent variables following [63], as in https://github. com/ilkhem/icebeem. In our MNIST-SVHN-Text experiments, we use code from [115], https: //github.com/thomassutter/MoPoE. Table 16: Unsupervised latent classification for different βs and models with shared latent variables only. Accuracy is computed with a linear classifier (logistic regression) trained on multi-modal inputs (M+S+T) or uni-modal inputs (M, S or T). Our bound Mixture bound (β, Aggregation) M+S+T M S T M+S+T M S T (0.1, PoE+) 0.983 (0.006) 0.919 (0.001) 0.561 (0.048) 0.988 (0.014) 0.992 (0.002) 0.979 (0.002) 0.846 (0.004) 1.000 (0.000) (0.1, SumPooling) 0.982 (0.004) 0.965 (0.002) 0.692 (0.047) 0.999 (0.001) 0.994 (0.000) 0.981 (0.002) 0.863 (0.005) 1.000 (0.000) (1.0, PoE+) 0.978 (0.002) 0.934 (0.001) 0.624 (0.040) 0.999 (0.001) 0.998 (0.000) 0.981 (0.000) 0.851 (0.000) 1.000 (0.000) (1.0, SumPooling) 0.981 (0.000) 0.962 (0.000) 0.704 (0.014) 0.992 (0.008) 0.994 (0.000) 0.983 (0.000) 0.866 (0.002) 1.000 (0.000) (4.0, PoE+) 0.981 (0.006) 0.943 (0.007) 0.630 (0.008) 0.993 (0.001) 0.998 (0.000) 0.981 (0.000) 0.846 (0.001) 1.000 (0.000) (4.0, SumPooling) 0.984 (0.004) 0.963 (0.001) 0.681 (0.009) 0.995 (0.000) 0.992 (0.002) 0.980 (0.001) 0.856 (0.001) 1.000 (0.000) 43Table 17: Encoder architectures for Gaussian models. (a) Modality-specific encoding functions hs(xs). Latent di- mension D = 30, modality dimension Ds ∼ U(30, 60). MoE/PoE SumPooling/SelfAttention Input: Ds Input: Ds Dense Ds × 512, ReLU Dense Ds × 256, ReLU Dense 512 × 512, ReLU Dense 256 × 256, ReLU Dense 512 × 60 Dense 256 × 60 (b) Model for outer aggregation function ρϑ for SumPooling and SelfAttention schemes. Outer Aggregation Input: 256 Dense 256 × 256, ReLU Dense 256 × 256, ReLU Dense 256 × 60 (c) Inner aggregation function χϑ. SumPooling SelfAttention Input: 256 Input: 256 Dense 256 × 256, ReLU Dense 256 × 256, ReLU Dense 256 × 256, ReLU Dense 256 × 256 Dense 256 × 256 (d) Transformer parameters. SelfAttention (1 Layer) Input: 256 Heads: 4 Attention size: 256 Hidden size FFN: 256 Table 18: Encoder architectures for Gaussian models with private latent variables. (a) Modality-specific encoding functions hs(xs). All private and shared latent variables are of dimension 10. Modality dimension Ds ∼ U(30, 60). PoE (hshared s and hprivate s ) SumPooling/SelfAttention (one hs) Input: Ds Input: Ds Dense Ds × 512, ReLU Dense Ds × 128, ReLU Dense 512 × 512, ReLU Dense 128 × 128, ReLU Dense 512 × 10 Dense 128 × 10 (b) Model for outer aggregation func- tion ρϑ for SumPooling scheme. Outer Aggregation (ρϑ) Input: 128 Dense 128 × 128, ReLU Dense 128 × 128, ReLU Dense 128 × 10 (c) Inner aggregation functions. SumPooling (χ0,ϑ, χ1,ϑ, χ2,ϑ) SelfAttention (χ1,ϑ, χ2,ϑ) Input: 128 Input: 128 Dense 128 × 128, ReLU Dense 128 × 128, ReLU Dense 128 × 128, ReLU Dense 128 × 128 Dense 128 × 128 (d) Transformer parameters. SelfAttention (1 Layer) Input: 128 Heads: 4 Attention size: 128 Hidden size FFN: 128 44Table 19: Encoder architectures for nonlinear model with auxiliary label. (a) Modality-specific encoding functions hs(xs). Modality dimension D1 = 2 (continuous modality) and D2 = 5 (la- bel). Embedding dimension DE = 4 for PoE and MoE and DE = 128 otherwise. Modality-specific encoders Input: Ds Dense Ds × 128, ReLU Dense 128 × 128, ReLU Dense 128 × DE (b) Model for outer aggregation function ρϑ for SumPooling and SelfAttention schemes and mixtures thereof. Output dimension is D0 = 25 for mixture densities and DO = 4 otherwise. Outer Aggregation Input: 128 Dense 128 × 128, ReLU Dense 128 × 128, ReLU Dense 128 × DO (c) Inner aggregation function χϑ. SumPooling SelfAttention Input: 128 Input: 128 Dense 128 × 128, ReLU Dense 128 × 128, ReLU Dense 128 × 128, ReLU Dense 128 × 128 Dense 128 × 128 (d) Transformer parameters. SelfAttention Input: 128 Heads: 4 Attention size: 128 Hidden size FFN: 128 Table 20: Encoder architectures for nonlinear model with five modalities. (a) Modality-specific encoding functions hs(xs). Modality dimensions Ds = 25. Latent dimension D = 25 MoE/PoE SumPooling/SelfAttention Input: Ds Input: Ds Dense Ds × 512, ReLU Dense Ds × 256, ReLU Dense 512 × 512, ReLU Dense 256 × 256, ReLU Dense 512 × 50 Dense 256 × 256 (b) Model for outer aggregation function ρϑ for SumPooling and SelfAttention schemes and mixtures thereof. Output dimension is D0 = 50 for mixture densities and DO = 25 otherwise. Outer Aggregation Input: 256 Dense 256 × 256, ReLU Dense 256 × 256, ReLU Dense 256 × DO (c) Inner aggregation function χϑ. SumPooling SelfAttention Input: 256 Input: 256 Dense 256 × 256, ReLU Dense 256 × 256, ReLU Dense 256 × 256, ReLU Dense ×256 Dense 256 × 256 (d) Transformer parameters. SelfAttention Input: 256 Heads: 4 Attention size: 256 Hidden size FFN: 256 45Table 21: Encoder architectures for MNIST-SVHN-Text. (a) MNIST-specific encoding functions hs(xs). Modality dimensions Ds = 28 × 28. Embed- ding dimension is DE = 2D for PoE/MoE and DE = 256 for SumPooling/SelfAttention. For PoE+/MoE+, we add four times a Dense layer of size 256 with ReLU layer before the last linear layer. MoE/PoE/SumPooling/SelfAttention Input: Ds, Dense Ds × 400, ReLU Dense 400 × 400, ReLU Dense 400 × DE (b) SVHN-specific encoding functions hs(xs). Modality dimensions Ds = 3 × 32 × 32. Embed- ding dimension is DE = 2D for PoE/MoE and DE = 256 for SumPooling/SelfAttention. For PoE+/MoE+, we add four times a Dense layer of size 256 with ReLU layer before the last linear layer. MoE/PoE/SumPooling/SelfAttention Input: Ds Conv(32, 4, 2), ReLU Conv(64, 4, 2), ReLU Conv(64, 4, 2), ReLU Conv(128, 4, 2), ReLU, Flatten Dense 2048 × DE (c) Text-specific encoding functions hs(xs). Modality dimensions Ds = 8 × 71. Embed- ding dimension is DE = 2D for PoE/MoE and DE = 256 for permutation-invariant mod- els (SumPooling/SelfAttention) and DE = 128 for permutation-equivariant models (SumPool- ing/SelfAttention). For PoE+/MoE+, we add four times a Dense layer of size 256 with ReLU layer before the last linear layer. MoE/PoE/SumPooling/SelfAttention Input: Ds Conv(128, 1, 1), ReLU Conv(128, 4, 2), ReLU Conv(128, 4, 2), ReLU, Flatten Dense 128 × DE (d) Model for outer aggregation function ρϑ for SumPooling and SelfAttention schemes. Output dimension is D0 = 2D = 80 for models with shared latent variables only and D0 = 10+10 for models with private and shared latent variables. DE = 256 for permutation-invariant and DI = 128 for permutation-invariant models. Outer Aggregation Input: DE Dense DE × DE, LReLU Dense DE × DE, LReLU Dense DE × DO (e) Inner aggregation function χϑ for permutation-invariant models (DE = 256) and permutaion-equivariant models (DE = 128). SumPooling SelfAttention Input: DE Input: DE Dense DE × DE, LReLU Dense DE × DE, LReLU Dense DE × DE, LReLU Dense ×DE Dense DE × DE (f) Transformer parameters for permutation- invariant models. DE = 256 for permutation- invariant and DI = 128 for permutation-invariant models. SelfAttention (2 Layers) Input: DE Heads: 4 Attention size: DE Hidden size FFN: DE 46Table 22: Decoder architectures for MNIST-SVHN-Text. (a) MNIST decoder. DI = 40 for models with shared latent variables only, and DI = 10 + 10 otherwise. MNIST Input: DI Dense 40 × 400, ReLU Dense 400 × 400, ReLU Dense 400 × Ds, Sigmoid (b) SVHN decoder. DI = 40 for models with shared latent variables only, and DI = 10 + 10 otherwise. SVHN Input: DI Dense DI × 128, ReLU tConv(64, 4, 3), ReLU tConv(64, 4, 2), ReLU tConv(32, 4, 2), ReLU tConv(3, 4, 2) (c) Text decoder. DI = 40 for models with shared latent variables only, and DI = 10+10 otherwise. Text Input: DI Dense DI × 128, ReLU tConv(128, 4, 3), ReLU tConv(128, 4, 2), ReLU tConv(71, 1, 1) 47