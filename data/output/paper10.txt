On the Implicit Bias of Adam Matias D. Cattaneo∗ Princeton University cattaneo@princeton.edu Jason M. Klusowski∗ Princeton University jason.klusowski@princeton.edu Boris Shigida∗ Princeton University bs1624@princeton.edu Abstract In previous literature, backward error analysis was used to find ordinary differential equations (ODEs) approximating the gradient descent trajectory. It was found that finite step sizes implicitly regularize solutions because terms appearing in the ODEs penalize the two-norm of the loss gradients. We prove that the exis- tence of similar implicit regularization in RMSProp and Adam depends on their hyperparameters and the training stage, but with a different “norm” involved: the corresponding ODE terms either penalize the (perturbed) one-norm of the loss gradients or, on the contrary, hinder its decrease (the latter case being typical). We also conduct numerical experiments and discuss how the proven facts can influence generalization. September 4, 2023 1 Introduction Gradient descent can be seen as a numerical method solving the ordinary differential equation (ODE) ˙θ = −∇E(θ), where E(·) is the loss function and ∇E(θ) denotes its gradient. Starting at θ(0), it creates a sequence of guesses θ(1), θ(2), . . ., which lie close to the solution trajectory θ(t) governed by aforementioned ODE. Since the step size h is finite, one could search for a modified differential equation ˙˜θ = −∇ �E(˜θ) such that θ(n) − ˜θ(nh) is exactly zero, or at least closer to zero than θ(n) − θ(nh), that is, all the guesses of the descent lie exactly on the new solution curve or closer compared to the original curve. This approach to analysing properties of numerical method is called backward error analysis in the numerical integration literature (see Chapter IX in [5]). [1] first used this idea for full-batch gradient descent and found that the modified loss function �E(˜θ) = E(˜θ) + (h/4)∥∇E(˜θ)∥2 makes the trajectory of the solution to ˙˜θ = −∇ �E(˜θ) approximate the sequence {θ(n)}∞ n=0 one order of h better than the original differential equation, where ∥ · ∥ denotes the Euclidean norm. In related work, [22] obtained the correction term for full-batch gradient descent up to any chosen order, also studying the global error (uniform in the iteration number) as opposed to the local (one-step) error. The analysis was later extended to mini-batch gradient descent in [28]. Assume that the training set is split in batches of size B and there are m batches per epoch (so the training set size is mB), the cost function is rewritten E(θ) = (1/m) �m−1 k=0 ˆEk(θ) with mini-batch costs denoted ∗Equal contribution Preprint. Under review. arXiv:2309.00079v1 [cs.LG] 31 Aug 2023ˆEk(θ) = (1/B) �kB+B j=kB+1 Ej(θ). It was obtained in that work that after one epoch, the mean iterate of the algorithm, averaged over all possible shuffles of the batch indices, is close to the solution to ˙θ = −∇ �ESGD(θ), where the modified loss is given by �ESGD(θ) = E(θ) + h 4m m−1 � k=0 ��∇ ˆE(θ) ��2. More recently, [8] studied the gradient descent with heavy-ball momentum iteration θ(n+1) = θ(n) − h∇E(θ(n)) + β(θ(n) − θ(n−1)), where β is the momentum parameter. They proved that it is close to the continuous trajectory of the piecewise first-order ODE ˙˜θ = −1 − βn+1 1 − β ∇E(˜θ(t)) − hγn(1 + β) 2(1 − β)3 ∇2E(˜θ(t))∇E(˜θ(t)), t ∈ [nh, (n + 1)h], where γn = � 1 − β2n+2� − 4(n + 1)βn+1 (1 − β) (1 + β). This result is stated here in a full-batch setting and is a special case of their main theorem, which includes mini-batches. In another recent work, [34] introduce a regularization term λ · ∥∇E(θ)∥ to the loss function as a way to ensure finding flatter minima, which have been observed empirically to have a smaller test error. The only difference between their term and the first-order correction coming from backward error analysis (up to a coefficient) is that the norm is not squared. Using backward error analysis to approximate the discrete dynamics with a modified ODE for adaptive algorithms such as RMSProp [30] and Adam [17] (which is an improvement over RMSProp and AdaGrad[4]) is currently missing in the literature. [1] note that “it would be interesting to use backward error analysis to calculate the modified loss and implicit regularization for other widely used optimizers such as momentum, Adam and RMSprop”. [28] reiterate that they “anticipate that backward error analysis could also be used to clarify the role of finite learning rates in adaptive optimizers like Adam”. In the same context, [8] agree that “RMSProp ... and Adam ..., albeit being powerful alternatives to SGD with faster convergence rates, are far from well-understood in the aspect of implicit regularization”. In a similar context, in Appendix G to [22] it is mentioned that “its [Adam’s] counter term and discretization error are open questions”. This paper fills the gap in the literature by conducting backward error analysis for (mini-batch, and full-batch as a special case) Adam and RMSProp. Our main contributions are listed below. • In Theorem 3.1 and Theorem 4.2, we provide a global second-order in h continuous ODE approxi- mation to Adam and RMSProp in the general mini-batch setting. For the full-batch special case, it was shown in prior work [21] that the continuous-time limit of both these algorithms is a (perturbed by ε) signGD flow ˙θ = − ∇E(θ) |∇E(θ)| + ε component-wise, where ε is the numerical stability parameter; we make this more precise by finding an additional “bias” term on the right (linearly depending on h). • We analyze the full-batch case in more detail. We find that the bias term does something different from penalizing the two-norm of the loss gradient as in the case of gradient descent: it either penalizes the perturbed one-norm of the loss gradient, defined as ∥v∥1,ε = �p i=1 � v2 i + ε, or, on the contrary, hinders its decrease (depending on hyperparameters and the training stage). See the summary of our theoretical finding for the full-batch case in Section 2. We also obtain the backward error analysis result for heavy-ball momentum gradient descent (which was derived before in [8]) as a special case: see Example 2.3. • We provide numerical evidence consistent with our results. In particular, we notice that often penalizing the perturbed one-norm appears to improve generalization, and hindering its decrease hurts it. The typical absence of implicit regularization appearing from backward error analysis in RMSProp and Adam (as opposed to GD) becomes one more previously unidentified possible explanation for poorer generalization of adaptive gradient algorithms compared to other methods. 2• Consistent with our theoretical results, we notice a phenomenon of rising and falling norm in our experiments, which is described as follows. The training of full-batch Adam first steeply decreases the training loss and the perturbed one-norm of the loss gradient, while increasing the test accuracy. Then, however, the perturbed norm rises, even though the training loss and test accuracy behave as expected: continue to decrease and increase respectively. Later the perturbed norm falls again. It seems that the height of the “hill” the perturbed norm graph rises to mid-training depends on how much the bias term hinders the decrease of this norm. To the best of our knowledge, this has not been noticed previously, though the phenomenon of the two-norm of the loss gradient rising while training convolutional neural networks with SGD may be related.2 Related work Backward error analysis of first-order methods. We provide the history of finding ordinary differential equations approximating different algorithms in the introduction. Recently, there have been other applications of backward error analysis related to machine learning. [18] show that the approximating continuous-time trajectories satisfy conservation laws that are broken in discrete time. [7] use backward error analysis while studying how to discretize continuous-time dynamical systems preserving stability and convergence rates. [27] find continuous-time approximations of discrete two-player differential games. Approximating gradient methods by differential equation trajectories. [21] prove that the trajectories of Adam and RMSProp are close to signGD dynamics, and investigate different training regimes of these algorithms empirically. SGD is approximated by stochastic differential equations and novel adaptive parameter adjustment policies are devised in [19]. Implicit bias of first-order methods. [29] prove that gradient descent trained to classify linearly separable data in the case of logistic loss converges to the direction of the max-margin vector (the solution to the hard margin SVM). This result has been extended to different loss functions in [24], to stochastic gradient descent in [25] and more generic optimization methods in [9], to the nonseparable case in [13], [14]. This line of research has been generalized to studying implicit biases of linear networks [12], [10], homogeneous neural networks [11], [23], [20]. [32] study the gradient flow of a diagonal linear network with squared loss and show that large initializations lead to minimum 2-norm solutions while small initializations lead to minimum 1-norm solutions. [6] extend this work to the case of non-zero step sizes and mini-batch training (SGD). [31] prove that Adam and RMSProp maximize the margin of homogeneous neural networks. Generalization of adaptive methods. [3] empirically investigate the edge-of-stability regime of adaptive gradient algorithms and the effect of sharpness (defined as the largest eigenvalue of the hessian) on generalization. [15] introduce a statistic that measures the uniformity of the hessian diagonal and argue that adaptive gradient algorithms are biased towards making this statistic smaller. [16] propose to improve generalization of adaptive methods by switching to SGD in the middle of training. Convergence of Adam. [26] investigate cases where Adam fails to converge to the optimal solution and argue that this is because of exponential averaging of the gradients only provides a short-term memory, and [33] propose ways of fixing the non-convergence issues by preventing the uncontrolled increase of the effective learning rate (i. e. learning rate divided by the square root of the exponential moving average of the squared gradients). Notation We denote the loss of the kth minibatch as a function of the network parameters θ ∈ Rp by Ek(θ), and in the full-batch setting we omit the index and write E(θ). ∇E means the gradient of E, and ∇ with indices means partial derivatives, e. g. ∇ijsE is a shortcut for ∂3E ∂θi∂θj∂θs . The norm without indices ∥·∥ is the two-norm of a vector, ∥·∥1 is the one-norm and ∥·∥1,ε is the perturbed one-norm 2See, e. g., the presentation http://videolectures.net/deeplearning2015_goodfellow_network_ optimization/. 3defined as ∥v∥1,ε = �p i=1 � v2 i + ε. (Of course, if ε > 0 the perturbed one-norm is not a norm, but ε = 0 makes it the one-norm.) 2 Implicit bias of full-batch Adam: an informal summary To avoid ambiguity and to provide the names and notations for hyperparameters, we define the algorithm below. Definition 2.1. The Adam algorithm is an optimization algorithm with numerical stability hyper- parameter ε > 0, squared gradient momentum hyperparameter ρ ∈ (0, 1), gradient momentum parameter β ∈ (0, 1), initialization θ(0) ∈ Rp, ν(0) = 0 ∈ Rp, m(0) = 0 ∈ Rp and the following update rule: for each n ≥ 0, j ∈ {1, . . . , p} ν(n+1) j = ρν(n) j + (1 − ρ) � ∇jEn(θ(n)) �2, m(n+1) j = βm(n) j + (1 − β)∇jEn(θ(n)), θ(n+1) j = θ(n) j − h m(n+1) j /(1 − βn+1) � ν(n+1) j /(1 − ρn+1) + ε . (1) Remark 2.2 (The ε hyperparameter is inside the square root). Note that the numerical stability hyperparameter ε > 0, which is introduced in these algorithms to avoid division by zero, is inside the square root in our definition. This way we avoid division by zero in the derivative too: the first derivative of x �→ �√x + ε �−1 is bounded for x ≥ 0. This is useful for our analysis. In the appendix, the original versions of RMSProp and Adam are also tackled, though with an additional assumption which requires that no component of the gradient can come very close to zero in the region of interest (in particular, no component can change the sign). This is true only for the initial period of learning (whereas the theorems below are true for the whole period). Practitioners do not seem to make a distinction between the version with ε inside vs. outside the square root: tutorials with both versions abound on machine learning related websites. Moreover, the popular Tensorflow variant of RMSProp has ε inside the square root3 even though in the documentation4 [17] is cited, where ε is outside. While conducting numerical experiments, we also noted that moving ε inside or outside the square root does not change the behavior of the algorithms qualitatively. Summary of our main result (in the full-batch case) Having provided the definition, we are ready to informally describe our theoretical result (in the full-batch special case). Assume E(θ) is the loss, whose partial derivatives up to the fourth order are bounded. Let {θ(n)} be iterations of Adam as defined in Definition 2.1. Our main result for this case is finding an ODE whose solution trajectory ˜θ(t) is h2-close to {θ(n)}, meaning that for any positive time horizon T > 0 there exists a constant C > 0 such that for any step size h ∈ (0, T) we have ∥˜θ(nh) − θ(n)∥ ≤ Ch2 (for n between 0 and ⌊T/h⌋). The ODE is written the following way (up to terms that rapidly go to zero as n grows): for the component number j ∈ {1, . . . , p} ˙˜θj(t) = − 1 � |∇jE(˜θ(t))|2 + ε � ∇jE(˜θ(t)) + bias � (2) with initial conditions ˜θj(0) = θ(0) j for all j, where the bias term is bias := h 2 � 1 + β 1 − β − 1 + ρ 1 − ρ + 1 + ρ 1 − ρ · ε |∇jE(˜θ(t))|2 + ε � ∇j ��∇E(˜θ(t)) �� 1,ε. (3) Depending on hyperparameter values and the training stage, the bias term can take two extreme forms, and during most of the training the reality is usually in between. The extreme cases are as follows. 3https://github.com/keras-team/keras/blob/f9336cc5114b4a9429a242deb264b707379646b7/ keras/optimizers/rmsprop.py#L190 4https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/experimental/ RMSprop 4ε “small” ε “large” β ≥ ρ ∥∇E(θ)∥1-penalized ∥∇E(θ)∥2 2-penalized ρ > β −∥∇E(θ)∥1-penalized ∥∇E(θ)∥2 2-penalized Table 1: Implicit bias of Adam: special cases. “Small” and “large” are in relation to squared gradient components. • If √ε is small compared to all components of ∇E(˜θ(t)), i. e. minj ��∇jE(˜θ(t)) �� ≫ √ε, which is the case during the initial learning stage, then bias = h 2 �1 + β 1 − β − 1 + ρ 1 − ρ � ∇j ��∇E(˜θ(t)) �� 1,ε. (4) For small ε, the perturbed one-norm is indistinguishable from the usual one-norm, and for β > ρ it is penalized (in much the same way as the squared two-norm is implicitly penalized in the case of GD), but for ρ > β its decrease is actually hindered by this term (so the bias is opposite to penalization). The ODE in (2) can be approximately rewritten as ˙˜θj(t) = − ∇j �E(˜θ(t)) ��∇jE(˜θ(t)) ��, �E(θ) = E(θ) + h 2 �1 + β 1 − β − 1 + ρ 1 − ρ � ��∇E(θ) �� 1. (5) • If √ε is large compared to all gradient components, i. e. maxj ��∇jE(˜θ(t)) �� ≪ √ε, which may happen during the later learning stage, the fraction with ε is the numerator in (3) approaches one, the dependence on ρ cancels out, and ��∇E(˜θ(t)) �� 1,ε ≈ p � i=1 √ε � 1 + ��∇iE(˜θ(t)) ��2 2ε � = p√ε + 1 2√ε ��∇E(˜θ(t)) ��2. (6) In other words, ∥ · ∥1,ε becomes ∥ · ∥2/(2√ε) up to an additive constant (which is “eaten” by the gradient): bias = h 4√ε 1 + β 1 − β ∇j ��∇E(˜θ(t)) ��2. The form of the ODE in this case is ˙˜θj(t) = −∇j �E(˜θ(t)), �E(θ) = 1 √ε � E(˜θ(t)) + h 4√ε 1 + β 1 − β ��∇E(˜θ(t)) ��2 � . (7) These two extreme cases are summarized in Table 1. In Figure 1, we use the one-dimensional (p = 1) case to illustrate what kind of term is being implicitly penalized. = 1e-05 =0.95, =0.995 =0.95, =0.99 =0.95, =0.951 =0.95, =0.95 =0.95, =0.9 =0.95, =0.8 =0.95, =0.75 =0.95, =0.5 =0.95, =0.1 = 1.0 =0.95, =0.995 =0.95, =0.99 =0.95, =0.951 =0.95, =0.95 =0.95, =0.9 =0.95, =0.8 =0.95, =0.75 =0.95, =0.5 =0.95, =0.1 = 100.0 =0.95, =0.995 =0.95, =0.99 =0.95, =0.951 =0.95, =0.95 =0.95, =0.9 =0.95, =0.8 =0.95, =0.75 =0.95, =0.5 =0.95, =0.1 Figure 1: The graphs of x �→ � x 0 � 1+β 1−β − 1+ρ 1−ρ + 1+ρ 1−ρ · ε y2+ε � d � ε + y2 for different β and ρ. 5Example 2.3 (Backward Error Analysis for GD with Heavy-ball Momentum). Assume ε is very large compared to all squared gradient components during the whole training process, so that the form of the ODE is approximated by (7). Since Adam with a large ε and after a certain number of iterations approximates SGD with heavy-ball momentum with step size h 1−β √ε , linear step size change (and corresponding time change) gives exactly the equations in Theorem 4.1 of [8]. Taking β = 0 (no momentum), we get the implicit regularization of GD from [1]. This overview also applies to RMSProp by setting β = 0. See Section 4 for the formal result. 3 ODE approximating mini-batch Adam trajectories: full statement We only make one assumption, which is standard in the literature: the loss for each mini-batch is 4 times continuously differentiable partial derivatives up to order 4 of each mini-batch loss Ek are bounded by constants, i. e. there exists a positive constant M such that for θ in the region of interest sup k sup θ � sup i |∇iEk(θ)| ∨ sup i,j |∇ijEk(θ)| ∨ sup i,j,s |∇ijsEk(θ)| ∨ sup i,j,s,r |∇ijsrEk(θ)| � ≤ M. (8) We now state the main result for mini-batch Adam, whose proof is in the supplemental appendix. Theorem 3.1. Assume (8) holds. Let {θ(n)} be iterations of Adam as defined in Definition 2.1, ˜θ(t) be the continuous solution to the piecewise ODE ˙˜θj(t) = − M (n) j (˜θ(t)) R(n) j (˜θ(t)) + h � M (n) j (˜θ(t)) � 2P (n) j (˜θ(t)) + ¯P (n) j (˜θ(t)) � 2R(n) j (˜θ(t))3 − 2L(n) j (˜θ(t)) + ¯L(n) j (˜θ(t)) 2R(n) j (˜θ(t)) � . (9) for t ∈ [nh, (n + 1)h] with the initial condition ˜θ(0) = θ(0), where R(n) j (θ) := � � � � n � k=0 ρn−k(1 − ρ)(∇jEk(θ))2/(1 − ρn+1) + ε, M (n) j (θ) := 1 1 − βn+1 n � k=0 βn−k(1 − β)∇jEk(θ), L(n) j (θ) := 1 1 − βn+1 n � k=0 βn−k(1 − β) p � i=1 ∇ijEk(θ) n−1 � l=k M (l) i (θ) R(l) i (θ) , ¯L(n) j (θ) := 1 1 − βn+1 n � k=0 βn−k(1 − β) p � i=1 ∇ijEk(θ)M (n) i (θ) R(n) i (θ) , P (n) j (θ) := 1 1 − ρn+1 n � k=0 ρn−k(1 − ρ)∇jEk(θ) p � i=1 ∇ijEk(θ) n−1 � l=k M (l) i (θ) R(l) i (θ) , ¯P (n) j (θ) := 1 1 − ρn+1 n � k=0 ρn−k(1 − ρ)∇jEk(θ) p � i=1 ∇ijEk(θ)M (n) i (θ) R(n) i (θ) . Then, for any fixed positive time horizon T > 0 there exists a constant C such that for any step size h ∈ (0, T) we have ��˜θ(nh) − θ(n)�� ≤ Ch2, n = 0, 1, . . . , ⌊T/h⌋. (10) Remark 3.2 (Backward error analysis of Adam in the full-batch setting). In the full-batch setting Ek ≡ E, the terms in Theorem 3.1 simplify to R(n) j (θ) = � |∇jE(θ)|2 + ε, M (n) j (θ) = ∇jE(θ), 6L(n) j (θ) = � β 1 − β − (n + 1)βn+1 1 − βn+1 � ∇j∥∇E(θ)∥1,ε, ¯L(n) j (θ) = ∇j∥∇E(θ)∥1,ε, P (n) j (θ) = � ρ 1 − ρ − (n + 1)ρn+1 1 − ρn+1 � ∇jE(θ)∇j∥∇E(θ)∥1,ε, ¯P (n) j (θ) = ∇jE(θ)∇j∥∇E(θ)∥1,ε. If the iteration number n is large, (9) rapidly becomes as described in (2) and (3). 4 ODE approximating mini-batch RMSProp trajectories: full statement We also study the properties of RMSProp using the same arguments as in Theorem 3.1. Up to rapidly vanishing terms, the resulting ODE is the same as for Adam with β = 0. See Remark 4.3 below. Definition 4.1. The RMSProp algorithm is an optimization algorithm with numerical stability parameter ε > 0, squared gradient momentum parameter ρ ∈ (0, 1), initialization θ(0) ∈ Rp, ν(0) = 0 ∈ Rp and the following update rule: for each n ≥ 0, j ∈ {1, . . . , p} ν(n+1) j = ρν(n) j + (1 − ρ) � ∇jEn(θ(n)) �2, θ(n+1) j = θ(n) j − h � ν(n+1) j + ε ∇jEn(θ(n)). (11) We now state the main result for mini-batch RMSProp, whose proof is in the supplemental appendix. Theorem 4.2. Assume (8) holds. Let {θ(n)} be iterations of RMSProp as defined in Definition 4.1, ˜θ(t) be the continuous solution to the piecewise ODE ˙˜θj(t) = −∇jEn(˜θ(t)) R(n) j (˜θ(t)) + h    ∇jEn(˜θ(t)) � 2P (n) j (˜θ(t)) + ¯P (n) j (˜θ(t)) � 2R(n) j (˜θ(t))3 − �p i=1 ∇ijEn(˜θ(t)) ∇iEn(˜θ(t)) R(n) i (˜θ(t)) 2R(n) j (˜θ(t))    . (12) for t ∈ [nh, (n + 1)h] with the initial condition ˜θ(0) = θ(0), where R(n) j (θ) := � � � � n � k=0 ρn−k(1 − ρ)(∇jEk(θ))2 + ε, P (n) j (θ) := n � k=0 ρn−k(1 − ρ)∇jEk(θ) p � i=1 ∇ijEk(θ) n−1 � l=k ∇iEl(θ) R(l) i (θ) , ¯P (n) j (θ) := n � k=0 ρn−k(1 − ρ)∇jEk(θ) p � i=1 ∇ijEk(θ)∇iEn(θ) R(n) i (θ) . Then, for any fixed positive time horizon T > 0 there exists a constant C such that for any step size h ∈ (0, T) we have ��˜θ(nh) − θ(n)�� ≤ Ch2, n = 0, 1, . . . , ⌊T/h⌋. Remark 4.3 (Backward error analysis of RMSProp in the full-batch setting). In the full-batch setting Ek ≡ E, the terms in Theorem 4.2 simplify to R(n) j (θ) = � |∇jE(θ)|2 (1 − ρn+1) + ε, P (n) j (θ) = n � k=0 ρn−k(1 − ρ)∇jE(θ) p � i=1 ∇ijE(θ) n−1 � l=k ∇iE(θ) � |∇iE(θ)|2(1 − ρl+1) + ε , ¯P (n) j (θ) = (1 − ρn+1)∇jE(θ) p � i=1 ∇ijE(θ) ∇iE(θ) � |∇iE(θ)|2(1 − ρn+1) + ε . If the iteration number n is large, (12) rapidly becomes as described in (2) and (3) with β = 0. 70.5 1.0 1.5 2.0 2.5 0.5 1.0 1.5 2.0 2.5 3.0 3.5 rho = 0.999, h = 0.001, eps = 1e-06 adam, beta = 0.9 adam, beta = 0.99 adam, beta = 0.999 0.5 1.0 1.5 2.0 2.5 0.5 1.0 1.5 2.0 2.5 3.0 3.5 beta = 0.9, h = 0.001, eps = 1e-06 adam, rho = 0.9 adam, rho = 0.99 adam, rho = 0.999 Figure 2: Increasing β moves the trajectory of Adam towards the regions with smaller one-norm of the gradient (if ε is sufficiently small); increasing ρ does the opposite. The violet line is the line of global minima, and the cross denotes the limiting point of minimal one-norm of the gradient. All Adam trajectories start at (2.8, 3.5). 5 Discussion First conclusion. Recall that from [8] the ODE approximating the dynamics of full-batch heavy-ball momentum GD is close to ˙θ = 1 1 − β ∇E(θ) + h 1 + β 4(1 − β)3 ∇ ∥∇E(θ)∥2 � �� � regularization . The first-order term regularizes the training process by penalizing the two-norm of the gradient of the loss. We can conclude with high confidence that this kind of regularization is typically absent in RMSProp (if ε is small) and Adam with ρ > β (if ε is small). This may partially explain why these algorithms generalize worse than their non-adaptive counterparts. Second conclusion. However, the bias term in (3) does contain a kind of “norm” which is the perturbed one-norm ∥v∥1,ε = �p i=1 � v2 i + ε. If √ε is small compared to gradient components, which is usually true except at the end of the training, we can conclude from (5) with moderate confidence that it is only in the case β > ρ that the perturbed norm is penalized, and decreasing ρ or increasing β moves the trajectory towards regions with lower “norm”. Third conclusion. There is currently no theory that would indicate that penalizing the (perturbed) one-norm of the gradient improves generalization. However, reasoning by analogy (with the case of the two-norm), we can conjecture with lower confidence that at least in some stable regimes of training increasing β and decreasing ρ should improve the test error. 6 Illustration: simple bilinear model We now analyze the effect of the first-order term for Adam in the same model as [1] and [8] have studied. Namely, assume the parameter θ = (θ1, θ2) is 2-dimensional, and the loss is given by E(θ) := 1/2(y − θ1θ2x)2, where x, y are fixed scalars x = 2, y = 3/2. The loss is minimized on the hyperbola θ1θ2 = y/x. We graph the trajectories of Adam in this case: Figure 2 shows that increasing β forces the trajectory to the region with smaller 1-norm of the gradient of the loss ∥∇E(θ)∥1, and increasing ρ does the opposite. Figure 3 shows that increasing the learning rate moves Adam towards the region with smaller ∥∇E(θ)∥1 if β > ρ (just like in the case of gradient descent, except the norm is different if ε is small compared to gradient components), and does the opposite if ρ > β. All these observations are exactly what Theorem 3.1 predicts. 80.5 1.0 1.5 2.0 2.5 0.5 1.0 1.5 2.0 2.5 3.0 3.5 beta = 0.995, rho=0.75, eps=1e-06 adam, h = 1e-05 adam, h = 0.0001 adam, h = 0.001 0.5 1.0 1.5 2.0 2.5 0.5 1.0 1.5 2.0 2.5 3.0 3.5 beta = 0.9, rho=0.999, eps=1e-06 adam, h = 1e-05 adam, h = 0.0001 adam, h = 0.001 Figure 3: The setting is the same as in Figure 2. Increasing the learning rate moves the Adam trajectory towards the regions with smaller one-norm of the gradient if β is significantly larger than ρ and does the opposite if ρ is larger than β. 7 Numerical experiments We offer some preliminary empirical evidence of how the first-order term shows up in deep neural networks. [21] divides training regimes of Adam into three categories: the spike regime when ρ is sufficiently larger than β, in which the training loss curve contains very large spikes and the training process is obviously unstable; the (stable) oscillation regime when ρ is sufficiently close to β, in which the loss curve contains fast and small oscillations; the divergence regime when β is sufficiently larger than ρ, in which the optimization diverges. We of course exclude the last regime. Since it is very unlikely that an unstable Adam trajectory is close to the piecewise ODE emerging from backward error analysis, we exclude this regime as well, and confine ourselves to considering the oscillation regime (in which ρ and β do not have to be equal, but should not be too far apart). This is the regime [21] recommend to use in practice. We train Resnet-50 on the CIFAR-10 dataset with full-batch Adam and calculate the quantity ∥∇E(θ)∥1,ε at the first point the training loss drops below 0.01. Figure 4 shows that in the stable oscillation regime increasing ρ seems to increase the perturbed one-norm. This is consistent with backward error analysis (the smaller ρ, the more this “norm” is penalized). We also observe that increasing ρ seems to decrease the test accuracy (see the same figure). The opposite effect was noticed in [3], which we think is the case for the spike regime (where the trajectory of Adam is definitely far from the piecewise ODE trajectory at the later stages of training): it is intuitively plausible that increasing the number and magnitude of the spikes should increase the test accuracy by reducing overfitting. The left part of Figure 5 shows that increasing β seems to decrease the perturbed one-norm. This is consistent with backward error analysis (the larger β, the more this norm is penalized). Similarly, the right part shows that increasing β seems to increase the test accuracy, if anything. Note that the effective learning rate does not depend on β (as is the case for gradient descent with heavy-ball momentum), so we compare training with the same effective learning rate. The picture confirms the finding in [8] (for momentum gradient descent) that increasing the momentum parameter almost always improves the test accuracy. We also train Resnet-101 on CIFAR-10 with full-batch Adam, investigating how increasing β influences the training process. Figure 6 shows the training loss curves and the perturbed one-norm curve (the graphs of ∥∇E∥1,ε as functions of the epoch number). Note that the training loss decreases monotonically to zero, the larger β the faster. The “norm” decreases, then rises again, and then decreases further until convergence. Throughout most of the training, the larger β the smaller the “norm” (so the norm behaves with respect to β in the opposite way the training loss does). The “hills” of the “norm” curves are higher with smaller β and almost unnoticeable when β = ρ. (This should be treated with caution: “hills” are not fully explained by ρ > β.) This is completely consistent with backward analysis because the larger ρ with respect to β, the more ∥∇E∥1,ε is prevented from 90.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99 3100 3200 3300 3400 3500 3600 3700 3800 Perturbed 1-norm 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99 81.5 82.0 82.5 83.0 83.5 84.0 84.5 85.0 Test accuracy Figure 4: Resnet-50 on CIFAR-10 trained with full-batch Adam. The test accuracy seems to fall as ρ increases (in the stable “small oscillations” regime of training). The hyperparameters are as follows: h = 7.5 · 10−5, ε = 10−8, β = 0.99. The test accuracies plotted here are maximal after more than 3600 epochs (they become almost constant much earlier). The perturbed norms are calculated at the same epoch number 900. (It is fair to compare Adam with different parameters at one epoch since the effective learning rates are the same.) 0.96 0.97 0.98 0.99 1.00 3000 3500 4000 4500 5000 Perturbed 1-norm 0.96 0.97 0.98 0.99 1.00 76 78 80 82 84 Test accuracy Figure 5: Resnet-50 on CIFAR-10 trained with full-batch Adam. The perturbed one-norm seems to fall as β increases (in the stable oscillation regime of training), and the test accuracy seems to rise. The hyperparameters are as follows: h = 10−4, ρ = 0.999, ε = 10−8. Both metrics are calculated when the loss first drops below the threshold 0.1. falling by the bias term in (10). The last picture in Figure 6 shows how penalizing the “norm” seems to correspond to increasing the test accuracy in this case. Further evidence on how the perturbed one-norm behaves during training is available in Figure 7, where we train Resnet-101 on CIFAR-100 with increasing ρ. We see that the “hills” are there even if β > ρ, but their height seems to be larger for larger ρ. 8 Limitations and future directions We think that backward error analysis applied to real-world machine learning optimization tasks has some limitations, some of them general to the whole literature and some of them specific to adaptive algorithms. First, the assumption similar to (8) is either explicitly or implicitly present in all previous work on backward error analysis of gradient-based machine learning algorithms, as far as we know. This relatively weak assumption is not true if at least one activation function in the neural network is ReLU: the loss is not even differentiable (though it is very common to ignore this). Moreover, there 100 500 1000 1500 2000 epoch 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 Train loss train h=0.0001, =0.9572, =0.999, =1e-08 train h=0.0001, =0.9687, =0.999, =1e-08 train h=0.0001, =0.9771, =0.999, =1e-08 train h=0.0001, =0.9833, =0.999, =1e-08 train h=0.0001, =0.9878, =0.999, =1e-08 train h=0.0001, =0.9911, =0.999, =1e-08 train h=0.0001, =0.9935, =0.999, =1e-08 train h=0.0001, =0.9952, =0.999, =1e-08 train h=0.0001, =0.9965, =0.999, =1e-08 train h=0.0001, =0.9974, =0.999, =1e-08 0 500 1000 1500 2000 epoch 10000 20000 30000 40000 50000 Perturbed one-norm train h=0.0001, =0.9572, =0.999, =1e-08 train h=0.0001, =0.9687, =0.999, =1e-08 train h=0.0001, =0.9771, =0.999, =1e-08 train h=0.0001, =0.9833, =0.999, =1e-08 train h=0.0001, =0.9878, =0.999, =1e-08 train h=0.0001, =0.9911, =0.999, =1e-08 train h=0.0001, =0.9935, =0.999, =1e-08 train h=0.0001, =0.9952, =0.999, =1e-08 train h=0.0001, =0.9965, =0.999, =1e-08 train h=0.0001, =0.9974, =0.999, =1e-08 0.960 0.965 0.970 0.975 0.980 0.985 0.990 0.995 74 76 78 80 Test accuracy Figure 6: Resnet-101 on CIFAR-10 trained with full-batch Adam. First picture from the top: training loss curves. Second picture: curves plotting ∥∇E∥1,ε after each epoch. Third picture: test accuracy the moment loss drops below 0.01 as a function of β. Hyperparameters: h = 10−4, ρ = 0.999, ε = 10−8. 110 200 400 600 800 1000 epoch 10000 20000 30000 40000 Perturbed one-norm train h=0.0001, =0.97, =0.92, =1e-08 train h=0.0001, =0.97, =0.9354, =1e-08 train h=0.0001, =0.97, =0.9479, =1e-08 train h=0.0001, =0.97, =0.9579, =1e-08 train h=0.0001, =0.97, =0.966, =1e-08 train h=0.0001, =0.97, =0.9726, =1e-08 train h=0.0001, =0.97, =0.9778, =1e-08 train h=0.0001, =0.97, =0.9821, =1e-08 train h=0.0001, =0.97, =0.9856, =1e-08 train h=0.0001, =0.97, =0.9883, =1e-08 train h=0.0001, =0.97, =0.9906, =1e-08 train h=0.0001, =0.97, =0.9924, =1e-08 train h=0.0001, =0.97, =0.9939, =1e-08 train h=0.0001, =0.97, =0.995, =1e-08 train h=0.0001, =0.97, =0.996, =1e-08 Figure 7: Resnet-101 trained on CIFAR-100 with full-batch Adam. We plot ∥∇E∥1,ε after each epoch. Hyperparameters: h = 10−4, β = 0.97, ε = 10−8. is evidence that large-batch algorithms often operate at the edge of stability ([2], [3]), in which the largest eigenvalue of the hessian can be quite large, making it unclear whether the higher-order partial derivatives can safely be assumed bounded near optimality. Second, note that the constant in (10) depends on ε and goes to infinity as ε goes to zero. Theoretically, very small ε may mean that the trajectory of the piecewise ODE is only close to the actual Adam trajectory for unrealistically small learning rates, at least at the later stages of learning. (For the initial learning period, this is not a problem.) It is also true of Proposition 1 in [21]: the real trajectory may be far away even from the sign-GD dynamics. This is especially noticeable in the large-spike regime of training (see Section 7 and [21]) which, despite being obviously pretty unstable, can still minimize the training loss well and lead to acceptable test errors. We believe that these considerations can fruitfully guide future work in this area. Acknowledgments and Disclosure of Funding We specially thank Boris Hanin for his insightful comments and suggestions. Cattaneo gratefully acknowledges financial support from the National Science Foundation through DMS-2210561 and SES-2241575. Klusowski gratefully acknowledges financial support from the National Science Foundation through CAREER DMS-2239448, DMS-2054808, and HDR TRIPODS CCF-1934924. References [1] David Barrett and Benoit Dherin. “Implicit Gradient Regularization”. In: International Con- ference on Learning Representations. 2021. URL: https://openreview.net/forum?id= 3q5IqUrkcF. [2] Jeremy Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar. “Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability”. In: International Conference on Learning Representations. 2021. URL: https://openreview.net/forum? id=jh-rTtvkGeM. [3] Jeremy M Cohen, Behrooz Ghorbani, Shankar Krishnan, Naman Agarwal, Sourabh Medapati, Michal Badura, Daniel Suo, David Cardoze, Zachary Nado, George E Dahl, et al. “Adaptive gradient methods at the edge of stability”. In: arXiv preprint arXiv:2207.14484 (2022). [4] John Duchi, Elad Hazan, and Yoram Singer. “Adaptive subgradient methods for online learning and stochastic optimization.” In: Journal of machine learning research 12.7 (2011). 12[5] Christian Lubich Ernst Hairer and Gerhard Wanner. Geometric numerical integration. 2nd ed. Springer-Verlag, Berlin, 2006. ISBN: 3-540-30663-3. [6] Mathieu Even, Scott Pesme, Suriya Gunasekar, and Nicolas Flammarion. “(S) GD over Diagonal Linear Networks: Implicit Regularisation, Large Stepsizes and Edge of Stability”. In: arXiv preprint arXiv:2302.08982 (2023). [7] Guilherme França, Michael I Jordan, and René Vidal. “On dissipative symplectic integration with applications to gradient-based optimization”. In: Journal of Statistical Mechanics: Theory and Experiment 2021.4 (2021), p. 043402. [8] Avrajit Ghosh, He Lyu, Xitong Zhang, and Rongrong Wang. “Implicit regularization in Heavy-ball momentum accelerated stochastic gradient descent”. In: The Eleventh International Conference on Learning Representations. 2023. URL: https://openreview.net/forum? id=ZzdBhtEH9yB. [9] Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. “Characterizing implicit bias in terms of optimization geometry”. In: International Conference on Machine Learning. PMLR. 2018, pp. 1832–1841. [10] Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. “Implicit bias of gradient descent on linear convolutional networks”. In: Advances in neural information processing systems 31 (2018). [11] Ziwei Ji and Matus Telgarsky. “Directional convergence and alignment in deep learning”. In: Advances in Neural Information Processing Systems 33 (2020), pp. 17176–17186. [12] Ziwei Ji and Matus Telgarsky. “Gradient descent aligns the layers of deep linear networks”. In: arXiv preprint arXiv:1810.02032 (2018). [13] Ziwei Ji and Matus Telgarsky. “Risk and parameter convergence of logistic regression”. In: arXiv preprint arXiv:1803.07300 (2018). [14] Ziwei Ji and Matus Telgarsky. “The implicit bias of gradient descent on nonseparable data”. In: Conference on Learning Theory. PMLR. 2019, pp. 1772–1798. [15] Kaiqi Jiang, Dhruv Malik, and Yuanzhi Li. “How Does Adaptive Optimization Impact Local Neural Network Geometry?” In: arXiv preprint arXiv:2211.02254 (2022). [16] Nitish Shirish Keskar and Richard Socher. “Improving generalization performance by switch- ing from adam to sgd”. In: arXiv preprint arXiv:1712.07628 (2017). [17] Diederick P Kingma and Jimmy Ba. “Adam: A method for stochastic optimization”. In: International Conference on Learning Representations. 2015. [18] Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel LK Yamins, and Hidenori Tanaka. “Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics”. In: arXiv preprint arXiv:2012.04728 (2020). [19] Qianxiao Li, Cheng Tai, and Weinan E. “Stochastic Modified Equations and Adaptive Stochas- tic Gradient Algorithms”. In: Proceedings of the 34th International Conference on Machine Learning. Ed. by Doina Precup and Yee Whye Teh. Vol. 70. Proceedings of Machine Learning Research. PMLR, Aug. 2017, pp. 2101–2110. URL: https://proceedings.mlr.press/ v70/li17f.html. [20] Kaifeng Lyu and Jian Li. “Gradient descent maximizes the margin of homogeneous neural networks”. In: arXiv preprint arXiv:1906.05890 (2019). [21] Chao Ma, Lei Wu, and E Weinan. “A qualitative study of the dynamic behavior for adap- tive gradient algorithms”. In: Mathematical and Scientific Machine Learning. PMLR. 2022, pp. 671–692. [22] Taiki Miyagawa. “Toward Equation of Motion for Deep Neural Networks: Continuous-time Gradient Descent and Discretization Error Analysis”. In: Advances in Neural Information Processing Systems. Ed. by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho. 2022. URL: https://openreview.net/forum?id=qq84D17BPu. [23] Mor Shpigel Nacson, Suriya Gunasekar, Jason Lee, Nathan Srebro, and Daniel Soudry. “Lexi- cographic and depth-sensitive margins in homogeneous and non-homogeneous deep models”. In: International Conference on Machine Learning. PMLR. 2019, pp. 4683–4692. [24] Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique Pamplona Savarese, Nathan Srebro, and Daniel Soudry. “Convergence of gradient descent on separable data”. In: The 22nd International Conference on Artificial Intelligence and Statistics. PMLR. 2019, pp. 3420–3428. 13[25] Mor Shpigel Nacson, Nathan Srebro, and Daniel Soudry. “Stochastic gradient descent on separable data: Exact convergence with a fixed learning rate”. In: The 22nd International Conference on Artificial Intelligence and Statistics. PMLR. 2019, pp. 3051–3059. [26] Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. “On the convergence of adam and beyond”. In: arXiv preprint arXiv:1904.09237 (2019). [27] Mihaela C Rosca, Yan Wu, Benoit Dherin, and David Barrett. “Discretization drift in two- player games”. In: International Conference on Machine Learning. PMLR. 2021, pp. 9064– 9074. [28] Samuel L Smith, Benoit Dherin, David Barrett, and Soham De. “On the Origin of Implicit Regularization in Stochastic Gradient Descent”. In: International Conference on Learning Representations. 2021. URL: https://openreview.net/forum?id=rq_Qr0c1Hyo. [29] Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. “The implicit bias of gradient descent on separable data”. In: The Journal of Machine Learning Research 19.1 (2018), pp. 2822–2878. [30] Tijmen Tieleman, Geoffrey Hinton, et al. “Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude”. In: COURSERA: Neural networks for machine learning 4.2 (2012), pp. 26–31. [31] Bohan Wang, Qi Meng, Wei Chen, and Tie-Yan Liu. “The Implicit Bias for Adaptive Optimiza- tion Algorithms on Homogeneous Neural Networks”. In: Proceedings of the 38th International Conference on Machine Learning. Ed. by Marina Meila and Tong Zhang. Vol. 139. Pro- ceedings of Machine Learning Research. PMLR, July 2021, pp. 10849–10858. URL: https: //proceedings.mlr.press/v139/wang21q.html. [32] Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. “Kernel and rich regimes in overparametrized models”. In: Conference on Learning Theory. PMLR. 2020, pp. 3635–3673. [33] Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. “Adaptive Methods for Nonconvex Optimization”. In: Advances in Neural Information Processing Systems. Ed. by S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett. Vol. 31. Curran Associates, Inc., 2018. URL: https://proceedings.neurips.cc/ paper_files/paper/2018/file/90365351ccc7437a1309dc64e4db32a3-Paper.pdf. [34] Yang Zhao, Hao Zhang, and Xiuyuan Hu. “Penalizing gradient norm for efficiently improving generalization in deep learning”. In: International Conference on Machine Learning. PMLR. 2022, pp. 26982–26992. 14Supplementary Material for the Manuscript “On the Implicit Bias of Adam” by Matias D. Cattaneo, Jason M. Klusowski, and Boris Shigida September 4, 2023 Contents 1 Overview 1 2 RMSProp with ε outside the square root 2 3 RMSProp with ε inside the square root 5 4 Adam with ε outside the square root 7 5 Adam with ε inside the square root 10 6 Technical bounding lemmas 12 7 Proof of Theorem SA-2.3 21 8 Numerical experiments 27 1 Overview SA-1.1. This appendix provides some omitted details and proofs. We consider two algorithms: RMSProp and Adam, and two versions of each algorithm (with the numerical stability ε parameter inside and outside of the square root in the denominator). This means there are four main theorems: Theorem SA-2.4, Theorem SA-3.4, Theorem SA-4.4 and Theorem SA-5.4, each residing in the section completely devoted to one algorithm. The simple induction argument taken from [1], essentially the same for each of these theorems, is based on an auxiliary result whose corresponding versions are Theorem SA-2.3, Theorem SA-3.3, Theorem SA-4.3 and Theorem SA-5.3. The proof of this result is also elementary but long, and it is done by a series of lemmas in Section 6 and Section 7, culminating in Section SA-7.6. Out of these four, we only prove Theorem SA-2.3 since the other three results are proven in the same way with obvious changes. Section 8 contains some details about the numerical experiments. SA-1.2 Notation. We denote the loss of the kth minibatch as a function of the network parameters θ ∈ Rp by Ek(θ), and in the full-batch setting we omit the index and write E(θ). As usual, ∇E means the gradient of E, and nabla with indices means partial derivatives, e. g. ∇ijsE is a shortcut for ∂3E ∂θi∂θj∂θs . The letter T > 0 will always denote a finite time horizon of the ODEs, h will always denote the training step size, and we will replace nh with tn when convenient, where n ∈ {0, 1, . . .} is the step number. We will use the same notation for the iteration of the discrete algorithm � θ(k)� k∈Z≥0 , the piecewise ODE solution ˜θ(t) and some auxiliary terms for each of the four algorithms: see Definition SA-2.1, Definition SA- 1 arXiv:2309.00079v1 [cs.LG] 31 Aug 20233.1, Definition SA-4.1, Definition SA-5.1. This way, we avoid cluttering the notation significantly. We are careful to reference the relevant definition in all theorem statements. 2 RMSProp with ε outside the square root Definition SA-2.1. In this section, for some θ(0) ∈ Rp, ν(0) = 0 ∈ Rp, ρ ∈ (0, 1), let the sequence of p-vectors � θ(k)� k∈Z≥0 be defined for n ≥ 0 by ν(n+1) j = ρν(n) j + (1 − ρ) � ∇jEn � θ(n)��2 , θ(n+1) j = θ(n) j − h � ν(n+1) j + ε ∇jEn � θ(n)� . (2.1) Let ˜θ(t) be defined as a continuous solution to the piecewise ODE ˙˜θj(t) = − ∇jEn � ˜θ(t) � R(n) j � ˜θ(t) � + ε + h       ∇jEn � ˜θ(t) � � 2P (n) j � ˜θ(t) � + ¯P (n) j � ˜θ(t) �� 2 � R(n) j � ˜θ(t) � + ε �2 R(n) j � ˜θ(t) � − �p i=1 ∇ijEn � ˜θ(t) � ∇iEn(˜θ(t)) R(n) i (˜θ(t))+ε 2 � R(n) j (˜θ(t)) + ε �       (2.2) with the initial condition ˜θ(0) = θ(0), where R(n)(θ), P(n)(θ) and ¯P(n)(θ) are p-dimensional functions with components R(n) j (θ) := � � � � n � k=0 ρn−k(1 − ρ) � ∇jEk(θ) �2, P (n) j (θ) := n � k=0 ρn−k(1 − ρ)∇jEk (θ) p � i=1 ∇ijEk (θ) n−1 � l=k ∇iEl (θ) R(l) i (θ) + ε , ¯P (n) j (θ) := n � k=0 ρn−k(1 − ρ)∇jEk (θ) p � i=1 ∇ijEk (θ) ∇iEn (θ) R(n) i (θ) + ε . Assumption SA-2.2. 1. For some positive constants M1, M2, M3, M4 we have sup i sup k sup θ ��∇iEk(θ) �� ≤ M1, sup i,j sup k sup θ ��∇ijEk(θ) �� ≤ M2, sup i,j,s sup k sup θ ��∇ijsEk(θ) �� ≤ M3, sup i,j,s,r sup k sup θ ��∇ijsrEk(θ) �� ≤ M4. 2. For some R > 0 we have for all n ∈ � 0, 1, . . . , ⌊T/h⌋ � R(n) j � ˜θ(tn) � ≥ R, n � k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(tk) ��2 ≥ R2, where ˜θ(t) is defined in Definition SA-2.1. 2Theorem SA-2.3 (RMSProp with ε outside: local error bound). Suppose Assumption SA-2.2 holds. Then for all n ∈ � 0, 1, . . . , ⌊T/h⌋ � ����������� ˜θj(tn+1) − ˜θj(tn) + h ∇jEn � ˜θ(tn) � � �n k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(tk) ��2 + ε ����������� ≤ C1h3 for a positive constant C1 depending on ρ. The proof of Theorem SA-2.3 is conceptually simple but very technical, and we delay it until Section 7. For now assuming it as given and combining it with a simple induction argument gives a global error bound which follows. Theorem SA-2.4 (RMSProp with ε outside: global error bound). Suppose Assumption SA-2.2 holds, and n � k=0 ρn−k(1 − ρ) � ∇jEk � θ(k)��2 ≥ R2 for � θ(k)� k∈Z≥0 defined in Definition SA-2.1. Then there exist positive constants d1, d2, d3 such that for all n ∈ � 0, 1, . . . , ⌊T/h⌋ � ∥en∥ ≤ d1ed2nhh2 and ∥en+1 − en∥ ≤ d3ed2nhh3, where en := ˜θ(tn) − θ(n). The constants can be defined as d1 := C1, d2 :=  1 + M2√p R + ε � M 2 1 R(R + ε) + 1 � d1   √p, d3 := C1d2. Proof. We will show this by induction over n, the same way an analogous bound is shown in [1]. The base case is n = 0. Indeed, e0 = ˜θ(0) − θ(0) = 0. Then the jth component of e1 − e0 is [e1 − e0]j = [e1]j = ˜θj(t1) − θ(0) j + h∇jE0 � θ(0)� � (1 − ρ) � ∇jE0 � θ(0)��2 + ε = ˜θj(t1) − ˜θj(t0) + h∇jE0 � ˜θ(t0) � � (1 − ρ) � ∇jE0 � ˜θ(t0) ��2 + ε . By Theorem SA-2.3, the absolute value of the right-hand side does not exceed C1h3, which means ∥e1 − e0∥ ≤ C1h3√p. Since C1√p ≤ d3, the base case is proven. Now suppose that for all k = 0, 1, . . . , n − 1 the claim ∥ek∥ ≤ d1ed2khh2 and ∥ek+1 − ek∥ ≤ d3ed2khh3 is proven. Then ∥en∥ (a) ≤ ∥en−1∥ + ∥en − en−1∥ ≤ d1ed2(n−1)hh2 + d3ed2(n−1)hh3 = d1ed2(n−1)hh2 � 1 + d3 d1 h � (b) ≤ d1ed2(n−1)hh2 (1 + d2h) 3(c) ≤ d1ed2(n−1)hh2 · ed2h = d1ed2nhh2, where (a) is by the triangle inequality, (b) is by d3/d1 ≤ d2, in (c) we used 1 + x ≤ ex for all x ≥ 0. Next, combining Theorem SA-2.3 with (2.1), we have ���[en+1 − en]j ��� ≤ C1h3 + h ������� ∇jEn � ˜θ(tn) � √ A + ε − ∇jEn � θ(n)� √ B + ε ������� , (2.3) where to simplify notation we put A := n � k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(tk) ��2 , B := n � k=0 ρn−k(1 − ρ) � ∇jEk � θ(k)��2 . Using A ≥ R2, B ≥ R2, we have ���� 1 √ A + ε − 1 √ B + ε ���� = |A − B| �√ A + ε � �√ B + ε � �√ A + √ B � ≤ |A − B| 2R (R + ε)2 . (2.4) But since ����� � ∇jEk � ˜θ(tk) ��2 − � ∇jEk � θ(k)��2����� = ����∇jEk � ˜θ(tk) � − ∇jEk � θ(k)����� · ����∇jEk � ˜θ(tk) � + ∇jEk � θ(k)����� ≤ 2M1 ����∇jEk � ˜θ(tk) � − ∇jEk � θ(k)����� ≤ 2M1M2 √p ���˜θ(tk) − θ(k)��� , we have |A − B| ≤ 2M1M2 √p n � k=0 ρn−k(1 − ρ) ���˜θ(tk) − θ(k)��� . (2.5) Combining (2.4) and (2.5), we obtain ������� ∇jEn � ˜θ(tn) � √ A + ε − ∇jEn � θ(n)� √ B + ε ������� ≤ ����∇jEn � ˜θ(tn) ����� · ���� 1 √ A + ε − 1 √ B + ε ���� + ����∇jEn � ˜θ(tn) � − ∇jEn � θ(n)����� √ B + ε ≤ M1 · 2M1M2√p �n k=0 ρn−k(1 − ρ) ���˜θ(tk) − θ(k)��� 2R(R + ε)2 + M2√p ���˜θ(tn) − θ(n)��� R + ε = M 2 1 M2√p R(R + ε)2 n � k=0 ρn−k(1 − ρ) ���˜θ(tk) − θ(k)��� + M2√p R + ε ���˜θ(tn) − θ(n)��� (a) ≤ M 2 1 M2√p R(R + ε)2 n � k=0 ρn−k(1 − ρ)d1ed2khh2 + M2√p R + ε d1ed2nhh2, (2.6) where in (a) we used the induction hypothesis and that the bound on ∥en∥ is already proven. Now note that since 0 < ρe−d2h ≤ ρ, we have �n k=0 � ρe−d2h�k ≤ �∞ k=0 ρk = 1 1−ρ, which is rewritten as n � k=0 ρn−k(1 − ρ)ed2kh ≤ ed2nh. 4Then we can continue (2.6): ������� ∇jEn � ˜θ(tn) � √ A + ε − ∇jEn � θ(n)� √ B + ε ������� ≤ M2√p R + ε � M 2 1 R(R + ε) + 1 � d1ed2nhh2 (2.7) Again using 1 ≤ ed2nh, we conclude from (2.3) and (2.7) that ∥en+1 − en∥ ≤  C1 + M2√p R + ε � M 2 1 R(R + ε) + 1 � d1   √p � �� � ≤d3 ed2nhh3, finishing the induction step. SA-2.5 RMSProp with ε outside: full-batch. In the full-batch setting Ek ≡ E, the terms in (2.2) simplify to R(n) j (θ) = ��∇jE(θ) �� � 1 − ρn+1, P (n) j (θ) = n � k=0 ρn−k(1 − ρ)∇jE(θ) p � i=1 ∇ijE(θ) n−1 � l=k ∇iE(θ) ��∇iE(θ) �� � 1 − ρl+1 + ε , ¯P (n) j (θ) = � 1 − ρn+1� ∇jE(θ) p � i=1 ∇ijE(θ) ∇iE(θ) ��∇iE(θ) �� � 1 − ρn+1 + ε . If ε is small and the iteration number n is large, (2.2) simplifies to ˙˜θj(t) = − sign ∇jE(˜θ(t)) + h ρ 1 − ρ · �p i=1 ∇ijE(˜θ(t)) sign ∇iE(˜θ(t)) ���∇jE(˜θ(t)) ��� = ���∇jE(˜θ(t)) ��� −1 � −∇jE(˜θ(t)) + h ρ 1 − ρ∇j ���∇E(˜θ(t)) ��� 1 � . 3 RMSProp with ε inside the square root Definition SA-3.1. In this section, for some θ(0) ∈ Rp, ν(0) = 0 ∈ Rp, ρ ∈ (0, 1), let the sequence of p-vectors � θ(k)� k∈Z≥0 be defined for n ≥ 0 by ν(n+1) j = ρν(n) j + (1 − ρ) � ∇jEn � θ(n)��2 , θ(n+1) j = θ(n) j − h � ν(n+1) j + ε ∇jEn � θ(n)� . (3.1) Let ˜θ(t) be defined as a continuous solution to the piecewise ODE ˙˜θj(t) = − ∇jEn � ˜θ(t) � R(n) j � ˜θ(t) � + h      ∇jEn � ˜θ(t) � � 2P (n) j � ˜θ(t) � + ¯P (n) j � ˜θ(t) �� 2R(n) j � ˜θ(t) �3 − �p i=1 ∇ijEn � ˜θ(t) � ∇iEn(˜θ(t)) R(n) i (˜θ(t)) 2R(n) j (˜θ(t))      . (3.2) 5with the initial condition ˜θ(0) = θ(0), where R(n)(θ), P(n)(θ) and ¯P(n)(θ) are p-dimensional functions with components R(n) j (θ) := � � � � n � k=0 ρn−k(1 − ρ) � ∇jEk(θ) �2 + ε, P (n) j (θ) := n � k=0 ρn−k(1 − ρ)∇jEk (θ) p � i=1 ∇ijEk (θ) n−1 � l=k ∇iEl (θ) R(l) i (θ) , ¯P (n) j (θ) := n � k=0 ρn−k(1 − ρ)∇jEk (θ) p � i=1 ∇ijEk (θ) ∇iEn (θ) R(n) i (θ) . (3.3) Assumption SA-3.2. For some positive constants M1, M2, M3, M4 we have sup i sup k sup θ ��∇iEk(θ) �� ≤ M1, sup i,j sup k sup θ ��∇ijEk(θ) �� ≤ M2, sup i,j,s sup k sup θ ��∇ijsEk(θ) �� ≤ M3, sup i,j,s,r sup k sup θ ��∇ijsrEk(θ) �� ≤ M4. Theorem SA-3.3 (RMSProp with ε inside: local error bound). Suppose Assumption SA-3.2 holds. Then for all n ∈ � 0, 1, . . . , ⌊T/h⌋ � ����������� ˜θj(tn+1) − ˜θj(tn) + h ∇jEn � ˜θ(tn) � � �n k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(tk) ��2 + ε ����������� ≤ C2h3 for a positive constant C2 depending on ρ, where ˜θ(t) is defined in Definition SA-3.1. We omit the proof since it is essentially the same argument as for Theorem SA-2.3. Theorem SA-3.4 (RMSProp with ε inside: global error bound). Suppose Assumption SA-3.2 holds. Then there exist positive constants d4, d5, d6 such that for all n ∈ � 0, 1, . . . , ⌊T/h⌋ � ∥en∥ ≤ d4ed5nhh2 and ∥en+1 − en∥ ≤ d6ed5nhh3, where en := ˜θ(tn) − θ(n); ˜θ(t) and � θ(k)� k∈Z≥0 are defined in Definition SA-3.1. The constants can be defined as d4 := C2, d5 :=  1 + M2√p √ε � M 2 1 ε + 1 � d4   √p, d6 := C2d5. We omit the proof since it is essentially the same argument as for Theorem SA-2.4. 64 Adam with ε outside the square root Definition SA-4.1. In this section, for some θ(0) ∈ Rp, ν(0) = 0 ∈ Rp, β, ρ ∈ (0, 1), let the sequence of p-vectors � θ(k)� k∈Z≥0 be defined for n ≥ 0 by ν(n+1) j = ρν(n) j + (1 − ρ) � ∇jEn � θ(n)��2 , m(n+1) j = βm(n) j + (1 − β)∇jEn � θ(n)� , θ(n+1) j = θ(n) j − h m(n+1) j / � 1 − βn+1� � ν(n+1) j / (1 − ρn+1) + ε or, rewriting, θ(n+1) j = θ(n) j − h 1 1−βn+1 �n k=0 βn−k (1 − β) ∇jEk � θ(k)� � 1 1−ρn+1 �n k=0 ρn−k(1 − ρ) � ∇jEk � θ(k)��2 + ε . (4.1) Let ˜θ(t) be defined as a continuous solution to the piecewise ODE ˙˜θj(t) = − M (n) j � ˜θ(t) � R(n) j � ˜θ(t) � + ε + h       M (n) j � ˜θ(t) � � 2P (n) j � ˜θ(t) � + ¯P (n) j � ˜θ(t) �� 2 � R(n) j � ˜θ(t) � + ε �2 R(n) j � ˜θ(t) � − 2L(n) j � ˜θ(t) � + ¯L(n) j � ˜θ(t) � 2 � R(n) j � ˜θ(t) � + ε �       . (4.2) with the initial condition ˜θ(0) = θ(0), where R(n)(θ), P(n)(θ), ¯P(n)(θ), M(n)(θ), L(n)(θ), ¯L(n)(θ) are p-dimensional functions with components R(n) j (θ) := � � � � n � k=0 ρn−k(1 − ρ) � ∇jEk(θ) �2 / (1 − ρn+1), M (n) j (θ) := 1 1 − βn+1 n � k=0 βn−k (1 − β) ∇jEk (θ) , L(n) j (θ) := 1 1 − βn+1 n � k=0 βn−k(1 − β) p � i=1 ∇ijEk(θ) n−1 � l=k M (l) i (θ) R(l) i (θ) + ε , ¯L(n) j (θ) := 1 1 − βn+1 n � k=0 βn−k(1 − β) p � i=1 ∇ijEk(θ) M (n) i (θ) R(n) i (θ) + ε , P (n) j (θ) := 1 1 − ρn+1 n � k=0 ρn−k(1 − ρ)∇jEk(θ) p � i=1 ∇ijEk(θ) n−1 � l=k M (l) i (θ) R(l) i (θ) + ε , ¯P (n) j (θ) := 1 1 − ρn+1 n � k=0 ρn−k(1 − ρ)∇jEk(θ) p � i=1 ∇ijEk(θ) M (n) i (θ) R(n) i (θ) + ε . (4.3) Assumption SA-4.2. 1. For some positive constants M1, M2, M3, M4 we have sup i sup k sup θ ��∇iEk(θ) �� ≤ M1, 7sup i,j sup k sup θ ��∇ijEk(θ) �� ≤ M2, sup i,j,s sup k sup θ ��∇ijsEk(θ) �� ≤ M3, sup i,j,s,r sup k sup θ ��∇ijsrEk(θ) �� ≤ M4. 2. For some R > 0 we have for all n ∈ � 0, 1, . . . , ⌊T/h⌋ � R(n) j � ˜θ(tn) � ≥ R, 1 1 − ρn+1 n � k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(tk) ��2 ≥ R2, where ˜θ(t) is defined in Definition SA-4.1. Theorem SA-4.3 (Adam with ε outside: local error bound). Suppose Assumption SA-4.2 holds. Then for all n ∈ � 0, 1, . . . , ⌊T/h⌋ � ����������� ˜θj(tn+1) − ˜θj(tn) + h 1 1−βn+1 �n k=0 βn−k (1 − β) ∇jEk � ˜θ(tk) � � 1 1−ρn+1 �n k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(tk) ��2 + ε ����������� ≤ C3h3 for a positive constant C3 depending on β and ρ. We omit the proof since it is essentially the same argument as for Theorem SA-2.3. Theorem SA-4.4 (Adam with ε outside: global error bound). Suppose Assumption SA-4.2 holds, and 1 1 − ρn+1 n � k=0 ρn−k(1 − ρ) � ∇jEk � θ(k)��2 ≥ R2 for � θ(k)� k∈Z≥0 defined in Definition SA-4.1. Then there exist positive constants d7, d8, d9 such that for all n ∈ � 0, 1, . . . , ⌊T/h⌋ � ∥en∥ ≤ d7ed8nhh2 and ∥en+1 − en∥ ≤ d9ed8nhh3, where en := ˜θ(tn) − θ(n). The constants can be defined as d7 := C3, d8 :=  1 + M2√p R + ε � M 2 1 R(R + ε) + 1 � d7   √p, d9 := C3d8. Proof. Analogously to Theorem SA-2.4, we will prove this by induction over n. The base case is n = 0. Indeed, e0 = ˜θ(0) − θ(0) = 0. Then the jth component of e1 − e0 is [e1 − e0]j = [e1]j = ˜θj(t1) − θ(0) j + h∇jE0 � θ(0)� ����∇jE0 � θ(0)����� + ε = ˜θj(t1) − ˜θj(t0) + h∇jE0 � ˜θ(t0) � �� ∇jE0 � ˜θ(t0) ��2 + ε . 8By Theorem SA-4.3, the absolute value of the right-hand side does not exceed C3h3, which means ∥e1 − e0∥ ≤ C3h3√p. Since C3√p ≤ d9, the base case is proven. Now suppose that for all k = 0, 1, . . . , n − 1 the claim ∥ek∥ ≤ d7ed8khh2 and ∥ek+1 − ek∥ ≤ d9ed8khh3 is proven. Then ∥en∥ (a) ≤ ∥en−1∥ + ∥en − en−1∥ ≤ d7ed8(n−1)hh2 + d9ed8(n−1)hh3 = d7ed8(n−1)hh2 � 1 + d9 d7 h � (b) ≤ d7ed8(n−1)hh2 (1 + d8h) (c) ≤ d7ed8(n−1)hh2 · ed8h = d7ed8nhh2, where (a) is by the triangle inequality, (b) is by d9/d7 ≤ d8, in (c) we used 1 + x ≤ ex for all x ≥ 0. Next, combining Theorem SA-4.3 with (4.1), we have ���[en+1 − en]j ��� ≤ C3h3 + h ���� N ′ √ D′ + ε − N ′′ √ D′′ + ε ���� , (4.4) where to simplify notation we put N ′ := 1 1 − βn+1 n � k=0 βn−k(1 − β)∇jEk � θ(k)� , N ′′ := 1 1 − βn+1 n � k=0 βn−k(1 − β)∇jEk � ˜θ(tk) � , D′ := 1 1 − ρn+1 n � k=0 ρn−k(1 − ρ) � ∇jEk � θ(k)��2 , D′′ := 1 1 − ρn+1 n � k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(tk) ��2 . Using D′ ≥ R2, D′′ ≥ R2, we have ���� 1 √ D′ + ε − 1 √ D′′ + ε ���� = ��D′ − D′′�� �√ D′ + ε � �√ D′′ + ε � �√ D′ + √ D′′ � ≤ ��D′ − D′′�� 2R (R + ε)2 . (4.5) But since ����� � ∇jEk � θ(k)��2 − � ∇jEk � ˜θ(tk) ��2����� = ����∇jEk � θ(k)� − ∇jEk � ˜θ(tk) ����� · ����∇jEk � θ(k)� + ∇jEk � ˜θ(tk) ����� ≤ 2M1 ����∇jEk � θ(k)� − ∇jEk � ˜θ(tk) ����� ≤ 2M1M2 √p ���θ(k) − ˜θ(tk) ��� , we have ��D′ − D′′�� ≤ 2M1M2√p 1 − ρn+1 n � k=0 ρn−k(1 − ρ) ���θ(k) − ˜θ(tk) ��� . (4.6) Similarly, ��N ′ − N ′′�� ≤ 1 1 − βn+1 n � k=0 βn−k(1 − β) ����∇jEk � θ(k)� − ∇jEk � ˜θ(tk) ����� ≤ 1 1 − βn+1 n � k=0 βn−k(1 − β)M2 √p ���θ(k) − ˜θ(tk) ��� . (4.7) 9Combining (4.5), (4.6) and (4.7), we get ���� N ′ √ D′ + ε − N ′′ √ D′′ + ε ���� ≤ ��N ′�� · ���� 1 √ D′ + ε − 1 √ D′′ + ε ���� + ��N ′ − N ′′�� √ D′′ + ε ≤ 1 1 − βn+1 n � k=0 βn−k(1 − β)M1 · 2M1M2√p 2R(R + ε)2 (1 − ρn+1) n � k=0 ρn−k(1 − ρ) ���θ(k) − ˜θ(tk) ��� + M2√p (R + ε) (1 − βn+1) n � k=0 βn−k(1 − β) ���θ(k) − ˜θ(tk) ��� = M 2 1 M2√p R(R + ε)2 (1 − ρn+1) n � k=0 ρn−k(1 − ρ) ���θ(k) − ˜θ(tk) ��� + M2√p (R + ε) (1 − βn+1) n � k=0 βn−k(1 − β) ���θ(k) − ˜θ(tk) ��� (a) ≤ M 2 1 M2√p R(R + ε)2 (1 − ρn+1) n � k=0 ρn−k(1 − ρ)d7ed8khh2 + M2√p (R + ε) (1 − βn+1) n � k=0 βn−k(1 − β)d7ed8khh2, (4.8) where in (a) we used the induction hypothesis and that the bound on ∥en∥ is already proven. Now note that since 0 < ρe−d8h < ρ, we have �n k=0 � ρe−d8h�k ≤ �n k=0 ρk = � 1 − ρn+1� / (1 − ρ), which is rewritten as 1 1 − ρn+1 n � k=0 ρn−k(1 − ρ)ed8kh ≤ ed8nh. By the same logic, 1 1 − βn+1 n � k=0 βn−k(1 − β)ed8kh ≤ ed8nh. Then we can continue (4.8): ���� N ′ √ D′ + ε − N ′′ √ D′′ + ε ���� ≤ M2√p R + ε � M 2 1 R(R + ε) + 1 � d7ed8nhh2 (4.9) Again using 1 ≤ ed8nh, we conclude from (4.4) and (4.9) that ∥en+1 − en∥ ≤  C3 + M2√p R + ε � M 2 1 R(R + ε) + 1 � d7   √p � �� � ≤d9 ed8nhh3, finishing the induction step. 5 Adam with ε inside the square root Definition SA-5.1. In this section, for some θ(0) ∈ Rp, ν(0) = 0 ∈ Rp, β, ρ ∈ (0, 1), let the sequence of p-vectors � θ(k)� k∈Z≥0 be defined for n ≥ 0 by ν(n+1) j = ρν(n) j + (1 − ρ) � ∇jEn � θ(n)��2 , m(n+1) j = βm(n) j + (1 − β)∇jEn � θ(n)� , θ(n+1) j = θ(n) j − h m(n+1) j / � 1 − βn+1� � ν(n+1) j / (1 − ρn+1) + ε . (5.1) 10Let ˜θ(t) be defined as a continuous solution to the piecewise ODE ˙˜θj(t) = − M (n) j � ˜θ(t) � R(n) j � ˜θ(t) � + h      M (n) j � ˜θ(t) � � 2P (n) j � ˜θ(t) � + ¯P (n) j � ˜θ(t) �� 2R(n) j � ˜θ(t) �3 − 2L(n) j � ˜θ(t) � + ¯L(n) j � ˜θ(t) � 2R(n) j � ˜θ(t) �      . (5.2) with the initial condition ˜θ(0) = θ(0), where R(n)(θ), P(n)(θ), ¯P(n)(θ), M(n)(θ), L(n)(θ), ¯L(n)(θ) are p-dimensional functions with components R(n) j (θ) := � � � � n � k=0 ρn−k(1 − ρ) � ∇jEk(θ) �2 / (1 − ρn+1) + ε, M (n) j (θ) := 1 1 − βn+1 n � k=0 βn−k (1 − β) ∇jEk (θ) , L(n) j (θ) := 1 1 − βn+1 n � k=0 βn−k(1 − β) p � i=1 ∇ijEk(θ) n−1 � l=k M (l) i (θ) R(l) i (θ) , ¯L(n) j (θ) := 1 1 − βn+1 n � k=0 βn−k(1 − β) p � i=1 ∇ijEk(θ)M (n) i (θ) R(n) i (θ) , P (n) j (θ) := 1 1 − ρn+1 n � k=0 ρn−k(1 − ρ)∇jEk(θ) p � i=1 ∇ijEk(θ) n−1 � l=k M (l) i (θ) R(l) i (θ) , ¯P (n) j (θ) := 1 1 − ρn+1 n � k=0 ρn−k(1 − ρ)∇jEk(θ) p � i=1 ∇ijEk(θ)M (n) i (θ) R(n) i (θ) . (5.3) Assumption SA-5.2. For some positive constants M1, M2, M3, M4 we have sup i sup k sup θ ��∇iEk(θ) �� ≤ M1, sup i,j sup k sup θ ��∇ijEk(θ) �� ≤ M2, sup i,j,s sup k sup θ ��∇ijsEk(θ) �� ≤ M3, sup i,j,s,r sup k sup θ ��∇ijsrEk(θ) �� ≤ M4. Theorem SA-5.3 (Adam with ε inside: local error bound). Suppose Assumption SA-5.2 holds. Then for all n ∈ � 0, 1, . . . , ⌊T/h⌋ � ����������� ˜θj(tn+1) − ˜θj(tn) + h 1 1−βn+1 �n k=0 βn−k (1 − β) ∇jEk � ˜θ(tk) � � 1 1−ρn+1 �n k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(tk) ��2 + ε ����������� ≤ C4h3 for a positive constant C4 depending on β and ρ. We omit the proof since it is essentially the same argument as for Theorem SA-2.3. Theorem SA-5.4 (Adam with ε inside: global error bound). Suppose Assumption SA-5.2 holds for � θ(k)� k∈Z≥0 defined in Definition SA-5.1. Then there exist positive constants d10, d11, d12 such that for all n ∈ � 0, 1, . . . , ⌊T/h⌋ � ∥en∥ ≤ d10ed11nhh2 and ∥en+1 − en∥ ≤ d12ed11nhh3, 11where en := ˜θ(tn) − θ(n). The constants can be defined as d10 := C4, d11 :=  1 + M2√p √ε � M 2 1 ε + 1 � d10   √p, d12 := C4d11. 6 Technical bounding lemmas We will need the following lemmas to prove Theorem SA-2.3. Lemma SA-6.1. Suppose Assumption SA-2.2 holds. Then sup θ ���P (n) j (θ) ��� ≤ C5, (6.1) sup θ ��� ¯P (n) j (θ) ��� ≤ C6, (6.2) with constants C5, C6 defined as follows: C5 := pM 2 1 M2 R + ε · ρ 1 − ρ, C6 := pM 2 1 M2 R + ε . Proof of Lemma SA-6.1. The proof is done in the following simple steps. SA-6.2 Proof of (6.1). This bound is straightforward: sup θ ���P (n) j (θ) ��� = sup θ ������ n � k=0 ρn−k(1 − ρ)∇jEk (θ) p � i=1 ∇ijEk (θ) n−1 � l=k ∇iEl (θ) R(l) i (θ) + ε ������ ≤ pM 2 1 M2 R + ε (1 − ρ) n � k=0 ρn−k(n − k) ≤ pM 2 1 M2 R + ε (1 − ρ) ∞ � k=0 ρkk = C5. SA-6.3 Proof of (6.2). This bound is straightforward: sup θ ��� ¯P (n) j (θ) ��� = sup θ ������ n � k=0 ρn−k(1 − ρ)∇jEk (θ) p � i=1 ∇ijEk (θ) ∇iEn (θ) R(n) i (θ) + ε ������ ≤ pM 2 1 M2 R + ε (1 − ρ) n � k=0 ρn−k ≤ pM 2 1 M2 R + ε = C6. This concludes the proof of Lemma SA-6.1. Lemma SA-6.4. Suppose Assumption SA-2.2 holds. Then the first derivative of t �→ ˜θj(t) is uniformly over j and t ∈ [0, T] bounded in absolute value by some positive constant, say D1. Proof. This follows immediately from h ≤ T, (6.1), (6.2) and the definition of ˜θ(t) given in (2.2). Lemma SA-6.5. Suppose Assumption SA-2.2 holds. Then sup t∈[0,T ] sup j ����� � ∇jEn � ˜θ(t) ��·����� ≤ C7, (6.3) 12sup n,k sup t∈[tn,tn+1] ������� p � i=1 ∇ijEk � ˜θ(t) �   ˙˜θi(t) + ∇iEn � ˜θ(t) � R(n) i � ˜θ(t) � + ε   ������� ≤ C8h, (6.4) sup k≤n sup t∈[0,T ] ������� p � i=1 ∇ijEk � ˜θ(t) � n−1 � l=k ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε ������� ≤ (n − k)C9, (6.5) ����� � P (n) j � ˜θ(t) ��·����� ≤ C10 + C14, (6.6) ���� � ¯P (n) j (˜θ(t)) �·���� ≤ C15, (6.7) ��������    p � i=1 ∇ijEk � ˜θ(t) � ∇iEn � ˜θ(t) � R(n) i � ˜θ(t) � + ε    ·�������� ≤ C13, (6.8) �����������       ∇jEn � ˜θ(t) � � 2P (n) j � ˜θ(t) � + ¯P (n) j � ˜θ(t) �� 2 � R(n) j � ˜θ(t) � + ε �2 R(n) j � ˜θ(t) �       ·����������� ≤ C17, (6.9) ���������     �p i=1 ∇ijEn � ˜θ(t) � ∇iEn(˜θ(t)) R(n) i (˜θ(t))+ε 2 � R(n) j (˜θ(t)) + ε �     ·��������� ≤ C18, (6.10) with constants C7, C8, C9, C10, C11, C12, C13, C14, C15, C16, C17, C18 defined as follows: C7 := pM2D1, C8 := pM2 �M1 (2C5 + C6) 2(R + ε)2R + pM1M2 2(R + ε)2 � , C9 := pM1M2 R + ε , C10 := D1p2 M1M 2 2 R + ε · ρ 1 − ρ, C11 := D1pM1M2 R , C12 := D1p2 M1M3 R + ε , C13 := C12 + pM2 �D1pM2 R + ε + M1 (R + ε)2 C11 � = D1p2 R + ε � M1M3 + M 2 2 + M 2 1 M 2 2 (R + ε)R � , C14 := M1C13 ρ 1 − ρ, C15 := D1p2M1M 2 2 R + ε + D1p2M 2 1 M3 R + ε + D1p2M1M 2 2 R + ε + pM 2 1 M2C11 (R + ε)2 , C16 := 2C11 R(R + ε)3 + C11 (R + ε)4 , C17 := D1pM2 · (2C5 + C6) 2 (R + ε)2 R + M1 � 2 (C10 + C14) + C15 � 2 (R + ε)2 R + M1 (2C5 + C6) C16 2 , 13C18 := 1 2(R + ε) � p2D1M1M3 R + ε + p2D1M 2 2 R + ε + pM1M2C11 (R + ε)2 � + 1 2 · pM1M2 R + ε · C11 (R + ε)2 . Proof of Lemma SA-6.5. We divide this argument in several steps. SA-6.6 Proof of (6.3). This bound is straightforward: ����� � ∇jEn � ˜θ(t) ��·����� = ������ p � i=1 ∇ijEn � ˜θ(t) � ˙˜θi(t) ������ ≤ C7. SA-6.7 Proof of (6.4). By (2.2) we have for t = t− n+1 ������� ˙˜θj(t) + ∇jEn � ˜θ(t) � R(n) j � ˜θ(t) � + ε ������� ≤ h �M1 (2C5 + C6) 2(R + ε)2R + pM1M2 2(R + ε)2 � , giving (6.4) immediately. SA-6.8 Proof of (6.5). This bound follows from the assumptions immediately. SA-6.9 Proof of (6.6). We will prove this by bounding the two terms in the expression d dtP (n) j � ˜θ(t) � = n � k=0 ρn−k(1 − ρ) p � u=1 ∇juEk � ˜θ(t) � ˙˜θu(t) p � i=1 ∇ijEk � ˜θ(t) � n−1 � l=k ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε + n � k=0 ρn−k(1 − ρ)∇jEk � ˜θ(t) � p � i=1 d dt      ∇ijEk � ˜θ(t) � n−1 � l=k ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε      . (6.11) It is easily shown that the first term in (6.11) is bounded in absolute value by C10: ������� n � k=0 ρn−k(1 − ρ) p � u=1 ∇juEk � ˜θ(t) � ˙˜θu(t) p � i=1 ∇ijEk � ˜θ(t) � n−1 � l=k ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε ������� ≤ D1p2 M1M 2 2 R + ε (1 − ρ) n � k=0 ρkk ≤ D1p2 M1M 2 2 R + ε (1 − ρ) ∞ � k=0 ρkk = C10. For the proof of (6.6), it is left to show that the second term in (6.11) is bounded in absolute value by C14. To bound �p i=1 d dt � ∇ijEk � ˜θ(t) � �n−1 l=k ∇iEl(˜θ(t)) R(l) i (˜θ(t))+ε � , we can use �������� p � i=1 d dt      ∇ijEk � ˜θ(t) � n−1 � l=k ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε      �������� 14≤ ������� p � i=1 d dt � ∇ijEk � ˜θ(t) �� n−1 � l=k ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε ������� + �������� p � i=1 ∇ijEk � ˜θ(t) � n−1 � l=k d dt      ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε      �������� By the Cauchy-Schwarz inequality applied twice, ������� p � i=1 d dt � ∇ijEk � ˜θ(t) �� n−1 � l=k ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε ������� ≤ � � � � p � i=1 p � s=1 � ∇ijsEk � ˜θ(t) ��2 � � � � p � u=1 ˙˜θu(t)2 � � � � � � p � i=1 ������� n−1 � l=k ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε ������� 2 ≤ M3p · D1 √p · � � � � � � p � i=1 ������� n−1 � l=k ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε ������� 2 ≤ (n − k)C12. Next, for any n and j ���� d dtR(n) j � ˜θ(t) ����� = 1 R(n) j � ˜θ(t) � ������ n � k=0 ρn−k(1 − ρ)∇jEk � ˜θ(t) � p � i=1 ∇ijEk � ˜θ(t) � ˙˜θi(t) ������ ≤ 1 R(n) j � ˜θ(t) �D1pM1M2 n � k=0 ρn−k(1 − ρ) ≤ C11. (6.12) This gives �������� d dt      ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε      �������� ≤ ���� �p s=1 ∇isEl � ˜θ(t) � ˙˜θs(t) ���� R(l) i � ˜θ(t) � + ε + ����∇iEl � ˜θ(t) ����� · ���� d dtR(l) i � ˜θ(t) ����� � R(l) i � ˜θ(t) � + ε �2 ≤ D1pM2 R + ε + M1 (R + ε)2 C11. We have obtained �������� p � i=1 d dt      ∇ijEk � ˜θ(t) � n−1 � l=k ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε      �������� ≤ (n − k)C13. (6.13) This gives a bound on the second term in (6.11): �������� n � k=0 ρn−k(1 − ρ)∇jEk � ˜θ(t) � p � i=1 d dt      ∇ijEk � ˜θ(t) � n−1 � l=k ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε      �������� ≤ M1 n � k=0 ρn−k(1 − ρ)(n − k)C13 ≤ C14, concluding the proof of (6.6). 15SA-6.10 Proof of (6.7). We will prove this by bounding the four terms in the expression d dt      n � k=0 ρn−k(1 − ρ)∇jEk � ˜θ(t) � p � i=1 ∇ijEk � ˜θ(t) � ∇iEn � ˜θ(t) � R(n) i � ˜θ(t) � + ε      = Term1 + Term2 + Term3 + Term4, where Term1 := n � k=0 ρn−k(1 − ρ) d dt � ∇jEk � ˜θ(t) �� p � i=1 ∇ijEk � ˜θ(t) � ∇iEn � ˜θ(t) � R(n) i � ˜θ(t) � + ε , Term2 := n � k=0 ρn−k(1 − ρ)∇jEk � ˜θ(t) � p � i=1 d dt � ∇ijEk � ˜θ(t) �� ∇iEn � ˜θ(t) � R(n) i � ˜θ(t) � + ε , Term3 := n � k=0 ρn−k(1 − ρ)∇jEk � ˜θ(t) � p � i=1 ∇ijEk � ˜θ(t) � d dt � ∇iEn � ˜θ(t) �� R(n) i � ˜θ(t) � + ε , Term4 := − n � k=0 ρn−k(1 − ρ)∇jEk � ˜θ(t) � p � i=1 ∇ijEk � ˜θ(t) � ∇iEn � ˜θ(t) � d dtR(n) i � ˜θ(t) � � R(n) i � ˜θ(t) � + ε �2 . To bound Term1, use ����� d dt � ∇jEk � ˜θ(t) ������� ≤ D1pM2, giving |Term1| ≤ D1p2M1M 2 2 R + ε n � k=0 ρn−k(1 − ρ) ≤ D1p2M1M 2 2 R + ε . To bound Term2, use ����� d dt � ∇ijEk � ˜θ(t) ������� ≤ D1pM3, giving |Term2| ≤ D1p2M 2 1 M3 R + ε n � k=0 ρn−k(1 − ρ) ≤ D1p2M 2 1 M3 R + ε . To bound Term3, use ����� d dt � ∇iEn � ˜θ(t) ������� ≤ D1pM2, giving |Term3| ≤ D1p2M1M 2 2 R + ε n � k=0 ρn−k(1 − ρ) ≤ D1p2M1M 2 2 R + ε . To bound Term4, use (6.12), giving |Term4| ≤ pM 2 1 M2C11 (R + ε)2 n � k=0 ρn−k(1 − ρ) ≤ pM 2 1 M2C11 (R + ε)2 . SA-6.11 Proof of (6.8). This is proven in (6.13). 16SA-6.12 Proof of (6.9). (6.12) gives �������� d dt      1 R(n) j � ˜θ(t) �      �������� = ���� d dtR(n) j � ˜θ(t) ����� R(n) j � ˜θ(t) �2 ≤ C11 R2 , (6.14) �������� d dt      1 R(n) j � ˜θ(t) � + ε      �������� = ���� d dtR(n) j � ˜θ(t) ����� � R(n) j � ˜θ(t) � + ε �2 ≤ C11 (R + ε)2 , (6.15) ����������� d dt            1 � R(n) j � ˜θ(t) � + ε �2            ����������� = 2 ���� d dtR(n) j � ˜θ(t) ����� � R(n) j � ˜θ(t) � + ε �3 ≤ 2C11 (R + ε)3 . (6.16) Combining two bounds above, we have ������ d dt �� R(n) j � ˜θ(t) � + ε �−2 R(n) j (˜θ(t))−1 ������� ≤ ������ d dt �� R(n) j � ˜θ(t) � + ε �−2������� R(n) j (˜θ(t)) + ���� d dt � R(n) j (˜θ(t))−1����� � R(n) j � ˜θ(t) � + ε �2 ≤ C16. We are ready to bound �����������       ∇jEn � ˜θ(t) � � 2P (n) j � ˜θ(t) � + ¯P (n) j � ˜θ(t) �� 2 � R(n) j � ˜θ(t) � + ε �2 R(n) j � ˜θ(t) �       ·����������� ≤ ���������� � ∇jEn � ˜θ(t) ��· � 2P (n) j � ˜θ(t) � + ¯P (n) j � ˜θ(t) �� 2 � R(n) j � ˜θ(t) � + ε �2 R(n) j � ˜θ(t) � ���������� + + ���������� ∇jEn � ˜θ(t) � � 2P (n) j � ˜θ(t) � + ¯P (n) j � ˜θ(t) ��· 2 � R(n) j � ˜θ(t) � + ε �2 R(n) j � ˜θ(t) � ���������� + ��������� ∇jEn � ˜θ(t) � � 2P (n) j � ˜θ(t) � + ¯P (n) j � ˜θ(t) �� 2 × �� R(n) j � ˜θ(t) � + ε �−2 R(n) j (˜θ(t))−1 �·������ ≤ C17. 17SA-6.13 Proof of (6.10). Since ������� p � i=1 ∇ijEn � ˜θ(t) � ∇iEn � ˜θ(t) � R(n) i � ˜θ(t) � + ε ������� ≤ pM1M2 R + ε and, as we have already seen in the argument for (6.7), ��������    p � i=1 ∇ijEn � ˜θ(t) � ∇iEn � ˜θ(t) � R(n) i � ˜θ(t) � + ε    ·�������� ≤ p2D1M1M3 R + ε + p2D1M 2 2 R + ε + pM1M2C11 (R + ε)2 , we are ready to bound ���������     �p i=1 ∇ijEn � ˜θ(t) � ∇iEn(˜θ(t)) R(n) i (˜θ(t))+ε 2 � R(n) j (˜θ(t)) + ε �     ·��������� ≤ C18. The proof of Lemma SA-6.5 is concluded. Lemma SA-6.14. Suppose Assumption SA-2.2 holds. Then the second derivative of t �→ ˜θj(t) is uniformly over j and t ∈ [0, T] bounded in absolute value by some positive constant, say D2. Proof. This follows from the definition of ˜θ(t) given in (2.2), h ≤ T and that the first derivatives of all three terms in (2.2) are bounded by Lemma SA-6.5. Lemma SA-6.15. Suppose Assumption SA-2.2 holds. Then ����� � ∇jEn � ˜θ(t) ��··����� ≤ C19, (6.17) ����� � R(n) j � ˜θ(t) ��··����� ≤ C20, (6.18) ������ �� R(n) j � ˜θ(t) � + ε �−2�··������ ≤ C21, (6.19) ����� � R(n) j � ˜θ(t) �−1�··����� ≤ C22, (6.20) ������ �� R(n) j � ˜θ(t) � + ε �−2 R(n) j � ˜θ(t) �−1 �··������ ≤ C23, (6.21) ��������    p � i=1 ∇ijEk � ˜θ(t) � n−1 � l=k ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε    ··�������� ≤ (n − k)C24, (6.22) with constants C19, C20, C21, C22, C23, C24 defined as follows: C19 := p2M3D2 1 + pM2D2, C20 := C11 R2 pM1M2D1 + 1 Rp2M 2 2 D2 1 + 1 Rp2M1M3D2 1 + 1 RpM1M2D2, C21 := 6C2 11 (R + ε)4 + 2C20 (R + ε)3 , C22 := 2C2 11 R3 + C20 R2 , 18C23 := C21 R + 4C2 11 R2(R + ε)3 + C22 (R + ε)2 , C24 := p  2C11 � D1M 2 2 p + D1M1M3p � (R + ε)2 + M1M2 � 2C2 11 (R + ε)3 + C20 (R + ε)2 � + 2D2 1M2M3p2 + M2 � D2 1M3p2 + D2M2p � + M1 � D2 1M4p2 + D2M3p � R + ε � . Proof of Lemma SA-6.15. We divide this argument in several steps. SA-6.16 Proof of (6.17). This bound is straightforward: ����� � ∇jEn � ˜θ(t) ��··����� = ������ p � i=1 p � s=1 ∇ijsEn � ˜θ(t) � ˙˜θs(t) ˙˜θi(t) + p � i=1 ∇ijEn � ˜θ(t) � ¨˜θt(t) ������ ≤ C19. SA-6.17 Proof of (6.18). Note that � R(n) j � ˜θ(t) ��·· = � R(n) j � ˜θ(t) �−1�· n � k=0 ρn−k(1 − ρ)∇jEk � ˜θ(t) � p � i=1 ∇ijEk � ˜θ(t) � ˙˜θi(t) + R(n) j � ˜θ(t) �−1 n � k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(t) ��· p � i=1 ∇ijEk � ˜θ(t) � ˙˜θi(t) + R(n) j � ˜θ(t) �−1 n � k=0 ρn−k(1 − ρ)∇jEk � ˜θ(t) � p � i=1 � ∇ijEk � ˜θ(t) ��· ˙˜θi(t) + R(n) j � ˜θ(t) �−1 n � k=0 ρn−k(1 − ρ)∇jEk � ˜θ(t) � p � i=1 ∇ijEk � ˜θ(t) � ¨˜θi(t), giving by (6.14) ����� � R(n) j � ˜θ(t) ��··����� ≤ C11 R2 pM1M2D1 n � k=0 ρn−k(1 − ρ) + 1 Rp2M 2 2 D2 1 n � k=0 ρn−k(1 − ρ) + 1 Rp2M1M3D2 1 n � k=0 ρn−k(1 − ρ) + 1 RpM1M2D2 � k=0 ρn−k(1 − ρ) ≤ C20. SA-6.18 Proof of (6.19). Note that �� R(n) j � ˜θ(t) � + ε �−2�·· = 6 �� R(n) j � ˜θ(t) ��·�2 � R(n) j � ˜θ(t) � + ε �4 − 2 � R(n) j � ˜θ(t) ��·· � R(n) j � ˜θ(t) � + ε �3 , giving by (6.12) and (6.18) ������ �� R(n) j � ˜θ(t) � + ε �−2�··������ ≤ C21. SA-6.19 Proof of (6.20). The bound follows from (6.12), (6.18) and � R(n) j � ˜θ(t) �−1�·· = 2 �� R(n) j � ˜θ(t) ��·�2 R(n) j � ˜θ(t) �3 − � R(n) j � ˜θ(t) ��·· R(n) j � ˜θ(t) �2 . 19SA-6.20 Proof of (6.21). Putting a := � R(n) j � ˜θ(t) � + ε �−2 , b := R(n) j � ˜θ(t) �−1 , use |a| ≤ 1 (R + ε)2 , |b| ≤ 1 R, |˙a| ≤ 2C11 (R + ε)3 , ���˙b ��� ≤ C11 R2 , |¨a| ≤ C21, ���¨b ��� ≤ C22, and (ab)·· = ¨ab + 2˙a˙b + a¨b. SA-6.21 Proof of (6.22). Putting a := ∇ijEk � ˜θ(t) � , b := ∇iEl � ˜θ(t) � , c := � R(l) i � ˜θ(t) � + ε �−1 , we have |a| ≤ M2, |˙a| ≤ pM3D1, |¨a| ≤ p2M4D2 1 + pM3D2, |b| ≤ M1, ���˙b ��� ≤ pM2D1, ���¨b ��� ≤ p2M3D2 1 + pM2D2, |c| ≤ 1 R + ε, |˙c| ≤ C11 (R + ε)2 , |¨c| ≤ 2C2 11 (R + ε)3 + C20 (R + ε)2 . (6.22) follows. The proof of Lemma SA-6.15 is concluded. Lemma SA-6.22. Suppose Assumption SA-2.2 holds. Then the third derivative of t �→ ˜θj(t) is uniformly over j and t ∈ [0, T] bounded in absolute value by some positive constant, say D3. Proof. By (6.5), (6.13) and (6.22) ������� p � i=1 ∇ijEk � ˜θ(t) � n−1 � l=k ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε ������� ≤ (n − k)C9, ��������    p � i=1 ∇ijEk � ˜θ(t) � n−1 � l=k ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε    ·�������� ≤ (n − k)C13, ��������    p � i=1 ∇ijEk � ˜θ(t) � n−1 � l=k ∇iEl � ˜θ(t) � R(l) i � ˜θ(t) � + ε    ··�������� ≤ (n − k)C24. From the definition of t �→ P (n) j � ˜θ(t) � , it means that its derivatives up to order two are bounded. Similarly, the same is true for t �→ ¯P (n) j � ˜θ(t) � . It follows from (6.19) and its proof that the derivatives up to order two of t �→ � R(n) j � ˜θ(t) � + ε �−2 R(n) j � ˜θ(t) �−1 20are also bounded. These considerations give the boundedness of the second derivative of the term t �→ ∇jEn � ˜θ(t) � � 2P (n) j � ˜θ(t) � + ¯P (n) j � ˜θ(t) �� 2 � R(n) j � ˜θ(t) � + ε �2 R(n) j � ˜θ(t) � in (2.2). The boundedness of the second derivatives of the other two terms is shown analogously. By (2.2) and since h ≤ T, this means sup j sup t∈[0,T ] ��� ... ˜θ j(t) ��� ≤ D3 for some positive constant D3. 7 Proof of Theorem SA-2.3 Lemma SA-7.1. Suppose Assumption SA-2.2 holds. Then for all n ∈ � 0, 1, . . . , ⌊T/h⌋ � , k ∈ {0, 1, . . . , n − 1} we have ����∇jEk � ˜θ(tk) � − ∇jEk � ˜θ(tn) ����� ≤ C7(n − k)h (7.1) Proof. (7.1) follows from the mean value theorem applied n − k times. Lemma SA-7.2. In the setting of Lemma SA-7.1, for any l ∈ {k, k + 1, . . . , n − 1} we have ������� ∇jEk � ˜θ(tl) � − ∇jEk � ˜θ(tl+1) � − h p � i=1 ∇ijEk � ˜θ(tn) � ∇iEl � ˜θ(tn) � R(l) i � ˜θ(tn) � + ε ������� ≤ � C19/2 + C8 + (n − l − 1)C13 � h2. Proof. By the Taylor expansion of t �→ ∇jEk � ˜θ(t) � on the segment [tl, tl+1] at tl+1 on the left ������ ∇jEk � ˜θ(tl) � − ∇jEk � ˜θ(tl+1) � + h p � i=1 ∇ijEk � ˜θ(tl+1) � ˙˜θi � t− l+1 � ������ ≤ C19 2 h2. Combining this with (6.4) gives ������� ∇jEk � ˜θ(tl) � − ∇jEk � ˜θ(tl+1) � − h p � i=1 ∇ijEk � ˜θ(tl+1) � ∇iEl � ˜θ(tl+1) � R(l) i � ˜θ(tl+1) � + ε ������� ≤ � C19/2 + C8 � h2. (7.2) Now applying the mean-value theorem n − l − 1 times, we have ������� p � i=1 ∇ijEk � ˜θ(tl+1) � ∇iEl � ˜θ(tl+1) � R(l) i � ˜θ(tl+1) � + ε − p � i=1 ∇ijEk � ˜θ(tl+2) � ∇iEl � ˜θ(tl+2) � R(l) i � ˜θ(tl+2) � + ε ������� ≤ C13h, · · · ������� p � i=1 ∇ijEl � ˜θ(tn−1) � ∇iEk � ˜θ(tn−1) � R(l) i � ˜θ(tn−1) � + ε − p � i=1 ∇ijEk � ˜θ(tn) � ∇iEl � ˜θ(tn) � R(l) i � ˜θ(tn) � + ε ������� ≤ C13h, 21and in particular ������� p � i=1 ∇ijEk � ˜θ(tl+1) � ∇iEl � ˜θ(tl+1) � R(l) i � ˜θ(tl+1) � + ε − p � i=1 ∇ijEk � ˜θ(tn) � ∇iEl � ˜θ(tn) � R(l) i � ˜θ(tn) � + ε ������� ≤ (n − l − 1)C13h. Combining this with (7.2), we conclude the proof of Lemma SA-7.2. Lemma SA-7.3. In the setting of Lemma SA-7.1, ������� ∇jEk � ˜θ(tk) � − ∇jEk � ˜θ(tn) � − h p � i=1 ∇ijEk � ˜θ(tn) � n−1 � l=k ∇iEl � ˜θ(tn) � R(l) i � ˜θ(tn) � + ε ������� ≤ � (n − k)(C19/2 + C8) + (n − k)(n − k − 1) 2 C13 � h2. Proof. Fix n ∈ Z≥0. Note that ������� ∇jEk � ˜θ(tk) � − ∇jEk � ˜θ(tn) � − h p � i=1 ∇ijEk � ˜θ(tn) � n−1 � l=k ∇iEl � ˜θ(tn) � R(l) i � ˜θ(tn) � + ε ������� = �������� n−1 � l=k      ∇jEk � ˜θ(tl) � − ∇jEk � ˜θ(tl+1) � − h p � i=1 ∇ijEk � ˜θ(tn) � ∇iEl � ˜θ(tn) � R(l) i � ˜θ(tn) � + ε      �������� ≤ n−1 � l=k ������� ∇jEk � ˜θ(tl) � − ∇jEk � ˜θ(tl+1) � − h p � i=1 ∇ijEk � ˜θ(tn) � ∇iEl � ˜θ(tn) � R(l) i � ˜θ(tn) � + ε ������� (a) ≤ n−1 � l=k � C19/2 + C8 + (n − l − 1)C13 � h2 = � (n − k)(C19/2 + C8) + (n − k)(n − k − 1) 2 C13 � h2, where (a) is by Lemma SA-7.2. Lemma SA-7.4. Suppose Assumption SA-2.2 holds. Then for all n ∈ � 0, 1, . . . , ⌊T/h⌋ � ������ n � k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(tk) ��2 − R(n) j � ˜θ(tn) �2 ������ ≤ C25h (7.3) and ������ n � k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(tk) ��2 − R(n) j � ˜θ(tn) �2 − 2hP (n) j � ˜θ(tn) � ������ ≤ C26h2 (7.4) with C25 and C26 defined as follows: C25(ρ) := 2M1C7 ρ 1 − ρ, C26(ρ) := M1 |C19 + 2C8 − C13| ρ 1 − ρ + � M1C13 + |C19 + 2C8 − C13| C9 + (C19 + 2C8 − C13)2 4 � ρ(1 + ρ) (1 − ρ)2 + � C13C9 + C13 2 |C19 + 2C8 − C13| � ρ � 1 + 4ρ + ρ2� (1 − ρ)3 + C2 13 4 · ρ � 1 + 11ρ + 11ρ2 + ρ3� (1 − ρ)4 . 22Proof. Note that ����� � ∇jEk � ˜θ(tk) ��2 − � ∇jEk � ˜θ(tn) ��2����� ≤ ����∇jEk � ˜θ(tk) � − ∇jEk � ˜θ(tn) ����� · ����∇jEk � ˜θ(tk) � + ∇jEk � ˜θ(tn) ����� (a) ≤ C7(n − k)h · 2M1, where (a) is by (7.1). Using the triangle inequality, we can conclude ������ n � k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(tk) ��2 − R(n) j � ˜θ(tn) �2 ������ ≤ 2M1C7h(1 − ρ) n � k=0 (n − k)ρn−k = 2M1C7h(1 − ρ) n � k=0 kρk = 2M1C7 ρ 1 − ρh. (7.3) is proven. We continue by showing ����� � ∇jEk � ˜θ(tk) ��2 − � ∇jEk � ˜θ(tn) ��2 −2∇jEk � ˜θ(tn) � h p � i=1 ∇ijEk � ˜θ(tn) � n−1 � l=k ∇iEl � ˜θ(tn) � R(l) i � ˜θ(tn) � + ε ������� ≤ 2M1 � (n − k)(C19/2 + C8) + (n − k)(n − k − 1) 2 C13 � h2 + 2(n − k)C9 � (n − k)(C19/2 + C8) + (n − k)(n − k − 1) 2 C13 � h3 + � (n − k)(C19/2 + C8) + (n − k)(n − k − 1) 2 C13 �2 h4. (7.5) To prove this, use ���a2 − b2 − 2bKh ��� ≤ 2|b| · |a − b − Kh| + 2|K| · h · |a − b − Kh| + (a − b − Kh)2 with a := ∇jEk � ˜θ(tk) � , b := ∇jEk � ˜θ(tn) � , K := p � i=1 ∇ijEk � ˜θ(tn) � n−1 � l=k ∇iEl � ˜θ(tn) � R(l) i � ˜θ(tn) � + ε , and bounding |a − b − Kh| (a) ≤ � (n − k)(C19/2 + C8) + (n − k)(n − k − 1) 2 C13 � h2, |b| ≤ M1, |K| ≤ (n − k)C9, where (a) is by Lemma SA-7.3. (7.5) is proven. We turn to the proof of (7.4). By (7.5) and the triangle inequality ������ n � k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(tk) ��2 − R(n) j � ˜θ(tn) �2 − 2hP (n) j � ˜θ(tn) � ������ 23≤ (1 − ρ) n � k=0 ρn−k � Poly1(n − k)h2 + Poly2(n − k)h3 + Poly3(n − k)h4� = (1 − ρ) n � k=0 ρk � Poly1(k)h2 + Poly2(k)h3 + Poly3(k)h4� , where Poly1(k) := 2M1 � k(C19/2 + C8) + k(k − 1) 2 C13 � = M1C13k2 + M1(C19 + 2C8 − C13)k, Poly2(k) := 2kC9 � k(C19/2 + C8) + k(k − 1) 2 C13 � = C13C9k3 + (C19 + 2C8 − C13) C9k2, Poly3(k) := � k(C19/2 + C8) + k(k − 1) 2 C13 �2 = C2 13 4 k4 + C13 2 (C19 + 2C8 − C13) k3 + 1 4 (C19 + 2C8 − C13)2 k2. It is left to combine this with n � k=0 kρk ≤ ∞ � k=0 kρk = ρ (1 − ρ)2 , n � k=0 k2ρk ≤ ∞ � k=0 k2ρk = ρ(1 + ρ) (1 − ρ)3 , n � k=0 k3ρk ≤ ∞ � k=0 k3ρk = ρ � 1 + 4ρ + ρ2� (1 − ρ)4 , n � k=0 k4ρk ≤ ∞ � k=0 k4ρk = ρ � 1 + 11ρ + 11ρ2 + ρ3� (1 − ρ)5 . This gives ������ n � k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(tk) ��2 − R(n) j � ˜θ(tn) �2 − 2hP (n) j � ˜θ(tn) � ������ ≤ � M1C13 ρ(1 + ρ) (1 − ρ)2 + M1 |C19 + 2C8 − C13| ρ 1 − ρ � h2 + � C13C9 ρ � 1 + 4ρ + ρ2� (1 − ρ)3 + |C19 + 2C8 − C13| C9 ρ(1 + ρ) (1 − ρ)2 � h3 + � C2 13 4 · ρ � 1 + 11ρ + 11ρ2 + ρ3� (1 − ρ)4 + C13 2 |C19 + 2C8 − C13| ρ � 1 + 4ρ + ρ2� (1 − ρ)3 + 1 4 (C19 + 2C8 − C13)2 ρ(1 + ρ) (1 − ρ)2 � h4 (a) ≤ � M1 |C19 + 2C8 − C13| ρ 1 − ρ + � M1C13 + |C19 + 2C8 − C13| C9 + (C19 + 2C8 − C13)2 4 � ρ(1 + ρ) (1 − ρ)2 + � C13C9 + C13 2 |C19 + 2C8 − C13| � ρ � 1 + 4ρ + ρ2� (1 − ρ)3 +C2 13 4 · ρ � 1 + 11ρ + 11ρ2 + ρ3� (1 − ρ)4 � h2, where in (a) we used that h < 1. (7.4) is proven. 24Lemma SA-7.5. Suppose Assumption SA-2.2 holds. Then ��������    � � � � n � k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(tk) ��2 + ε    −1 − � R(n) j � ˜θ(tn) � + ε �−1 +h P (n) j � ˜θ(tn) � � R(n) j � ˜θ(tn) � + ε �2 R(n) j � ˜θ(tn) � ���������� ≤ C25(ρ)2 + R2C26(ρ) 2R3(R + ε)2 h2. Proof. Note that if a ≥ R2, b ≥ R2, we have �������� 1 √a + ε − 1 √ b + ε + a − b 2 �√ b + ε �2 √ b �������� = (a − b)2 2 √ b �√ b + ε � �√ a + ε � �√ a + √ b � � 1 √ b + ε + 1 √ a + √ b � � �� � ≤2/R ≤ (a − b)2 2R3(R + ε)2 . By the triangle inequality, �������� 1 √a + ε − 1 √ b + ε + c 2 �√ b + ε �2 √ b �������� ≤ (a − b)2 2R3(R + ε)2 + |a − b − c| 2 �√ b + ε �2 √ b ≤ (a − b)2 2R3(R + ε)2 + |a − b − c| 2R (R + ε)2 Apply this with a := n � k=0 ρn−k(1 − ρ) � ∇jEk � ˜θ(tk) ��2 , b := R(n) j � ˜θ(tn) �2 , c := 2hP (n) j � ˜θ(tn) � and use bounds |a − b| ≤ 2M1C7 ρ 1 − ρh, |a − b − c| ≤ C26(ρ)h2 by Lemma SA-7.4. SA-7.6. We are finally ready to prove Theorem SA-2.3. Proof of Theorem SA-2.3. By (6.9) and (6.10), the first derivative of the function t �→       ∇jEn � ˜θ(t) � � 2P (n) j � ˜θ(t) � + ¯P (n) j � ˜θ(t) �� 2 � R(n) j � ˜θ(t) � + ε �2 R(n) j � ˜θ(t) � − �p i=1 ∇ijEn � ˜θ(t) � ∇iEn(˜θ(t)) R(n) i (˜θ(t))+ε 2 � R(n) j (˜θ(t)) + ε �       25is bounded in absolute value by a positive constant C27 = C17 + C18. By (2.2), this means ������� ¨˜θj(t) + d dt    ∇jEn � ˜θ(t) � R(n) j � ˜θ(t) � + ε    ������� ≤ C27h. Combining this with ������ ˜θj(tn+1) − ˜θj(tn) − ˙˜θj � t+ n � h − ¨˜θj � t+ n � 2 h2 ������ ≤ D3 6 by Taylor expansion, we get �������� ˜θj(tn+1) − ˜θj(tn) − ˙˜θj � t+ n � h + h2 2 · d dt    ∇jEn � ˜θ(t) � R(n) j � ˜θ(t) � + ε    ������� t=t+ n �������� ≤ �D3 6 + C27 2 � h3. (7.6) Using ������� ˙˜θj(tn) + ∇jEn � ˜θ(tn) � R(n) j � ˜θ(tn) � + ε ������� ≤ C28h with C28 defined as C28 := M1 (2C5 + C6) 2(R + ε)2R + pM1M2 2(R + ε)2 by (2.2), and calculating the derivative, it is easy to show �������� d dt    ∇jEn � ˜θ(t) � R(n) j � ˜θ(t) � + ε    ������� t=t+ n − FrDer �������� ≤ C29h (7.7) for a positive constant C29, where FrDer := FrDerNum � R(n) j � ˜θ(tn) � + ε �2 R(n) j � ˜θ(tn) � FrDerNum := ∇jEn � ˜θ(tn) � ¯P (n) j � ˜θ(tn) � − � R(n) j � ˜θ(tn) � + ε � R(n) j � ˜θ(tn) � p � i=1 ∇ijEn � ˜θ(tn) � ∇iEn � ˜θ(tn) � R(n) i � ˜θ(tn) � + ε , C29 := � pM2 R + ε + M 2 1 M2p (R + ε)2R � C28. From (7.6) and (7.7), by the triangle inequality ����� ˜θj(tn+1) − ˜θj(tn) − ˙˜θj � t+ n � h + h2 2 FrDer ����� ≤ �D3 6 + C27 + C29 2 � h3, which, using (2.2), is rewritten as ���������� ˜θj(tn+1) − ˜θj(tn) + h ∇jEn � ˜θ(tn) � R(n) j � ˜θ(tn) � + ε − h2 ∇jEn � ˜θ(tn) � P (n) j � ˜θ(tn) � � R(n) j � ˜θ(tn) � + ε �2 R(n) j � ˜θ(tn) � ���������� 26≤ �D3 6 + C27 + C29 2 � h3. It is left to combine this with Lemma SA-7.5, giving the assertion of the theorem with C1 = D3 6 + C27 + C29 2 + M1 C2 25 + R2C26 2R3(R + ε)2 . 8 Numerical experiments SA-8.1 Models. We use small modifications of default Keras Resnet-50 and Resnet-101 architectures1 for training on CIFAR-10 and CIFAR-100 (since image sizes are not the same as Imagenet), after verifying their correctness. The first convolution layer conv1 has 3 × 3 kernel, stride 1 and “same” padding. Then comes batch normalization, and relu. Max pooling is removed, and otherwise conv2_x to conv5_x are as described in [2], see Table 1 there (downsampling is performed by the first convolution of each bottleneck block, same as in this original paper, not the middle one as in version 1.52; all convolution layers have learned biases). After conv5 there is global average pooling, 10 or 100-way fully connected layer (for CIFAR-10 and CIFAR-100 respectively), and softmax. SA-8.2 Data augmentation. We subtract the per-pixel mean and divide by standard deviation, and we use the data augmentation scheme from [3], following [2], section 4.2. We take inspiration and some code snippets from [4] (though we do not use their models). During each pass over the training dataset, each 32 × 32 initial image is padded evenly with zeros so that it becomes 36 × 36, then random crop is applied so that the picture becomes 32 × 32 again, and finally random (probability 0.5) horizontal (left to right) flip is used. References [1] Avrajit Ghosh, He Lyu, Xitong Zhang, and Rongrong Wang. “Implicit regularization in Heavy-ball momentum accelerated stochastic gradient descent”. In: The Eleventh International Conference on Learning Representations. 2023. url: https://openreview.net/forum?id=ZzdBhtEH9yB. [2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. “Deep residual learning for image recognition”. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2016, pp. 770–778. [3] Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu. “Deeply-supervised nets”. In: Artificial intelligence and statistics. Pmlr. 2015, pp. 562–570. [4] Chia-Hung Yuan. Training CIFAR-10 with TensorFlow2(TF2). https : / / github . com / lionelmessi6410/tensorflow2-cifar. 2021. 1https://github.com/keras-team/keras/blob/v2.13.1/keras/applications/resnet.py 2https://catalog.ngc.nvidia.com/orgs/nvidia/resources/resnet_50_v1_5_for_pytorch 27