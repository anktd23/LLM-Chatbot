Branches of a Tree: Taking Derivatives of Programs with Discrete and Branching Randomness in High Energy Physics Michael Kagan1, ∗ and Lukas Heinrich2, ∗ 1SLAC National Accelerator Laboratory 2Technical University of Munich We propose to apply several gradient estimation techniques to enable the differentiation of programs with discrete randomness in High Energy Physics. Such programs are common in High Energy Physics due to the presence of branching processes and clustering-based analysis. Thus differentiating such programs can open the way for gradient based optimization in the context of detector design optimization, simulator tuning, or data analysis and reconstruction optimization. We discuss several possible gradient estimation strategies, including the recent Stochastic AD method, and compare them in simplified detector design experiments. In doing so we develop, to the best of our knowledge, the first fully differentiable branching program. I. INTRODUCTION Gradient-based optimization methods are at the core of many modern successes in Machine Learning (ML) and Artificial Intelligence (AI), especially Deep Learning. The development and application of these ML methods in High Energy Physics (HEP) has similarly enabled large performance improvements in a wide array of tasks, such as particle reconstruction, fast simulation, and parame- ter inference (for recent topical reviews, see e.g. [1–8]). Gradient-based optimization methods rely on automatic differentiation (AD) [9, 10], an algorithmic way to effi- ciently compute the derivatives of numerical software. AD is a general tool that can be applied to scientific software beyond ML, such as simulators and inference algorithms, and used for optimizing the parameters of this software. The broader application of AD to numerical software, potentially mixed with ML components, is often referred to as Differentiable Programming (DP). For instance, combining AD-enabled HEP software with ML can lead to optimizable hybrid physics-AI models with built-in physics knowledge from the HEP software, such as AI- augmented / AI-guided simulation and reconstruction, or analysis-by-synthesis inference methods with simula- tors in the loop [11–13]. Using such AD-integrated HEP software in ML models can be considered a complimen- tary approach to adding inductive bias into ML models through structure and architecture, such as symmetry equivariance and relational inductive bias. While interest is quickly growing to apply gradient- based optimization methods to a broader set challenges in HEP, such as detector design or end-to-end reconstruction, a major limitation has been the fact that standard AD can only compute derivatives of deterministic continuous functions or stochastic functions with reparametrized con- tinuous random variables [14–16]. Specifically, in HEP, many programs are both stochastic and rely on sampling discrete random variables or decisions, such as branching ∗ Corresponding authors contributed equally to this work: makagan@slac.stanford.edu,l.heinrich@tum.de points in parton showers, particle-material interactions, or clustering steps in jet building. Standard AD may not compute the desired derivative of such programs correctly, particularly when the discrete stochasticity depends on the parameter one aims to optimize (and thus differentiate with respect to). Instead, more careful consideration on how to compute the appropriate derivative is required. There are several methods for gradient-based optimiza- tion in programs with discrete randomness, which we ex- plore within the context of HEP applications. One method uses the score-based approach to estimating derivatives of expectations values [17], which has been examined sparsely within HEP and not for tasks such as detector design optimization. Recently, Arya et al. [18] proposed a new AD method for handling and composing programs with discrete randomness. Using these tools, we develop simplified differentiable HEP simulators,which nonetheless exhibit the critical behaviors which until now hampered progress, and case studies to examine the behavior of and the variance of these gradient estimators. A review of methods for computing derivatives of stochastic programs is found in Sec. II. Related work is discussed in Sec. III. Sec. IV presents applications and comparisons of different gradient estimators in HEP case studies, with an emphasis on detector design optimization. Contributions1: We introduce several methods to en- able differentiation through the discrete randomness to HEP programs. We show how the score function based approach can be used for design optimization in HEP. While the score function is often used outside of HEP for design optimization, and has been used for other tasks within HEP, it has not yet been explored within the context of HEP detector design optimization. We also introduce stochastic AD [18] to HEP and its ability to enable differentiable programming even in programs with discrete randomness. We provide the first application of stochastic AD to branching processes and in doing 1 Project code can be found at https://github.com/lukasheinrich/branches_of_a_tree/ arXiv:2308.16680v1 [stat.ML] 31 Aug 20232 1 def f(θ): 2 p = 0.5 3 b ∼ Bernoulli(p) 4 return g(θ) + b 1 def f(θ): 2 p = h(θ) 3 b ∼ Bernoulli(p) 4 return g(θ) + b FIG. 1: Assuming differentiable g : R → R and h : R → (0, 1), Left: Toy program without θ-dependence in the discrete stochastic behavior. As such, the derivative of the expected value of this program is dE[f(θ)] dθ = dg(θ) dθ and standard AD can correctly estimate this. Right: Toy program with θ-dependence in the discrete stochastic behavior through the Bernoulli parameter p. Standard AD would ignore this dependence, and the resulting derivative estimator would be the same as the program on the left. However the correct derivative should be dE[f(θ)] dθ = dg(θ) dθ + dh(θ) dθ . so we develop the first (to the best of our knowledge) differentiable branching program. To test these methods, we provide comparisons of gradient methods on detector design optimization case studies. II. REVIEW OF DIFFERENTIATION OF STOCHASTIC PROGRAMS In many HEP applications, a quantity of interest can be formulated as an expectated value of a function f(x, θ) over a parametrized density pθ(x): ¯f(θ) = Epθ(x)[f(x, θ)]. For optimizing such quantities one requires the gradients of these expectation values of stochastic programs, e.g. d dθEpθ(x)[f(x, θ)]. Importantly, the expected value of such stochastic programs may be continuous and differentiable, even when they depend on discrete randomness. For in- stance, the expected value of a Bernoulli random variable b ∼Bernoulli(θ) has the derivative d dθE[b] = d dθθ = 1. However, standard AD tools applied to such expecta- tions may not produce the desired result. For instance, Figure 1 shows two programs with discrete stochasticity. On the left, the discrete stochasticity does not depend on the parameter of differentiation θ, and standard AD will produce the correct derivative. On the right the discrete stochasticity depends on θ, standard AD will ignore this dependence and the resulting derivative will be incorrect as it will ignore this dependence. Handling these challenges requires more dedicated consideration on home to compute the appropriate derivative. We briefly review several approaches to gradient estimation below (see Ref. [19] for a detail review). Finite Differences (Numerical Differentiation): Fi- nite difference methods estimate derivatives by computing the difference between forward evaluations of a program and a perturbed version of the program, for instance: d dθEpθ(x)[f(x)] ≈ Epθ+ϵ(x)[f(x)] − Epθ(x)[f(x)] ϵ (1) Finite difference methods are prone to high vari- ance [10], and require large numbers of program eval- uations when θ is high dimensional. Central difference methods can reduce error. One large contributor to this large variance is that multiple independent evaluations of the program are used to estimate this gradient, introduc- ing separate stochastic evaluation paths of the program. Reparameterization Trick: In many cases, when sam- pling x ∼ pθ(x), we can smoothly reparametrize this sampling as z ∼ p(z) and x = g(z, θ), where p(z) is often a simple “base" distribution and g(·, θ) provides a differ- entiable, θ-dependent transformation of samples from the base to the desired distribution. For example, the normal distribution x ∼ N(µ, σ) is location-scale reparameteri- zable through z ∼ N(0, 1) and x = σz + µ ∼ N(µ, σ). This is particularly convenient for computing derivatives of expectation values of differentiable functions f(·), d dθEpθ(x)[f(x)] = d dθ � p(z)f(g(z, θ))dz = � p(z) df dg dg(z, θ) dθ dz (2) Many HEP simulators, which implement a structural causal model, can be considered as a reparametrization, mapping from noise variables to random variables x with physical meaning. However if the random variables x are discrete, the map is not differentiable, which limits the applicability of the reparametrization trick within a HEP context. Surrogate Methods: When a reparameterization is not possible, either because f(·) is non differentiable or pθ(x) does not admit a smooth reparameterization, surrogate methods can be used. In this case, an ML model S(z, θ), with z ∼ p(z) a chosen distribution, is trained to mimic the stochastic program. As such, surrogate methods try enable reparameterization though a ML model and thus enable differentiation, for instance: d dθEpθ(x)[f(x)] ≈ d dθ � p(z)S(z, θ)dz = � p(z)dS(z, θ) dθ dz (3) The quality of this derivative estimator will depend significantly on the quality of the surrogate model as an approximation of the original program. Moreover, the surrogate will learn a smooth approximation of non- differentiable elements of the program, but how this ap- proximation is performed and if bias is introduced is difficult to asses. Score function: When the parameter dependence of a differentiable distribution pθ(x) is known and differen-3 tiable with respect to the parameters, one can compute: d dθEpθ(x)[f(x)] = � pθ(x)d log pθ(x) dθ f(x)dx = Epθ(x) �d log pθ(x) dθ f(x) � (4) where d dθ log pθ(x) is known as the score function. This gradient estimator is also known as REINFORCE [17], and is commonly used in reinforcement learning. The benefit of this approach is that the f(x) does not need to be differentiable, only pθ(x) must be differentiable with respect to θ. As such, discrete random variables can be used in the computation of f(x). Control Variates: Score function based gradient esti- mates often have a large variance, which can make tasks like optimization with gradient descent slow and more difficult. A control variate, c(x, θ), can be subtracted from f(x) in Eqn. 4 to reduce the variance of the es- timator as long as it does not bias the estimator, i.e. as long as Epθ(x) � d log pθ(x) dθ c(x, θ) � = 0. Noting that Epθ(x) � d log pθ(x) dθ � = 0, one way to find a control vari- ate is to choose a c(θ) which does not depend on x. A common control variate, often also called a baseline, is the function mean ¯fθ = � pθ(x)f(x); in practice ¯fθ is often estimated using the mean of a mini-batch. We will use this baseline for the experiments in Sec. IV. More details on variance reduction methods for score based gradient estimation can be found in Ref. [20]. Proposal Distributions: In some case, we may not know or have access to pθ(x), for example when g(θ) is a simulator with parameters as input and sampling is done internally within the program. One approach can be to imagine the input to the program as a sample from a proposal distribution θ ∼ πψ(θ), where ψ are the parameters of the proposal distribution. For instance one could choose a normal distribution N(ψ, 1) for the proposal. One would then aim to optimize the mean of this proposal, d dψ Eπψ(θ)[g(θ)] = Eπψ �d log πψ(θ) dψ g(θ) � (5) Stochastic AD: The stochastic derivative of the expected value of a function f(·) of a discrete random variable x ∼ pθ(x) has the form [18]: d dθEpθ(x)[f(x)] = Epθ(x,y)[δ + β � f(y) − f(x) � ] (6) where δ is the standard derivative ∂f/∂θ as computed with AD, and the second term corresponds to the effect of a change in θ on the sampling of the discrete random variable x. The weight β depends on the underlying sampling distribution and y is an alternative value of the discrete random variable. Conceptually, for discrete X with consecutive integer range one can understand this result through the lens of reparameterization. One can reparamaterize the discrete distribution via the inversion method, e.g.: ω ∼ U[0, 1] x = {X : ω ∈ [CDFpθ(X), CDFpθ(X + 1) ) } For example, if x is a Bernoulli random variable with pa- rameter θ, then one can reparameterize x = H(ω > 1 − θ) where H(·) is the Heaviside step function. The boundaries which define the set of values of ω that result in a value of X are now dependent on the parameters θ. A change in parameters changes the boundaries, and thus changes the probabilities of different outcomes x. The second term of the stochastic AD derivative accounts for the infinitesimal change in probabilities as the boundaries are changed as well as the alternative value of the program y that would result from such a change. Importantly, one can define this derivative at each program evaluation and at each stochastic sampling within the program, allowing for the development of composition rules and of forward mode automatic differentiation. For a more detailed discussion of Stochastic AD, see Ref. [18]. The expectation on the right hand side of Equation 6 is taken with respect to the joint distribution pθ(x, y), which is often denoted the coupling. The marginal distributions of this coupling must be the same as the original sampling distribution, e.g. � dypθ(x, y) = � dxpθ(x, y) = pθ(x) to ensure both the primal evaluation of the program and the alternative proceed under the normal operation of the program. As such, there is a distribution over possible alternative programs. In practice, out of all possible alternatives from all of the discrete samplings within a program, only one alternative is randomly chosen. This pruning process treats the set of alternatives as a categorical distribution with the probability of a given alternative being the weight of the alternative relative to the total event weight (computed using the composition rules). This alternative may occur in the middle of the program, and is then tracked in parallel to the primal until completion of the alternative program. One can then use this program alternative y for computing even gradients using Equation 6. Variance Reduction with Random Number Reuse: While the marginals of the coupling are fixed, one has consider- able flexibility to choose the correlation structure between x and y. This is important because once an alternative is determined from a discrete sampling within the program, the alternative program will be run to completion and thus may require additional sampling of discrete random variables. The more correlated are the evaluations of the alternative completion to that of the primal evaluation, the lower the variance of the gradient estimator may be. As downstream discrete samplings also occur in the pri- mal program, one can reuse the reparameterized sampling in the primal program, i.e. the ω values sampled for the inversion method. By reusing ω values, less additional4 variance is added to the alternative program then if the downstream discrete random variable were sampled inde- pendently. In this work for the experiments in Sec. IV, we use a first-in first-out (FIFO) approach, where at each time step we store ω values in the FIFO while iterating over branches (particle) in the primal, and then pull ω values from the FIFO while iterating over branches in the alternative. If additional ω values are needed by the alternative, they are sampled independently of the primal. III. RELATED WORK Automatic differentiation [9, 10], and its use in gradi- ent based optimization, is ubiquitous in ML, statistics, and applied math. AD uses the chain rule to evaluate derivatives of a function that is represented as a computer program. AD takes as input program code and produces new code for evaluating the program and derivatives. AD typically builds a computational graph, or a directed acyclic graph of mathematical operations applied to an input. Gradients are defined for each operation of the graph, and the total gradient can be evaluated from input to output, called forward mode, and from output to input, called reverse mode or backpropagation in ML. AD is backbone of ML / DP frameworks like TensorFlow [21], JAX [22], and PyTorch [23]. Significant work has been performed on developing Monte Carlo estimators for gradients in machine learning, as discussed in the recent review [19], and gradients of stochastic computation graphs [24]. More recently meth- ods such as Stochastic AD [18, 25] have been developed to target derivatives of programs with discrete stochastic behavior in a compositional way, as well as developing specific applications with dedicated variance reduction methods (e.g. developing coupling for these applications). Extensions to AD have recently been proposed for differ- entiating the expectations of stochastic programs [26, 27], and to account for parametric discontinuities [28]. Differentiable programming approaches have begun to be explored in HEP. Examples include histogram fitting in pyHF [29, 30], in analysis optimization in Neos [12, 31], for modeling parton distribution functions (pdf) used by matrix element generators [32, 33], and for developing dif- ferentiable matrix element simulations with MadJax [11]. Related to our work, Ref. [34] studies differentiating a parton shower, which focuses on the derivative of the primal shower program but does not examine differen- tiation through discrete randomness in such branching programs. Within cosmology, the Differentiable Universe Initiative is developing a differentiable simulation and analysis toolchain for comological data analyses [35]. Within HEP, score function gradient estimators have been used within the context of jet grooming [36] and hierarchical jet clustering [37]. To the best of our knowl- edge, this work is the first application to detector design optimization in HEP. On HEP detector design, surrogate based optimization methods have been developed and explored for particle detectors in Ref. [38]. Surrogate methods have also been applied to neutrino detector design [39]. Detector design optimization with standard AD tools and with surrogate methods is discussed in Ref. [40]. A branch-and-bound type algorithm is explored in Ref. [41]. IV. APPLICATIONS We present a series of applications in a simplified sim- ulation of particles interacting with detector material. In these experiments, we examine how different gradient estimation methods can be applied and compare their performance in terms of the variance of the estimators. A. Particle Interaction Simulator The simplified simulator in this work aims to capture the salient features of particle physics simulators that model the traversal of particles through matter but be simple enough allow reimplementation in a programming language of choice for a detailed study of various gradient estimation in a self-contained setting. Algorithm1 Simplified Particle Interaction Simulator Require: E0 : energy threshold Require: ϵ : energy loss at interaction Require: m(x, θ) : R3 × Rn → [0, 1] : material map Require: P = {pi = (⃗xi⃗pi, Ei)}: initial particle list Require: H = ∅: list of hits 1: function Simulate(P,θ) 2: while not all p ∈ P below threshold do 3: P ← ∅ ▷ list of surviving particles 4: for for all particles in P do 5: if Ei < E0 then 6: continue ▷ particle below thr. 7: end if 8: pi = propagate(pi) 9: ρE ← sample Ber(ρE|mθ(xi)) 10: if ρE then ▷ particle interacts 11: H ← H ∪ {xi} ▷ add position to hits 12: ρsplit ← sample Ber(ρsplit|mθ(xi)) 13: if ρsplit then ▷ particle splits 14: pL, pR ← split(pi) 15: P ← P ∪ {pR, pL} 16: else 17: Ei ← Ei − ϵ ▷ lose energy 18: P ← P ∪ {pi} 19: end if 20: else 21: P ← P ∪ {pi} 22: end if 23: end for 24: end while 25: return H 26: end function The simulator Sθ(p) models the stochastic evolution of5 (a) Single particle energy loss (b) Particle shower with splitting FIG. 2: Event Displays of a single particle energy loss (left) and a particle shower with particle splitting (right). A primal event is shown in purple, while an alternative event, as determined using Stochastic AD, is shown in yellow. The material map is shown in grey. particles through two main processes. A binary splitting process p0 → pL, pR that splits a parent particle momen- tum evenly across two child particles and a energy loss process E → E − ϵ. The probability of a particle interac- tion is modeled as a function of the material map m(x). The simulation is performed by fixed time steps, first propagating particles in their direction of travel, and then querying the material map to determine if an interaction occurs, and if so which type of interaction (i.e. splitting or energy loss). Pseudo-code for the simulator can be found in Alg. 1. The detector is simulated as a continuous material map mθ(x0, x1) which takes as input a position (x0, x1) and outputs an interaction probability. This interaction probability is dependent on detector parameters θ, and we will examine examples where derivatives with respect to θ are sought. Only a single detector parameter is used in the following experiments, which is the detector inner radius which will be denoted θR. With r = � x2 0 + x2 1, and ϕ = arctan x0 x1 , the material map is defined as: mθ(x, y) = 1 2mstart(r, θR) mΦ(r, ϕ) mR(r) mend(r, θR) (7) where mstart(r, θR) = 1 1 + e−β(r−θR) mΦ(r, ϕ) = 1 1 + eβ sin(ω(ϕ+2r)) mR(r) = 1 1 + eβ cos(ω(r−2)) mend(r, θR) = 1 1 + eβ(r−θR−Rmax) . The terms mstart(r, θR) and mend(r, θR) determine the inner and outer radius of the detector, respectively. The terms mΦ(r, ϕ) and mR(r) determine the segmentation in ϕ and r, respectively. The constants β, ω, and Rmax control the sharpness of the smooth material map, the segmentation frequency in the azimuthal direction, and the maximum depth of the detector, respectively. The parameter that will be optimized is θR, the inner radius of the detector. Example material maps can be seen in grey in the event displays of Figure 2, where the darkness of the shade of grey indicates the strength of interaction. B. Single Particle Energy Loss In the single particle energy loss setting, interactions which cause splitting are turned off, i.e. there are no show- ers. At each step, the particle interacts with the detector with a probability pEloss = m(x, θ) which is dependent on material map parameters θ. This probability is large is high density regions of detector material and small in low density regions. A Bernoulli distribution with parameter m(x, θ) is sampled at each time step to determine if the interaction occurs and, if so, the particle deterministically loses energy ϵ = 1 GeV. All particles are set to have initial energy of 25 GeV and when the particle energy falls below E0 = 0.5 GeV, the particle is stopped. An example event display can be seen in Fig. 2a, where the primal particle trajectory is seen in purple, the alternative trajectory determined with Stochastic AD is seen in yellow, and the material map is seen in grey. In this example, there is only one detector parameter6 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 parameter 0 2 4 6 8 10 Loss per event primal primal median poly. fit 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 parameter 15 10 5 0 5 10 15 20 Grad Numeric StochAD SCORB SCORE grad from fit StochAD Score w/ Baseline Score 15 10 5 0 5 10 15 g [X( )] Score Numeric 50 0 50 FIG. 3: For simulations of single particle energy loss: (Left) The loss function and various gradient estimators of the loss are shown as a function of the detector radius parameter. Sample primal evaluations of the loss are shown as markers, and the interquantile interval is shown in black. The red dashed line shows the derivative of an polynomial interpolation of the mean loss. (Middle) The mean and standard deviation of the numeric, Stochastic AD (STAD), score function (SCORE), and score function with baseline (SCORB) gradient estimators as a function of the detector radius parameter. The gradient of the polynomial interpolation of the mean loss is shown in dashed black. (Right) Box plot of the four variance estimators evaluated at parameter value θR = 2.5m, with the mean shown as a dashed line. θ ≡ R, the inner radius of the detector, and derivatives are computed with respect to R using several methods of estimating gradients. The mean squared error between the radial position of points of particle interaction and a target radius ¯RT is used as a loss function that one may minimize for the purposes of design optimization. In this example we set ¯RT = 2m. The loss as a function of the detector radius parameter can be seen on the left in Fig. 3. The loss from individual primal simulation samples can be seen in grey, the median loss and interquartile range in black, and a polynomial interpolation of the average loss. Even though the simu- lation is stochastic, and one can see the variations of the loss at each parameter in the grey points, there is a clear minimum to the loss function. The gradient estimators and their standard deviations, calculated over the 5000 simulation runs, can be seen as a function of the detector radius parameter in the middle in Fig. 3. The distributions of gradient estimators evaluated at the parameter value θR = 2.5m can be seen in the box plot on the right in Fig. 3. As expected, numerical derivatives have the largest standard deviation, though it should be noted that this can depend highly on the size of the finite different ϵ and the method for calculating the numerical derivative. Similarly, score function gradi- ent estimators without baseline shows a high standard deviation, especially at large radius parameter where the loss function has larger standard deviation over differ- ent simulation samples. The score with baseline has a much reduced standard deviation over the score function without baseline across all parameter values. Stochastic AD shows the smallest standard deviation of all estima- tors, likely owing to the ability to couple much of the alternative program evaluation to that of the primal up to the alternative branching point. Across all gradient estimators, the mean of the gradient estimator, shown in solid lines, are close to the gradient of the polynomial fit of the loss (which serves as a rough guide to the gradient of the expected loss) within one standard deviation. A numerical comparison of gradient estimator mean and variance can be found in Tab. I. C. Branching Shower In the branching shower example, the same material map, and thus interaction probability, as the single par- ticle energy loss simulation is used but if an interaction occurs, the particle is deterministically split into two daughter particles each with half the energy of the parent particle and with an opening angle of 0.1 radians. Initial particles are set to have starting energy of 25 GeV and when any particle energy falls below 0.5 GeV, the parti- cle is stopped. The same loss function as in the single particle energy loss example is used here. An example event display can be seen in Fig. 2b, where the primal particle shower is seen in purple, the alternative shower determined with Stochastic AD is seen in yellow, and the material map is seen in grey. The loss function and the standard deviation of the gradients, as functions of the detector radius parameter, can be seen on the left and middle, respectively, in Fig. 4. As in the single particle energy loss example, the nu- merical gradients are found to have the largest standard deviation of gradients, with the score function without baseline estimator having the second largest standard deviation. Notably, the score function with baseline esti-7 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 parameter 0 2 4 6 8 10 Loss per event primal primal median poly. fit 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 parameter 15 10 5 0 5 10 15 20 Grad Numeric StochAD SCORB SCORE grad from fit StochAD Score w/ Baseline Score 15 10 5 0 5 10 15 g [X( )] Score Numeric 50 0 50 FIG. 4: For simulations of particle showers with splitting: (Left) The loss function and various gradient estimators of the loss are shown as a function of the detector radius parameter. Sample primal evaluations of the loss are shown as markers, and the interquantile interval is shown in black. The red dashed line shows the derivative of an polynomial interpolation of the mean loss. (Middle) The mean and standard deviation of the numeric, Stochastic AD (STAD), score function (SCORE), and score function with baseline (SCORB) gradient estimators as a function of the detector radius parameter. The gradient of the polynomial interpolation of the mean loss is shown in dashed black. (Right) Box plot of the four variance estimators evaluated at parameter value θR = 2.5m, with the mean shown as a dashed line. mator is found to have the smallest standard deviation, slightly smaller than the Stochastic AD gradient esti- mator. Unlike the single particle example, the splitting shower has many program branching points which can create alternative outputs that are significantly different from the primal shower. In turn, this leads to a reduc- tion in the correlation between the primal and alternative showers and ultimately to an increase in the gradient estimator standard deviation. A comparison of the distri- bution of gradient estimators, at detector parameter value θR = 2.5m, can be seen on the right in Fig. 4. While the mean values (dotted lines) in each box agree well across estimators, the variance estimates as well as the tails are significantly better behaved for Stochastic AD and score function with baseline estimators. Similarly, a compari- son of the mean and standard deviation of the gradient estimators evaluated at the parameter value θR = 2.5m for both the single particle energy loss and the splitting shower can be found in Tab. I. Estimator E-loss Shower StochAD 3.17 ± 4.47 2.53 ± 6.37 Score w/ Baseline 3.01 ± 6.59 2.47 ± 4.42 Score w/o Baseline 2.68 ± 17.18 2.76 ± 12.20 Numerical 3.83 ± 139.96 2.43 ± 74.85 TABLE I: Gradient estimator mean and standard deviation, for both the single particle energy loss and splitting shower, evaluated at parameter value θR = 2.5m and determined from 5,000 samples. The estimator with lowest standard deviation is shown in bold. It should be noted that there is considerable flexibility in Stochastic AD for how to couple the randomness in the primal and alternative programs after the point at which the alternative is produced, i.e. how to choose the join distribution over random variables in the primal and alternative programs. This selection of coupling can have a considerable impact on the Stochastic AD gradient estimator variance. In this work, we have used a simple approach of re-using random variables sampled in the primal for the alternative, without regard for where those random variables are re-used in the alternative. We have seen that this re-use can have a large impact; we observed that removing the re-use of random variables in the alternative can increase the Stochastic AD gradient estimator standard deviation by factors of 1.5 or more. More generally, a more careful strategy of re-using of random variables may considerably reduce the Stochastic AD gradient estimator variance. D. Design Optimization with Splitting Shower We test the ability to use the various gradient estima- tors to perform a gradient based optimization using of the detector radius parameter using the aforementioned loss with a target radial shower depth of ¯RT = 2. Each epoch consists of a single step of the optimization, with a mini-batch size of only 2 simulation runs used to esti- mate gradients in each epoch. The Adam optimizer [42] is used. A learning rate of 0.01 is used for the gradient descent parameter update. For all optimizations, the ini- tial detector radius parameter value is set to θinit = 3m and the optimization is run for 500 gradient steps. Each gradient method is used in 10 separate optimizations,8 and the average and standard deviation of the loss at each optimization step is shown in Fig. 5. As expected, the score function estimator without baseline and the numeric gradients shows the largest standard deviation to the extent that optimization is not feasible in this setting. We also see that the numeric and score function without baseline estimators are significantly slower at optimiz- ing the objective. The score with baseline estimator and Stochastic AD estimators show similar standard deviation and similar progress towards the loss minimum as a func- tion of optimization step. This suggests that Stochastic AD and score function with baseline estimators provide significantly better gradient estimates, even with very small sample sizes, and are likely interesting estimators for further study of detector design optimizations in more complex settings. 0 100 200 300 400 500 Steps 0 1 2 3 4 Loss Design Optimization numeric SCORE SCORB STAD FIG. 5: The mean and inter-quantile range of the loss versus epoch of detector design optimization is shown. Mean and quantiles are computed from 10 optimizations. V. CONCLUSION In this work, we discuss several strategies for differ- entiating stochastic programs, with a focus on methods capable of differentiating programs with discrete random- ness, and discuss their application to High Energy Physics detector design optimization. We develop the first appli- cation of Stochastic AD to branching processes and, more generally, the first differentiable branching program capa- ble of estimating gradients through the discrete processes within a particle shower. We also introduce score func- tion gradient estimators within this HEP detector design context. We find that Stochastic AD and score function gradient estimators, using control variates, provide the best gradient estimators in terms of smallest standard deviation among the gradient estimators examined within a case study of detector design. We show that both tech- niques can successfully be used for gradient-based HEP detector design on a toy detector simulator. More broadly, we believe that the careful study and application of techniques like Stochastic AD and score function estimation can open the way to a wide array of new differentiable programming applications in HEP and other sciences. ACKNOWLEDGEMENTS We thank Gaurav Arya, Frank Schäfer, and Moritz Schauer for the helpful discussions regarding Stochastic AD, and thank Gaurav Arya for the helpful feedback on the manuscript. We thank Michael Brenner for the helpful discussions regarding score function gradient estimators at the Aspen Center for Physics, as this work was par- tially performed at the Aspen Center for Physics, which is supported by National Science Foundation grant PHY- 2210452. We also thank the Munich Institute for Astro-, Particle and BioPhysics (MIAPbP) which is funded by the Deutsche Forschungsgemeinschaft (DFG, German Re- search Foundation) under Germany´s Excellence Strategy – EXC-2094 – 390783311, as this work was partially per- formed at the MIAPbP workshop on Differentiable and Probabilistic Programming for Fundamental Physics. MK is supported by the US Department of Energy (DOE) under grant DE-AC02-76SF00515. LH is sup- ported by the Excellence Cluster ORIGINS, which is funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excel- lence Strategy - EXC-2094-390783311. [1] A. Radovic et al., “Machine learning at the energy and intensity frontiers of particle physics,” Nature 560, 41–48 (2018). [2] K. Cranmer, U. Seljak, K. Terao, and the Particle Data Group, “Review of Particle Physics,” Progress of Theo- retical and Experimental Physics 2022, 083C01 (2022), chapter 41. [3] P. Calafiura, D. Rousseau, and K. Terao, Artificial Intel- ligence for High Energy Physics (WORLD SCIENTIFIC, 2022).9 [4] K. Cranmer, J. Brehmer, and G. Louppe, “The frontier of simulation-based inference,” Proceedings of the National Academy of Sciences 117, 30055–30062 (2020). [5] J. Shlomi, P. Battaglia, and J.-R. Vlimant, “Graph neural networks in particle physics,” Machine Learning: Science and Technology 2, 021001 (2020). [6] S. Thais et al., “Graph neural networks in particle physics: Implementations, innovations, and challenges,” (2022), arXiv:2203.12852. [7] A. Butter et al., “Machine learning and LHC event gener- ation,” SciPost Phys. 14, 079 (2023). [8] A. Adelmann et al., “New directions for surrogate models and differentiable programming for high energy physics detector simulation,” (2022), arXiv:2203.08806. [9] H. M. Bücker, G. F. Corliss, P. D. Hovland, U. Naumann, and B. Norris, eds., Automatic Differentiation: Applica- tions, Theory, and Implementations, Lecture Notes in Computational Science and Engineering (Springer, New York, NY, 2005). [10] A. G. Baydin, B. A. Pearlmutter, A. A. Radul, and J. M. Siskind, “Automatic differentiation in machine learning: a survey,” Journal of Machine Learning Research 18, 1–43 (2018). [11] L. Heinrich and M. Kagan, “Differentiable Matrix Ele- ments with MadJax,” J. Phys. Conf. Ser. 2438, 012137 (2023), arXiv:2203.00057. [12] N. Simpson and L. Heinrich, “neos: End-to-End- Optimised Summary Statistics for High Energy Physics,” J. Phys. Conf. Ser. 2438, 012105 (2023), arXiv:2203.05570. [13] S. Cheong et al., “Novel light field imaging device with enhanced light collection for cold atom clouds,” Journal of Instrumentation 17, P08021 (2022). [14] P. Glasserman, Monte Carlo Methods in Financial Engi- neering (Springer, 2013). [15] D. P. Kingma and M. Welling, “Auto-encoding vari- ational bayes,” Proceedings of the International Con- ference on Learning Representations, ICLR’14 (2014), arXiv:1312.6114. [16] D. Rezende, S. Mohamed, and D. Wierstra, “Stochas- tic backpropagation and approximate inference in deep generative models,” in Proceedings of the 31st Inter- national Conference on International Conference on Machine Learning - Volume 32, ICML’14 (2014) p. II–1278–II–1286. [17] R. Williams, “Simple statistical gradient-following algo- rithms for connectionist reinforcement learning,” Machine Learning 8, 229–256 (1992). [18] G. Arya, M. Schauer, F. Schäfer, and C. Rackauckas, “Automatic Differentiation of Programs with Discrete Ran- domness,” in Advances in Neural Information Processing Systems, Vol. 35 (Curran Associates, Inc., 2022). [19] S. Mohamed, M. Rosca, M. Figurnov, and A. Mnih, “Monte carlo gradient estimation in machine learning,” (2020), arXiv:1906.10652. [20] E. Greensmith, P. L. Bartlett, and J. Baxter, “Variance reduction techniques for gradient estimates in reinforce- ment learning,” Journal of Machine Learning Research 5, 1471–1530 (2004). [21] Martín Abadi et al., “TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems,” (2015), software available from tensorflow.org. [22] J. Bradbury et al., “JAX: composable transformations of Python+NumPy programs,” (2018). [23] Adam Paszke et al., “PyTorch: An Imperative Style, High- Performance Deep Learning Library,” in Advances in Neu- ral Information Processing Systems 32 (Curran Associates, Inc., 2019) pp. 8024–8035. [24] J. Schulman, N. Heess, T. Weber, and P. Abbeel, “Gra- dient estimation using stochastic computation graphs,” in Advances in Neural Information Processing Systems, Vol. 28 (Curran Associates, Inc., 2015). [25] G. Arya et al., “Differentiating metropolis-hastings to optimize intractable densities,” (2023), arXiv:2306.07961. [26] A. K. Lew, M. Huot, S. Staton, and V. K. Mansinghka, “ADEV: Sound automatic differentiation of expected val- ues of probabilistic programs,” Proceedings of the ACM on Programming Languages 7, 121–153 (2023). [27] E. Krieken, J. Tomczak, and A. Ten Teije, “Storchastic: A framework for general stochastic automatic differen- tiation,” in Advances in Neural Information Processing Systems, Vol. 34 (Curran Associates, Inc., 2021) pp. 7574– 7587. [28] S. Bangaru, J. Michel, K. Mu, G. Bernstein, T.-M. Li, and J. Ragan-Kelley, “Systematically differentiating paramet- ric discontinuities,” ACM Trans. Graph. 40, 107:1–107:17 (2021). [29] L. Heinrich, M. Feickert, and G. Stark, “pyhf: v0.6.3,” https://github.com/scikit-hep/pyhf/releases/tag/v0.6.3. [30] L. Heinrich, M. Feickert, G. Stark, and K. Cranmer, “pyhf: pure-Python implementation of HistFactory statistical models,” Journal of Open Source Software 6, 2823 (2021). [31] L. Heinrich and N. Simpson, “pyhf/neos: initial zenodo release [Link],” (2020). [32] S. Carrazza, J. M. Cruz-Martinez, and M. Rossi, “Pdfflow: Parton distribution functions on gpu,” Computer Physics Communications 264, 107995 (2021). [33] R. D. Ball et al., “An open-source machine learning frame- work for global analyses of parton distributions,” (2021), arXiv:2109.02671. [34] B. Nachman and S. Prestel, “Morphing parton showers with event derivatives,” (2022), arXiv:2208.02274. [35] Differentiable Universe Initiative, home page & software. [36] S. Carrazza and F. A. Dreyer, “Jet grooming through reinforcement learning,” Phys. Rev. D 100, 014014 (2019), arXiv:1903.09644. [37] J. Brehmer et al., “Hierarchical clustering in parti- cle physics through reinforcement learning,” (2020), arXiv:2011.08191. [38] S. Shirobokov et al., “Black-box optimization with local generative surrogates,” in Advances in Neural Informa- tion Processing Systems, Vol. 33 (Curran Associates, Inc., 2020) pp. 14650–14662. [39] C. Haack and L. Schumacher, “Machine-learning aided detector optimization of the pacific ocean neutrino exper- iment,” ICRC2023, 1059. [40] T. Dorigo et al. (MODE), “Toward the end-to-end op- timization of particle physics instruments with differ- entiable programming,” Rev. Phys. 10, 100085 (2023), arXiv:2203.13818. [41] T. Gorordo et al., “Geometry Optimization for Long-lived Particle Detectors,” (2022), arXiv:2211.08450. [42] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in 3rd International Conference on Learn- ing Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings (2015).