Information Theoretically Optimal Sample Complexity of Learning Dynamical Directed Acyclic Graphs Mishfad Shaikh Veedu∗, Deepjyoti Deka †, and Murti V. Salapaka∗ Abstract In this article, the optimal sample complexity of learning the underlying interac- tion/dependencies of a Linear Dynamical System (LDS) over a Directed Acyclic Graph (DAG) is studied. The sample complexity of learning a DAG’s structure is well-studied for static systems, where the samples of nodal states are independent and identically distributed (i.i.d.). However, such a study is less explored for DAGs with dynamical systems, where the nodal states are temporally correlated. We call such a DAG underly- ing an LDS as dynamical DAG (DDAG). In particular, we consider a DDAG where the nodal dynamics are driven by unobserved exogenous noise sources that are wide-sense stationary (WSS) in time but are mutually uncorrelated, and have the same power spec- tral density (PSD). Inspired by the static settings, a metric and an algorithm based on the PSD matrix of the observed time series are proposed to reconstruct the DDAG. The equal noise PSD assumption can be relaxed such that identifiability conditions for DDAG reconstruction are not violated. For the LDS with WSS (sub) Gaussian exogenous noise sources, it is shown that the optimal sample complexity (or length of state trajectory) needed to learn the DDAG is n = Θ(q log(p/q)), where p is the number of nodes and q is the maximum number of parents per node. To prove the sample complexity upper bound, a concentration bound for the PSD estimation is derived, under two different sampling strategies. A matching min-max lower bound using generalized Fano’s inequality also is provided, thus showing the order optimality of the proposed algorithm. 1 Introduction Learning the interdependency structure in a network of agents, from passive time series observations, is a salient problem with applications in neuroscience Bower and Beeman (2012), finance Kim et al. (2011), meteorology Ghil et al. (2002), etc. Reconstructing the exact structure with the dependency/causation directions has a wide range of applications. For example, the identification of causation structure among the shares helps in obtaining robust portfolio management in the stock market Kim et al. (2011). Similarly, causal graphs are useful in understanding dynamics and identifying contributing factors of a public epidemic emergency situation Yang et al. (2020). ∗Department of Electrical Engineering, University of Minnesota Twin Cities †Theoretical Division T-5, Los Alamos National Laboratory. The authors acknowledge support from the Information Science and Technology Institute (ISTI) at Los Alamos National Laboratory. The research work conducted at Los Alamos National Laboratory is done under the auspices of the National Nuclear Security Administration of the U.S. Department of Energy under Contract No. 89233218CNA000001. The first and the third authors acknowledge the support of NSF through Award Number 2030096 and ARPA-E through the Award number DE-AR0001016. 1 arXiv:2308.16859v1 [stat.ML] 31 Aug 2023The structure of directed interactions in a network of agents is conveniently represented using directed graphs, with the agents as nodes and the directed interactions as directed edges. If the underlying graph doesn’t have cycles, it is called a Directed Acyclic Graph (DAG). In general, it is not possible to reconstruct the exact structure with the direction of different edges. Instead, in many networks it is possible to retrieve only the Markov equivalence graphs, the set of graphs satisfying the same conditional dependence property, from data without any intervention Ghoshal and Honorio (2018). In applications such as finance Kim et al. (2011), climate science Ghil et al. (2002) etc., the agent states, instead of being temporally independent, can evolve over time due to past directed interactions. Such temporal evolution can be represented by a linear dynamical system (LDS). In LDS, the interaction between agent states is captured by a linear time-invariant function. In this paper, we study the identifiability and present the first sample complexity results for learning a DAG of LDS, which we term as Dynamical DAG or DDAG. This is distinguished from static DAG, where the agent states are temporally independent and the DAG does not correspond to temporal dynamics. 1.1 Related Work Static DAG Learning: The problem of obtaining an upper bound on the sample complexity of learning static DAGs goes back twenty-five years Friedman and Yakhini (1996), Zuk et al. (2006). However, tight characterization of optimal rates for DAG learning is a harder problem compared to undirected networks Gao et al. (2022), primarily due to the order identification step. Identifiability conditions for learning static DAGs with linear interactions and excited by equal variance Gaussian noise were given in Peters and B¨uhlmann (2014). Several polynomial time algorithms have been proposed for static DAG reconstruction using samples of states at the graph nodes; see Ghoshal and Honorio (2017a,b, 2018); Chen et al. (2019); Gao et al. (2022); Park (2020); Park and Raskutti (2017), and the reference therein. An information- theoretic lower bound on structure estimation was studied in Ghoshal and Honorio (2017a). In Gao et al. (2022), it was shown that the order optimal sample complexity for static Gaussian graphical model with equal variance is n = Θ(q log(p/q)), where p is the number of nodes and q is the maximum number of parents. The authors showed that the algorithm given in Chen et al. (2019) provides an upper bound that matches a min-max lower bound for the number of samples. However, similar results for DDAGs, with underlying temporal directed interaction between agent states, have not been studied, to the best of our knowledge. LDS Learning: Graph reconstruction, in general, is challenging in a network of LDS (di- rected, undirected, or bi-directed), as it involves time-dependencies between collected samples of nodal states. Learning the conditional independence structure in LDS with independent and identically distributed (white) excitation was explored in Basu and Michailidis (2015), Loh and Wainwright (2011), Songsiri and Vandenberghe (2010), Simchowitz et al. (2018), Faradonbeh et al. (2018), and the references therein. However, the methods in the cited papers do not extend to LDS that is excited by WSS (wide-sense stationary) noise, which makes correlations in state samples more pronounced. For LDS with WSS noise, Tank et al. (2015), Dahlhaus (2000), Materassi and Salapaka (2013), and Materassi and Innocenti (2009), estimated the conditional correlation structure, which contains true edges in the network and extra edges between some two-hop neighbors Materassi and Salapaka (2012). A consistent algorithm for the recovery of exact topology in a large class of applications with WSS noise was provided in Talukdar et al. (2020), with the corresponding sample-complexity analysis developed in Doddi et al. (2022), using a neighborhood-based regression framework. However, 2the developed algorithms and related sample complexity results do not extend to directed graphs and hence exclude DDAG reconstruction. DDAG reconstruction from the time-series data has been explored using the framework of directed mutual information in Quinn et al. (2015) but without rate characterization for learning from finite samples. Contribution: This article presents an information-theoretically optimal sample com- plexity analysis for learning a dynamical DAG (DDAG, i.e., a DAG underlying an LDS excited by WSS noise of equal power spectral density), using samples of state trajectories of the corresponding LDS. To the best of our knowledge, this is the first paper to study and prove sample complexity analysis for DDAGs. We consider learning under two sampling scenarios, viz; 1) restart and record, 2) continuous sampling. While the former pertains to samples collected from multiple disjoint (independent) trajectories of state evolution, the latter includes samples from a single but longer trajectory of the state evolution (see Fig. 2). Surprisingly, the results in this article show that the estimation errors are not influenced by the sampling strategy (restart and record or continuous) as long as the collected samples are over a determined threshold given by n = O(q log(p/q)) is obtained, where p is the number of nodes and q is the maximum number of parents per node. We also provide a match- ing information-theoretic lower-bound, max � log p 2β2+β4 , q log(p/q) M2−1 � , where β and M are system parameters (see Definition 2.4); thus obtaining an order optimal bound n = Θ(q log(p/q)). Our learning algorithm relies on first deriving rules for DAG estimation using the Power Spectral Density Matrix (PSDM) of nodal states, inspired by the estimator for static DAGs based on covariance matrices in Chen et al. (2019). Subsequently, the sample complexity associated with learning is derived by obtaining concentration bounds for the PSDM. In this regard, characterization of non-asymptotic bounds of PSDMs for a few spectral estimators have been obtained Fiecas et al. (2019); Veedu et al. (2021); Zhang and Wu (2021) previ- ously. A unified framework of concentration bounds for a general class of PSDM estimators was recently presented in Lamperski (2023). Our concentration bounds of the PSDM are reached using different proof steps, based on Rademacher random variables and symmetriza- tion argument Wainwright (2019). The rest of the paper is organized as follows. Section 2 introduces the system model and the preliminary definitions for LDS and DDAGs. Section 3 discusses Algorithm 1 and the main results for DDAG reconstruction from PSDM. Section 4 provides a concentration bound for the error in estimating the PSDM and a sample complexity upper bound for DDAG reconstruction using Algorithm 1. Section 5 contains a sample complexity lower bound. Notations: Bold faced small letters, x denote vectors; Bold faced capital letters, A denote matrices; For a time-series, x, ˘x(t) denotes the value of x at time t, x(ω) denotes the discrete time Fourier transform of x, x(ω) := �∞ k=−∞ ˘x(k)e−iωk, ω ∈ Ω = [0, 2π]; diag(v1, . . . , vp) operator creates a diagonal matrix with diagonal entries v1, . . . , vp; ΦxAB or ΦAB denotes the matrix obtained by selecting rows A and columns B in Φx; A∗ denotes conjugate transpose of A. ∥A∥ is the spectral norm of A and ∥v∥2 is the Euclidean norm of vector v. 2 System model of DDAG We describe different aspects of DDAG (DAG underlying an LDS) and the sampling strategies considered. We begin with some necessary DAG terminologies. DDAG terminology: The dynamical directed acyclic graph (DDAG), is given by G := (V, E), where V = {1, . . . , p}, and E is edge set of directed edge i −→ j. A directed path from i to j is a path of the form i = v0 −→ v1 −→ . . . −→ vℓ −→ vℓ+1 = j, where vk ∈ V and 3(vk, vk+1) ∈ E for every k = 0, . . . , ℓ. A cycle is a directed path from i to i, which does not exist in DDAG G. For G, pa(i) := {j ∈ V : (j, i) ∈ E} denotes the parents set and desc(i) denotes the descendants of i, the nodes that have a directed path from i. The set nd(i) := V \ desc(i) denotes the non-descendants set and the set an(i) ⊂ nd(i) denotes the ancestors set, the nodes that have a directed path to i. At set C ⊆ V is called ancestral if for every i ∈ C, pa(i) ⊆ C. Figure 2 shows an example DDAG with the node definitions. A node set C ⊆ V is said to be a topological ordering on G if for every i, j ∈ C, i ∈ desc(j) in G implies i > j. Gp,q denotes the family of DDAGs with p nodes and at most q parents per node. Without a loss of generalizability, we use the same terminology for the DDAG and the 1 2 5 3 6 4 7 Q1 Q2 Q4 Q3 Q5 R2 R4 R6 R8 R10 R12 R15 R16 R18 R20 Fig. 1: An electric circuit with transistors and tuned ampl transistors introduce direction, and inductors and capaci time-lag dependency across the circuit. Fig. 2: A causal graph representation of the electric circu Here, each node represents a transistor Figure 1: An example DDAG. Node 1 is an ancestor and node 7 is a descendant of every node in the graph. The set {1, 2, 5} is an ancestral set but {2, 5} is not. an(3) = {1, 2}, desc(3) = {4, 7}, nd(3) = {1, 2, 5, 6}. underlying DAG. LDS Model excited by equal PSD WSS noise: For the DDAG G = (V, E) ∈ Gp,q, we consider a linear dynamical system (LDS) with p scalar state variables, corresponding to nodes in V . Node i is equipped with time series measurements, {˘xi(k)}k∈Z, 1 ≤ i ≤ p. The LDS evolves according to the linear time-invariant model, ˘xi(k) = p � (i,j)∈E,j̸=i (˘hij ⋆ ˘xj)(k) + ˘ei(k), k ∈ Z, (1) where transfer function ˘hij ̸= 0 when directed edge (i, j) ∈ E. The exogenous noise {˘ei(k)}k∈Z, 1 ≤ i ≤ p, are zero mean wide sense stationary Gaussian processes, uncorre- lated across nodes. Taking the discrete-time Fourier transform (DTFT) of (1) provides the frequency representation for every ω ∈ Ω = [0, 2π], xi(ω) = p � (i,j)∈E,j̸=i Hij(ω)xj(ω) + ei(ω), 1 ≤ i ≤ p, (2) where xi(ω) = F{˘xi} := �∞ k=−∞ ˘xi(k)e−iωk, ei(ω) = F{˘ei}, and Hij(ω) = F{˘hij}. The model in (2) can be represented in the matrix form to obtain the following LDS, x(ω) = H(ω)x(ω) + e(ω), ∀ω ∈ Ω, (3) where e(ω) is the WSS noise. In this article, we are interested in the LDS with Φe(ω) = σ(ω)diag(α1, . . . , αp), where αi are known and can be a function of ω. For the simplicity of analysis, henceforth it is assumed that Φe(ω) = σ(ω)I. Remark 2.1. The assumption Φe(ω) = σ(ω)I is a restrictive assumption. However, we would like to remark that some form of restriction is required due to the impossibility of DAG 4reconstruction in a general setup due to identifiability issues Shimizu et al. (2006). However, the assumption can be relaxed to incorporate the identifiability conditions on Φei and H to retrieve the topological ordering, similar to Ghoshal and Honorio (2018). Furthermore, our results on DDAG reconstruction require the equal PSD to hold only at some known ω ∈ Ω which is less restrictive. The power spectral density matrix (PSDM) of the time-series x at the angular frequency ω ∈ Ω is given by Φx(ω) = F {Rx(t)} = ∞ � k=−∞ Rx(k)e−iωk, (4) where Rx(k) := E[˘x(k)˘xT (0)] is the auto-correlation matrix of the time-series x at lag k. The (i, j)-th entry of Φx is denoted by Φij. For the LDS (3), the PSDM is given by Φx(ω) = (I − H(ω))−1Φe(ω)((I − H(ω))−1)∗. (5) Consider the following additional non-restrictive assumptions on the power spectral density and correlation matrix of the LDS states. Assumption 2.2. There exists an M ∈ R such that 1 M ≤ λmin(Φx) ≤ λmax(Φx) ≤ M, where λmin and λmax respectively denote minimum and maximum eigenvalues. Assumption 2.3. The auto-correlation matrix of the time-series x at lag k, Rx(k) := E[˘x(k)˘xT (0)] satisfies ∥Rx(k)∥ ≤ Cρ−|k|, for some positive constants C, ρ ∈ R, ρ > 1. In the remaining paper, following these assumptions, our interest will be limited to the following family of DDAGs and corresponding LDS. Definition 2.4. Hp,q(β, σ, M) denotes the family of LDS given by (3) such that the corre- sponding DDAG, G(V, E) ∈ Gp,q (p nodes with each node having a maximum q parents), with |Hij(ω)| ≥ β, ∀(i, j) ∈ E, ω, Φe(ω) = σ(ω)I, and M−1 ≤ λmin(Φx(ω)) ≤ λmax(Φx(ω)) ≤ M, ∀ω ∈ Ω Sampling Strategy for LDS states: We consider two sampling settings (see Fig. 2 for details): i) restart and record: The sampling is performed as follows: start recording and stop it after N measurements. For the next trajectory, the procedure is restarted with an indepen- dent realization and record for another epoch of N samples; repeat the process another n − 2 times providing n i.i.d trajectories of N samples each. ii) continuous sampling: Here, a single trajectory of length n × N is taken consecutively. Then, the observations are divided into n segments, each having N consecutive samples. The data collected using either strategy is thus grouped into n trajectories of N samples each. For the finite length rth trajectory, {˘xr(t)}N−1 t=0 , define xr(ω) = 1 √ N N−1 � k=0 ˘xr(k)e−iωk, r = 1, . . . , n, ω ∈ Ω. (6) Note that xr(ω), if exists, is a zero mean random variable with the covariance matrix given by �Φx(ω) := E {xr(ω)[xr(ω)]∗} = 1 N N−1 � k=−(N−1) (N − |k|)Rx(k)e−iωk, ∀ω ∈ Ω. (7) 5Start Start Stop x1 Stop x1 Start Stop x2 Start Stop xn x2 xn Figure 2: (a) shows restart and record sampling. (b) shows continuous sampling. For the restart and record setting (unlike for continuous sampling), {xr(ω)}n r=1 are i.i.d. Further, as N −→ ∞, �Φx(ω) −→ Φx(ω) uniformly in Ω Stoica et al. (2005). 3 Reconstructing DDAGs from PSDM In this section, we discuss some results on how the PSDM, Φx, can be employed to completely reconstruct the DDAG, G when the time series is generated according to (3). Applying these results, inspired from the algorithm for static setting Chen et al. (2019), we propose Algorithm 1 for reconstructing the DDAG, G. First, we prove that conditional PSD (CPSD) of i conditioned on C, defined as f(i, C, ω) := Φii(ω) − ΦiC(ω)Φ−1 CC(ω)ΦCi(ω), (8) is a metric sufficient to obtain a topological ordering on the DDAG, G, which aids in the reconstruction of G. Notice that unlike Chen et al. (2019)’s static setting, our algorithm will require the use of CPSD to unveil the dependencies in the DDAG and further affect the sample complexity of learning. 3.1 CPSD and Topological Ordering Here, we will show that CPSD is the minimum for the nodes that have all the parents included in the conditioning set. We start by proving the result for the source nodes. Lemma 3.1. Consider the LDS described by (3). For any ω ∈ Ω, let α∗ := min k∈V Φkk(ω). Then Φii(ω) = α∗ if and only if i is a source node. Proof. Let T(ω) = (I − H(ω))−1 and Φe(ω) = σ(ω)In. Then, Φx(ω) = σ(ω)T(ω)T∗(ω). By Cayley-Hamilton theorem, there exists constants a0(ω), . . . , an(ω) such that (I − H(ω))−1 = �n−1 k=0 ak(ω)(I − H(ω))k. Using induction, it can be shown that non-diagonal entries of (I − H(ω))k are zero if and only if there no k − hop path between i and j (almost always). Similarly, (i, i)th entry is 1 if and only if there is no k-hop path between them (almost always). Then (ignoring (ω)), Φii = (Φx)ii = σ �n k=1 TikTik = σ(T2 ii + � k̸=i T2 ik). If i is a source node, then Tik = 0 for every k ̸= i and Tii = 1, which implies Φii = σT2 ii. For non-source nodes, Tik ̸= 0 for some k, which gives Φii > σ. 3.1.1 Conditional PSD deficit The following is the definition of conditional PSD deficit, which is helpful in proving the subsequent results and in retrieving the DDAG. 6Definition 3.2 (The CPSD deficit). ∆ := min ω∈[−π,π] min j∈V min C⊊nd(j), Pa(j)\C̸=∅ f(j, C, ω) − σ(ω) (9) The following lemma shows that f(i, C, ω) from Eq. 8 can be used as a metric to obtain a topological ordering of G. Lemma 3.3. Consider the DDAG, G, governed by (3). Let j ∈ V and let C ⊆ V \ {j} be an ancestral set. Then for every ω ∈ Ω, f(j, C, ω) = σ(ω) : if Pa(j) ⊆ C, f(j, C, ω) ≥ ∆ + σ(ω) > σ(ω) : if Pa(j) ⊈ C. Proof. See Appendix A Corollary 3.4. If Pa(i) ⊆ C and Pa(i) ⊈ D, then f(i, C, ω) − f(i, D, ω) ≥ ∆. Lemma 3.5. ∆ ≥ β2σ. Proof. Applying (15), f(j, C, ω) − σ = HjD(ΦD − ΦDCΦ−1 CCΦCD)H∗ jD ≥ σ|D|β2. Then ∆ ≥ min ω min j |σ|D|β2 = σ|D|β2 (a) ≥ σβ2, where (a) follows because |D| ≥ 1 if C ⊈ Pa(j). To determine the DDAG G’s structure, first determine a topological ordering of nodes as follows: Beginning with an empty set S, we iteratively add node i in S where (i, C∗ i ) ∈ arg min C⊆S,|C|≤q 1≤j≤p, j /∈S f(j, C, ω), (10) and f comes from (8). The following Lemma shows that S is a valid topological ordering w.r.t. G. Lemma 3.6. S is a valid topological ordering with respect to G. Proof. In the first step, C = ∅ and f(i, C, ω) = Φii. By Lemma 3.1 and Lemma 3.3, Φii = σ if i is a source node, where as Φii ≥ σ + ∆ if i is not a source node. Thus, the first node in the ordered set S, S1, is always a source node. Induction assumption: Nodes S1 to Sn in S follow topological order. For the n + 1, by Lemma 3.3, for every C ⊆ S, f(k, C, ω) is minimum for k ∈ V \ S if and only if Pa(k) ⊆ C. Thus nodes S1 to Sn+1 follow a topological order, which proves the result. Identification of the parents: Parents of a node are identified from the ordered set by applying Corollary 3.4. Let D = C \ {k}. As shown in Corollary 3.4, if Pa(i) ⊆ C and k ∈ Pa(i), then f(i, C, ω) − f(i, D, ω) ≥ ∆. Thus, from the set S, for every node Si, one can eliminate nodes S1, . . . , Si−1 by checking if the difference is greater than ∆. If the difference is greater than ∆ for some Sk, then Sk is a parent of Si. That is, 7Lemma 3.7. Let (i, C∗ i ) be a solution of (10) and let Pi := {j ∈ C∗ i | |f(i, C∗ i , ω) − f(i, C∗ i \ j, ω)| ≥ ∆} . Then, Pa(i) = Pi. Applying the above procedure and Lemma 3.3-Lemma 3.7, one can formulate Algorithm 1 to obtain the ordering of the DDAG, G and eventually reconstruct the DDAG exactly. In Algorithm 1, �f(i, C, ω) := �Φii(ω)− �ΦiC(ω)�Φ−1 CC(ω)�ΦCi(ω), an empirical estimate of f(i, C, ω), is employed instead of f and γ = ∆/2. The following lemma proves that if the empirical estimate �f(·) is close enough to the original f(·), then Algorithm 1 reconstructs the DDAG exactly. Algorithm 1 Ordering algorithm Input: Estimated PSDM, �Φ(ω): ∆, z = ejω, ω ∈ (−π, π] Output: �G 1. Initialize the ordering, S ←−() 2. For i = 1, . . . , p (a) Compute � j∗, C∗ j � ∈ arg min C⊆S,|C|≤q 1≤j≤p, j /∈S �f(j, C, ω) (b) S ←− (S, j∗) 3. �G = (V, �E), V ←− {1, . . . , p}, �E ←− ∅ 4. For i = 1, . . . , p (a) Parents of i, Pi := � j ∈ C∗ i ��� | �f(Si, C∗ i , ω) − �f(Si, C∗ i \ j, ω)| ≥ γ � (b) For k ∈ Pi i. Do �E ←− �E ∪ (k, i) 5. Return �G Lemma 3.8. If | �f(i, C, ω)−f(i, C, ω)| < ∆/4, for every i, ω, C, then Algorithm 1 reconstructs the DAG, G successfully. That is, G = �G. Proof. See Appendix B Therefore, it suffices to derive the conditions under which |f(·) − �f(·)| < ∆/4. In the following section, we derive a concentration bound to guarantee a small error, which in turn is applied in obtaining the upper bound on the sample complexity of estimating G. 4 Finite Sample Analysis of Reconstructing DDAGs In Lemma 3.8, it was shown that the DDAG, G, can be reconstructed exactly if the error in estimating f(i, C, ω) (given by (8)) is small enough. In this section, a concentration bound 8on the error in estimating Φx from finite data is obtained, which is used later to obtain a concentration bound on the error in estimating the metric f. Recall that we consider n state trajectories (see Fig. 2 for the two sampling strategies) with each trajectory being of length N samples, i.e � {˘xr(k)}N−1 k=0 �n r=1. The DFTs for each trajectory, xr(ω), is a complex Gaussian with mean zero and covariance matrix, �Φx(ω)), i.e., for every r = 1, . . . , n, xr(ω) ∼ N(0, �Φx(ω)), as given in Eq. 7. Increasing N ensures that �Φx is close to Φx. To estimate the PSDM, we thus rely on the spectrogram method and estimate �Φx(ω)) using finite n samples for xr. 4.1 Non-Asymptotic Estimation Error in Spectrogram Method Let �Φx(ω) := 1 n �n r=1 xr(ω)[xr(ω)]∗. Let the estimation error in estimating Φx by �Φx(ω) be Q := �Φx(ω) − Φx(ω). Applying the triangle inequality, ∥Q∥ ≤ ∥Qapprox∥+∥Q2∥, where Qapprox := �Φx(ω)−Φx(ω) and Q2 := �Φx(ω) − �Φx(ω). Note that �Φx(ω) is the covariance of the DFT of each trajectory. To bound the estimation error, we bound both Qapprox and Q2. The following Lemma shows that Qapprox is small if N (length of each trajectory) is large. Lemma 4.1 (Lemma 5.1, Doddi et al. (2022)). Consider an LDS given by (3) that satis- fies Assumption 2.3. Let Qapprox = �Φx(ω) − Φx(ω) where �Φx(ω) is given in Eq. 7. Then ∥Qapprox∥ < ε1 if N > 2Cρ−1 (1−ρ−1)2ε1 . The next step is to characterize Q2, the error in estimating �Φx using �Φx(ω). Since E (xr(ω)[xr(ω)]∗) = �Φx(ω)), �Φx(ω) is an unbiased estimator of �Φx(ω). The following theorem provides a concentration bound on ∥Q2∥. The concentration bound is applicable under two sampling scenarios; the restart and record setting and continuous sampling setting, as shown in Fig. 2. Theorem 4.2. Suppose {˘xr(k)}N−1 k=0 , 1 ≤ r ≤ n be the time series measurements obtained from an LDS governed by (3), satisfying Assumption 2.2. Then P �����Φx(ω) − �Φx(ω)) ��� ≥ ϵ � ≤ exp � − ϵ2n 128M2 + 6p � , ∀ω ∈ [−π, π]. (11) Proof. See Appendix C By combining Lemma 4.1 and Theorem 4.2, the following corollary is obtained, which gives a concentration bound on the estimation error, ∥Q∥. Corollary 4.3. Consider an LDS governed by (3) that satisfies Assumptions 2.2 and 2.3. Let {˘xr(k)}N−1 k=0 , 1 ≤ r ≤ n be the time series measurements obtained for the LDS. Suppose that N > 2Cρ−1 (1−ρ−1)2ε1 , where 0 < ε1. Let 0 < ε1, ε2 < ε be such that ε2 = ε − ε1. Then ∀ω ∈ Ω, P ����Φxx(ω) − �Φxx(ω)) ��� ≥ ε � ≤ exp � − ε2 2n 128M2 + 6p � , (12) P ����ΦCC(ω) − �ΦCC(ω)) ��� ≥ ε � ≤ exp � − ε2 2n 128M2 + 6q � , and (13) P ����Φii(ω) − �Φii(ω)) ��� ≥ ε � ≤ exp � − ε2 2n 128M2 + 6 � . (14) 94.2 Sample Complexity Bounds: Upper Bound In the previous subsection, concentration bounds on the estimation errors in PSDM were obtained. Here, a concentration bound on the error in estimating f is obtained, which is used to obtain a concentration bound in reconstructing the DDAG, G. The following result provides a concentration bound on |f − �f|. Lemma 4.4. Consider an LDS governed by (3) that satisfies Assumptions 2.2 and 2.3. Let {˘xr(k)}N−1 k=0 , 1 ≤ r ≤ n be the time series measurements obtained for the LDS. Suppose that N > 2Cρ−1 (1−ρ−1)2ε1 , where 0 < ε1. Let 0 < ε1, ε2 < ε be such that ε2 = ε − ε1. Then there exists a c0 ∈ R such that, for any ω ∈ Ω, P � |f(i, C, ω) − �f(i, C, ω) ≥ ε � ≤ c0e � − ε2 2n 10368M6 +6(q+1) � , where q is the maximum number of parents any node has in G. Proof. See Appendix D Based on Lemma 4.4, the following upper bound on the probability of error in estimating G can be obtained. Theorem 4.5. Suppose q ≤ p/2. Consider an LDS that belongs to Hp,q(β, σ, M) (Definition 2.4) that satisfies Assumptions 2.2 and 2.3. Let {˘xr(k)}N−1 k=0 , 1 ≤ r ≤ n be the time series measurements of the LDS and let �G be the DDAG reconstructed by Algorithm 1. Suppose that N > 2Cρ−1 (1−ρ−1)2ε1 , where 0 < ε1 < ∆/4. Let 0 < ε1, ε2 < ε < ∆/4 be such that ε2 = ε − ε1. Then P � G(ω) ̸= �G(ω) � ≤ δ if n ≳ M6 (q log(p/q) − log δ) ϵ2 2 . Proof. Applying the bound in Lemma 4.4, P � G(ω) ̸= ˆG(ω) � = P   � k∈V,C⊆V \{k},|C|≤q � |f(i, C, ω) − �f(i, C, ω)| > ϵ �   (a) ≤ � k∈V,C⊆V \{k},|C|≤q P �� |f(i, C, ω) − �f(i, C, ω)| > ϵ �� (b) ≤ p ��(p − 1) 1 � + · · · + �(p − 1) q �� c0e � − ϵ2n 10368M6 +6(q+1) � (c) ≲ c1p × q × (p/q)qe � − ϵ2n 10368M6 +6(q+1) � ≈ exp � log p + log q + q log (p/q) + � − ϵ2n 10368M6 + 6(q + 1) �� ≈ exp � log p + log q + q log (p/q) + 6q − ϵ2n M6 � ≲ exp � q log (p/q) − ϵ2n M6 � < δ, 10where (a) follows by union bound, (b) follows since |V | = p and there are �p−1 k � number of combinations with |C| = k. (c) follows by applying Stirling’s approximation, �n k � ≤ (ne/k)k. Thus, n ≳ M6(q log(p/q)−log δ) ϵ2 . By selecting the threshold γ in the algorithm appropriately, we can get a sample complexity n ≳ M6q log(p/q) ∆2 . In the following section, a matching lower bound is derived. 5 Sample Complexity Bounds: Lower Bound The lower bound for reconstructing DDAG is derived using information-theoretic techniques, in particular Fano’s inequality, and by restricting the interested family of graphs to a finite set. Notice that, except for a couple of non-trivial facts in dynamic setup, this is a direct extension of the lower bound for the static case provided in Gao et al. (2022). The approach is to construct restricted ensembles of graphical models and then to lower bound the probability of error using Generalized Fano’s inequality. Theorem 5.1 below provides the lower bound. For completeness, the proof is provided in the Appendix E.2. Theorem 5.1. Suppose q ≤ p/2. If n ≤ (1 − 2δ) max � log p 2β2 + β4 , q log(p/q) M2 − 1 � then inf �G max H∈Hp,q(β,σ,M) P{(G(H) ̸= �G)} ≥ δ, That is, if n ≤ (1 − 2δ) max � log p 2β2+β4 , q log(p/q) M2−1 � , then any given estimator fails to recon- struct the DDAG with probability greater than δ. The lower-bound provides the fundamental limit on reconstructing the DDAG from finite samples. Theorem 5.1 provides the lower bound Ω � log p 2β2+β4 � q log(p/q) M2−1 � . Notice that the upper bound in Theorem 4.5 is O(q log p q). Thus we obtain a matching order bound when the lower bound is dominated by the second term. Conclusion In this article, we characterized the optimal sample complexity for structure identification with directions in linear dynamical networks. Inspired by the static setting, a metric and an algorithm were proposed based on the power spectral density matrix to exactly reconstruct the DAG. It is shown that the optimal sample complexity is n = Θ(q log(p/q)). For the upper bound characterization, we obtained a tight concentration bound for power spectral density matrix. An information-theoretic min-max lower bound also was provided for (sub) Gaussian linear dynamical sysytems. It was shown that the upper-bound is order optimal with respect to the lower bound. 11Appendices A Proof of Lemma 3.3 Let C ⊆ V \ {j} be an ancestral set and let D = nd(j) \ C. Then, xj(ω) = HjC(ω)XC(ω) + HjD(ω)XD(ω) + ej(ω). Applying ΦejC = ΦejD = 0, we obtain ΦjC(ω) = HjC(ω)ΦCC(ω) + HjD(ω)ΦDC(ω) and Φj(ω) = HjC(ω)ΦCHjC(ω)∗ + HjD(ω)ΦDC(ω)H∗ jC(ω) + HjC(ω)ΦCD(ω)H∗ jD(ω) + HjD(ω)ΦD(ω)H∗ jD(ω) + Φejej(ω). Then f(j, C, ω) = Φj − ΦjCΦ−1 C ΦCj = Φejej + HjD(ΦD − ΦDCΦ−1 CCΦCD)H∗ jD. Notice that when Pa(j) ⊆ C, HjD = 0, and f(j, C, ω) = Φejej, which shows the first part. To prove the second part, suppose Pa(j) ∩ D ̸= ∅. We need to show that HjD(ΦD − ΦDCΦ−1 CCΦCD)H∗ jD > 0. Let A = nd(j) = C ∪ D and B = desc(j) ∪ {j}. From Talukdar et al. (2018); Veedu et al. (2021), Φ−1 AA =S + L, where S = (IA − H∗ AA)Φ−1 eA (IA − HAA), L = HBAΦ−1 eB HBA − Ψ∗Λ−1Ψ, Ψ = H∗ ABΦ−1 eA (I − HAA) + (I − H∗ BB)Φ−1 eB HBA, and Λ = H∗ ABΦ−1 eA HAB + (I − H∗ BB)Φ−1 eB (I − HBB). Notice that since B is the set of descendants of j, HAB = 0, as cycles can be formed otherwise. Then, L = 0 and Φ−1 AA = (IA − H∗ AA)Φ−1 eA (IA − HAA). Φ−1 AA = � ΦDD ΦDC ΦCD ΦCC �−1 = � KDD KDC KCD KCC � = 1 σ (I − H∗ AA) (I − HAA) By Scur’s complement, (ΦD−ΦDCΦ−1 CCΦCD)−1 = KDD = 1 σ(ID−H∗ DD−HDD+(H∗ AAHAA)D×D). Moreover, HAA = � HDD HDC HCD HCC � and (H∗ AAHAA)D×D = H∗ DDHDD + H∗ CDHCD. Since C is ancestral, HCD = 0 and KDD = 1 σ(ID − HDD)∗(ID − HDD). Since G is a DAG, the rows and columns of H can be rearranged to obtain a lower triangular matrix with zeros on the diagonal. Thus eigenvalues of (ID −HDD) and its inverse are all ones. Hence minimum eigenvalue of K−1 DD is greater than σ. Applying Rayleigh Ritz theorem on HjDK−1 DDH∗ jD, we have HjD(ΦD − ΦDCΦ−1 CCΦCD)H∗ jD = HjDK−1 DDH∗ jD ≥ σ|D|β2 (15) which is strictly greater than zero if D is non-empty. 12B Proof of Lemma 3.8 The proof is done in two steps. First, we show that S in Algorithm 1 is a topological ordering. Then, we show that step (4) in Algorithm 1 can identify the parents of every node in G. The first step is shown via induction. Since | �f(i, C, ω) − f(i, C, ω)| < ∆/4 for empty set, |Φii − �Φii| < ∆/4 for every i. Recall from Lemma 3.3 that Φii − Φjj > ∆ if i is a source node and j is a non-source node. Then, �Φjj ≥ Φjj − ∆/4 ≥ Φii + 3∆/4 ≥ �Φii + ∆/2. Thus, i ∈ arg min 1≤k≤p �Φkk if and only if i ∈ arg min 1≤k≤p Φkk and thus S1 is always a source node. For the induction step, assume that S1, . . . , Sn forms a correct topologically ordered set w.r.t. G. Let C ⊆ S(1 : n). If Pa(i) ⊆ C and Pa(j) ⊈ C, then by applying Lemma 3.3, �f(j, C, ω) > f(j, C, ω) − ∆/4 ≥ σ + 3∆/4 = f(i, C, ω) + 3∆/4 ≥ �f(i, C, ω) + ∆/2. Thus, i ∈ arg min k∈V \S �f(k, C, ω) if and only if i ∈ arg min k∈V \S f(k, C, ω) and thus (S, Sn+1) forms a topological order w.r.t. G, by Lemma 3.6. To prove the second step, let C ⊆ S(1 : i). Since S(1 : i) is a valid topological ordering, Pa(i) ⊆ S(1 : i − 1). Let k ∈ Pa(i) and let D = C \ {k}. Then, as shown in Corollary 3.4 f(i, C, ω) − f(i, D, ω) ≥ ∆, and ∆ ≤ |f(i, C, ω) − f(i, D, ω)| ≤ |f(i, C, ω) − �f(i, C, ω)| + | �f(i, C, ω) − �f(i, D, ω)| + | �f(i, D, ω) − f(i, D, ω)| < ∆/4 + ∆/4 + | �f(i, C, ω) − �f(i, D, ω)| =⇒ | �f(i, C, ω) − �f(i, D, ω)| > ∆/2. Suppose k /∈ Pa(i) but k ∈ S(1 : i). Then, for D = C \ {k}, f(i, C, ω) − f(i, D, ω) = 0. Repeating the same series of inequalities above by exchanging f and �f, we obtain | �f(i, C, ω)− �f(i, D, ω)| < ∆/2. Thus, from the set S, for every node Si, one can check nodes S1, . . . , Si−1 and verify if the difference of including and excluding the node is greater than ∆/2. If the difference is greater than ∆/2 for some k, then k is a parent of i, and if not, then the node is not a parent of i. That is, let Ci = {S1, . . . , Si−1}, i > 1, and let �Pi := � j ∈ Ci ��� | �f(Si, Ci, ω) − �f(Si, Ci \ {j}, ω)| > ∆/2 � . Then, Pa(i) = �Pi. C Proof of Theorem 4.2 By the variational form of spectral norm Horn and Johnson (2012), ∥Q∥ = sup v∈Cp,∥v∥=1 |v∗Qv|, where the max is taken over a p-dimensional unit complex sphere, Sp := {v ∈ Cp : ∥v∥2 = 1}. The first step here is to reduce supremum to finite maximization using finite covers of a unit ball, which is done using a δ cover. A δ-cover of a set A is a set v1, . . . , vm such that for every v ∈ A, there exists an i ∈ 1, . . . , m such that ∥vi − v∥2 ≤ δ. The following Lemma is obtained by extending example 5.8 in Wainwright (2019) to the complex field. 13Lemma C.1. Let v1, . . . , vm be a δ-covering of the unit sphere Sp. Then there exists such a covering with m ≤ (1 + 2/δ)2p vectors. Proof. The proof follows by extending (5.9) in Wainwright (2019), to the complex field. Let v ∈ Sp and let vj be such that v = vj + ∆, where ∥∆∥ ≤ δ. Then, v∗Qv = (vj)∗Qvj + 2ℜ{∆∗Qvj} + ∆∗Q∆. Applying triangle inequality, |v∗Qv| ≤ |(vj)∗Qvj| + 2∥∆∥∥Q∥∥vj∥ + |∆∥2∥Q∥ ≤ |(vj)∗Qvj| + 2δ∥Q∥ + δ2∥Q∥ ≤ |(vj)∗Qvj| + 1 2∥Q∥ for δ ≤ 0.22474. Thus, ∥Q∥ = max v∈Sp |v∗Qv| ≤ max j=1,...,m |(vj)∗Qvj| + 1 2∥Q∥ and ∥Q∥ ≤ 2 max j=1,...,m |(vj)∗Qvj| Next, we find an upper bound for E � eλ∥Q∥� , which is treated with Chernoff-type bounding technique to obtain the desired result. E � eλ∥Q∥� ≤ E � exp � 2λ max j=1,...,m |(vj)∗Qvj| �� ≤ m � j=1 E � e2λ(vj)∗Qvj� + E � e−2λ(vj)∗Qvj� (16) Next, we complete the proof for the restart and record sampling and the continuous sampling separately. C.1 Restart and Record Sampling Under the restart and record sampling settings, for any given ω ∈ Ω, {xr(ω)}n r=1 is i.i.d. Thus E � exp � t(vj)∗Qvj�� = E � exp � t(vj)∗(�Φx(ω) − �Φx(ω)))vj�� = E � exp � t n n � r=1 (vj)∗xr(ω)[xr(ω)]∗vj − (vj)∗�Φx(ω)vj �� = n � r=1 E � exp � t n(vj)∗xr(ω)[xr(ω)]∗vj − (vj)∗�Φx(ω)vj �� = � E � exp � t n(vj)∗x1(ω)[x1(ω)]∗vj − (vj)∗�Φx(ω)vj ���n = � E � exp � t n|v∗xr(ω)|2 − v∗�Φx(ω)v ���n 14Let ε ∈ {−1, +1} be a Rademacher variable independent of xr. It can be shown that Proposition 4.11 in Wainwright (2019) will hold for complex numbers also. Then Exr(ω) � exp � t n|v∗xr(ω)|2 − v∗�Φx(ω)v �� ≤ Exr(ω),ε � exp �2tε n |v∗xr(ω)|2 �� (17) = ∞ � k=0 (2t/n)2k 2k! E � |v∗xr(ω)|4k� (18) Recall that �Φx is a positive definite matrix and v∗xr ∼ N(0, η), where η = v∗�Φxv ≤ λmax(�Φx) ≤ M. The even moments of y ∼ N(0, η) is given by E{y2k} = η2k(2k − 1)!! = (2k)! 2kk! η2k. Then E � |v∗xr(ω)|4k� ≤ (4k)! 22k(2k)!M2k. Therefore using the inequality (4k)! ≤ 22k[(2k)!]2, Exr(ω) � exp � t n|v∗xr(ω)|2 − v∗�Φx(ω)v �� ≤ 1 + ∞ � k=1 (2t/n)2k 2k! (4k)! 22k(2k)!M2k ≤ 1 + ∞ � k=1 (2t/n)2k 2k! 22k[(2k)!]2 22k(2k)! M2k = 1 + ∞ � k=1 �2Mt n �2k = 1 1 − � 2Mt n �2 ≤ exp �8M2t2 n2 � whenever 2Mt n < 3/4, where the final inequality follows by applying 1 − x ≥ e−2x for x ∈ [0, 3/4] (to be precise 0.77). Thus, E � exp � t(vj)∗Qvj�� ≤ exp �8M2t2 n � , ∀|t| ≤ 3n 8M . Applying Lemma C.1 and the bound 2m ≤ 2(1 + 2/0.22474)2p ≤ 2e4.6p ≤ e5p+0.693 ≤ e6p, From (16), E � eλ∥Q∥� ≤ E � exp � 2λ max j=1,...,m |(vj)∗Qvj| �� ≤ m � j=1 E � e2λ(vj)∗Qvj� + E � e−2λ(vj)∗Qvj� ≤ 2m exp �32M2λ2 n � ≤ exp �32M2λ2 n + 6p � , ∀|λ| ≤ 3n 16M . Applying Chernoff-type bounding approach, P (∥Q∥ ≥ t) ≤ e−λtE � eλ∥Q∥� ≤ exp � −λt + 32M2λ2 n + 6p � , ∀|λ| ≤ 3n 16M . 15The tightest bound is given by g∗(t) := inf |λ|≤ 3n 16M � −λt + 32M2λ2 n + 6p � , where the objective is convex. Taking derivative w.r.t. λ and equating to zero, λ∗ = tn 64M2 and g∗ = − t2n 64M2 + 32M2 n t2n2 642M4 +6p = 6p− t2n 128M2 , if t is such that t ≤ 12M, which is reasonable as we can always pick M ≥ 1. Thus, P (∥Q∥ ≥ t) ≤ exp � − t2n 128M2 + 6p � The theorem statement follows. C.2 Continuous Sampling In the continuous sampling setting, the samples ˘x(0), . . . , ˘x(N −1), ˘x(N), . . . , ˘x(2N −1), . . . , ˘x((n − 1)N), . . . , ˘x(nN − 1) are sampled continuously and are correlated with each other. Thus, xr(ω) and xs(ω), r ̸= s, 1 ≤ r, s ≤ n, can be correlated, in contrast to the restart and record (RR) setting, where the xr(ω) and xs(ω), r ̸= s are i.i.d. For any given ω ∈ Ω, let x(ω) := [[x1(ω)]T , [x2(ω)]T , . . . , [xn(ω)]T ]T ∈ Cpn×1 be the vectorized form of {xr(ω)}n r=1 and let C(ω) := E{x(ω)x∗(ω)} be the covariance matrix of x(ω). Under the RR setting, C(ω) ∈ Cpn×nn will be a block-diagonal matrix (of block size p×p), whereas in the continuous sampling, the non-block-diagonal entries of C(ω) can be non-zero. However, the vector x(ω), with correlated entries, can be written as a linear transformation of i.i.d. vector w ∈ Cpn×1 with unit variance, i.e. x(ω) = C1/2(ω)w, where C1/2 is the square-root of C. As shown in Appendix E.1, when {˘e(k)}n k=1 in the linear time-invariant model (1) are Gaussian, xr(ω) and thus x(ω) are Gaussian distributed. In this case, a candidate is w ∼ N(0, Ipn). It can be verified that E{x(ω)x∗(ω)} = C1/2(ω)E{ww∗}C1/2(ω) = C(ω). Notice that the covariance matrix C(ω) is a block matrix, defined as C =   C11 C12 . . . C1n C21 C22 . . . C2n ... Cn1 Cn2 . . . Cnn   , where Crs(ω) ∈ Cp×p, 1 ≤ r, s ≤ n, where the entries of Crs(ω) is given by E {xr(ω)[xs(ω)]∗}. Recall that xr(ω) = 1 √ N N−1 � ℓ=0 ˘x((r − 1)N + ℓ)e−iωℓ. Let Ir = � 0| . . . |0|Ip×p| . . . |0 � ∈ Rp×np be such that rth block is identity matrix. Then xr(ω) = Irx(ω). The estimated PSDM is then given by �Φx(ω) = 1 n n � r=1 xr(ω)[xr(ω)]∗ = 1 n n � r=1 Irx(ω)x∗(ω)I∗ r. Substituting x(ω) = C1/2(ω)w, and letting B(ω) := (C1/2)∗(ω) �n r=1 I∗ ruu∗IrC1/2(ω) 16E [exp (tu∗Qu)]=E � exp � t n n � r=1 u∗xr(ω)[xr(ω)]∗u − u∗�Φx(ω)u �� =E � exp � t n n � r=1 [x∗(ω)I∗ ruu∗Irx(ω) − E {x∗(ω)I∗ ruu∗Irx(ω)}] �� =E � exp � t n n � r=1 � w∗(C1/2)∗(ω)I∗ ruu∗IrC1/2(ω)w − E � w∗(C1/2)∗(ω)I∗ ruu∗IrC1/2(ω)w ���� =E � exp � t n � w∗(C1/2)∗(ω) n � r=1 (I∗ ruu∗Ir)C1/2(ω)w − E � w∗(C1/2)∗(ω) n � r=1 (I∗ ruu∗Ir)C1/2(ω)w ���� = E � exp � t n [w∗B(ω)w − E {w∗B(ω)w}] �� Notice that I∗ ru = � 0, . . . , 0, uT , , . . . , 0 �T is a column vector, I∗ ruu∗Ir =   0 0 0 . . . 0 . . . . . . 0 ... . . . uu∗ ���� (r,r)th block . . . 0 ... ... · · · ... 0 0 0 . . . 0   and n � r=1 I∗ ruu∗Ir =   uu∗ 0 . . . 0 0 uu∗ . . . 0 ... ... · · · 0 0 0 . . . uu∗   , i.e., rank(I∗ ruu∗Ir) = 1 and rank(B(ω)) ≤ n. Let B(ω) = U(ω)Λ(ω)U∗(ω) be the eigen value decomposition of B(ω), where Λ = diag(λ1, . . . , λn). Consequently, omitting ω from the notations, E [exp (tu∗Qu)] = E � exp � t n [w∗Bw − E {w∗Bw}] �� = E � exp � t n [w∗UΛU∗w − E {w∗UΛU∗w}] �� (a) = E � exp � t n [w∗Λw − E {w∗Λw}] �� = E � exp � t n n � i=1 λi � w2 i − E � w2 i �� �� = n � i=1 E � exp �tλi n � w2 i − E � w2 i ���� , where (a) follows because w is invariant under unitary transformations Cui et al. (2019). Let ε ∈ {+1, −1} be a uniform random variable independent of w. Similar to (17), we can now 17apply the Rademacher random variable trick. Ewi � exp � λ � w2 i − E � w2 i ���� ≤ Ewi,ε � exp � 2λεw2 i �� = ∞ � k=0 (2λ)2k (2k)! E � w4k i � ≤ ∞ � k=0 (2λ)2k 2k! (4k)! (2k)!22k ≤ ∞ � k=0 (2λ)2k = 1 1 − 4λ2 ≤ exp(8λ2), for every |λ| < 3/8. Thus, (with the substitution λ = tλi/n and the upperbound λi ≤ ∥B∥) E [exp (tu∗Qu)] = n � i=1 E � exp �tλi n � w2 i − E � w2 i ���� ≤ n � i=1 exp �8t2λ2 i n2 � = exp � 8t2 n2 n � i=1 λ2 i � ≤ exp �8t2 n ∥C∥2 � , ∀ |t| ≤ 3n 8∥C∥, where we have used ∥B∥ ≤ ∥C∥ in the final equality. Now, combining this with the δ−cover argument, E � et∥Q∥� ≤ m � j=1 E � e2t(vj)∗Qvj� + E � e−2t(vj)∗Qvj� ≤ 2m exp �32∥C∥2t2 n � ≤ exp �32∥C∥2t2 n + 6p � , ∀|t| ≤ 3n 16∥C∥. Finally, applying Chernoff bound, P (∥Q∥ ≥ t) ≤ exp � − t2n 128∥C∥2 + 6p � . 18C.2.1 Tight upper bound for ∥C∥ An explicit expression for C is given as follows: Crs(ω) := E {xr(ω)[xs(ω)]∗} = 1 N N−1 � ℓ=0 N−1 � k=0 E � ˘x((r − 1)N + ℓ)[˘x((s − 1)N + k)]T � e−iω(ℓ−k) = 1 N N−1 � ℓ=0 N−1 � k=0 R˘x((r − s)N + ℓ − k)e−iω(ℓ−k) = 1 N N−1 � τ=−N+1 (N − |τ|)R˘x((r − s)N + τ)e−iωτ = N−1 � τ=−N+1 � 1 − |τ| N � R˘x((r − s)N + τ)e−iωτ = N−1 � τ=−N+1 � 1 − |τ| N � e−iωτR˘x((r − s)N + τ). Let ατ = e−iωτ � 1 − |τ| N � . Then C = N−1 � τ=−N+1 ατ   R˘x(τ) R˘x(−N + τ) . . . R˘x((1 − n)N + τ) R˘x(N + τ) R˘x(τ) . . . R˘x((2 − n)N + τ) ... ... ... R˘x((n − 1)N + τ) R˘x((n − 2)N + τ) . . . R˘x(τ)   . Notice that ˘g(τ) := 1 − |τ|/N is a triangle function, and the Fourier transform of ˘g(τ), g(ω) has the property that |g(ω)| ≤ 1. Then for any u ∈ Cnp such that ∥u∥2 ≤ 1, u∗Cu = n � i,j=1 [ui]∗ N−1 � τ=−N+1 ατR˘x((i − j)N + τ)uj (a) ≤ Fτ{u}Fτ{Rx(tN + τ)}Fτ{u} (b) ≤ ∥Φx(ω)∥ ≤ M, where (a) follows by taking Fourier transform with respect to τ (Fτ denotes Fourier trans- form with respect to the variable τ) and (b) since ∥Fτ{u}∥2 ≤ 1. Thus, P (∥Q∥ ≥ t) ≤ exp � − t2n 128M2 + 6p � , similar to the restart and record case. D Proof of Lemma 4.4 Notice that ∥Ax∥2 ≤ ∥A∥∥x∥2 for every matrix A and vector x. Applying this identity with x = [1, 0 . . . , 0], ∥ΦCi∥2 ≤ ∥ΦAA∥, where A = [k, C]. Then, applying CBS inequality for 19complex vectors, |x∗Ay| ≤ ∥x∥2∥Ay∥2 ≤ ∥x∥2∥A∥∥y∥2, the error can be upper bounded as |f(i, C, ω) − �f(i, C, ω)| = |(Φii − ΦiCΦ−1 CCΦCi) − (�Φii − �ΦiC �Φ−1 CC �ΦCi)| = |(Φii − �Φii) + (�ΦiC �Φ−1 CC �ΦCi − ΦiCΦ−1 CCΦCi)| ≤ |Φii − �Φii| + |�ΦiC(�Φ−1 CC − Φ−1 CC)�ΦCi)| + |(�ΦiC − ΦiC)Φ−1 CC �ΦCi)| + |ΦiCΦ−1 CC(�ΦCi − ΦCi)| ≤ |Φii − �Φii| + ∥�ΦiC∥2∥�Φ−1 CC − Φ−1 CC∥∥�ΦCi∥2 + ∥�ΦiC − ΦiC∥2∥Φ−1 CC∥∥�ΦCi∥2 + ∥ΦiC∥2∥Φ−1 CC∥∥�ΦCi − ΦCi∥2 ≤ |Φii − �Φii| + ∥�Φ−1 CC − Φ−1 CC∥∥�ΦCi∥2 2 + M∥�ΦiC − ΦiC∥2∥�ΦCi∥2 + M2∥�ΦCi − ΦCi∥2 ≤ |Φii − �Φii| + ∥�Φ−1 CC − Φ−1 CC∥∥�ΦCi − ΦCi∥2 2 + ∥�Φ−1 CC − Φ−1 CC∥∥ΦCi∥2 2 + M∥�ΦiC − ΦiC∥2 � ∥�ΦCi − ΦCi∥2 + ∥ΦCi∥2 � + M2∥�ΦCi − ΦCi∥2 ≤ |Φii − �Φii| + ∥�Φ−1 CC − Φ−1 CC∥∥�ΦCi − ΦCi∥2 2 + ∥�Φ−1 CC − Φ−1 CC∥M2 + M∥�ΦiC − ΦiC∥2 � ∥�ΦCi − ΦCi∥2 + M � + M2∥�ΦCi − ΦCi∥2 = |Φii − �Φii| + ∥�Φ−1 CC − Φ−1 CC∥∥�ΦCi − ΦCi∥2 2 + ∥�Φ−1 CC − Φ−1 CC∥M2 + M∥�ΦCi − ΦCi∥2 2 + M2∥�ΦCi − ΦCi∥2 + M2∥�ΦCi − ΦCi∥2 ≤ |Φii − �Φii| + ∥�Φ−1 CC − Φ−1 CC∥∥�ΦCi − ΦCi∥2 2 + M2∥�Φ−1 CC − Φ−1 CC∥ + M∥�ΦCi − ΦCi∥2 2 + 2M2∥�ΦCi − ΦCi∥2. The above expression can be bounded above if we can bound the three errors, ∥�Φii−Φii∥ = ϵi, ∥�ΦAA − ΦAA∥ = ϵA, and ∥�Φ−1 CC − Φ−1 CC∥ = ϵCinv. Simplifying the above expression, |f(i, C, ω) − �f(i, C, ω)| ≤ |Φii − �Φii| + ∥�Φ−1 CC − Φ−1 CC∥∥�ΦCi − ΦCi∥2 2 + M2∥�Φ−1 CC − Φ−1 CC∥ + M∥�ΦCi − ΦCi∥2 2 + 2M2∥�ΦCi − ΦCi∥2 ≤ ϵi + ϵCinvϵ2 A + 2M2ϵCinv + Mϵ2 A + 2M2ϵA ≤ ϵi + ϵCinv(ϵ2 A + 2M2) + 3M2ϵA ≤ ϵi + 3M2ϵCinv + 3M2ϵA Pick ϵi = 3M2ϵCinv = 3M2ϵA = ϵ/3. Then |f(i, C, ω) − �f(i, C, ω)| < ϵ. From Section 5.8 in Horn and Johnson (2012), ∥ΦCC − �ΦCC∥ ≤ ∥ΦCC∥∥Φ−1 CC∥−1∥�Φ−1 CC − Φ−1 CC∥ M2 1 − M2 ∥�Φ−1 CC−Φ−1 CC∥ ∥Φ−1 CC∥ , ≤ M4∥�Φ−1 CC − Φ−1 CC∥ 1 − M∥�Φ−1 CC − Φ−1 CC∥ ≤ ϵ =⇒ ∥�Φ−1 CC − Φ−1 CC∥ ≤ ϵ M4 + Mϵ ≤ ϵ M4 . 20Therefore, to guarantee that ∥�Φ−1 CC − Φ−1 CC∥ < ϵ, it is sufficient to guarantee that ∥�ΦCC − ΦCC∥ < ϵ since M ≥ 1. Rewriting Corollary 4.3, P � |Φii − �Φii| ≥ ϵ � ≤ e− ϵ2n 128M2 +6, (19) P � ∥ΦAA − �ΦAA∥ > ϵ � ≤ e− ϵ2n 128M2 +6(q+1), and (20) P � ∥ΦCC − �ΦCC∥ > ϵ � ≤ e− ϵ2n 128M2 +6q, ∀ϵ ≥ 0. (21) Plugging these bounds in the above expressions gives the concentration upper bound P � |f(i, C, ω) − �f(i, C, ω) ≥ ϵ � ≤ P � |Φii − �Φii| ≥ ϵ/3 � + P � ∥�Φ−1 CC − Φ−1 CC∥ ≥ ϵ/(9M2) � + P � ∥�ΦAA − ΦAA∥ ≥ ϵ/(9M2) � ≤ P � |Φii − �Φii| ≥ ϵ/3 � + P � ∥�ΦCC − ΦCC∥ ≥ ϵM2/(9) � + P � ∥�ΦAA − ΦAA∥ ≥ ϵ/(9M2) � ≤ e � − ϵ2n 1152M2 +6 � + e � − ϵ2M2n 10368 +6q � + e � − ϵ2n 10368M6 +6(q+1) � ≤ c0e � − ϵ2n 10368M6 +6(q+1) � . E Lower bound: Proof of Theorem 5.1 E.1 Density function in frequency domain Consider an autoregressive (AR) model ˘x(k) = T1 � l=0 ˘H(l)˘x(k − l) + ˘e(k), ∀k ∈ Z, (22) where ˘e(k) = [˘e1(k), . . . , ˘en(k)]T , �x(k) = [˘x1(k), . . . , ˘xn(k)]T , ˘ei is a stochastic process such that the Fourier transform e(ω) exists. To understand the problem, let us assume ˘e(k) ∼ N(0, σ2I) for k = 0, . . . , T2, i.i.d. and zero otherwise. i.e., ˘x(k) is non zero only for k = 210, . . . , T1 + T2. Then e(ω) = T2 � k=0 ˘e(k)e−jωk = T2 � k=0 ˘e(k) cos(ωk) − j T2 � k=0 ˘e(k) sin(ωk). =⇒ cov(e(ω)) = E{e(ω)e∗(ω)} = E      T2 � k1=0 ˘e(k1)e−jωk1     T2 � k2=0 ˘e(k2)e−jωk2   ∗   = E      T2 � k1=0 T2 � k2=0 ˘e(k1)[˘e(k2)]∗e−jωk1ejωk2      = E �� T2 � k=0 ˘e(k)[˘e(k)]∗ �� = T2 � k=0 E {˘e(k)[˘e(k)]∗} = σ2I T2 � k=0 1. Thus, e(ω) ∼ N � 0, (T2 + 1)σ2I � . Now, consider the general LDS (3), with ˘e(k) ∼ N(0, σkI), k ∈ Z. Then as shown above, e(ω) ∼ N (0, Φe(ω)), where Φe(ω) = � k∈Z σkI. It follows that x(ω) = (I − H(ω))−1e(ω) and x(ω) ∼ N(0, Φx), where Φx(ω) = (I − H(ω))−1Φe(ω)((I − H(ω))−1)∗. Thus, the density function f(ω) is Gaussian, where the covariance matrix is the power spectral density matrix, a function of ω. E.2 Proof of lowerbound The proof is based on Generalized Fano’s inequality. Lemma E.1 (Generalized Fano’s method). Gao et al. (2022) Consider a class of obser- vational distribution F and a subclass F′ = {F1, . . . , Fr} ⊆ F with r distributions and the estimators �θ. Then inf �θ max F∈F E{I(θ(F) ̸= �θ)} ≥ αr 2 � 1 − nβr + log 2 log r � , where n is the number of samples, αr := max k̸=j I(θ(Fk) ̸= θ(Fj)), βr := max k̸=j KL(Fk||Fj), with KL(P||Q) := EP � log P Q � = EP [log P] − EP [log Q] being the KL divergence. Corollary E.2. Consider subclass of graphs G′ = {G1, . . . , Gr} ⊆ Gp,q, and let Hi be the distribution corresponding to a distinct Gi ∈ G′. Then, any estimator �G := � ω∈Ω �G(ω) of Gi 22is δ unreliable, inf �G sup Gi∈G′ P{(G(Hi) ̸= �G} ≥ δ, if n ≤ (1 − 2δ) log r − log 2 βr Therefore, building a lower bound involves finding a subclass that has 1) small βr and 2) large r. First, we can find an upper bound of r by upper bounding the number of directed graphs possible with at most q parents. Overall, p2 number of possible positions are there and at most pq many edges. The number of possible ways to choose k edges is �p2 k � . Thus r = pq � k=1 �p2 k � ≤ pq �p2 pq � ≲ pq(p/q)pq. Therefore, log r ≲ log(pq) + pq log(p/q) ≲ pq log(p/q) Similarly, it can be shown that log r ≳ pq log(p/q) Gao et al. (2022). Consider the LDS (3), with ˘e(k) ∼ N(0, σkI), i.i.d. across time. Then as shown in Appendix E.1, e(ω) ∼ N (0, Φe(ω)) and x(ω) ∼ N(0, Φx), where Φe(ω) = � k∈Z σkI and Φx(ω) = (I − H(ω))−1Φe(ω)((I − H(ω))−1)∗. Ensemble A: Consider all possible DAGs in G′ with i.i.d. Gaussian exogenous distribu- tion such that Φe(ω) exists. For the two distributions, Fk and Fj such that for any ω ∈ Ω, Fk(ω) ∼ N(0, Φ(k)(ω)) and Fj(ω) ∼ N(0, Φ(j)(ω)), KL(Fk(ω)||Fj(ω)) = 1 2 � EFj(ω)[x∗(ω)[Φ(k)(ω)]−1x(ω)] − EFj[x∗(ω)[Φ(j)(ω)]−1x(ω)] � = 1 2 � EFj(ω)[tr(x(ω)[Φ(k)(ω)]−1x∗(ω))] − p � = 1 2 � tr([Φ(k)(ω)]−1Φ(j)(ω)) − p � ≤ 1 2 �� ∥[Φ(k)(ω)]−1∥2 F ∥Φ(j)(ω)∥2 F − p � ≤ 1 2 � pM2 − p � ≤ (M2 − 1)p Therefore one of the lower bounds is inf �G sup G∈G′ P{(G ̸= �G)} ≥ δ, if n ≤ (1 − 2δ)pq log(p/q)) − log 2 (M2 − 1)p ≲ q log(p/q) M2 − 1 . Ensemble B: Here, we consider graphs in Hp,q(β, σ, M) (recall the definition from 3.1.1) with a single edge u −→ v with Hvu(ω) = β for every ω ∈ Ω, i.e. constant matrix. For LDS with i.i.d. Gaussian noise with PSD matrix Φx that satisfies this condition, H is such that Hvu ̸= 0 and Hij = 0 otherwise. Here, the total number of graphs, r = 2 �p 2 � = (p2 − p) ≈ p2 23Notice that [Φ−1 x ]ij = (Iij − Hij − H∗ ji + �p k=1 H∗ kiHkj)/σ. Thus (ignoring ω), [Φ−1 x ]uv = −H∗ vu+�p k=1 H∗ kuHkv σ = −H∗ vu σ = −β/σ and [Φ−1 x ]ij = 0 if i, j ̸= u, v. Then, x∗Φ−1 x x = � ij x∗ i [Φ−1 x ]ijxj = 1 σ � p � i=1 |xi|2(1 + � k |Hki|2) + x∗ u[Φ−1 x ]uvxv + x∗ v[Φ−1 x ]vuxu � = 1 σ  � i̸=u |xi|2 + (1 + |Hvu|2)|xu|2 − 2βℜ{x∗ uxv}   = 1 σ �� i |xi|2 + β2|xu|2 − 2βℜ{x∗ uxv} � = 1 σ  � i̸=v |xi|2 + |xv − βxu|2   Therefore, KL(F uv||F jk) = EF uv � log F uv − log F jk� = 1 2σEF uv � |xv|2 + |xv − βxu|2 − |xk|2 − |xk − βxj|2� = 1 2σ � β2σ + EF uv � β2|xj|2 − 2βℜ{x∗ jxk} �� Considering all the cases of (u, v) vs (j, k) it can be shown that KL(F uv||F jk) ≤ β2 + β4/2 Gao et al. (2022). Thus, n ≳ log p β2+β4/2 gives the second lower bound. The lower bound follows by combining ensembles A and B. References S. Basu and G. Michailidis. Regularized estimation in sparse high-dimensional time series models. The Annals of Statistics, pages 1535–1567, 2015. J. M. Bower and D. Beeman. The book of GENESIS: exploring realistic neural models with the GEneral NEural SImulation System. Springer Science & Business Media, 2012. W. Chen, M. Drton, and Y. S. Wang. On causal discovery with an equal-variance assumption. Biometrika, 106(4):973–980, 2019. W. Cui, X. Zhang, and Y. Liu. Covariance matrix estimation from linearly-correlated gaussian samples. IEEE Transactions on Signal Processing, 67(8):2187–2195, 2019. R. Dahlhaus. Graphical interaction models for multivariate time series. Metrika, 51:157–172, 2000. H. Doddi, D. Deka, S. Talukdar, and M. Salapaka. Efficient and passive learning of networked dynamical systems driven by non-white exogenous inputs. In International Conference on Artificial Intelligence and Statistics, pages 9982–9997. PMLR, 2022. 24M. K. S. Faradonbeh, A. Tewari, and G. Michailidis. Finite time identification in unstable linear systems. Automatica, 96:342–353, 2018. M. Fiecas, C. Leng, W. Liu, and Y. Yu. Spectral analysis of high-dimensional time series. Electronic Journal of Statistics, 13(2):4079–4101, 2019. N. Friedman and Z. Yakhini. On the sample complexity of learning bayesian networks. In Proceedings of the Twelfth international conference on Uncertainty in artificial intelligence, pages 274–282, 1996. M. Gao, W. M. Tai, and B. Aragam. Optimal estimation of gaussian dag models. In In- ternational Conference on Artificial Intelligence and Statistics, pages 8738–8757. PMLR, 2022. M. Ghil, M. R. Allen, M. D. Dettinger, K. Ide, D. Kondrashov, M. E. Mann, A. W. Robertson, A. Saunders, Y. Tian, F. Varadi, and P. Yiou. Advanced spectral methods for climatic time series. Reviews of Geophysics, 40(1):3–1–3–41, 2002. doi: 10.1029/2000RG000092. URL https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2000RG000092. A. Ghoshal and J. Honorio. Information-theoretic limits of bayesian network structure learn- ing. In Artificial Intelligence and Statistics, pages 767–775. PMLR, 2017a. A. Ghoshal and J. Honorio. Learning identifiable gaussian bayesian networks in polynomial time and sample complexity. Advances in Neural Information Processing Systems, 30, 2017b. A. Ghoshal and J. Honorio. Learning linear structural equation models in polynomial time and sample complexity. In A. Storkey and F. Perez-Cruz, editors, Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, volume 84 of Proceedings of Machine Learning Research, pages 1466–1475. PMLR, 09–11 Apr 2018. URL https://proceedings.mlr.press/v84/ghoshal18a.html. R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, USA, 2nd edition, 2012. ISBN 0521548233. Y. S. Kim, S. T. Rachev, M. L. Bianchi, I. Mitov, and F. J. Fabozzi. Time series analysis for financial market meltdowns. Journal of Banking & Finance, 35(8):1879–1891, 2011. A. Lamperski. Non-asymptotic pointwise and worst-case bounds for classical spectrum esti- mators. arXiv preprint arXiv:2303.11908, 2023. P.-L. Loh and M. J. Wainwright. High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity. Advances in neural information processing sys- tems, 24, 2011. D. Materassi and G. Innocenti. Unveiling the connectivity structure of financial networks via high-frequency analysis. Physica A: Statistical Mechanics and its Applications, 388(18): 3866–3878, 2009. ISSN 0378-4371. doi: https://doi.org/10.1016/j.physa.2009.06.003. URL https://www.sciencedirect.com/science/article/pii/S0378437109004324. D. Materassi and M. V. Salapaka. On the problem of reconstructing an unknown topology via locality properties of the wiener filter. IEEE Transactions on Automatic Control, 57 (7):1765–1777, July 2012. ISSN 0018-9286. doi: 10.1109/TAC.2012.2183170. 25D. Materassi and M. V. Salapaka. Reconstruction of directed acyclic networks of dynamical systems. In 2013 American Control Conference, pages 4687–4692. IEEE, 2013. G. Park. Identifiability of additive noise models using conditional variances. The Journal of Machine Learning Research, 21(1):2896–2929, 2020. G. Park and G. Raskutti. Learning quadratic variance function (qvf) dag models via overdis- persion scoring (ods). J. Mach. Learn. Res., 18(224):1–44, 2017. J. Peters and P. B¨uhlmann. Identifiability of gaussian structural equation models with equal error variances. Biometrika, 101(1):219–228, 2014. C. J. Quinn, N. Kiyavash, and T. P. Coleman. Directed information graphs. IEEE Transac- tions on Information Theory, 61(12):6887–6909, 2015. doi: 10.1109/TIT.2015.2478440. S. Shimizu, P. O. Hoyer, A. Hyv¨arinen, A. Kerminen, and M. Jordan. A linear non-gaussian acyclic model for causal discovery. Journal of Machine Learning Research, 7(10), 2006. M. Simchowitz, H. Mania, S. Tu, M. I. Jordan, and B. Recht. Learning without mixing: Towards a sharp analysis of linear system identification. In Conference On Learning Theory, pages 439–473. PMLR, 2018. J. Songsiri and L. Vandenberghe. Topology selection in graphical models of autoregressive processes. The Journal of Machine Learning Research, 11:2671–2705, 2010. P. Stoica, R. L. Moses, et al. Spectral analysis of signals, volume 452. Pearson Prentice Hall Upper Saddle River, NJ, 2005. S. Talukdar, D. Deka, M. Chertkov, and M. Salapaka. Topology learning of radial dynamical systems with latent nodes. In 2018 Annual American Control Conference (ACC), pages 1096–1101. IEEE, 2018. S. Talukdar, D. Deka, H. Doddi, D. Materassi, M. Chertkov, and M. V. Salapaka. Physics informed topology learning in networks of linear dynamical systems. Automatica, 112: 108705, 2020. ISSN 0005-1098. doi: https://doi.org/10.1016/j.automatica.2019.108705. A. Tank, N. J. Foti, and E. B. Fox. Bayesian structure learning for stationary time series. In Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence, pages 872–881, 2015. M. S. Veedu, D. Harish, and M. V. Salapaka. Topology learning of linear dynamical systems with latent nodes using matrix decomposition. IEEE Transactions on Automatic Control, Early Access:1–1, 2021. doi: 10.1109/TAC.2021.3124979. M. J. Wainwright. Random matrices and covariance estimation, page 159–193. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019. doi: 10.1017/9781108627771.006. X. Yang, T. Xu, P. Jia, H. Xia, L. Guo, L. Zhang, and K. Ye. Transportation, germs, culture: a dynamic graph model of covid-19 outbreak. Quantitative Biology, 8:238–244, 2020. D. Zhang and W. B. Wu. Convergence of covariance and spectral density estimates for high- dimensional locally stationary processes. The Annals of Statistics, 49(1):233–254, 2021. 26O. Zuk, S. Margel, and E. Domany. On the number of samples needed to learn the cor- rect structure of a bayesian network. In Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence, pages 560–567, 2006. 27